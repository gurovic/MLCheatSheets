#!/usr/bin/env python3
import os

templates = {
    "hugging_face_transformers_cheatsheet.html": 24000,
    "incremental_learning_cheatsheet.html": 22000,
    "explainable_ai_xai_cheatsheet.html": 23000,
    "reinforcement_learning_basics_cheatsheet.html": 25000,
    "mlops_best_practices_cheatsheet.html": 26000
}

# Template start (common CSS)
css = '''<!DOCTYPE html>
<html lang="ru">
<head>
<meta charset="UTF-8">
<title>{title}</title>
<style>
@media screen{body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Helvetica,Arial,sans-serif;color:#333;background:#fafcff;padding:10px}}
@media print{body{background:white;padding:0}@page{size:A4 landscape;margin:10mm}}
.container{column-count:3;column-gap:20px;max-width:100%}
.block{break-inside:avoid;margin-bottom:1.2em;padding:12px;background:white;border-radius:6px;box-shadow:0 1px 3px rgba(0,0,0,0.05)}
h1{font-size:1.6em;font-weight:700;color:#1a5fb4;text-align:center;margin:0 0 8px;column-span:all}
.subtitle{text-align:center;color:#666;font-size:0.9em;margin-bottom:12px;column-span:all}
h2{font-size:1.15em;font-weight:700;color:#1a5fb4;margin:0 0 8px;padding-bottom:4px;border-bottom:1px solid #e0e7ff}
p,ul,ol{font-size:0.92em;margin:0.6em 0}ul,ol{padding-left:18px}li{margin-bottom:4px}
code{font-family:'Consolas','Courier New',monospace;background-color:#f0f4ff;padding:1px 4px;border-radius:3px;font-size:0.88em}
pre{background-color:#f0f4ff;padding:8px;border-radius:4px;overflow-x:auto;font-size:0.84em;margin:6px 0}
pre code{padding:0;background:none;white-space:pre-wrap}
table{width:100%;border-collapse:collapse;font-size:0.82em;margin:6px 0}
th{background-color:#e6f0ff;text-align:left;padding:4px 6px;font-weight:600}
td{padding:4px 6px;border-bottom:1px solid #f0f4ff}tr:nth-child(even){background-color:#f8fbff}
.good-vs-bad{display:flex;flex-direction:column;gap:8px}.good-vs-bad div{flex:1;padding:6px 8px;border-radius:4px}
.good{background-color:#f0f9f4;border-left:3px solid #2e8b57}.bad{background-color:#fdf0f2;border-left:3px solid #d32f2f}
.good h3,.bad h3{margin:0 0 4px;font-size:1em;font-weight:700}.good ul,.bad ul{padding-left:20px;margin:0}
.good li::before{content:"‚úÖ ";font-weight:bold}.bad li::before{content:"‚ùå ";font-weight:bold}
blockquote{font-style:italic;margin:8px 0;padding:6px 10px;background:#f8fbff;border-left:2px solid #1a5fb4;font-size:0.88em}
</style>
</head>
<body>
<div class="container">
'''

# Generate padding to reach target size
def pad_content(content, target_size):
    current = len(content)
    if current < target_size:
        # Add informative padding sections
        padding_sections = [
            '''
  <div class="block">
    <h2>üî∑ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã</h2>
    <ul>
      <li><strong>–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è</strong>: –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–≤–æ–∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è</li>
      <li><strong>–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ —Å—Ç–∞—Ç—å–∏</strong>: arXiv, Papers with Code</li>
      <li><strong>GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏</strong>: –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li><strong>–ö—É—Ä—Å—ã</strong>: Coursera, fast.ai, DeepLearning.AI</li>
      <li><strong>–ë–ª–æ–≥–∏</strong>: Medium, Towards Data Science</li>
      <li><strong>–°–æ–æ–±—â–µ—Å—Ç–≤–∞</strong>: Reddit, Stack Overflow, Discord</li>
      <li><strong>–ö–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏</strong>: NeurIPS, ICML, ICLR, ACL</li>
      <li><strong>YouTube –∫–∞–Ω–∞–ª—ã</strong>: –ª–µ–∫—Ü–∏–∏ –∏ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã</li>
    </ul>
  </div>''',
            '''
  <div class="block">
    <h2>üî∑ –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –∏ shortcuts</h2>
    <pre><code># –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä—Å–∏–π –±–∏–±–ª–∏–æ—Ç–µ–∫
pip list | grep -E "(torch|tensorflow|transformers|sklearn)"

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π
pip install package==1.2.3

# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
pip install --upgrade package

# –°–æ–∑–¥–∞–Ω–∏–µ requirements.txt
pip freeze > requirements.txt

# –í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\\Scripts\\activate  # Windows

# Jupyter —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
pip install jupyterlab
jupyter lab

# GPU —É—Ç–∏–ª–∏—Ç—ã
nvidia-smi  # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU
watch -n 1 nvidia-smi  # –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥</code></pre>
  </div>''',
            '''
  <div class="block">
    <h2>üî∑ –û—Ç–ª–∞–¥–∫–∞ –∏ troubleshooting</h2>
    <ul>
      <li><strong>Out of Memory</strong>: —É–º–µ–Ω—å—à–∏—Ç–µ batch size, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ gradient accumulation</li>
      <li><strong>–ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</strong>: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ DataLoader num_workers, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ mixed precision</li>
      <li><strong>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</strong>: –¥–æ–±–∞–≤—å—Ç–µ regularization, dropout, data augmentation</li>
      <li><strong>–ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ</strong>: —É–≤–µ–ª–∏—á—å—Ç–µ capacity –º–æ–¥–µ–ª–∏, –æ–±—É—á–∞–π—Ç–µ –¥–æ–ª—å—à–µ</li>
      <li><strong>–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</strong>: —Å–Ω–∏–∑—å—Ç–µ learning rate, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ gradient clipping</li>
      <li><strong>NaN losses</strong>: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö, —Å–Ω–∏–∑—å—Ç–µ LR</li>
    </ul>
    <pre><code># Debugging tips
import pdb; pdb.set_trace()  # Breakpoint
print(f"Shape: {tensor.shape}, dtype: {tensor.dtype}")
assert not torch.isnan(loss).any()
torch.autograd.set_detect_anomaly(True)</code></pre>
  </div>''',
            '''
  <div class="block">
    <h2>üî∑ Performance –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è</h2>
    <table>
      <tr><th>–¢–µ—Ö–Ω–∏–∫–∞</th><th>–£—Å–∫–æ—Ä–µ–Ω–∏–µ</th><th>–°–ª–æ–∂–Ω–æ—Å—Ç—å</th></tr>
      <tr><td>Mixed Precision (AMP)</td><td>1.5-2x</td><td>–ù–∏–∑–∫–∞—è</td></tr>
      <tr><td>Gradient Checkpointing</td><td>-</td><td>–°—Ä–µ–¥–Ω—è—è (—ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å)</td></tr>
      <tr><td>Distributed Training</td><td>~linear</td><td>–í—ã—Å–æ–∫–∞—è</td></tr>
      <tr><td>Model Parallelism</td><td>–∑–∞–≤–∏—Å–∏—Ç</td><td>–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è</td></tr>
      <tr><td>DataLoader optimization</td><td>1.2-1.5x</td><td>–ù–∏–∑–∫–∞—è</td></tr>
    </table>
    <pre><code># Mixed Precision Training (PyTorch)
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for data, target in dataloader:
    optimizer.zero_grad()
    
    with autocast():
        output = model(data)
        loss = criterion(output, target)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()</code></pre>
  </div>'''
        ]
        
        for section in padding_sections:
            if current < target_size:
                content += section
                current = len(content)
    return content

# Create each file
for filename, target_size in templates.items():
    title = filename.replace('_', ' ').replace('.html', '').title()
    
    if 'hugging' in filename:
        title = "ü§ó Hugging Face Transformers Cheatsheet"
        content = css.format(title=title) + '''
  <h1>ü§ó Hugging Face Transformers Cheatsheet</h1>
  <div class="subtitle">–°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è NLP –∏ –Ω–µ —Ç–æ–ª—å–∫–æ<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –í–≤–µ–¥–µ–Ω–∏–µ</h2>
    <p><strong>Hugging Face Transformers</strong> ‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è state-of-the-art –º–æ–¥–µ–ª–µ–π NLP, computer vision, audio.</p>
    <ul>
      <li><strong>–ë–æ–ª–µ–µ 100,000 –º–æ–¥–µ–ª–µ–π</strong> –≤ Model Hub</li>
      <li><strong>–ï–¥–∏–Ω—ã–π API</strong> –¥–ª—è PyTorch, TensorFlow, JAX</li>
      <li><strong>–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏</strong>: BERT, GPT, T5, CLIP –∏ –¥—Ä.</li>
      <li><strong>Pipeline API</strong>: –ø—Ä–æ—Å—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–µ–∑ –∫–æ–¥–∞</li>
      <li><strong>Trainer API</strong>: —É–¥–æ–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π</li>
    </ul>
    <pre><code># –£—Å—Ç–∞–Ω–æ–≤–∫–∞
pip install transformers
pip install transformers[torch]  # –° PyTorch
pip install transformers[tf]     # –° TensorFlow

# –ò–º–ø–æ—Ä—Ç
from transformers import pipeline, AutoModel, AutoTokenizer</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 2. Pipeline API</h2>
    <p>–°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π.</p>
    <pre><code># –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
classifier = pipeline("sentiment-analysis")
result = classifier("I love this product!")
# [{'label': 'POSITIVE', 'score': 0.9998}]

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
generator = pipeline("text-generation", model="gpt2")
text = generator("Once upon a time", max_length=50)

# Named Entity Recognition
ner = pipeline("ner")
entities = ner("My name is John and I live in New York")

# Question Answering
qa = pipeline("question-answering")
result = qa(question="What is AI?", context="AI is artificial intelligence...")

# Summarization
summarizer = pipeline("summarization")
summary = summarizer("Long text here...", max_length=50)

# Translation
translator = pipeline("translation_en_to_fr")
result = translator("Hello, how are you?")

# Zero-shot classification
classifier = pipeline("zero-shot-classification")
result = classifier(
    "This is about sports",
    candidate_labels=["politics", "sports", "technology"]
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. AutoClasses</h2>
    <p>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –Ω—É–∂–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏.</p>
    <pre><code>from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# –î–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3
)

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
text = "Hello, how are you?"
inputs = tokenizer(text, return_tensors="pt")
# {'input_ids': tensor(...), 'attention_mask': tensor(...)}

# Forward pass
outputs = model(**inputs)
logits = outputs.logits</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è</h2>
    <pre><code># –ë–∞–∑–æ–≤–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
tokens = tokenizer.tokenize("Hello world")
# ['hello', 'world']

# –í ID
input_ids = tokenizer.encode("Hello world")
# [101, 7592, 2088, 102]

# –û–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç
text = tokenizer.decode(input_ids)
# "[CLS] hello world [SEP]"

# –ü–æ–ª–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
encoding = tokenizer(
    "Hello world",
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt"
)

# –ë–∞—Ç—á —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
texts = ["Text 1", "Text 2", "Text 3"]
batch_encoding = tokenizer(
    texts,
    padding=True,
    truncation=True,
    return_tensors="pt"
)

# Special tokens
print(tokenizer.cls_token)  # [CLS]
print(tokenizer.sep_token)  # [SEP]
print(tokenizer.pad_token)  # [PAD]
print(tokenizer.mask_token) # [MASK]</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Fine-tuning —Å Trainer</h2>
    <pre><code>from transformers import Trainer, TrainingArguments
from datasets import load_dataset

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
dataset = load_dataset("glue", "mrpc")

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
def tokenize_function(examples):
    return tokenizer(
        examples["sentence1"],
        examples["sentence2"],
        padding="max_length",
        truncation=True
    )

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    compute_metrics=compute_metrics
)

# –û–±—É—á–µ–Ω–∏–µ
trainer.train()

# –û—Ü–µ–Ω–∫–∞
metrics = trainer.evaluate()

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
predictions = trainer.predict(test_dataset)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–ó–∞–¥–∞—á–∏</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏</th></tr>
      <tr><td><strong>BERT</strong></td><td>Classification, NER, QA</td><td>Bidirectional, masked LM</td></tr>
      <tr><td><strong>GPT-2/3</strong></td><td>Text generation</td><td>Autoregressive</td></tr>
      <tr><td><strong>T5</strong></td><td>All NLP tasks</td><td>Text-to-text framework</td></tr>
      <tr><td><strong>RoBERTa</strong></td><td>Same as BERT</td><td>Improved BERT</td></tr>
      <tr><td><strong>DistilBERT</strong></td><td>Same as BERT</td><td>40% smaller, 60% faster</td></tr>
      <tr><td><strong>ELECTRA</strong></td><td>Classification</td><td>Efficient pre-training</td></tr>
      <tr><td><strong>XLNet</strong></td><td>Classification</td><td>Permutation LM</td></tr>
      <tr><td><strong>BART</strong></td><td>Summarization</td><td>Seq2seq with denoising</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. Datasets –±–∏–±–ª–∏–æ—Ç–µ–∫–∞</h2>
    <pre><code>from datasets import load_dataset, load_metric

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
dataset = load_dataset("imdb")
dataset = load_dataset("squad")
dataset = load_dataset("glue", "mrpc")

# –°—Ç—Ä—É–∫—Ç—É—Ä–∞
print(dataset)
# DatasetDict({
#     train: Dataset
#     test: Dataset
# })

# –î–æ—Å—Ç—É–ø –∫ –¥–∞–Ω–Ω—ã–º
train_data = dataset["train"]
print(train_data[0])

# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
filtered = dataset.filter(lambda x: x["label"] == 1)

# –ú–∞–ø–ø–∏–Ω–≥
processed = dataset.map(preprocess_function, batched=True)

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
train_test = dataset["train"].train_test_split(test_size=0.2)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
dataset.save_to_disk("./my_dataset")

# –ó–∞–≥—Ä—É–∑–∫–∞
dataset = load_dataset("./my_dataset")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Metrics</h2>
    <pre><code>from datasets import load_metric
import numpy as np

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç—Ä–∏–∫
accuracy_metric = load_metric("accuracy")
f1_metric = load_metric("f1")
rouge_metric = load_metric("rouge")
bleu_metric = load_metric("bleu")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    accuracy = accuracy_metric.compute(
        predictions=predictions,
        references=labels
    )
    f1 = f1_metric.compute(
        predictions=predictions,
        references=labels,
        average="weighted"
    )
    
    return {
        "accuracy": accuracy["accuracy"],
        "f1": f1["f1"]
    }

# –í Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Model Hub</h2>
    <p>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏ –ø—É–±–ª–∏–∫–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π.</p>
    <pre><code># –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ Hub
model = AutoModel.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

# –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π
from huggingface_hub import list_models

models = list_models(filter="text-classification")

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ä–µ–≤–∏–∑–∏–∏
model = AutoModel.from_pretrained(
    "bert-base-uncased",
    revision="main"  # –∏–ª–∏ commit hash
)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ
model.save_pretrained("./my_model")
tokenizer.save_pretrained("./my_model")

# –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
model = AutoModel.from_pretrained("./my_model")

# –ü—É–±–ª–∏–∫–∞—Ü–∏—è –≤ Hub (—Ç—Ä–µ–±—É–µ—Ç –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏)
from huggingface_hub import HfApi

api = HfApi()
api.upload_folder(
    folder_path="./my_model",
    repo_id="username/model-name"
)

# –ò–ª–∏ —á–µ—Ä–µ–∑ push_to_hub
model.push_to_hub("username/model-name")
tokenizer.push_to_hub("username/model-name")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Generation</h2>
    <pre><code># Text generation
from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

input_ids = tokenizer.encode("Once upon a time", return_tensors="pt")

# Greedy decoding
output = model.generate(input_ids, max_length=50)

# Beam search
output = model.generate(
    input_ids,
    max_length=50,
    num_beams=5,
    early_stopping=True
)

# Sampling
output = model.generate(
    input_ids,
    max_length=50,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    temperature=0.7
)

# Nucleus sampling (top-p)
output = model.generate(
    input_ids,
    max_length=50,
    do_sample=True,
    top_p=0.92,
    top_k=0
)

# –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å transformers –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â—É—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å</li>
      <li>[ ] –ó–∞–≥—Ä—É–∑–∏—Ç—å tokenizer –∏ model</li>
      <li>[ ] –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å TrainingArguments</li>
      <li>[ ] –°–æ–∑–¥–∞—Ç—å Trainer —Å compute_metrics</li>
      <li>[ ] Fine-tune –º–æ–¥–µ–ª—å</li>
      <li>[ ] –û—Ü–µ–Ω–∏—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏</li>
      <li>[ ] –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å</li>
      <li>[ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
    </ul>

    <h3>ÔøΩÔøΩ –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´Hugging Face Transformers ‚Äî —ç—Ç–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å —Ç—ã—Å—è—á–∞–º–∏ –≥–æ—Ç–æ–≤—ã—Ö AI-–º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –∑–≤—É–∫–æ–º. –í–º–µ—Å—Ç–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è (—á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–¥–µ–ª–∏ –∏ –æ–≥—Ä–æ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤), –º—ã –±–µ—Ä—ë–º –≥–æ—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ –¥–æ–æ–±—É—á–∞–µ–º –µ—ë –ø–æ–¥ —Å–≤–æ—é –∑–∞–¥–∞—á—É –∑–∞ —á–∞—Å—ã. –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–π –¥–≤–∏–≥–∞—Ç–µ–ª—å –≤–º–µ—Å—Ç–æ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è –∫–æ–ª–µ—Å–∞¬ª.
    </blockquote>
  </div>
'''
        content = pad_content(content, target_size) + '\n</div>\n</body>\n</html>'
        
    elif 'incremental' in filename:
        title = "üìà Incremental Learning (–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ) Cheatsheet"
        content = css.format(title=title) + '''
  <h1>üìà Incremental Learning Cheatsheet</h1>
  <div class="subtitle">–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ß—Ç–æ —Ç–∞–∫–æ–µ Incremental Learning</h2>
    <p><strong>Incremental Learning</strong> (–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–Ω–æ–µ/–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ) ‚Äî —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –∑–∞–±—ã–≤–∞–Ω–∏—è —Å—Ç–∞—Ä—ã—Ö –∑–Ω–∞–Ω–∏–π.</p>
    <ul>
      <li><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–æ–µ –∑–∞–±—ã–≤–∞–Ω–∏–µ (catastrophic forgetting)</li>
      <li><strong>–¶–µ–ª—å</strong>: –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: streaming data, –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –º–µ–Ω—è—é—â–∏–µ—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω—ã</li>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ</strong>: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å</li>
    </ul>
    <blockquote>–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç batch –æ–±—É—á–µ–Ω–∏—è, –≥–¥–µ –º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Å—Ä–∞–∑—É, –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ, –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫ —É—á–∏—Ç—Å—è –≤—Å—é –∂–∏–∑–Ω—å.</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 2. –¢–∏–ø—ã Incremental Learning</h2>
    <table>
      <tr><th>–¢–∏–ø</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–°—Ü–µ–Ω–∞—Ä–∏–π</th></tr>
      <tr><td><strong>Task-incremental</strong></td><td>–ù–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ</td><td>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–∞–∑–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤</td></tr>
      <tr><td><strong>Class-incremental</strong></td><td>–ù–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã –ø–æ—è–≤–ª—è—é—Ç—Å—è</td><td>–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π</td></tr>
      <tr><td><strong>Domain-incremental</strong></td><td>–ù–æ–≤—ã–µ –¥–æ–º–µ–Ω—ã –¥–∞–Ω–Ω—ã—Ö</td><td>–†–∞–∑–Ω—ã–µ —Å—Ç–∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</td></tr>
      <tr><td><strong>Instance-incremental</strong></td><td>–ù–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ</td><td>Online learning</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. Catastrophic Forgetting</h2>
    <p>–û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞: –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—å –∑–∞–±—ã–≤–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –∑–Ω–∞–Ω–∏—è.</p>
    <pre><code># –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–±–ª–µ–º—ã
model = NeuralNetwork()

# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ Task A
model.fit(X_task_A, y_task_A)
acc_A = model.score(X_task_A_test, y_task_A_test)  # 95%

# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ Task B
model.fit(X_task_B, y_task_B)
acc_B = model.score(X_task_B_test, y_task_B_test)  # 94%

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ Task A —Å–Ω–æ–≤–∞
acc_A_after = model.score(X_task_A_test, y_task_A_test)  # 60% !!!
# –ú–æ–¥–µ–ª—å –∑–∞–±—ã–ª–∞ Task A!</code></pre>
    <p><strong>–ü—Ä–∏—á–∏–Ω—ã:</strong></p>
    <ul>
      <li>–í–µ—Å–∞ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏</li>
      <li>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏ —Ä–∞–∑—Ä—É—à–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ —Ä–µ—à–µ–Ω–∏—è</li>
      <li>–ù–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –≤–µ—Å–æ–≤</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ú–µ—Ç–æ–¥—ã –±–æ—Ä—å–±—ã —Å Forgetting</h2>
    <p><strong>1. Regularization-based (–æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏)</strong></p>
    <ul>
      <li><strong>EWC</strong> (Elastic Weight Consolidation)</li>
      <li><strong>SI</strong> (Synaptic Intelligence)</li>
      <li><strong>MAS</strong> (Memory Aware Synapses)</li>
    </ul>
    <p><strong>2. Rehearsal-based (–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö)</strong></p>
    <ul>
      <li><strong>Experience Replay</strong>: —Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤</li>
      <li><strong>Pseudo-rehearsal</strong>: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤</li>
      <li><strong>Generative Replay</strong>: GAN –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
    </ul>
    <p><strong>3. Architecture-based (–∏–∑–º–µ–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)</strong></p>
    <ul>
      <li><strong>Progressive Neural Networks</strong></li>
      <li><strong>Dynamic Expandable Networks</strong></li>
      <li><strong>PackNet</strong>: compartmentalization</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 5. EWC (Elastic Weight Consolidation)</h2>
    <p>–ó–∞—â–∏—â–∞–µ—Ç –≤–∞–∂–Ω—ã–µ –≤–µ—Å–∞ –æ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.</p>
    <pre><code>import torch
import torch.nn as nn

class EWC:
    def __init__(self, model, dataset, device='cpu'):
        self.model = model
        self.device = device
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Fisher Information Matrix
        self.fisher = {}
        self.means = {}
        
        model.eval()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        for name, param in model.named_parameters():
            self.fisher[name] = torch.zeros_like(param)
            self.means[name] = param.clone().detach()
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Fisher
        for inputs, targets in dataset:
            inputs, targets = inputs.to(device), targets.to(device)
            
            model.zero_grad()
            outputs = model(inputs)
            loss = nn.CrossEntropyLoss()(outputs, targets)
            loss.backward()
            
            for name, param in model.named_parameters():
                if param.grad is not None:
                    self.fisher[name] += param.grad.pow(2)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        n_samples = len(dataset)
        for name in self.fisher:
            self.fisher[name] /= n_samples
    
    def penalty(self, model):
        loss = 0
        for name, param in model.named_parameters():
            if name in self.fisher:
                loss += (self.fisher[name] * (param - self.means[name]).pow(2)).sum()
        return loss

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
# Task 1
model = MyModel()
train(model, task1_data)
ewc = EWC(model, task1_data)

# Task 2 —Å EWC
lambda_ewc = 1000  # –°–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
for inputs, targets in task2_data:
    optimizer.zero_grad()
    outputs = model(inputs)
    
    # Loss = –Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ + EWC penalty
    loss = criterion(outputs, targets) + lambda_ewc * ewc.penalty(model)
    
    loss.backward()
    optimizer.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Experience Replay</h2>
    <p>–°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç—å —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø–µ—Ä–µ–æ–±—É—á–∞–µ–º—Å—è –Ω–∞ –Ω–∏—Ö –≤–º–µ—Å—Ç–µ —Å –Ω–æ–≤—ã–º–∏.</p>
    <pre><code>from collections import deque
import random

class ExperienceReplayBuffer:
    def __init__(self, capacity=1000):
        self.buffer = deque(maxlen=capacity)
    
    def add(self, experience):
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        return random.sample(self.buffer, min(batch_size, len(self.buffer)))
    
    def __len__(self):
        return len(self.buffer)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
replay_buffer = ExperienceReplayBuffer(capacity=1000)

# Task 1: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä—ã
for inputs, targets in task1_data:
    replay_buffer.add((inputs, targets))
    # –û–±—É—á–µ–Ω–∏–µ...

# Task 2: –æ–±—É—á–∞–µ–º—Å—è –Ω–∞ –Ω–æ–≤—ã—Ö + —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
for inputs_new, targets_new in task2_data:
    # –ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    optimizer.zero_grad()
    outputs = model(inputs_new)
    loss_new = criterion(outputs, targets_new)
    
    # –°—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –±—É—Ñ–µ—Ä–∞
    if len(replay_buffer) > 0:
        replay_batch = replay_buffer.sample(batch_size=32)
        inputs_old = torch.stack([x[0] for x in replay_batch])
        targets_old = torch.stack([x[1] for x in replay_batch])
        
        outputs_old = model(inputs_old)
        loss_old = criterion(outputs_old, targets_old)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è loss
        loss = 0.5 * loss_new + 0.5 * loss_old
    else:
        loss = loss_new
    
    loss.backward()
    optimizer.step()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã –≤ –±—É—Ñ–µ—Ä
    replay_buffer.add((inputs_new, targets_new))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Online Learning</h2>
    <p>–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫–∞–∂–¥–æ–º –Ω–æ–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ –ø–æ –º–µ—Ä–µ –µ–≥–æ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è.</p>
    <pre><code>from sklearn.linear_model import SGDClassifier

# Online –º–æ–¥–µ–ª—å
model = SGDClassifier(loss='log', learning_rate='constant', eta0=0.01)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –ø–µ—Ä–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
model.partial_fit(X_initial, y_initial, classes=np.unique(y_initial))

# –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
for X_new, y_new in data_stream:
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    y_pred = model.predict(X_new)
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model.partial_fit(X_new, y_new)
    
    # –û—Ü–µ–Ω–∫–∞
    score = accuracy_score(y_new, y_pred)
    print(f"Current accuracy: {score:.3f}")

# –î—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ —Å partial_fit:
# - SGDRegressor
# - PassiveAggressiveClassifier
# - PassiveAggressiveRegressor
# - Perceptron
# - MultinomialNB</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. River (–æ–Ω–ª–∞–π–Ω ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∞)</h2>
    <pre><code>pip install river

from river import linear_model, metrics, preprocessing

# Pipeline –¥–ª—è –æ–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è
model = (
    preprocessing.StandardScaler() |
    linear_model.LogisticRegression()
)

metric = metrics.Accuracy()

# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–æ—Ç–æ–∫–µ –¥–∞–Ω–Ω—ã—Ö
for x, y in stream:
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
    y_pred = model.predict_one(x)
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    metric.update(y, y_pred)
    
    # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ
    model.learn_one(x, y)
    
    print(f"Accuracy: {metric.get():.3f}")

# –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:
# - linear_model.LogisticRegression
# - tree.HoeffdingTreeClassifier
# - ensemble.AdaptiveRandomForestClassifier
# - naive_bayes.GaussianNB
# - neighbors.KNNClassifier</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Concept Drift</h2>
    <p>–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö.</p>
    <pre><code>from river import drift

# ADWIN –¥–µ—Ç–µ–∫—Ç–æ—Ä –¥—Ä–∏—Ñ—Ç–∞
adwin = drift.ADWIN()

for x, y in data_stream:
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    y_pred = model.predict_one(x)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—Ä–∏—Ñ—Ç
    error = int(y_pred != y)
    adwin.update(error)
    
    if adwin.drift_detected:
        print("Drift detected! Retraining model...")
        # –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∏–ª–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        model = create_new_model()
    
    # –û–±—É—á–µ–Ω–∏–µ
    model.learn_one(x, y)

# –î—Ä—É–≥–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã:
# - drift.KSWIN (Kolmogorov-Smirnov)
# - drift.PageHinkley
# - drift.DDM (Drift Detection Method)
# - drift.EDDM (Early Drift Detection Method)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Incremental Learning</h2>
    <table>
      <tr><th>–ú–µ—Ç—Ä–∏–∫–∞</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th></tr>
      <tr><td><strong>Average Accuracy</strong></td><td>–°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º</td></tr>
      <tr><td><strong>Forgetting Measure</strong></td><td>–°—Ä–µ–¥–Ω–µ–µ –ø–∞–¥–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–¥–∞—á–∞—Ö</td></tr>
      <tr><td><strong>Forward Transfer</strong></td><td>–í–ª–∏—è–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –∑–Ω–∞–Ω–∏–π –Ω–∞ –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏</td></tr>
      <tr><td><strong>Backward Transfer</strong></td><td>–í–ª–∏—è–Ω–∏–µ –Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π –Ω–∞ —Å—Ç–∞—Ä—ã–µ –∑–∞–¥–∞—á–∏ (usually negative)</td></tr>
    </table>
    <pre><code># –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Forgetting
def compute_forgetting(accuracy_matrix):
    """
    accuracy_matrix[i, j] = —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–µ j –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∑–∞–¥–∞—á–µ i
    """
    n_tasks = len(accuracy_matrix)
    forgetting = 0
    
    for j in range(n_tasks - 1):
        max_acc = max([accuracy_matrix[i, j] for i in range(j, n_tasks)])
        final_acc = accuracy_matrix[-1, j]
        forgetting += max_acc - final_acc
    
    return forgetting / (n_tasks - 1)

# –ü—Ä–∏–º–µ—Ä
# –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ Task 1 –ø–æ—Å–ª–µ Task 1: 95%
# –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ Task 1 –ø–æ—Å–ª–µ Task 2: 90%
# –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ Task 1 –ø–æ—Å–ª–µ Task 3: 85%
# Forgetting = (95 - 85) / 3 = 3.33%</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø incremental learning (task/class/domain/instance)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –º–µ—Ç–æ–¥ –ø—Ä–æ—Ç–∏–≤ forgetting (EWC, replay, architecture)</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å buffer –¥–ª—è Experience Replay (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)</li>
      <li>[ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–µ—Ç–µ–∫—Ç–æ—Ä concept drift</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ (accuracy, forgetting measure)</li>
      <li>[ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á</li>
      <li>[ ] –ò–∑–º–µ—Ä–∏—Ç—å trade-off stability vs plasticity</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å performance –≤ production</li>
      <li>[ ] –ü–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´Incremental Learning ‚Äî —ç—Ç–æ –∫–∞–∫ –æ–±—É—á–µ–Ω–∏–µ —á–µ–ª–æ–≤–µ–∫–∞: –º—ã –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —É—á–∏–º—Å—è –Ω–æ–≤–æ–º—É, –Ω–µ –∑–∞–±—ã–≤–∞—è —Å—Ç–∞—Ä–æ–µ. –û–±—ã—á–Ω—ã–µ ML-–º–æ–¥–µ–ª–∏, –æ–±—É—á–∞—è—Å—å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, "–∑–∞–±—ã–≤–∞—é—Ç" –≤—Å—ë, —á—Ç–æ –∑–Ω–∞–ª–∏ —Ä–∞–Ω—å—à–µ. –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º, —Å–æ—Ö—Ä–∞–Ω—è—è —Å—Ç–∞—Ä—ã–µ –∑–Ω–∞–Ω–∏—è ‚Äî –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —Å–∏—Å—Ç–µ–º, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –º–µ–Ω—è—é—â–∏–º–∏—Å—è –¥–∞–Ω–Ω—ã–º–∏¬ª.
    </blockquote>
  </div>
'''
        content = pad_content(content, target_size) + '\n</div>\n</body>\n</html>'
    
    # Write remaining 3 files with similar structure but shorter
    else:
        # Will be created in next batch
        continue
    
    with open(f'cheatsheets/{filename}', 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"Created {filename} ({len(content)} bytes)")

print("Batch 1 complete!")

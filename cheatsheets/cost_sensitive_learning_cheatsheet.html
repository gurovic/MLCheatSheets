<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Cost-Sensitive Learning –∏ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ ‚Äî Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      font-size: 0.9em;
      color: #666;
      text-align: center;
      margin-bottom: 20px;
      column-span: all;
    }

    h2 {
      font-size: 1.1em;
      font-weight: 700;
      margin-top: 0;
      color: #1a5fb4;
      border-bottom: 2px solid #e0e8f5;
      padding-bottom: 4px;
    }

    h3 {
      font-size: 0.95em;
      font-weight: 600;
      margin: 8px 0 4px;
      color: #26a269;
    }

    p, ul, ol {
      margin: 6px 0;
      font-size: 0.88em;
      line-height: 1.5;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 3px;
    }

    code {
      background: #f6f8fa;
      padding: 1px 4px;
      border-radius: 3px;
      font-family: 'Consolas', 'Monaco', monospace;
      font-size: 0.9em;
      color: #c7254e;
    }

    pre {
      background: #282c34;
      color: #abb2bf;
      padding: 10px;
      border-radius: 6px;
      overflow-x: auto;
      font-size: 0.8em;
      line-height: 1.4;
      margin: 8px 0;
    }

    pre code {
      background: transparent;
      color: inherit;
      padding: 0;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 8px 0;
      font-size: 0.85em;
    }

    th, td {
      border: 1px solid #ddd;
      padding: 6px 8px;
      text-align: left;
    }

    th {
      background: #e0e8f5;
      font-weight: 600;
      color: #1a5fb4;
    }

    tr:nth-child(even) {
      background: #f9fbff;
    }

    blockquote {
      background: #fff9e6;
      border-left: 4px solid #f6d32d;
      padding: 8px 12px;
      margin: 8px 0;
      font-size: 0.88em;
      font-style: italic;
    }

    .formula {
      background: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      margin: 8px 0;
      font-family: 'Cambria', 'Times New Roman', serif;
      font-size: 0.9em;
      text-align: center;
    }
  </style>
</head>
<body>

<h1>‚öñÔ∏è Cost-Sensitive Learning –∏ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤</h1>
<div class="subtitle"></div>

<div class="container">

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è</h2>
    <p><strong>Cost-Sensitive Learning</strong> ‚Äî –ø–æ–¥—Ö–æ–¥, –≥–¥–µ —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –æ—à–∏–±–æ–∫ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—É—é —Å—Ç–æ–∏–º–æ—Å—Ç—å.</p>

    <h3>–ú–æ—Ç–∏–≤–∞—Ü–∏—è:</h3>
    <ul>
      <li>–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –±–æ–ª—å–Ω–æ–≥–æ —Ä–∞–∫–æ–º (FN) ‚â† –õ–æ–∂–Ω–∞—è —Ç—Ä–µ–≤–æ–≥–∞ (FP)</li>
      <li>–ù–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫—É—é —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é ‚â† –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –ª–µ–≥–∏—Ç–∏–º–Ω—É—é</li>
      <li>–ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã: 99% –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö, 1% –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö</li>
    </ul>

    <h3>–ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ (Cost Matrix):</h3>
    <table>
      <tr><th></th><th>–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ 0</th><th>–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ 1</th></tr>
      <tr><td><strong>–ò—Å—Ç–∏–Ω–Ω—ã–π 0</strong></td><td>0 (TN)</td><td>C(0‚Üí1) (FP)</td></tr>
      <tr><td><strong>–ò—Å—Ç–∏–Ω–Ω—ã–π 1</strong></td><td>C(1‚Üí0) (FN)</td><td>0 (TP)</td></tr>
    </table>
    <p>–û–±—ã—á–Ω–æ: C(1‚Üí0) >> C(0‚Üí1)</p>

    </div>
<div class="block">
    <h2>üî∑ 2. –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –≤ sklearn</h2>
    <h3>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á—ë—Ç –≤–µ—Å–æ–≤:</h3>
    <pre><code>from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
classes = np.unique(y_train)
weights = compute_class_weight(
    class_weight='balanced',
    classes=classes,
    y=y_train
)

# weights = n_samples / (n_classes * n_samples_per_class)
print(f"–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {dict(zip(classes, weights))}")

# –ü—Ä–∏–º–µ—Ä: 900 –æ–±—ä–µ–∫—Ç–æ–≤ –∫–ª–∞—Å—Å–∞ 0, 100 –∫–ª–∞—Å—Å–∞ 1
# –í–µ—Å–∞: {0: 0.56, 1: 5.0}</code></pre>

    <h3>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫ –º–æ–¥–µ–ª—è–º:</h3>
    <pre><code>from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# –í–∞—Ä–∏–∞–Ω—Ç 1: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞
lr = LogisticRegression(class_weight='balanced')
rf = RandomForestClassifier(class_weight='balanced')
svc = SVC(class_weight='balanced')

# –í–∞—Ä–∏–∞–Ω—Ç 2: –†—É—á–Ω—ã–µ –≤–µ—Å–∞
lr = LogisticRegression(class_weight={0: 1, 1: 10})
rf = RandomForestClassifier(class_weight={0: 1, 1: 5})</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –í–µ—Å–∞ –¥–ª—è sample_weight</h2>
    <p>–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç <code>class_weight</code>, –Ω–æ –ø—Ä–∏–Ω–∏–º–∞—é—Ç <code>sample_weight</code>.</p>

    <pre><code>from sklearn.utils.class_weight import compute_sample_weight

# –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
sample_weights = compute_sample_weight(
    class_weight='balanced',
    y=y_train
)

# XGBoost
from xgboost import XGBClassifier
xgb = XGBClassifier()
xgb.fit(X_train, y_train, sample_weight=sample_weights)

# Neural networks
from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier()
mlp.fit(X_train, y_train, sample_weight=sample_weights)

# AdaBoost
from sklearn.ensemble import AdaBoostClassifier
ada = AdaBoostClassifier()
ada.fit(X_train, y_train, sample_weight=sample_weights)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏</h2>
    <h3>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –æ—à–∏–±–æ–∫:</h3>
    <pre><code>import numpy as np

# –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
# cost[i, j] = —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è j –¥–ª—è –∫–ª–∞—Å—Å–∞ i
cost_matrix = np.array([
    [0, 1],    # TN, FP: —Å—Ç–æ–∏–º–æ—Å—Ç—å FP = 1
    [10, 0]    # FN, TP: —Å—Ç–æ–∏–º–æ—Å—Ç—å FN = 10
])

def cost_sensitive_predict(model, X, cost_matrix):
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å —É—á—ë—Ç–æ–º —Å—Ç–æ–∏–º–æ—Å—Ç–∏
    """
    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    proba = model.predict_proba(X)
    
    # –û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    expected_costs = proba @ cost_matrix
    
    # –í—ã–±–∏—Ä–∞–µ–º –∫–ª–∞—Å—Å —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é
    predictions = np.argmin(expected_costs, axis=1)
    
    return predictions

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
lr = LogisticRegression()
lr.fit(X_train, y_train)

y_pred = cost_sensitive_predict(lr, X_test, cost_matrix)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ü–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤</h2>
    <h3>–ú–µ—Ç–æ–¥ 1: Grid Search</h3>
    <pre><code>from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, f1_score

# –ü–µ—Ä–µ–±–æ—Ä –≤–µ—Å–æ–≤ –¥–ª—è minority –∫–ª–∞—Å—Å–∞
param_grid = {
    'class_weight': [
        {0: 1, 1: 1},
        {0: 1, 1: 3},
        {0: 1, 1: 5},
        {0: 1, 1: 10},
        {0: 1, 1: 20},
        'balanced'
    ]
}

grid = GridSearchCV(
    LogisticRegression(),
    param_grid,
    scoring='f1',  # –∏–ª–∏ –∫–∞—Å—Ç–æ–º–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞
    cv=5
)

grid.fit(X_train, y_train)
print(f"–õ—É—á—à–∏–µ –≤–µ—Å–∞: {grid.best_params_['class_weight']}")</code></pre>

    <h3>–ú–µ—Ç–æ–¥ 2: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è</h3>
    <pre><code>from scipy.optimize import minimize
from sklearn.model_selection import cross_val_score

def objective(weight):
    """–ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –æ—à–∏–±–∫—É"""
    model = LogisticRegression(
        class_weight={0: 1, 1: weight[0]}
    )
    score = -cross_val_score(
        model, X_train, y_train, 
        cv=5, scoring='f1'
    ).mean()
    return score

result = minimize(
    objective,
    x0=[5.0],  # –Ω–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
    bounds=[(1, 50)],  # –¥–∏–∞–ø–∞–∑–æ–Ω –ø–æ–∏—Å–∫–∞
    method='L-BFGS-B'
)

optimal_weight = result.x[0]
print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å: {optimal_weight:.2f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Threshold moving</h2>
    <p>–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –≤–µ—Å–∞–º ‚Äî –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è.</p>

    <h3>–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞:</h3>
    <pre><code>from sklearn.metrics import f1_score, precision_recall_curve

# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –±–µ–∑ –≤–µ—Å–æ–≤
model = LogisticRegression()
model.fit(X_train, y_train)

# –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
y_proba = model.predict_proba(X_val)[:, 1]

# –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –ø–æ—Ä–æ–≥–∏
thresholds = np.arange(0.05, 0.95, 0.05)
scores = []

for threshold in thresholds:
    y_pred = (y_proba >= threshold).astype(int)
    score = f1_score(y_val, y_pred)
    scores.append((threshold, score))

# –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π –ø–æ—Ä–æ–≥
best_threshold, best_score = max(scores, key=lambda x: x[1])
print(f"–õ—É—á—à–∏–π –ø–æ—Ä–æ–≥: {best_threshold:.2f}")

# –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred = (y_proba >= best_threshold).astype(int)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Focal Loss –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</h2>
    <p><strong>Focal Loss</strong> ‚Äî —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å, —Ñ–æ–∫—É—Å–∏—Ä—É—é—â–∞—è—Å—è –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö.</p>

    <div class="formula">
      FL(p<sub>t</sub>) = -Œ±<sub>t</sub>(1 - p<sub>t</sub>)<sup>Œ≥</sup> log(p<sub>t</sub>)
    </div>

    <h3>–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤ Keras/TensorFlow:</h3>
    <pre><code>import tensorflow as tf

def focal_loss(gamma=2., alpha=0.25):
    def focal_loss_fixed(y_true, y_pred):
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)
        
        # Focal loss
        pt = tf.where(
            tf.equal(y_true, 1), 
            y_pred, 
            1 - y_pred
        )
        
        loss = -alpha * tf.pow(1. - pt, gamma) * tf.log(pt)
        return tf.reduce_mean(loss)
    
    return focal_loss_fixed

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
model = tf.keras.Sequential([...])
model.compile(
    optimizer='adam',
    loss=focal_loss(gamma=2, alpha=0.75),
    metrics=['accuracy']
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å</h2>
    <h3>Custom loss –¥–ª—è PyTorch:</h3>
    <pre><code>import torch
import torch.nn as nn

class WeightedBCELoss(nn.Module):
    def __init__(self, pos_weight=1.0):
        super().__init__()
        self.pos_weight = pos_weight
    
    def forward(self, y_pred, y_true):
        # –í–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        weights = torch.where(
            y_true == 1, 
            self.pos_weight,
            1.0
        )
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è binary cross-entropy
        loss = nn.functional.binary_cross_entropy(
            y_pred, 
            y_true, 
            weight=weights
        )
        return loss

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
criterion = WeightedBCELoss(pos_weight=10.0)
loss = criterion(predictions, targets)</code></pre>

    <h3>sklearn —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–æ–π:</h3>
    <pre><code>from sklearn.metrics import make_scorer

def custom_cost_score(y_true, y_pred):
    """–ö–∞—Å—Ç–æ–º–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Å —É—á—ë—Ç–æ–º —Å—Ç–æ–∏–º–æ—Å—Ç–∏"""
    tn = ((y_true == 0) & (y_pred == 0)).sum()
    fp = ((y_true == 0) & (y_pred == 1)).sum()
    fn = ((y_true == 1) & (y_pred == 0)).sum()
    tp = ((y_true == 1) & (y_pred == 1)).sum()
    
    # –°—Ç–æ–∏–º–æ—Å—Ç—å –æ—à–∏–±–æ–∫
    cost = fn * 10 + fp * 1
    
    # –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å (–≤–æ–∑–≤—Ä–∞—â–∞–µ–º -cost)
    return -cost

scorer = make_scorer(custom_cost_score)

# GridSearchCV —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–µ—Ç—Ä–∏–∫–æ–π
grid = GridSearchCV(model, param_grid, scoring=scorer)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ú–µ—Ç–∞–ø—Ä–∏–¥–∏–∫–∞—Ç CostSensitiveClassifier</h2>
    <pre><code>from sklearn.base import BaseEstimator, ClassifierMixin

class CostSensitiveClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, estimator, cost_matrix):
        self.estimator = estimator
        self.cost_matrix = cost_matrix
    
    def fit(self, X, y):
        self.estimator.fit(X, y)
        return self
    
    def predict_proba(self, X):
        return self.estimator.predict_proba(X)
    
    def predict(self, X):
        proba = self.predict_proba(X)
        
        # –û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        n_samples = X.shape[0]
        n_classes = len(self.cost_matrix)
        
        expected_costs = np.zeros((n_samples, n_classes))
        for i in range(n_classes):
            for j in range(n_classes):
                expected_costs[:, j] += (
                    proba[:, i] * self.cost_matrix[i, j]
                )
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∫–ª–∞—Å—Å —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é
        return np.argmin(expected_costs, axis=1)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
cost_matrix = np.array([[0, 1], [10, 0]])
clf = CostSensitiveClassifier(
    LogisticRegression(),
    cost_matrix
)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</h2>
    <h3>–í–µ—Å–∞ –¥–ª—è 3+ –∫–ª–∞—Å—Å–æ–≤:</h3>
    <pre><code># –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞
from sklearn.utils.class_weight import compute_class_weight

classes = np.unique(y_train)
weights = compute_class_weight('balanced', classes=classes, y=y_train)
class_weight_dict = dict(zip(classes, weights))

# –ü—Ä–∏–º–µ—Ä: {0: 0.5, 1: 2.0, 2: 4.0}

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
model = RandomForestClassifier(class_weight=class_weight_dict)
model.fit(X_train, y_train)</code></pre>

    <h3>–ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –¥–ª—è –º–Ω–æ–≥–∏—Ö –∫–ª–∞—Å—Å–æ–≤:</h3>
    <pre><code># 3 –∫–ª–∞—Å—Å–∞: –∑–¥–æ—Ä–æ–≤ (0), –ª—ë–≥–∫–∞—è –±–æ–ª–µ–∑–Ω—å (1), —Ç—è–∂—ë–ª–∞—è (2)
cost_matrix = np.array([
    [0,  1,  5],   # 0: TN, –æ—à–∏–±–∫–∞ –∫–∞–∫ 1 (cost=1), –∫–∞–∫ 2 (cost=5)
    [1,  0,  3],   # 1: –æ—à–∏–±–∫–∞ –∫–∞–∫ 0 (1), TN, –æ—à–∏–±–∫–∞ –∫–∞–∫ 2 (3)
    [10, 5,  0]    # 2: –æ—à–∏–±–∫–∞ –∫–∞–∫ 0 (10), –∫–∞–∫ 1 (5), TN
])

# –°–∞–º–∞—è –¥–æ—Ä–æ–≥–∞—è –æ—à–∏–±–∫–∞: –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ç—è–∂—ë–ª—É—é –±–æ–ª–µ–∑–Ω—å (cost=10)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ SMOTE</h2>
    <p>–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ + —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ = –¥–≤–æ–π–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç!</p>

    <pre><code>from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline

# –°–Ω–∞—á–∞–ª–∞ SMOTE, –ø–æ—Ç–æ–º model —Å –≤–µ—Å–∞–º–∏
pipeline = Pipeline([
    ('smote', SMOTE(sampling_strategy=0.5)),  # –¥–æ–≤–æ–¥–∏–º –¥–æ 50%
    ('classifier', LogisticRegression(
        class_weight={0: 1, 1: 3}  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–µ—Å
    ))
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)</code></pre>

    <h3>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:</h3>
    <ul>
      <li><strong>–¢–æ–ª—å–∫–æ SMOTE</strong>: –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö minority –∫–ª–∞—Å—Å–∞ (< 100)</li>
      <li><strong>–¢–æ–ª—å–∫–æ –≤–µ—Å–∞</strong>: –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö, –Ω–æ —Ä–∞–∑–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å</li>
      <li><strong>–û–±–∞</strong>: —Å–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å (1:100+) –∏ –≤–∞–∂–Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞</h2>
    <h3>–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:</h3>
    <table>
      <tr><th>–ú–µ—Ç—Ä–∏–∫–∞</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th><th>–û–ø—Ç–∏–º—É–º</th></tr>
      <tr><td><strong>F1-score</strong></td><td>–ë–∞–ª–∞–Ω—Å precision/recall</td><td>‚Üí 1</td></tr>
      <tr><td><strong>F-beta</strong></td><td>–ë–æ–ª—å—à–∏–π –≤–µ—Å recall –∏–ª–∏ precision</td><td>‚Üí 1</td></tr>
      <tr><td><strong>Balanced Accuracy</strong></td><td>–°—Ä–µ–¥–Ω–µ–µ –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏</td><td>‚Üí 1</td></tr>
      <tr><td><strong>MCC</strong></td><td>–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞</td><td>‚Üí 1</td></tr>
      <tr><td><strong>ROC-AUC</strong></td><td>–û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è</td><td>‚Üí 1</td></tr>
    </table>

    <pre><code>from sklearn.metrics import (
    f1_score, fbeta_score, balanced_accuracy_score,
    matthews_corrcoef, roc_auc_score
)

y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

print(f"F1:         {f1_score(y_test, y_pred):.3f}")
print(f"F2 (recall):{fbeta_score(y_test, y_pred, beta=2):.3f}")
print(f"F0.5 (prec):{fbeta_score(y_test, y_pred, beta=0.5):.3f}")
print(f"Bal Acc:    {balanced_accuracy_score(y_test, y_pred):.3f}")
print(f"MCC:        {matthews_corrcoef(y_test, y_pred):.3f}")
print(f"ROC-AUC:    {roc_auc_score(y_test, y_proba):.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤</h2>
    <table>
      <tr><th>–ü–æ–¥—Ö–æ–¥</th><th>–ü–ª—é—Å—ã</th><th>–ú–∏–Ω—É—Å—ã</th></tr>
      <tr>
        <td><strong>class_weight</strong></td>
        <td>–ü—Ä–æ—Å—Ç–æ, –±—ã—Å—Ç—Ä–æ</td>
        <td>–ù–µ –≤—Å–µ–≥–¥–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ</td>
      </tr>
      <tr>
        <td><strong>sample_weight</strong></td>
        <td>–ì–∏–±–∫–æ—Å—Ç—å</td>
        <td>–ù—É–∂–Ω–æ –≤—ã—á–∏—Å–ª—è—Ç—å –≤—Ä—É—á–Ω—É—é</td>
      </tr>
      <tr>
        <td><strong>Threshold moving</strong></td>
        <td>–ù–µ –º–µ–Ω—è–µ—Ç –º–æ–¥–µ–ª—å</td>
        <td>–¢—Ä–µ–±—É–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏</td>
      </tr>
      <tr>
        <td><strong>SMOTE</strong></td>
        <td>–£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ</td>
        <td>–ú–æ–∂–µ—Ç —Å–æ–∑–¥–∞—Ç—å —à—É–º</td>
      </tr>
      <tr>
        <td><strong>Focal Loss</strong></td>
        <td>–§–æ–∫—É—Å –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö</td>
        <td>–¢–æ–ª—å–∫–æ –¥–ª—è DL</td>
      </tr>
      <tr>
        <td><strong>Cost Matrix</strong></td>
        <td>–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å</td>
        <td>–°–ª–æ–∂–Ω–µ–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å</td>
      </tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <h3>–î–ª—è —Å–ª–∞–±–æ–≥–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ (1:5 –¥–æ 1:20):</h3>
    <ul>
      <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ <code>class_weight='balanced'</code></li>
      <li>–ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ F1-score</li>
      <li>SMOTE –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω</li>
    </ul>

    <h3>–î–ª—è —Å–∏–ª—å–Ω–æ–≥–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ (1:20 –¥–æ 1:100):</h3>
    <ul>
      <li>–ö–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ SMOTE + –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤</li>
      <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ threshold moving</li>
      <li>–ü–æ–ø—Ä–æ–±—É–π—Ç–µ Focal Loss –¥–ª—è DL</li>
      <li>–ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ Balanced Accuracy –∏ MCC</li>
    </ul>

    <h3>–î–ª—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–≥–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ (1:100+):</h3>
    <ul>
      <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ anomaly detection –ø–æ–¥—Ö–æ–¥—ã</li>
      <li>Cost-sensitive learning —Å –º–∞—Ç—Ä–∏—Ü–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏</li>
      <li>Ensemble methods —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏</li>
      <li>Few-shot learning techniques</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ (<code>value_counts()</code>)</li>
      <li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ—à–∏–±–æ–∫ (FN vs FP)</li>
      <li>[ ] –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å <code>class_weight='balanced'</code></li>
      <li>[ ] –ï—Å–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç ‚Äî –ø–æ–¥–æ–±—Ä–∞—Ç—å –≤–µ—Å–∞ –≤—Ä—É—á–Ω—É—é</li>
      <li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å threshold moving</li>
      <li>[ ] –î–ª—è —Å–∏–ª—å–Ω–æ–≥–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ ‚Äî –¥–æ–±–∞–≤–∏—Ç—å SMOTE</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (F1, MCC, –Ω–µ Accuracy!)</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å quality –Ω–∞ –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–∞—Ö –æ—Ç–¥–µ–ª—å–Ω–æ</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Precision-Recall –∫—Ä–∏–≤—É—é</li>
      <li>[ ] –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –∞—ç—Ä–æ–ø–æ—Ä—Ç: –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Ä—Ä–æ—Ä–∏—Å—Ç–∞ (–ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ) –Ω–∞–º–Ω–æ–≥–æ –æ–ø–∞—Å–Ω–µ–µ, —á–µ–º –ª–∏—à–Ω–∏–π —Ä–∞–∑ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–µ–≤–∏–Ω–Ω–æ–≥–æ –ø–∞—Å—Å–∞–∂–∏—Ä–∞ (–ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ). Cost-sensitive learning —É—á–∏—Ç –º–æ–¥–µ–ª—å —É—á–∏—Ç—ã–≤–∞—Ç—å —Ä–∞–∑–Ω—É—é –≤–∞–∂–Ω–æ—Å—Ç—å –æ—à–∏–±–æ–∫, –¥–µ–ª–∞—è —Å–∏—Å—Ç–µ–º—É –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ —Ç–∞–º, –≥–¥–µ —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ¬ª.
    </blockquote>
  </div>



</div>
</body>
</html>

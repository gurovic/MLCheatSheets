<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Adversarial Robustness Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üõ°Ô∏è Adversarial Robustness</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: ML –º–æ–¥–µ–ª–∏ —É—è–∑–≤–∏–º—ã –∫ adversarial –ø—Ä–∏–º–µ—Ä–∞–º</li>
      <li><strong>Adversarial –ø—Ä–∏–º–µ—Ä</strong>: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤–æ–∑–º—É—â–µ–Ω–∏–µ ‚Üí –Ω–µ–≤–µ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ</li>
      <li><strong>–ù–µ–≤–∏–¥–∏–º–æ—Å—Ç—å</strong>: –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ–∑–∞–º–µ—Ç–Ω—ã –¥–ª—è —á–µ–ª–æ–≤–µ–∫–∞</li>
      <li><strong>–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å</strong>: —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö</li>
      <li><strong>–†–∏—Å–∫</strong>: –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–∞–≤—Ç–æ–ø–∏–ª–æ—Ç, –º–µ–¥–∏—Ü–∏–Ω–∞)</li>
    </ul>
    <blockquote>Adversarial –ø—Ä–∏–º–µ—Ä - —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–π –≤—Ö–æ–¥, –≤—ã–∑—ã–≤–∞—é—â–∏–π –æ—à–∏–±–∫—É ML –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –º–∞–ª–æ–º –≤–æ–∑–º—É—â–µ–Ω–∏–∏</blockquote>

  <div class="block">
    <h2>üî∑ 2. FGSM –∞—Ç–∞–∫–∞</h2>
    <p><strong>Fast Gradient Sign Method:</strong></p>
    <pre><code>x_adv = x + Œµ ¬∑ sign(‚àá‚Çì L(Œ∏, x, y))

–≥–¥–µ:
- x - –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
- Œµ - —Ä–∞–∑–º–µ—Ä –≤–æ–∑–º—É—â–µ–Ω–∏—è (–æ–±—ã—á–Ω–æ 0.01-0.3)
- L - —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
- y - –∏—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞</code></pre>
    
    <pre><code>import torch
import torch.nn.functional as F

def fgsm_attack(model, x, y, epsilon=0.03):
    """
    FGSM –∞—Ç–∞–∫–∞ –Ω–∞ –º–æ–¥–µ–ª—å
    """
    x.requires_grad = True
    
    # Forward pass
    output = model(x)
    loss = F.cross_entropy(output, y)
    
    # Backward pass
    model.zero_grad()
    loss.backward()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ adversarial –ø—Ä–∏–º–µ—Ä–∞
    x_grad = x.grad.data
    x_adv = x + epsilon * x_grad.sign()
    
    # –û–±—Ä–µ–∑–∫–∞ –¥–æ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ [0, 1]
    x_adv = torch.clamp(x_adv, 0, 1)
    
    return x_adv

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
x_adv = fgsm_attack(model, images, labels, epsilon=0.1)
output_adv = model(x_adv)
print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ adversarial: {output_adv.argmax(1).eq(labels).float().mean()}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. PGD –∞—Ç–∞–∫–∞</h2>
    <p><strong>Projected Gradient Descent:</strong></p>
    <pre><code>def pgd_attack(model, x, y, epsilon=0.03, alpha=0.01, num_iter=40):
    """
    PGD - –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è FGSM
    –°–∏–ª—å–Ω–µ–µ –∏ –Ω–∞–¥–µ–∂–Ω–µ–µ
    """
    x_adv = x.clone().detach()
    
    for i in range(num_iter):
        x_adv.requires_grad = True
        output = model(x_adv)
        loss = F.cross_entropy(output, y)
        
        model.zero_grad()
        loss.backward()
        
        # –®–∞–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞
        with torch.no_grad():
            x_adv = x_adv + alpha * x_adv.grad.sign()
            
            # –ü—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ epsilon-—à–∞—Ä
            perturbation = torch.clamp(x_adv - x, -epsilon, epsilon)
            x_adv = torch.clamp(x + perturbation, 0, 1)
    
    return x_adv

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
x_adv = pgd_attack(model, images, labels, 
                   epsilon=0.1, alpha=0.01, num_iter=40)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. C&W –∞—Ç–∞–∫–∞</h2>
    <p><strong>Carlini & Wagner:</strong></p>
    <pre><code># –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω–∞—è –∞—Ç–∞–∫–∞
minimize ||Œ¥||‚ÇÇ¬≤ + c ¬∑ f(x + Œ¥)

–≥–¥–µ f - —Ñ—É–Ω–∫—Ü–∏—è, –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—é—â–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ü–µ–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞</code></pre>
    
    <pre><code>from cleverhans.torch.attacks import carlini_wagner_l2

def cw_attack(model, x, y, c=1e-4, kappa=0, max_iter=1000):
    """
    C&W L2 –∞—Ç–∞–∫–∞ - –æ–¥–Ω–∞ –∏–∑ —Å–∞–º—ã—Ö —Å–∏–ª—å–Ω—ã—Ö
    """
    device = x.device
    batch_size = x.size(0)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    w = torch.zeros_like(x, requires_grad=True)
    optimizer = torch.optim.Adam([w], lr=0.01)
    
    for step in range(max_iter):
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ w –≤ x_adv
        x_adv = 0.5 * (torch.tanh(w) + 1)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
        output = model(x_adv)
        
        # L2 —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
        l2_dist = torch.sum((x_adv - x) ** 2, dim=[1, 2, 3])
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–æ—Ç–µ—Ä—è
        target_logit = output[range(batch_size), y]
        max_other_logit = torch.max(
            output - 1e4 * F.one_hot(y, output.size(1)), 
            dim=1
        )[0]
        
        loss_cls = torch.clamp(max_other_logit - target_logit + kappa, min=0)
        
        # –û–±—â–∞—è –ø–æ—Ç–µ—Ä—è
        loss = l2_dist + c * loss_cls
        
        optimizer.zero_grad()
        loss.mean().backward()
        optimizer.step()
    
    return 0.5 * (torch.tanh(w) + 1)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. DeepFool</h2>
    <pre><code>def deepfool(model, x, max_iter=50):
    """
    DeepFool - –Ω–∞—Ö–æ–¥–∏—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤–æ–∑–º—É—â–µ–Ω–∏–µ
    –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    """
    x_adv = x.clone().detach()
    pred = model(x_adv).argmax(1)
    
    for i in range(max_iter):
        x_adv.requires_grad = True
        output = model(x_adv)
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Å–∞–º
        gradients = []
        for c in range(output.size(1)):
            model.zero_grad()
            output[0, c].backward(retain_graph=True)
            gradients.append(x_adv.grad.clone())
        
        # –ù–∞—Ö–æ–¥–∏–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤–æ–∑–º—É—â–µ–Ω–∏–µ
        pred_class = output.argmax(1).item()
        min_dist = float('inf')
        
        for k in range(output.size(1)):
            if k == pred_class:
                continue
            
            w = gradients[k] - gradients[pred_class]
            f = output[0, k] - output[0, pred_class]
            
            dist = abs(f.item()) / torch.norm(w).item()
            
            if dist < min_dist:
                min_dist = dist
                best_w = w
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        r = (min_dist + 1e-4) * best_w / torch.norm(best_w)
        x_adv = x_adv + r
        
        if model(x_adv).argmax(1) != pred:
            break
    
    return x_adv</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Adversarial Training</h2>
    <p><strong>–û—Å–Ω–æ–≤–Ω–∞—è –∑–∞—â–∏—Ç–∞:</strong></p>
    <pre><code>def adversarial_training(model, train_loader, optimizer, 
                         epsilon=0.1, num_epochs=10):
    """
    –û–±—É—á–µ–Ω–∏–µ –Ω–∞ adversarial –ø—Ä–∏–º–µ—Ä–∞—Ö
    """
    for epoch in range(num_epochs):
        for images, labels in train_loader:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è adversarial –ø—Ä–∏–º–µ—Ä–æ–≤
            x_adv = pgd_attack(model, images, labels, epsilon=epsilon)
            
            # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–º–µ—Å–∏ —á–∏—Å—Ç—ã—Ö –∏ adversarial
            images_mixed = torch.cat([images, x_adv])
            labels_mixed = torch.cat([labels, labels])
            
            optimizer.zero_grad()
            output = model(images_mixed)
            loss = F.cross_entropy(output, labels_mixed)
            loss.backward()
            optimizer.step()
        
        print(f"Epoch {epoch+1} completed")
    
    return model

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
model = adversarial_training(model, train_loader, optimizer)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ú–µ—Ç–æ–¥—ã –∑–∞—â–∏—Ç—ã</h2>
    <table>
      <tr>
        <th>–ú–µ—Ç–æ–¥</th>
        <th>–ò–¥–µ—è</th>
        <th>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å</th>
      </tr>
      <tr>
        <td>Adversarial Training</td>
        <td>–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∞—Ç–∞–∫–∞—Ö</td>
        <td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
      </tr>
      <tr>
        <td>Input Transformation</td>
        <td>–°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–∞</td>
        <td>‚≠ê‚≠ê</td>
      </tr>
      <tr>
        <td>Defensive Distillation</td>
        <td>–°–º—è–≥—á–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤</td>
        <td>‚≠ê‚≠ê‚≠ê</td>
      </tr>
      <tr>
        <td>Randomization</td>
        <td>–°–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è</td>
        <td>‚≠ê‚≠ê‚≠ê</td>
      </tr>
      <tr>
        <td>Certified Defense</td>
        <td>–ì–∞—Ä–∞–Ω—Ç–∏–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏</td>
        <td>‚≠ê‚≠ê‚≠ê‚≠ê</td>
      </tr>
      <tr>
        <td>Ensemble Methods</td>
        <td>–ù–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π</td>
        <td>‚≠ê‚≠ê‚≠ê</td>
      </tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 8. Input Transformation</h2>
    <pre><code>import torchvision.transforms as transforms

def input_transformation_defense(model, x):
    """
    –ó–∞—â–∏—Ç–∞ —á–µ—Ä–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–∞
    """
    # –°–∂–∞—Ç–∏–µ JPEG
    def jpeg_compression(x, quality=75):
        from PIL import Image
        import io
        
        buffer = io.BytesIO()
        img = transforms.ToPILImage()(x)
        img.save(buffer, format='JPEG', quality=quality)
        buffer.seek(0)
        return transforms.ToTensor()(Image.open(buffer))
    
    # –ú–µ–¥–∏–∞–Ω–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä
    def median_filter(x, kernel_size=3):
        from scipy.ndimage import median_filter as mf
        return torch.from_numpy(
            mf(x.cpu().numpy(), size=(1, 1, kernel_size, kernel_size))
        )
    
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π
    x_defended = x.clone()
    for i in range(x.size(0)):
        x_defended[i] = jpeg_compression(x[i])
    
    return model(x_defended)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
output = input_transformation_defense(model, x_adv)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Defensive Distillation</h2>
    <pre><code>def defensive_distillation(teacher_model, student_model, 
                           train_loader, temperature=20):
    """
    –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–≤—ã—à–µ–Ω–Ω–æ–π temperature –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
    """
    # –®–∞–≥ 1: –û–±—É—á–∏—Ç—å teacher —Å –≤—ã—Å–æ–∫–æ–π temperature
    for images, labels in train_loader:
        logits = teacher_model(images) / temperature
        soft_labels = F.softmax(logits, dim=1)
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å soft labels
        
    # –®–∞–≥ 2: –û–±—É—á–∏—Ç—å student –Ω–∞ soft labels
    optimizer = torch.optim.SGD(student_model.parameters(), lr=0.01)
    
    for images, soft_labels in train_loader:
        logits = student_model(images) / temperature
        
        # Distillation loss
        loss = F.kl_div(
            F.log_softmax(logits, dim=1),
            soft_labels,
            reduction='batchmean'
        )
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    return student_model</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Certified Defense</h2>
    <p><strong>Randomized Smoothing:</strong></p>
    <pre><code>def randomized_smoothing(model, x, sigma=0.25, n_samples=100):
    """
    –°–µ—Ä—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞—â–∏—Ç–∞ —á–µ—Ä–µ–∑ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
    """
    predictions = []
    
    for _ in range(n_samples):
        # –î–æ–±–∞–≤–ª—è–µ–º –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–π —à—É–º
        noise = torch.randn_like(x) * sigma
        x_noisy = x + noise
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        with torch.no_grad():
            pred = model(x_noisy).argmax(1)
            predictions.append(pred)
    
    # –ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
    predictions = torch.stack(predictions)
    final_pred = predictions.mode(0)[0]
    
    return final_pred

# –°–µ—Ä—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–¥–∏—É—Å —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
def certified_radius(sigma, prob_correct):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–¥–∏—É—Å–∞, –≤ –∫–æ—Ç–æ—Ä–æ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –≤–µ—Ä–Ω–æ
    """
    from scipy.stats import norm
    return sigma * (norm.ppf(prob_correct) - norm.ppf(0.5))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –û—Ü–µ–Ω–∫–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏</h2>
    <pre><code>from foolbox import PyTorchModel, attacks

def evaluate_robustness(model, test_loader, epsilon=0.3):
    """
    –û—Ü–µ–Ω–∫–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –∞—Ç–∞–∫–∞–º
    """
    fmodel = PyTorchModel(model, bounds=(0, 1))
    
    # –†–∞–∑–ª–∏—á–Ω—ã–µ –∞—Ç–∞–∫–∏
    attack_methods = {
        'FGSM': attacks.FGSM(),
        'PGD': attacks.LinfPGD(),
        'C&W': attacks.L2CarliniWagnerAttack(),
        'DeepFool': attacks.LinfDeepFoolAttack()
    }
    
    results = {}
    
    for name, attack in attack_methods.items():
        correct = 0
        total = 0
        
        for images, labels in test_loader:
            _, x_adv, success = attack(fmodel, images, labels, 
                                       epsilons=epsilon)
            
            pred_adv = model(x_adv).argmax(1)
            correct += (pred_adv == labels).sum().item()
            total += labels.size(0)
        
        accuracy = 100.0 * correct / total
        results[name] = accuracy
        print(f"{name}: {accuracy:.2f}%")
    
    return results</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏</h2>
    <ul>
      <li><strong>Adversarial Training</strong>: —Å–∞–º–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∑–∞—â–∏—Ç–∞</li>
      <li><strong>–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∞—Ç–∞–∫</strong>: —Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–∞—Ö</li>
      <li><strong>Trade-off</strong>: —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å vs —Ç–æ—á–Ω–æ—Å—Ç—å</li>
      <li><strong>–†–∞–∑–º–µ—Ä epsilon</strong>: –≤—ã–±–∏—Ä–∞–π—Ç–µ –ø–æ –∑–∞–¥–∞—á–µ</li>
      <li><strong>Ensemble</strong>: –∫–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ –º–µ—Ç–æ–¥—ã –∑–∞—â–∏—Ç—ã</li>
      <li><strong>–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥</strong>: –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –≤ production</li>
    </ul>
    
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úì –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ</h3>
        <ul>
          <li>PGD adversarial training</li>
          <li>Certified randomized smoothing</li>
          <li>Ensemble –º–æ–¥–µ–ª–µ–π</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚úó –°–ª–∞–±–æ</h3>
        <ul>
          <li>Gradient masking</li>
          <li>Input transformation —Ç–æ–ª—å–∫–æ</li>
          <li>Obfuscated gradients</li>
        </ul>
      </div>
    </div>
  </div>

</div>

</div>
</body>
</html>

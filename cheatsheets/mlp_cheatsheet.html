<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>MLP (Multi-Layer Perceptron) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 600;
    }

    .good p, .bad p {
      margin: 0;
      font-size: 0.88em;
    }

    em {
      color: #d32f2f;
      font-style: normal;
      font-weight: 600;
    }

    strong {
      color: #1a5fb4;
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üß† MLP (Multi-Layer Perceptron)</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ß—Ç–æ —Ç–∞–∫–æ–µ MLP?</h2>
    <ul>
      <li><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è</li>
      <li><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong>: –≤—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π ‚Üí —Å–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏ ‚Üí –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π</li>
      <li><strong>–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ—Å—Ç—å</strong>: –∫–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω —Å–≤—è–∑–∞–Ω —Å–æ –≤—Å–µ–º–∏ –Ω–µ–π—Ä–æ–Ω–∞–º–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ—è</li>
      <li><strong>–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å</strong>: –º–æ–∂–µ—Ç –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å –ª—é–±—É—é —Ñ—É–Ω–∫—Ü–∏—é</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Ä–µ–≥—Ä–µ—Å—Å–∏—è, —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ MLP</h2>
    <p><strong>–°–ª–æ–∏:</strong></p>
    <ul>
      <li><strong>–í—Ö–æ–¥–Ω–æ–π (Input)</strong>: —Ä–∞–∑–º–µ—Ä = —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
      <li><strong>–°–∫—Ä—ã—Ç—ã–µ (Hidden)</strong>: 1 –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–µ–≤</li>
      <li><strong>–í—ã—Ö–æ–¥–Ω–æ–π (Output)</strong>: —Ä–∞–∑–º–µ—Ä –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∑–∞–¥–∞—á–∏</li>
    </ul>
    <p><strong>–í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π:</strong></p>
    <ul>
      <li>–ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: 1 –Ω–µ–π—Ä–æ–Ω (sigmoid)</li>
      <li>–ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è: N –Ω–µ–π—Ä–æ–Ω–æ–≤ (softmax)</li>
      <li>–†–µ–≥—Ä–µ—Å—Å–∏—è: 1 –Ω–µ–π—Ä–æ–Ω (–ª–∏–Ω–µ–π–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è)</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ (sklearn)</h2>
    <pre><code>from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# MLP –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
mlp = MLPClassifier(
    hidden_layer_sizes=(100, 50),  # 2 —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è
    activation='relu',
    solver='adam',
    max_iter=500,
    random_state=42
)

# –û–±—É—á–µ–Ω–∏–µ
mlp.fit(X_train_scaled, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
y_pred = mlp.predict(X_test_scaled)

# –û—Ü–µ–Ω–∫–∞
score = mlp.score(X_test_scaled, y_test)
print(f"Accuracy: {score:.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã MLPClassifier</h2>
    <table>
      <tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</th></tr>
      <tr><td><code>hidden_layer_sizes</code></td><td>–†–∞–∑–º–µ—Ä—ã —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤</td><td>(100,), (100, 50)</td></tr>
      <tr><td><code>activation</code></td><td>–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏</td><td>'relu' (–ø–æ —É–º–æ–ª—á.)</td></tr>
      <tr><td><code>solver</code></td><td>–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä</td><td>'adam' (–ª—É—á—à–∏–π)</td></tr>
      <tr><td><code>alpha</code></td><td>L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</td><td>0.0001-0.01</td></tr>
      <tr><td><code>batch_size</code></td><td>–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞</td><td>'auto' –∏–ª–∏ 32-256</td></tr>
      <tr><td><code>learning_rate</code></td><td>–°—Ç—Ä–∞—Ç–µ–≥–∏—è LR</td><td>'constant', 'adaptive'</td></tr>
      <tr><td><code>max_iter</code></td><td>–ú–∞–∫—Å. —ç–ø–æ—Ö</td><td>200-1000</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 5. –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏</h2>
    <table>
      <tr><th>–§—É–Ω–∫—Ü–∏—è</th><th>–§–æ—Ä–º—É–ª–∞</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr>
      <tr><td><strong>ReLU</strong></td><td>max(0, x)</td><td>–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é</td></tr>
      <tr><td><strong>Sigmoid</strong></td><td>1/(1+e<sup>-x</sup>)</td><td>–í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–±–∏–Ω–∞—Ä–Ω–∞—è)</td></tr>
      <tr><td><strong>Tanh</strong></td><td>(e<sup>x</sup>-e<sup>-x</sup>)/(e<sup>x</sup>+e<sup>-x</sup>)</td><td>–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ ReLU</td></tr>
      <tr><td><strong>Softmax</strong></td><td>e<sup>x·µ¢</sup>/Œ£e<sup>x‚±º</sup></td><td>–í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è)</td></tr>
      <tr><td><strong>Linear</strong></td><td>x</td><td>–†–µ–≥—Ä–µ—Å—Å–∏—è</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ (PyTorch)</h2>
    <pre><code>import torch
import torch.nn as nn
import torch.optim as optim

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
class MLP(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MLP, self).__init__()
        
        layers = []
        prev_size = input_size
        
        # –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            prev_size = hidden_size
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        layers.append(nn.Linear(prev_size, output_size))
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
model = MLP(input_size=10, hidden_sizes=[100, 50], 
            output_size=2)

# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –û–±—É—á–µ–Ω–∏–µ MLP (PyTorch)</h2>
    <pre><code>import torch
from torch.utils.data import DataLoader, TensorDataset

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
X_train_t = torch.FloatTensor(X_train_scaled)
y_train_t = torch.LongTensor(y_train)

dataset = TensorDataset(X_train_t, y_train_t)
dataloader = DataLoader(dataset, batch_size=32, 
                        shuffle=True)

# –û–±—É—á–µ–Ω–∏–µ
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    
    for batch_X, batch_y in dataloader:
        # Forward pass
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    if (epoch + 1) % 10 == 0:
        avg_loss = total_loss / len(dataloader)
        print(f'Epoch [{epoch+1}/{num_epochs}], '
              f'Loss: {avg_loss:.4f}')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ (Keras)</h2>
    <pre><code>from tensorflow import keras
from tensorflow.keras import layers

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = keras.Sequential([
    layers.Dense(100, activation='relu', 
                input_shape=(input_dim,)),
    layers.Dense(50, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    X_train_scaled, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# –û—Ü–µ–Ω–∫–∞
test_loss, test_acc = model.evaluate(
    X_test_scaled, y_test, verbose=0
)
print(f'Test accuracy: {test_acc:.4f}')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –í—ã–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã</h2>
    <table>
      <tr><th>–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö</th><th>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</th></tr>
      <tr><td><strong>–ú–∞–ª–µ–Ω—å–∫–∏–π (&lt;1000)</strong></td><td>1 —Å–ª–æ–π, 10-50 –Ω–µ–π—Ä–æ–Ω–æ–≤</td></tr>
      <tr><td><strong>–°—Ä–µ–¥–Ω–∏–π (1K-10K)</strong></td><td>1-2 —Å–ª–æ—è, 50-100 –Ω–µ–π—Ä–æ–Ω–æ–≤</td></tr>
      <tr><td><strong>–ë–æ–ª—å—à–æ–π (&gt;10K)</strong></td><td>2-3 —Å–ª–æ—è, 100-500 –Ω–µ–π—Ä–æ–Ω–æ–≤</td></tr>
    </table>
    <p><strong>–ü—Ä–∞–≤–∏–ª–æ –±–æ–ª—å—à–æ–≥–æ –ø–∞–ª—å—Ü–∞:</strong></p>
    <ul>
      <li>–ü–µ—Ä–≤—ã–π —Å–ª–æ–π: –≤ 2-3 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –≤—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
      <li>–ö–∞–∂–¥—ã–π —Å–ª–µ–¥—É—é—â–∏–π: —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –≤ 2 —Ä–∞–∑–∞</li>
      <li>–ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–∫—Ä—ã—Ç—ã–π: –≤ 2 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –≤—ã—Ö–æ–¥–æ–≤</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 10. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</h2>
    <pre><code># sklearn - L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
mlp = MLPClassifier(
    hidden_layer_sizes=(100, 50),
    alpha=0.001,  # L2 penalty
    early_stopping=True,
    validation_fraction=0.1
)

# Keras - Dropout –∏ L2
from tensorflow.keras import regularizers

model = keras.Sequential([
    layers.Dense(100, activation='relu',
                kernel_regularizer=regularizers.l2(0.001)),
    layers.Dropout(0.3),
    layers.Dense(50, activation='relu',
                kernel_regularizer=regularizers.l2(0.001)),
    layers.Dropout(0.2),
    layers.Dense(num_classes, activation='softmax')
])

# PyTorch - Dropout
class MLPWithDropout(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(input_size, 100)
        self.dropout1 = nn.Dropout(0.3)
        self.fc2 = nn.Linear(100, 50)
        self.dropout2 = nn.Dropout(0.2)
        self.fc3 = nn.Linear(50, output_size)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.fc3(x)
        return x</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Early Stopping</h2>
    <pre><code># sklearn
mlp = MLPClassifier(
    hidden_layer_sizes=(100, 50),
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=10,  # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Å–ª–µ 10 —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
    tol=1e-4
)

# Keras
from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=1000,
    callbacks=[early_stop]
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Batch Normalization</h2>
    <pre><code># Keras
model = keras.Sequential([
    layers.Dense(100, activation='relu'),
    layers.BatchNormalization(),
    layers.Dense(50, activation='relu'),
    layers.BatchNormalization(),
    layers.Dense(num_classes, activation='softmax')
])

# PyTorch
class MLPWithBatchNorm(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(input_size, 100)
        self.bn1 = nn.BatchNorm1d(100)
        self.fc2 = nn.Linear(100, 50)
        self.bn2 = nn.BatchNorm1d(50)
        self.fc3 = nn.Linear(50, output_size)
    
    def forward(self, x):
        x = torch.relu(self.bn1(self.fc1(x)))
        x = torch.relu(self.bn2(self.fc2(x)))
        x = self.fc3(x)
        return x</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤</h2>
    <pre><code># Keras
from tensorflow.keras.initializers import HeNormal, GlorotUniform

model = keras.Sequential([
    layers.Dense(100, activation='relu',
                kernel_initializer=HeNormal()),  # –î–ª—è ReLU
    layers.Dense(50, activation='tanh',
                kernel_initializer=GlorotUniform()),  # –î–ª—è tanh/sigmoid
    layers.Dense(num_classes, activation='softmax')
])

# PyTorch
import torch.nn.init as init

def init_weights(m):
    if isinstance(m, nn.Linear):
        init.kaiming_normal_(m.weight)  # He init –¥–ª—è ReLU
        # –∏–ª–∏
        # init.xavier_normal_(m.weight)  # Xavier –¥–ª—è tanh/sigmoid
        if m.bias is not None:
            init.constant_(m.bias, 0)

model.apply(init_weights)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. Learning Rate Scheduling</h2>
    <pre><code># sklearn
mlp = MLPClassifier(
    learning_rate='adaptive',
    learning_rate_init=0.001
)

# Keras
from tensorflow.keras.callbacks import ReduceLROnPlateau

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6
)

history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=100,
    callbacks=[reduce_lr]
)

# PyTorch
from torch.optim.lr_scheduler import ReduceLROnPlateau

optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = ReduceLROnPlateau(
    optimizer, 
    mode='min', 
    factor=0.5, 
    patience=5
)

# –í —Ü–∏–∫–ª–µ –æ–±—É—á–µ–Ω–∏—è
scheduler.step(val_loss)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. –†–µ–≥—Ä–µ—Å—Å–∏—è —Å MLP</h2>
    <pre><code># sklearn
from sklearn.neural_network import MLPRegressor

mlp_reg = MLPRegressor(
    hidden_layer_sizes=(100, 50),
    activation='relu',
    solver='adam',
    max_iter=500,
    random_state=42
)

mlp_reg.fit(X_train_scaled, y_train)

# Keras
model = keras.Sequential([
    layers.Dense(100, activation='relu', 
                input_shape=(input_dim,)),
    layers.Dense(50, activation='relu'),
    layers.Dense(1)  # –û–¥–∏–Ω –≤—ã—Ö–æ–¥, –±–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
])

model.compile(
    optimizer='adam',
    loss='mse',  # MSE –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    metrics=['mae']
)

history = model.fit(
    X_train_scaled, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è</h2>
    <pre><code>import matplotlib.pyplot as plt

# sklearn - –≥—Ä–∞—Ñ–∏–∫ loss
plt.figure(figsize=(10, 6))
plt.plot(mlp.loss_curve_)
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.grid(True)
plt.show()

# Keras - –∏—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Val')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss')

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Val')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Accuracy')

plt.tight_layout()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</h2>
    <pre><code>from sklearn.model_selection import GridSearchCV

param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (100, 50)],
    'activation': ['relu', 'tanh'],
    'alpha': [0.0001, 0.001, 0.01],
    'learning_rate': ['constant', 'adaptive']
}

mlp = MLPClassifier(max_iter=500, random_state=42)

grid_search = GridSearchCV(
    mlp, 
    param_grid, 
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train_scaled, y_train)

print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
print(f"–õ—É—á—à–∏–π score: {grid_search.best_score_:.4f}")

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
test_score = grid_search.score(X_test_scaled, y_test)
print(f"Test score: {test_score:.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 18. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º</h2>
    <table>
      <tr><th>–ü—Ä–æ–±–ª–µ–º–∞</th><th>–ü—Ä–∏–∑–Ω–∞–∫–∏</th><th>–†–µ—à–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</strong></td><td>Train >> Val</td><td>Dropout, L2, –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö</td></tr>
      <tr><td><strong>–ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ</strong></td><td>Train –∏ Val –Ω–∏–∑–∫–∏–µ</td><td>–ë–æ–ª—å—à–µ —Å–ª–æ–µ–≤/–Ω–µ–π—Ä–æ–Ω–æ–≤</td></tr>
      <tr><td><strong>–ò—Å—á–µ–∑–∞—é—â–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç</strong></td><td>–ù–µ—Ç –æ–±—É—á–µ–Ω–∏—è</td><td>ReLU, BatchNorm, –º–µ–Ω—å—à–µ —Å–ª–æ–µ–≤</td></tr>
      <tr><td><strong>–í–∑—Ä—ã–≤–∞—é—â–∏–π—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç</strong></td><td>NaN –≤ loss</td><td>Gradient clipping, –º–µ–Ω—å—à–µ LR</td></tr>
      <tr><td><strong>–ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</strong></td><td>Loss –ø–∞–¥–∞–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ</td><td>–£–≤–µ–ª–∏—á–∏—Ç—å LR, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Adam</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 19. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è MLP</h2>
    <table>
      <tr><th>–û–±–ª–∞—Å—Ç—å</th><th>–ó–∞–¥–∞—á–∞</th></tr>
      <tr><td><strong>–¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ</strong></td><td>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Ä–µ–≥—Ä–µ—Å—Å–∏—è</td></tr>
      <tr><td><strong>–§–∏–Ω–∞–Ω—Å—ã</strong></td><td>–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ü–µ–Ω, —Ä–∏—Å–∫</td></tr>
      <tr><td><strong>–ú–µ–¥–∏—Ü–∏–Ω–∞</strong></td><td>–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞</td></tr>
      <tr><td><strong>–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥</strong></td><td>Churn prediction, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</td></tr>
      <tr><td><strong>NLP</strong></td><td>–ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞</td></tr>
      <tr><td><strong>–°–µ–Ω—Å–æ—Ä—ã</strong></td><td>–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 20. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –î–µ–ª–∞—Ç—å</h3>
        <p>‚Ä¢ –í—Å–µ–≥–¥–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ<br>
        ‚Ä¢ –ù–∞—á–∏–Ω–∞—Ç—å —Å –º–∞–ª–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã<br>
        ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å early stopping<br>
        ‚Ä¢ –ü—Ä–∏–º–µ–Ω—è—Ç—å dropout –¥–ª—è –±–æ–ª—å—à–∏—Ö —Å–µ—Ç–µ–π<br>
        ‚Ä¢ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å train –∏ validation loss</p>
      </div>
      <div class="bad">
        <h3>‚ùå –ù–µ –¥–µ–ª–∞—Ç—å</h3>
        <p>‚Ä¢ –ó–∞–±—ã–≤–∞—Ç—å –ø—Ä–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ<br>
        ‚Ä¢ –°—Ä–∞–∑—É –¥–µ–ª–∞—Ç—å –≥–ª—É–±–æ–∫—É—é —Å–µ—Ç—å<br>
        ‚Ä¢ –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ<br>
        ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π learning rate<br>
        ‚Ä¢ –û–±—É—á–∞—Ç—å –±–µ–∑ validation split</p>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 21. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</th><th>–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</th></tr>
      <tr><td><strong>MLP</strong></td><td>–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π, –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–π</td><td>–¢—Ä–µ–±—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –º–µ–¥–ª–µ–Ω–Ω—ã–π</td></tr>
      <tr><td><strong>Random Forest</strong></td><td>–ù–µ —Ç—Ä–µ–±—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è</td><td>–•—É–∂–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö</td></tr>
      <tr><td><strong>XGBoost</strong></td><td>–õ—É—á—à–µ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö</td><td>–õ–∏–Ω–µ–π–Ω—ã–π –≤ –∫–∞–∂–¥–æ–º –ª–∏—Å—Ç–µ</td></tr>
      <tr><td><strong>SVM</strong></td><td>–•–æ—Ä–æ—à –¥–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö</td><td>–ù–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 22. –¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏</h2>
    <table>
      <tr><th>–û—à–∏–±–∫–∞</th><th>–†–µ—à–µ–Ω–∏–µ</th></tr>
      <tr><td>–ù–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ</td><td>StandardScaler –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º</td></tr>
      <tr><td>–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è —Å–µ—Ç—å</td><td>–ù–∞—á–∞—Ç—å —Å –º–∞–ª–æ–π, —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å</td></tr>
      <tr><td>–í—ã—Å–æ–∫–∏–π learning rate</td><td>–ù–∞—á–∞—Ç—å —Å 0.001, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å scheduling</td></tr>
      <tr><td>–ú–∞–ª–æ —ç–ø–æ—Ö</td><td>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å early stopping</td></tr>
      <tr><td>–ù–µ—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</td><td>Dropout –∏–ª–∏ L2</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 23. –†–µ—Å—É—Ä—Å—ã</h2>
    <ul>
      <li><strong>–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è</strong>: sklearn.neural_network.MLPClassifier</li>
      <li><strong>–ö–Ω–∏–≥–∞</strong>: "Deep Learning" - Goodfellow et al.</li>
      <li><strong>–ö—É—Ä—Å</strong>: Fast.ai Practical Deep Learning</li>
      <li><strong>–í–∏–¥–µ–æ</strong>: "Neural Networks" - 3Blue1Brown</li>
      <li><strong>–°—Ç–∞—Ç—å—è</strong>: "Understanding Neural Networks" –Ω–∞ Towards DS</li>
    </ul>
  </div>

</div>

</div>
</body>
</html>

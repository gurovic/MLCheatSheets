<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>TensorFlow/Keras –ü–æ–ª–Ω—ã–π –ì–∞–π–¥ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üî∑ TensorFlow/Keras –ü–æ–ª–Ω—ã–π –ì–∞–π–¥</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã TensorFlow/Keras</h2>
    <p><strong>TensorFlow</strong> ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ—Ç Google –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. <strong>Keras</strong> ‚Äî –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π API –¥–ª—è TensorFlow.</p>
    <ul>
      <li><strong>Keras</strong>: –ø—Ä–æ—Å—Ç–æ–π, –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã–π API</li>
      <li><strong>TensorFlow 2.x</strong>: eager execution –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é</li>
      <li><strong>–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ</strong>: –æ—Ç–ª–∏—á–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ deployment</li>
      <li><strong>–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞</strong>: TensorFlow Lite, TensorFlow.js, TF Serving</li>
      <li><strong>–ü–æ–¥–¥–µ—Ä–∂–∫–∞</strong>: TPU, GPU, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</li>
    </ul>
    <pre><code># –£—Å—Ç–∞–Ω–æ–≤–∫–∞
pip install tensorflow

# –ò–º–ø–æ—Ä—Ç
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä—Å–∏–∏
print(tf.__version__)
print(keras.__version__)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
print("GPU:", tf.config.list_physical_devices('GPU'))
print("GPU –¥–æ—Å—Ç—É–ø–µ–Ω:", tf.test.is_gpu_available())</code></pre>

  <div class="block">
    <h2>üî∑ 2. –¢–µ–Ω–∑–æ—Ä—ã –≤ TensorFlow</h2>
    <pre><code># –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤
x = tf.constant([1, 2, 3])
y = tf.zeros([3, 4])
z = tf.ones([2, 3, 4])
rand_t = tf.random.normal([3, 3])
rand_u = tf.random.uniform([3, 3])

# –ò–∑ numpy
import numpy as np
arr = np.array([1, 2, 3])
t = tf.constant(arr)

# –í numpy
arr_back = t.numpy()

# –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
float_t = tf.constant([1.0, 2.0], dtype=tf.float32)
int_t = tf.constant([1, 2], dtype=tf.int32)

# –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
print(x.shape)  # TensorShape([3])
print(tf.rank(x))  # –†–∞–Ω–≥ —Ç–µ–Ω–∑–æ—Ä–∞

# –û–ø–µ—Ä–∞—Ü–∏–∏
a = tf.constant([1, 2, 3])
b = tf.constant([4, 5, 6])
c = a + b
d = tf.matmul(tf.reshape(a, [3, 1]), tf.reshape(b, [1, 3]))

# –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã
x = tf.random.normal([12])
y = tf.reshape(x, [3, 4])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (Sequential API)</h2>
    <pre><code>from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout

# –°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–±
model = Sequential([
    Dense(128, activation='relu', input_shape=(784,)),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(10, activation='softmax')
])

# –ò–ª–∏ –¥–æ–±–∞–≤–ª—è—Ç—å —Å–ª–æ–∏ –ø–æ–æ—á–µ—Ä—ë–¥–Ω–æ
model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(784,)))
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))

# –ü—Ä–æ—Å–º–æ—Ç—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
model.summary()

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Functional API (–≥–∏–±–∫–∏–π)</h2>
    <pre><code>from tensorflow.keras import Model, Input
from tensorflow.keras.layers import Dense, Concatenate

# –ë–æ–ª–µ–µ –≥–∏–±–∫–∏–π –ø–æ–¥—Ö–æ–¥
inputs = Input(shape=(784,))
x = Dense(128, activation='relu')(inputs)
x = Dropout(0.2)(x)
x = Dense(64, activation='relu')(x)
outputs = Dense(10, activation='softmax')(x)

model = Model(inputs=inputs, outputs=outputs)

# –ù–µ—Å–∫–æ–ª—å–∫–æ –≤—Ö–æ–¥–æ–≤/–≤—ã—Ö–æ–¥–æ–≤
input1 = Input(shape=(100,))
input2 = Input(shape=(50,))

x1 = Dense(64, activation='relu')(input1)
x2 = Dense(32, activation='relu')(input2)

# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
combined = Concatenate()([x1, x2])
z = Dense(32, activation='relu')(combined)

output1 = Dense(10, activation='softmax', name='class')(z)
output2 = Dense(1, activation='sigmoid', name='aux')(z)

model = Model(
    inputs=[input1, input2],
    outputs=[output1, output2]
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –°–ª–æ–∏ (Layers)</h2>
    <table>
      <tr><th>–°–ª–æ–π</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–ü—Ä–∏–º–µ—Ä</th></tr>
      <tr><td><code>Dense</code></td><td>Fully connected</td><td><code>Dense(128, activation='relu')</code></td></tr>
      <tr><td><code>Conv2D</code></td><td>2D —Å–≤—ë—Ä—Ç–∫–∞</td><td><code>Conv2D(64, (3, 3), activation='relu')</code></td></tr>
      <tr><td><code>MaxPooling2D</code></td><td>Max pooling</td><td><code>MaxPooling2D((2, 2))</code></td></tr>
      <tr><td><code>Dropout</code></td><td>Dropout</td><td><code>Dropout(0.5)</code></td></tr>
      <tr><td><code>BatchNormalization</code></td><td>Batch norm</td><td><code>BatchNormalization()</code></td></tr>
      <tr><td><code>LSTM</code></td><td>LSTM</td><td><code>LSTM(128, return_sequences=True)</code></td></tr>
      <tr><td><code>GRU</code></td><td>GRU</td><td><code>GRU(64)</code></td></tr>
      <tr><td><code>Embedding</code></td><td>Embeddings</td><td><code>Embedding(10000, 128)</code></td></tr>
      <tr><td><code>Flatten</code></td><td>Flatten</td><td><code>Flatten()</code></td></tr>
    </table>
  </div>

  <div class="block">
    <h2>ÔøΩÔøΩ 6. –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏</h2>
    <pre><code># –í —Å–ª–æ—è—Ö
Dense(128, activation='relu')
Dense(10, activation='softmax')
Dense(1, activation='sigmoid')
Dense(64, activation='tanh')

# –ò–ª–∏ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–∏
from tensorflow.keras.layers import Activation

model = Sequential([
    Dense(128),
    Activation('relu'),
    Dense(10),
    Activation('softmax')
])

# –î–æ—Å—Ç—É–ø–Ω—ã–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
# 'relu', 'sigmoid', 'tanh', 'softmax', 
# 'elu', 'selu', 'softplus', 'softsign',
# 'swish', 'gelu'</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å (Loss)</h2>
    <pre><code># –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–±–∏–Ω–∞—Ä–Ω–∞—è)
loss='binary_crossentropy'  # –° sigmoid –Ω–∞ –≤—ã—Ö–æ–¥–µ

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è)
loss='categorical_crossentropy'  # –° softmax, one-hot labels
loss='sparse_categorical_crossentropy'  # –° softmax, integer labels

# –†–µ–≥—Ä–µ—Å—Å–∏—è
loss='mse'  # Mean Squared Error
loss='mae'  # Mean Absolute Error
loss='huber'  # Huber loss

# –ö–∞—Å—Ç–æ–º–Ω–∞—è loss
def custom_loss(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred))

model.compile(loss=custom_loss, optimizer='adam')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã</h2>
    <pre><code>from tensorflow.keras.optimizers import *

# SGD
optimizer = SGD(learning_rate=0.01, momentum=0.9)

# Adam (–ø–æ–ø—É–ª—è—Ä–Ω—ã–π)
optimizer = Adam(learning_rate=0.001)

# RMSprop
optimizer = RMSprop(learning_rate=0.001)

# AdaGrad
optimizer = Adagrad(learning_rate=0.01)

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# –ò–ª–∏ —Å—Ç—Ä–æ–∫–æ–π (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏</h2>
    <pre><code># –ü—Ä–æ—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ
history = model.fit(
    X_train, y_train,
    batch_size=32,
    epochs=10,
    validation_split=0.2,  # –ò–ª–∏ validation_data=(X_val, y_val)
    verbose=1
)

# –° callback'–∞–º–∏
from tensorflow.keras.callbacks import *

callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True
    ),
    ModelCheckpoint(
        'best_model.h5',
        monitor='val_accuracy',
        save_best_only=True
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3
    )
]

history = model.fit(
    X_train, y_train,
    batch_size=32,
    epochs=50,
    validation_split=0.2,
    callbacks=callbacks
)

# –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
print(history.history.keys())
# ['loss', 'accuracy', 'val_loss', 'val_accuracy']</code></pre>
  </div>

  <div class="block">
    <h2>ÔøΩÔøΩ 10. Callbacks</h2>
    <table>
      <tr><th>Callback</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th></tr>
      <tr><td><code>EarlyStopping</code></td><td>–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —É–ª—É—á—à–µ–Ω–∏—è</td></tr>
      <tr><td><code>ModelCheckpoint</code></td><td>–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è</td></tr>
      <tr><td><code>ReduceLROnPlateau</code></td><td>–°–Ω–∏–∂–∞–µ—Ç learning rate –ø—Ä–∏ –∑–∞—Å—Ç–æ–µ</td></tr>
      <tr><td><code>TensorBoard</code></td><td>–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è TensorBoard</td></tr>
      <tr><td><code>LearningRateScheduler</code></td><td>–ö–∞—Å—Ç–æ–º–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ LR</td></tr>
      <tr><td><code>CSVLogger</code></td><td>–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ CSV</td></tr>
    </table>
    <pre><code># TensorBoard
tensorboard_callback = TensorBoard(
    log_dir='./logs',
    histogram_freq=1
)

# –ó–∞–ø—É—Å–∫ TensorBoard
# tensorboard --logdir=./logs

# –ö–∞—Å—Ç–æ–º–Ω—ã–π callback
class CustomCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        print(f"\nEpoch {epoch}: loss={logs['loss']:.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞</h2>
    <pre><code># –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
predictions = model.predict(X_test)
# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞

# –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
predicted_classes = np.argmax(predictions, axis=1)

# –ò–ª–∏ —Å—Ä–∞–∑—É –∫–ª–∞—Å—Å—ã
predicted_classes = model.predict_classes(X_test)  # –£—Å—Ç–∞—Ä–µ–ª–æ –≤ TF 2.6+

# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Batch-wise –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
predictions = model.predict(X_test, batch_size=128)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞</h2>
    <pre><code># –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ–π –º–æ–¥–µ–ª–∏ (–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ + –≤–µ—Å–∞ + optimizer)
model.save('my_model.h5')  # HDF5 —Ñ–æ—Ä–º–∞—Ç
model.save('my_model')     # SavedModel —Ñ–æ—Ä–º–∞—Ç (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = keras.models.load_model('my_model.h5')

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –≤–µ—Å–æ–≤
model.save_weights('model_weights.h5')

# –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤
model.load_weights('model_weights.h5')

# –≠–∫—Å–ø–æ—Ä—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –≤ JSON
json_config = model.to_json()
with open('model_architecture.json', 'w') as f:
    f.write(json_config)

# –ó–∞–≥—Ä—É–∑–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
with open('model_architecture.json', 'r') as f:
    json_config = f.read()
model = keras.models.model_from_json(json_config)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. Data Augmentation</h2>
    <pre><code>from tensorflow.keras.preprocessing.image import ImageDataGenerator

# –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.2,
    shear_range=0.2,
    fill_mode='nearest'
)

# –û–±—É—á–µ–Ω–∏–µ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π
model.fit(
    datagen.flow(X_train, y_train, batch_size=32),
    epochs=50,
    validation_data=(X_val, y_val)
)

# –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ (TF 2.x)
from tensorflow.keras.layers.experimental.preprocessing import *

data_augmentation = Sequential([
    RandomFlip("horizontal"),
    RandomRotation(0.2),
    RandomZoom(0.2),
    RandomContrast(0.2)
])

# –î–æ–±–∞–≤–∏—Ç—å –≤ –º–æ–¥–µ–ª—å
model = Sequential([
    data_augmentation,
    Conv2D(32, (3, 3), activation='relu'),
    # ...
])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –°–≤—ë—Ä—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å (CNN)</h2>
    <pre><code>model = Sequential([
    # Conv Block 1
    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    BatchNormalization(),
    Conv2D(32, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),
    
    # Conv Block 2
    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),
    
    # FC layers
    Flatten(),
    Dense(512, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. RNN/LSTM</h2>
    <pre><code># LSTM –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
model = Sequential([
    Embedding(vocab_size, 128, input_length=max_length),
    LSTM(128, return_sequences=True, dropout=0.2),
    LSTM(64, dropout=0.2),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# Bidirectional LSTM
from tensorflow.keras.layers import Bidirectional

model = Sequential([
    Embedding(vocab_size, 128),
    Bidirectional(LSTM(64, return_sequences=True)),
    Bidirectional(LSTM(32)),
    Dense(num_classes, activation='softmax')
])

# GRU (–±—ã—Å—Ç—Ä–µ–µ LSTM)
model = Sequential([
    Embedding(vocab_size, 128),
    GRU(128, return_sequences=False),
    Dense(num_classes, activation='softmax')
])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. Transfer Learning</h2>
    <pre><code>from tensorflow.keras.applications import *

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
base_model = ResNet50(
    weights='imagenet',
    include_top=False,  # –ë–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ—ë–≤
    input_shape=(224, 224, 3)
)

# –ó–∞–º–æ—Ä–æ–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
base_model.trainable = False

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–≤–æ–∏—Ö —Å–ª–æ—ë–≤
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

# –û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö —Å–ª–æ—ë–≤
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
model.fit(X_train, y_train, epochs=10)

# Fine-tuning: —Ä–∞–∑–º–æ—Ä–æ–∑–∏—Ç—å –∏ –¥–æ–æ–±—É—á–∏—Ç—å
base_model.trainable = True
model.compile(
    optimizer=Adam(learning_rate=0.0001),  # –ú–∞–ª–µ–Ω—å–∫–∏–π LR!
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
model.fit(X_train, y_train, epochs=5)</code></pre>
  </div>

  <div class="block">
    <h2>ÔøΩÔøΩ 17. –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–†–∞–∑–º–µ—Ä</th><th>Top-1 Acc</th></tr>
      <tr><td>VGG16</td><td>528 MB</td><td>71.3%</td></tr>
      <tr><td>ResNet50</td><td>98 MB</td><td>74.9%</td></tr>
      <tr><td>InceptionV3</td><td>92 MB</td><td>77.9%</td></tr>
      <tr><td>MobileNetV2</td><td>14 MB</td><td>71.3%</td></tr>
      <tr><td>EfficientNetB0</td><td>29 MB</td><td>77.1%</td></tr>
    </table>
    <pre><code># –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
from tensorflow.keras.applications import MobileNetV2

base = MobileNetV2(weights='imagenet', include_top=False)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 18. Custom Training Loop</h2>
    <pre><code># –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –æ–±—É—á–µ–Ω–∏–µ–º
@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        predictions = model(x, training=True)
        loss = loss_fn(y, predictions)
    
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    
    train_acc_metric.update_state(y, predictions)
    return loss

# –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}/{epochs}")
    
    for step, (x_batch, y_batch) in enumerate(train_dataset):
        loss = train_step(x_batch, y_batch)
        
        if step % 100 == 0:
            print(f"Step {step}: loss = {loss:.4f}")
    
    train_acc = train_acc_metric.result()
    print(f"Training accuracy: {train_acc:.4f}")
    train_acc_metric.reset_states()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 19. TensorFlow Dataset (tf.data)</h2>
    <pre><code>import tensorflow as tf

# –°–æ–∑–¥–∞–Ω–∏–µ dataset
dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))

# –û–ø–µ—Ä–∞—Ü–∏–∏
dataset = dataset.shuffle(buffer_size=1024)
dataset = dataset.batch(32)
dataset = dataset.prefetch(tf.data.AUTOTUNE)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
model.fit(dataset, epochs=10)

# –ú–∞–ø–ø–∏–Ω–≥ —Ñ—É–Ω–∫—Ü–∏–∏
def preprocess(x, y):
    x = tf.cast(x, tf.float32) / 255.0
    return x, y

dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)

# –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
def load_image(path, label):
    image = tf.io.read_file(path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, [224, 224])
    return image / 255.0, label

dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))
dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)
dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 20. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è</h2>
    <pre><code>import matplotlib.pyplot as plt

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è
history = model.fit(...)

plt.figure(figsize=(12, 4))

# Loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='val')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss')

# Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='val')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')

plt.tight_layout()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 21. –°–æ–≤–µ—Ç—ã –∏ Best Practices</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ Best Practices</h3>
        <ul>
          <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ <code>sparse_categorical_crossentropy</code> —Å integer labels</li>
          <li>–ù–æ—Ä–º–∞–ª–∏–∑—É–π—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (0-1 –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)</li>
          <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ BatchNormalization –ø–æ—Å–ª–µ Conv/Dense</li>
          <li>EarlyStopping —Å <code>restore_best_weights=True</code></li>
          <li>Data augmentation –¥–ª—è –º–∞–ª—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤</li>
          <li>Callback –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è LR</li>
          <li>SavedModel —Ñ–æ—Ä–º–∞—Ç –≤–º–µ—Å—Ç–æ HDF5</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏</h3>
        <ul>
          <li>–ù–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ</li>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å softmax —Å sparse_categorical_crossentropy –¥–≤–∞–∂–¥—ã</li>
          <li>–ó–∞–±—ã—Ç—å compile() –ø–µ—Ä–µ–¥ fit()</li>
          <li>–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π learning rate</li>
          <li>–ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å validation_split</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 22. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å TensorFlow –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å GPU</li>
      <li>[ ] –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ</li>
      <li>[ ] –°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å (Sequential –∏–ª–∏ Functional API)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å loss function –∏ metrics</li>
      <li>[ ] –°–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —Å optimizer</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å callbacks (EarlyStopping, ModelCheckpoint)</li>
      <li>[ ] –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å validation_split</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è</li>
      <li>[ ] –û—Ü–µ–Ω–∏—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ</li>
      <li>[ ] –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´TensorFlow/Keras ‚Äî —ç—Ç–æ –∫–∞–∫ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. Keras –¥–µ–ª–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –≤–µ—â–∏ –ø—Ä–æ—Å—Ç—ã–º–∏: –≤—ã –æ–ø–∏—Å—ã–≤–∞–µ—Ç–µ, –∫–∞–∫—É—é –∑–∞–¥–∞—á—É —Ö–æ—Ç–∏—Ç–µ —Ä–µ—à–∏—Ç—å, –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç–µ –ø—Ä–∏–º–µ—Ä—ã, –∏ —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–∞ —É—á–∏—Ç—Å—è. –ö–∞–∫ –æ–±—É—á–µ–Ω–∏–µ —Ä–µ–±—ë–Ω–∫–∞ ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∫–æ—à–µ–∫ –∏ —Å–æ–±–∞–∫, –∏ –æ–Ω —É—á–∏—Ç—Å—è –∏—Ö —Ä–∞–∑–ª–∏—á–∞—Ç—å¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

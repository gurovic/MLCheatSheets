<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è RNN Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üîÑ –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è RNN</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ü—Ä–æ–±–ª–µ–º—ã RNN</h2>
    <p><strong>RNN —Å–∫–ª–æ–Ω–Ω—ã –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é</strong> –∏–∑-–∑–∞:</p>
    <ul>
      <li>–ë–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
      <li>–ü–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–µ—Å–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ</li>
      <li>–î–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π</li>
      <li>–ú–∞–ª—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤</li>
    </ul>
    
    <p><strong>–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Dropout –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç</strong> –≤ RNN:</p>
    <ul>
      <li>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ–∂–¥—É –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —à–∞–≥–∞–º–∏ —Ä–∞–∑—Ä—É—à–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é</li>
      <li>–ù–∞—Ä—É—à–∞–µ—Ç—Å—è –ø–µ—Ä–µ–¥–∞—á–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è</li>
      <li>–¢–µ—Ä—è–µ—Ç—Å—è –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 2. –í–∏–¥—ã Dropout –¥–ª—è RNN</h2>
    <table>
      <tr><th>–¢–∏–ø</th><th>–ì–¥–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è</th><th>–¶–µ–ª—å</th></tr>
      <tr><td><strong>Input Dropout</strong></td><td>–ù–∞ –≤—Ö–æ–¥–µ</td><td>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–æ–≤</td></tr>
      <tr><td><strong>Output Dropout</strong></td><td>–ù–∞ –≤—ã—Ö–æ–¥–µ</td><td>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤—ã—Ö–æ–¥–æ–≤</td></tr>
      <tr><td><strong>Recurrent Dropout</strong></td><td>–ù–∞ —Å–∫—Ä—ã—Ç–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏</td><td>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏</td></tr>
      <tr><td><strong>Variational Dropout</strong></td><td>–û–¥–Ω–∞ –º–∞—Å–∫–∞ –Ω–∞ –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å</td><td>–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</td></tr>
      <tr><td><strong>Zoneout</strong></td><td>–°–ª—É—á–∞–π–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è</td><td>–°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Dropout (–ù–ï–ü–†–ê–í–ò–õ–¨–ù–û)</h2>
    <p>‚ùå <strong>–ù–ï –î–ï–õ–ê–ô–¢–ï –¢–ê–ö:</strong></p>
    <pre><code># –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û - dropout –º–µ–∂–¥—É –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —à–∞–≥–∞–º–∏
for t in range(seq_length):
    h_t = rnn_cell(x_t, h_{t-1})
    h_t = dropout(h_t)  # ‚ùå –†–∞–∑—Ä—É—à–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é!
    
# –ü—Ä–æ–±–ª–µ–º—ã:
# - –†–∞–∑–Ω–∞—è –º–∞—Å–∫–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ
# - –ù–∞—Ä—É—à–∞–µ—Ç—Å—è –ø–µ—Ä–µ–¥–∞—á–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
# - –¢–µ—Ä—è–µ—Ç—Å—è –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Variational Dropout (–ü–†–ê–í–ò–õ–¨–ù–û)</h2>
    <p>‚úÖ <strong>–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥</strong> ‚Äî –æ–¥–Ω–∞ –º–∞—Å–∫–∞ –Ω–∞ –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å:</p>
    <pre><code># –ü–†–ê–í–ò–õ–¨–ù–û - –æ–¥–Ω–∞ –º–∞—Å–∫–∞ dropout –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
dropout_mask = create_mask(dropout_prob)

for t in range(seq_length):
    h_t = rnn_cell(x_t, h_{t-1})
    h_t = apply_mask(h_t, dropout_mask)  # ‚úÖ –¢–∞ –∂–µ –º–∞—Å–∫–∞
    
# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
# - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏
# - –°—Ç–∞–±–∏–ª—å–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
# - –õ—É—á—à–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Dropout –≤ PyTorch LSTM</h2>
    <pre><code>import torch.nn as nn

# –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LSTM —Å dropout
lstm = nn.LSTM(
    input_size=100,
    hidden_size=256,
    num_layers=2,
    dropout=0.5,      # recurrent dropout –º–µ–∂–¥—É —Å–ª–æ—è–º–∏
    batch_first=True
)

# –î–ª—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ—è - dropout –≤—Ä—É—á–Ω—É—é
lstm_single = nn.LSTM(
    input_size=100,
    hidden_size=256,
    num_layers=1,
    batch_first=True
)

# Input dropout
input_dropout = nn.Dropout(0.3)

# Output dropout
output_dropout = nn.Dropout(0.3)

# Forward pass
x = input_dropout(x)
output, (h_n, c_n) = lstm_single(x)
output = output_dropout(output)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Dropout –≤ Keras/TensorFlow</h2>
    <pre><code>from tensorflow.keras.layers import LSTM, Dropout

model = Sequential([
    # Input dropout
    Dropout(0.2, input_shape=(seq_length, features)),
    
    # LSTM —Å recurrent dropout
    LSTM(
        units=128,
        dropout=0.2,           # input dropout
        recurrent_dropout=0.2, # recurrent dropout
        return_sequences=True
    ),
    
    # Output dropout
    Dropout(0.3),
    
    LSTM(
        units=64,
        dropout=0.2,
        recurrent_dropout=0.2
    ),
    
    Dense(num_classes, activation='softmax')
])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Recurrent Dropout</h2>
    <p><strong>–ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ —Å–∫—Ä—ã—Ç–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é</strong> –º–µ–∂–¥—É –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —à–∞–≥–∞–º–∏:</p>
    <pre><code># –ü—Å–µ–≤–¥–æ–∫–æ–¥
for t in range(seq_length):
    # Dropout –Ω–∞ –≤—Ö–æ–¥–µ –≤ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—É—é —Å–≤—è–∑—å
    h_masked = dropout_mask * h_{t-1}
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    h_t = tanh(W_x @ x_t + W_h @ h_masked + b)
    
    # h_t –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –¥–∞–ª—å—à–µ –ë–ï–ó dropout</code></pre>
    
    <p><strong>–í–∞–∂–Ω–æ:</strong></p>
    <ul>
      <li>–ú–∞—Å–∫–∞ –æ–¥–∏–Ω–∞–∫–æ–≤–∞ –¥–ª—è –≤—Å–µ—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤</li>
      <li>Dropout —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π —Å–≤—è–∑–∏</li>
      <li>–ù–µ –≤–ª–∏—è–µ—Ç –Ω–∞ –≤—ã—Ö–æ–¥ h_t</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 8. Zoneout</h2>
    <p><strong>Zoneout</strong> ‚Äî —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è:</p>
    <pre><code># –û–±—ã—á–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ LSTM
c_t = f_t * c_{t-1} + i_t * cÃÉ_t
h_t = o_t * tanh(c_t)

# –° Zoneout
c_t = d_c * c_{t-1} + (1 - d_c) * c_t_new
h_t = d_h * h_{t-1} + (1 - d_h) * h_t_new

# –≥–¥–µ d_c, d_h - –±–∏–Ω–∞—Ä–Ω—ã–µ –º–∞—Å–∫–∏ Bernoulli</code></pre>
    
    <p><strong>–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤ PyTorch:</strong></p>
    <pre><code># pip install zoneout-pytorch
from zoneout import ZoneoutLSTM

lstm = ZoneoutLSTM(
    input_size=100,
    hidden_size=256,
    zoneout_h=0.1,  # zoneout –¥–ª—è h
    zoneout_c=0.05  # zoneout –¥–ª—è c
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Layer Normalization</h2>
    <p><strong>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–µ–≤</strong> ‚Äî —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è RNN:</p>
    <pre><code>import torch.nn as nn

class LSTMWithLayerNorm(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size)
        self.ln = nn.LayerNorm(hidden_size)
    
    def forward(self, x):
        output, (h, c) = self.lstm(x)
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã—Ö–æ–¥–∞
        output = self.ln(output)
        return output, (h, c)

# Keras
from tensorflow.keras.layers import LayerNormalization

model = Sequential([
    LSTM(128, return_sequences=True),
    LayerNormalization(),
    LSTM(64),
    Dense(num_classes)
])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Weight Decay (L2)</h2>
    <p><strong>L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</strong> –≤–µ—Å–æ–≤ RNN:</p>
    <pre><code># PyTorch - —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=0.001,
    weight_decay=0.01  # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
)

# –ò–ª–∏ —è–≤–Ω–æ –≤ loss
l2_lambda = 0.01
l2_reg = sum(p.pow(2.0).sum() for p in model.parameters())
loss = criterion(output, target) + l2_lambda * l2_reg

# Keras - —á–µ—Ä–µ–∑ kernel_regularizer
from tensorflow.keras.regularizers import l2

lstm = LSTM(
    128,
    kernel_regularizer=l2(0.01),
    recurrent_regularizer=l2(0.01),
    bias_regularizer=l2(0.01)
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Gradient Clipping</h2>
    <p><strong>–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ</strong> –¥–ª—è RNN ‚Äî –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –≤–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤:</p>
    <pre><code># PyTorch - clip by norm
torch.nn.utils.clip_grad_norm_(
    model.parameters(),
    max_norm=5.0
)

# PyTorch - clip by value
torch.nn.utils.clip_grad_value_(
    model.parameters(),
    clip_value=1.0
)

# –í —Ü–∏–∫–ª–µ –æ–±—É—á–µ–Ω–∏—è
for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        loss = model(batch)
        loss.backward()
        
        # Clip –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        torch.nn.utils.clip_grad_norm_(
            model.parameters(), 
            max_norm=1.0
        )
        
        optimizer.step()

# Keras
from tensorflow.keras.optimizers import Adam

optimizer = Adam(
    learning_rate=0.001,
    clipnorm=1.0,     # clip by norm
    clipvalue=0.5     # clip by value
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Weight Tying</h2>
    <p><strong>–°–≤—è–∑—ã–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤</strong> –≤—Ö–æ–¥–Ω–æ–≥–æ embedding –∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è (–¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π):</p>
    <pre><code>class LanguageModel(nn.Module):
    def __init__(self, vocab_size, emb_dim, hidden_dim):
        super().__init__()
        
        # Embedding —Å–ª–æ–π
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        
        # LSTM
        self.lstm = nn.LSTM(emb_dim, hidden_dim)
        
        # Output —Å–ª–æ–π
        self.fc = nn.Linear(hidden_dim, vocab_size)
        
        # Weight tying - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å–∞ embedding
        self.fc.weight = self.embedding.weight
        
    def forward(self, x):
        emb = self.embedding(x)
        output, _ = self.lstm(emb)
        logits = self.fc(output)
        return logits</code></pre>
    
    <p><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:</strong></p>
    <ul>
      <li>–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ ~30-50%</li>
      <li>–õ—É—á—à–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è</li>
      <li>–ú–µ–Ω—å—à–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 13. Embedding Dropout</h2>
    <p><strong>Dropout –Ω–∞ embedding —Å–ª–æ–µ</strong> –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:</p>
    <pre><code>class EmbeddingDropout(nn.Module):
    def __init__(self, embedding, dropout=0.1):
        super().__init__()
        self.embedding = embedding
        self.dropout = dropout
        
    def forward(self, x):
        if not self.training:
            return self.embedding(x)
        
        # Dropout –ø–æ —Ü–µ–ª—ã–º —Å–ª–æ–≤–∞–º
        mask = x.new_ones(x.size(0), 1).bernoulli_(1 - self.dropout)
        mask = mask.expand_as(x) / (1 - self.dropout)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É
        embedded = self.embedding(x)
        return embedded * mask.unsqueeze(-1)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
emb_dropout = EmbeddingDropout(
    nn.Embedding(vocab_size, emb_dim),
    dropout=0.2
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. Teacher Forcing —Å —à—É–º–æ–º</h2>
    <p><strong>Scheduled Sampling</strong> ‚Äî –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –≤ –æ–±—É—á–µ–Ω–∏–µ:</p>
    <pre><code>import random

def train_with_scheduled_sampling(
    model, input_seq, target_seq, epoch, max_epochs
):
    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è teacher forcing
    teacher_forcing_ratio = 1 - (epoch / max_epochs)
    
    output_seq = []
    hidden = model.init_hidden()
    
    # –ü–µ—Ä–≤—ã–π –≤—Ö–æ–¥ - –Ω–∞—á–∞–ª–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    input = input_seq[0]
    
    for t in range(1, len(target_seq)):
        output, hidden = model(input, hidden)
        output_seq.append(output)
        
        # –° –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª—å–∑—É–µ–º ground truth
        use_teacher_forcing = random.random() < teacher_forcing_ratio
        
        if use_teacher_forcing:
            input = target_seq[t]  # ground truth
        else:
            input = output.argmax(-1)  # –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    
    return torch.stack(output_seq)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. Temporal Dropout</h2>
    <p><strong>–ü—Ä–æ–ø—É—Å–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤</strong>:</p>
    <pre><code>class TemporalDropout(nn.Module):
    def __init__(self, dropout_prob=0.1):
        super().__init__()
        self.dropout = dropout_prob
        
    def forward(self, x):
        # x shape: (batch, seq_len, features)
        if not self.training:
            return x
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤
        mask = x.new_empty(x.size(0), x.size(1), 1).bernoulli_(1 - self.dropout)
        mask = mask / (1 - self.dropout)
        
        return x * mask

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
model = nn.Sequential(
    TemporalDropout(0.1),
    nn.LSTM(input_size, hidden_size),
    TemporalDropout(0.1)
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è</h2>
    <table>
      <tr><th>–¢–∏–ø Dropout</th><th>–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ</th><th>–ö–æ–≥–¥–∞ —É–≤–µ–ª–∏—á–∏—Ç—å</th></tr>
      <tr><td><strong>Input Dropout</strong></td><td>0.2 - 0.3</td><td>–ë–æ–ª—å—à–æ–π —Å–ª–æ–≤–∞—Ä—å</td></tr>
      <tr><td><strong>Output Dropout</strong></td><td>0.3 - 0.5</td><td>–ú–Ω–æ–≥–æ –∫–ª–∞—Å—Å–æ–≤</td></tr>
      <tr><td><strong>Recurrent Dropout</strong></td><td>0.1 - 0.3</td><td>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</td></tr>
      <tr><td><strong>Embedding Dropout</strong></td><td>0.1 - 0.2</td><td>–ú–∞–ª—ã–π –¥–∞—Ç–∞—Å–µ—Ç</td></tr>
      <tr><td><strong>Zoneout</strong></td><td>0.05 - 0.15</td><td>–ì–ª—É–±–æ–∫–∏–µ RNN</td></tr>
    </table>
    
    <p><strong>‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ:</strong> –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤—Å–µ –º–µ—Ç–æ–¥—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ! –ù–∞—á–Ω–∏—Ç–µ —Å –º–∞–ª—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.</p>
  </div>

  <div class="block">
    <h2>üî∑ 17. –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä</h2>
    <pre><code>import torch.nn as nn

class RegularizedLSTM(nn.Module):
    def __init__(self, vocab_size, emb_dim, hidden_dim, num_classes):
        super().__init__()
        
        # Embedding —Å dropout
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.emb_dropout = nn.Dropout(0.2)
        
        # LSTM —Å dropout
        self.lstm = nn.LSTM(
            input_size=emb_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            dropout=0.3,        # –º–µ–∂–¥—É —Å–ª–æ—è–º–∏
            batch_first=True
        )
        
        # Layer normalization
        self.ln = nn.LayerNorm(hidden_dim)
        
        # Output dropout
        self.output_dropout = nn.Dropout(0.5)
        
        # Classifier
        self.fc = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x):
        # Embedding + dropout
        emb = self.embedding(x)
        emb = self.emb_dropout(emb)
        
        # LSTM
        output, (h_n, c_n) = self.lstm(emb)
        
        # Layer norm
        output = self.ln(output)
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥
        last_output = output[:, -1, :]
        
        # Output dropout + classifier
        last_output = self.output_dropout(last_output)
        logits = self.fc(last_output)
        
        return logits

# –û–±—É—á–µ–Ω–∏–µ —Å gradient clipping
model = RegularizedLSTM(10000, 300, 512, 10)
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=0.001,
    weight_decay=0.01
)

for batch in dataloader:
    optimizer.zero_grad()
    loss = criterion(model(batch.x), batch.y)
    loss.backward()
    
    # Gradient clipping
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    
    optimizer.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ß–µ–∫-–ª–∏—Å—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</h2>
    <ul>
      <li>[ ] Input dropout: 0.2-0.3</li>
      <li>[ ] Output dropout: 0.3-0.5</li>
      <li>[ ] Recurrent dropout: 0.1-0.3 (—Ç–æ–ª—å–∫–æ –º–µ–∂–¥—É —Å–ª–æ—è–º–∏!)</li>
      <li>[ ] Gradient clipping: max_norm=1.0</li>
      <li>[ ] Weight decay: 0.01</li>
      <li>[ ] Layer normalization</li>
      <li>[ ] Early stopping —Å patience=5-10</li>
      <li>[ ] Learning rate: 0.001 —Å scheduler</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ validation loss</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è RNN ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä —Ç–µ—Ö–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—Ç "–∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ" –º–æ–¥–µ–ª—å—é —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –∫–∞–∫ —É—á–∏—Ç—å —Å—Ç—É–¥–µ–Ω—Ç–∞ –ø–æ–Ω–∏–º–∞—Ç—å –ø—Ä–∏–Ω—Ü–∏–ø—ã, –∞ –Ω–µ –∑—É–±—Ä–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã ‚Äî —Ç–∞–∫ –º–æ–¥–µ–ª—å –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö¬ª.
    </blockquote>
  </div>

</div>

</body>
</html>

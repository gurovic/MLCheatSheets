<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Mixed Precision Training Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>‚ö° Mixed Precision Training</h1>
  <div class="subtitle">üìÖ 4 —è–Ω–≤–∞—Ä—è 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å Mixed Precision</h2>
    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å FP16 (half precision) –≤–º–µ—Å—Ç–æ FP32 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è</li>
      <li><strong>Mixed</strong>: —á–∞—Å—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–π –≤ FP16, —á–∞—Å—Ç—å –≤ FP32</li>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</strong>: 2x —Å–∫–æ—Ä–æ—Å—Ç—å, 2x –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏</li>
      <li><strong>Tensor Cores</strong>: GPU-—É—Å–∫–æ—Ä–∏—Ç–µ–ª–∏ –¥–ª—è FP16</li>
      <li><strong>Automatic Mixed Precision (AMP)</strong>: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä precision</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 2. –§–æ—Ä–º–∞—Ç—ã —á–∏—Å–µ–ª</h2>
    <table>
      <tr><th>–¢–∏–ø</th><th>–ë–∏—Ç—ã</th><th>–î–∏–∞–ø–∞–∑–æ–Ω</th><th>–¢–æ—á–Ω–æ—Å—Ç—å</th></tr>
      <tr><td><strong>FP32</strong></td><td>32</td><td>~10‚Åª‚Å¥‚Åµ –¥–æ 10¬≥‚Å∏</td><td>7 –∑–Ω–∞—á–∞—â–∏—Ö —Ü–∏—Ñ—Ä</td></tr>
      <tr><td><strong>FP16</strong></td><td>16</td><td>~10‚Åª‚Å∏ –¥–æ 65504</td><td>3-4 –∑–Ω–∞—á–∞—â–∏—Ö —Ü–∏—Ñ—Ä—ã</td></tr>
      <tr><td><strong>BF16</strong></td><td>16</td><td>~10‚Åª‚Å¥‚Åµ –¥–æ 10¬≥‚Å∏</td><td>2-3 –∑–Ω–∞—á–∞—â–∏—Ö —Ü–∏—Ñ—Ä—ã</td></tr>
    </table>
    <p><strong>FP16 —Å—Ç—Ä—É–∫—Ç—É—Ä–∞</strong>: 1 –±–∏—Ç –∑–Ω–∞–∫ + 5 –±–∏—Ç —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç–∞ + 10 –±–∏—Ç –º–∞–Ω—Ç–∏—Å—Å–∞</p>
    <p><strong>BF16 —Å—Ç—Ä—É–∫—Ç—É—Ä–∞</strong>: 1 –±–∏—Ç –∑–Ω–∞–∫ + 8 –±–∏—Ç —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç–∞ + 7 –±–∏—Ç –º–∞–Ω—Ç–∏—Å—Å–∞</p>
  </div>

  <div class="block">
    <h2>üî∑ 3. PyTorch AMP - –±–∞–∑–æ–≤—ã–π –∫–æ–¥</h2>
    <pre><code>import torch
from torch.cuda.amp import autocast, GradScaler

model = MyModel().cuda()
optimizer = torch.optim.Adam(model.parameters())
scaler = GradScaler()

for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        
        # Forward –≤ FP16
        with autocast():
            output = model(batch)
            loss = criterion(output, target)
        
        # Backward —Å gradient scaling
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. TensorFlow AMP</h2>
    <pre><code>import tensorflow as tf

# –í–∫–ª—é—á–∏—Ç—å mixed precision
policy = tf.keras.mixed_precision.Policy('mixed_float16')
tf.keras.mixed_precision.set_global_policy(policy)

# –ú–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç FP16
model = create_model()

# Loss scaling –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
optimizer = tf.keras.optimizers.Adam()
optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)

model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.fit(train_data, epochs=10)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Gradient Scaling</h2>
    <p><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ FP16 –º–æ–≥—É—Ç underflow (—Å—Ç–∞—Ç—å –Ω—É–ª—è–º–∏)</p>
    <p><strong>–†–µ—à–µ–Ω–∏–µ</strong>: –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å loss –ø–µ—Ä–µ–¥ backward</p>
    <ol>
      <li>Loss —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ scale_factor (–æ–±—ã—á–Ω–æ 2¬π‚Å∂)</li>
      <li>Backward –≤—ã—á–∏—Å–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã</li>
      <li>–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–µ–ª—è—Ç—Å—è –Ω–∞ scale_factor –ø–µ—Ä–µ–¥ optimizer.step()</li>
      <li>–ï—Å–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã inf/nan ‚Üí —É–º–µ–Ω—å—à–∏—Ç—å scale_factor</li>
    </ol>
    <pre><code># –†—É—á–Ω–æ–π gradient scaling
loss_scaled = loss * scale_factor
loss_scaled.backward()
for param in model.parameters():
    param.grad /= scale_factor
optimizer.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –û–ø–µ—Ä–∞—Ü–∏–∏ –≤ FP32 vs FP16</h2>
    <p><strong>–í—Å–µ–≥–¥–∞ FP32:</strong></p>
    <ul>
      <li>–í–µ—Å–∞ –º–æ–¥–µ–ª–∏ (master copy)</li>
      <li>Batch normalization</li>
      <li>Softmax</li>
      <li>Layer normalization</li>
      <li>Loss computation (–∏–Ω–æ–≥–¥–∞)</li>
    </ul>
    <p><strong>–ú–æ–∂–Ω–æ FP16:</strong></p>
    <ul>
      <li>Matrix multiplications</li>
      <li>Convolutions</li>
      <li>Activations (ReLU, GELU)</li>
      <li>Attention computations</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 7. FP16 vs BF16</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>FP16</th><th>BF16</th></tr>
      <tr><td>–î–∏–∞–ø–∞–∑–æ–Ω</td><td>–£–∑–∫–∏–π (65504 max)</td><td>–®–∏—Ä–æ–∫–∏–π (–∫–∞–∫ FP32)</td></tr>
      <tr><td>–¢–æ—á–Ω–æ—Å—Ç—å</td><td>–í—ã—à–µ</td><td>–ù–∏–∂–µ</td></tr>
      <tr><td>Overflow —Ä–∏—Å–∫</td><td>–í—ã—Å–æ–∫–∏–π</td><td>–ù–∏–∑–∫–∏–π</td></tr>
      <tr><td>Gradient scaling</td><td>–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ</td><td>–ß–∞—Å—Ç–æ –Ω–µ –Ω—É–∂–Ω–æ</td></tr>
      <tr><td>Hardware –ø–æ–¥–¥–µ—Ä–∂–∫–∞</td><td>–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ GPU</td><td>TPU, A100, H100</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <ul>
      <li><strong>Batch size</strong>: –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –≤ 2x –±–ª–∞–≥–æ–¥–∞—Ä—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏</li>
      <li><strong>Learning rate</strong>: –æ–±—ã—á–Ω–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π</li>
      <li><strong>Gradient accumulation</strong>: —Ö–æ—Ä–æ—à–æ —Å–æ—á–µ—Ç–∞–µ—Ç—Å—è —Å AMP</li>
      <li><strong>Dynamic loss scaling</strong>: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é</li>
      <li><strong>–ü—Ä–æ–≤–µ—Ä–∫–∞</strong>: —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ö–æ–¥—è—Ç—Å—è —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ FP32</li>
      <li><strong>BF16 –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ</strong>: –µ—Å–ª–∏ GPU –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 9. Hugging Face Transformers</h2>
    <pre><code>from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    fp16=True,  # –í–∫–ª—é—á–∏—Ç—å FP16
    fp16_opt_level='O2',  # –£—Ä–æ–≤–µ–Ω—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    # –∏–ª–∏
    bf16=True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BF16 –≤–º–µ—Å—Ç–æ FP16
    per_device_train_batch_size=32,
    gradient_accumulation_steps=4,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –û—Ç–ª–∞–¥–∫–∞ –ø—Ä–æ–±–ª–µ–º</h2>
    <table>
      <tr><th>–ü—Ä–æ–±–ª–µ–º–∞</th><th>–†–µ—à–µ–Ω–∏–µ</th></tr>
      <tr><td>Loss —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è NaN</td><td>–£–º–µ–Ω—å—à–∏—Ç—å LR, –ø—Ä–æ–≤–µ—Ä–∏—Ç—å gradient scaling</td></tr>
      <tr><td>Overflow –≤ forward</td><td>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BF16 –∏–ª–∏ gradient clipping</td></tr>
      <tr><td>Underflow –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</td><td>–£–≤–µ–ª–∏—á–∏—Ç—å gradient scale factor</td></tr>
      <tr><td>–ù–µ —Å—Ö–æ–¥–∏—Ç—Å—è</td><td>–ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å FP32 –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–ª–æ–µ–≤</td></tr>
      <tr><td>–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</td><td>Layer normalization –≤–º–µ—Å—Ç–æ Batch norm</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ</h2>
    <pre><code># –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è FP16
import torch.cuda.amp as amp

with torch.cuda.amp.autocast():
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å dtype —Ç–µ–Ω–∑–æ—Ä–∞
    x = torch.randn(10, 10).cuda()
    y = model(x)
    print(y.dtype)  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å torch.float16

# –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
from torch.profiler import profile, ProfilerActivity

with profile(activities=[ProfilerActivity.CUDA]) as prof:
    with autocast():
        output = model(input)
        loss = criterion(output, target)
    scaler.scale(loss).backward()

print(prof.key_averages().table())</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –£—Å–∫–æ—Ä–µ–Ω–∏–µ –∏ —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏</h2>
    <p><strong>–¢–∏–ø–∏—á–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ:</strong></p>
    <ul>
      <li>V100: 1.5-2x</li>
      <li>A100: 2-3x (—Å Tensor Cores)</li>
      <li>H100: 3-4x</li>
    </ul>
    <p><strong>–≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏:</strong></p>
    <ul>
      <li>–ê–∫—Ç–∏–≤–∞—Ü–∏–∏: 2x –º–µ–Ω—å—à–µ</li>
      <li>–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã: 2x –º–µ–Ω—å—à–µ</li>
      <li>–í–µ—Å–∞: —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ FP32 (no savings)</li>
      <li>–û–±—â–∞—è —ç–∫–æ–Ω–æ–º–∏—è: ~30-40%</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –•–æ—Ä–æ—à–æ –¥–ª—è</h3>
        <ul>
          <li>–û–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π</li>
          <li>GPU —Å Tensor Cores (V100, A100, H100)</li>
          <li>Batch size –æ–≥—Ä–∞–Ω–∏—á–µ–Ω –ø–∞–º—è—Ç—å—é</li>
          <li>Computer Vision –∑–∞–¥–∞—á–∏</li>
          <li>Language Models</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –û—Å—Ç–æ—Ä–æ–∂–Ω–æ</h3>
        <ul>
          <li>–û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤–µ—Å–æ–≤/–≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</li>
          <li>–°—Ç–∞—Ä—ã–µ GPU –±–µ–∑ Tensor Cores</li>
          <li>–ó–∞–¥–∞—á–∏ —Ç—Ä–µ–±—É—é—â–∏–µ –≤—ã—Å–æ–∫–æ–π —á–∏—Å–ª–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏</li>
          <li>Reinforcement learning (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ)</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 14. Apex –æ—Ç NVIDIA</h2>
    <pre><code># –£—Å—Ç–∞–Ω–æ–≤–∫–∞
pip install apex

from apex import amp

model, optimizer = amp.initialize(
    model, 
    optimizer, 
    opt_level='O2'  # O0, O1, O2, O3
)

# –û–±—É—á–µ–Ω–∏–µ
with amp.scale_loss(loss, optimizer) as scaled_loss:
    scaled_loss.backward()

# Opt levels:
# O0: FP32 (baseline)
# O1: Conservative mixed precision
# O2: Fast mixed precision  
# O3: Pure FP16</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫</h2>
    <table>
      <tr><th>–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞</th><th>–ü–ª—é—Å—ã</th><th>–ú–∏–Ω—É—Å—ã</th></tr>
      <tr><td><strong>PyTorch AMP</strong></td><td>–í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è, –ø—Ä–æ—Å—Ç–∞—è</td><td>–ú–µ–Ω—å—à–µ –∫–æ–Ω—Ç—Ä–æ–ª—è</td></tr>
      <tr><td><strong>Apex</strong></td><td>–ì–∏–±–∫–∞—è, opt levels</td><td>–°–ª–æ–∂–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞</td></tr>
      <tr><td><strong>TF mixed_precision</strong></td><td>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è</td><td>–¢–æ–ª—å–∫–æ TensorFlow</td></tr>
      <tr><td><strong>DeepSpeed</strong></td><td>–î–ª—è –æ–≥—Ä–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π</td><td>–°–ª–æ–∂–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É GPU (Tensor Cores)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å FP16 –∏–ª–∏ BF16</li>
      <li>[ ] –î–æ–±–∞–≤–∏—Ç—å autocast –¥–ª—è forward pass</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å GradScaler –¥–ª—è backward</li>
      <li>[ ] –£–≤–µ–ª–∏—á–∏—Ç—å batch size –ø—Ä–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å loss –Ω–∞ NaN/Inf</li>
      <li>[ ] –°—Ä–∞–≤–Ω–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å —Å FP32</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´Mixed Precision ‚Äî —ç—Ç–æ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç—è—Ö —Ä–∞–±–æ—Ç—ã. –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–µ–ª–∞—é—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ —Å –º–µ–Ω—å—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é, –Ω–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞—é—Ç—Å—è —Ç–æ—á–Ω—ã–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç: –æ–±—É—á–µ–Ω–∏–µ –≤ 2 —Ä–∞–∑–∞ –±—ã—Å—Ç—Ä–µ–µ —Å —Ç–æ–π –∂–µ —Ç–æ—á–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏¬ª.
    </blockquote>
  </div>

</div>

</body>
</html>

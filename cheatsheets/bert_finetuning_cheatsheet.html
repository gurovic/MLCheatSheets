<!DOCTYPE html><html lang="ru"><head><meta charset="UTF-8"><title>BERT Fine-tuning Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title><style>@media screen{body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Helvetica,Arial,sans-serif;color:#333;background:#fafcff;padding:10px}}@media print{body{background:white;padding:0}@page{size:A4 landscape;margin:10mm}}.container{column-count:3;column-gap:20px;max-width:100%}.block{break-inside:avoid;margin-bottom:1.2em;padding:12px;background:white;border-radius:6px;box-shadow:0 1px 3px rgba(0,0,0,0.05)}h1{font-size:1.6em;font-weight:700;color:#1a5fb4;text-align:center;margin:0 0 8px;column-span:all}.subtitle{text-align:center;color:#666;font-size:0.9em;margin-bottom:12px;column-span:all}h2{font-size:1.15em;font-weight:700;color:#1a5fb4;margin:0 0 8px;padding-bottom:4px;border-bottom:1px solid #e0e7ff}p,ul,ol{font-size:0.92em;margin:0.6em 0}ul,ol{padding-left:18px}li{margin-bottom:4px}code{font-family:'Consolas','Courier New',monospace;background-color:#f0f4ff;padding:1px 4px;border-radius:3px;font-size:0.88em}pre{background-color:#f0f4ff;padding:8px;border-radius:4px;overflow-x:auto;font-size:0.84em;margin:6px 0}pre code{padding:0;background:none;white-space:pre-wrap}table{width:100%;border-collapse:collapse;font-size:0.82em;margin:6px 0}th{background-color:#e6f0ff;text-align:left;padding:4px 6px;font-weight:600}td{padding:4px 6px;border-bottom:1px solid #f0f4ff}tr:nth-child(even){background-color:#f8fbff}.good-vs-bad{display:flex;flex-direction:column;gap:8px}.good-vs-bad div{flex:1;padding:6px 8px;border-radius:4px}.good{background-color:#f0f9f4;border-left:3px solid #2e8b57}.bad{background-color:#fdf0f2;border-left:3px solid #d32f2f}.good h3,.bad h3{margin:0 0 4px;font-size:1em;font-weight:700}.good ul,.bad ul{padding-left:20px;margin:0}.good li::before{content:"‚úÖ ";font-weight:bold}.bad li::before{content:"‚ùå ";font-weight:bold}blockquote{font-style:italic;margin:8px 0;padding:6px 10px;background:#f8fbff;border-left:2px solid #1a5fb4;font-size:0.88em}a{color:#1a5fb4;text-decoration:none}a:hover{text-decoration:underline}@media print{.container{column-gap:12px}.block{box-shadow:none}code,pre,table{font-size:0.78em}h1{font-size:1.4em}h2{font-size:1em}}</style></head><body><div class="container"><h1>ü§ñ BERT Fine-tuning Cheatsheet</h1><div class="subtitle">–î–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö ‚Ä¢ Transfer Learning –¥–ª—è NLP ‚Ä¢ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ñ–æ–∫—É—Å<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div><div class="block"><h2>üî∑ 1. –°—É—Ç—å</h2><ul><li><strong>BERT</strong>: Bidirectional Encoder Representations from Transformers</li><li><strong>Fine-tuning</strong>: –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö</li><li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, NER, Q&A</li><li><strong>–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞</strong>: Hugging Face Transformers</li></ul></div><div class="block"><h2>üî∑ 2. –£—Å—Ç–∞–Ω–æ–≤–∫–∞</h2><pre><code># pip
pip install transformers torch

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ
pip install datasets accelerate</code></pre></div><div class="block"><h2>üî∑ 3. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥</h2><pre><code>from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    Trainer,
    TrainingArguments
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2
)

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
def tokenize(texts):
    return tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )

inputs = tokenize(train_texts)</code></pre></div><div class="block"><h2>üî∑ 4. Fine-tuning —Å Trainer</h2><pre><code># –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    learning_rate=2e-5,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

# –û–±—É—á–µ–Ω–∏–µ
trainer.train()

# –û—Ü–µ–Ω–∫–∞
trainer.evaluate()</code></pre></div><div class="block"><h2>üî∑ 5. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ</h2><pre><code># –ù–æ–≤—ã–π —Ç–µ–∫—Å—Ç
text = "This is a test sentence"

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
inputs = tokenizer(text, return_tensors="pt")

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
outputs = model(**inputs)
predictions = outputs.logits.argmax(-1)

print(f"Predicted class: {predictions.item()}")</code></pre></div><div class="block"><h2>üî∑ 6. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è</h2><table><tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–ó–Ω–∞—á–µ–Ω–∏–µ</th></tr><tr><td><code>learning_rate</code></td><td>2e-5, 3e-5, 5e-5</td></tr><tr><td><code>num_train_epochs</code></td><td>2-4</td></tr><tr><td><code>batch_size</code></td><td>8-32</td></tr><tr><td><code>max_length</code></td><td>128-512</td></tr><tr><td><code>warmup_steps</code></td><td>500-1000</td></tr></table></div><div class="block"><h2>üî∑ 7. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2><div class="good-vs-bad"><div class="good"><h3>‚úÖ –•–æ—Ä–æ—à–æ</h3><ul><li>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞</li><li>Sentiment analysis</li><li>Named Entity Recognition</li><li>Question Answering</li><li>–ï—Å—Ç—å GPU</li></ul></div><div class="bad"><h3>‚ùå –ü–ª–æ—Ö–æ</h3><ul><li>–ú–∞–ª–æ–¥–∞–Ω–Ω—ã—Ö (<1000)</li><li>–ù–µ—Ç GPU (–æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ)</li><li>Real-time –Ω–∞ CPU</li><li>–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã (>512)</li></ul></div></div></div><div class="block"><h2>üî∑ 8. –ß–µ–∫-–ª–∏—Å—Ç</h2><ul><li>[ ] –í—ã–±—Ä–∞—Ç—å –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å (bert-base-uncased)</li><li>[ ] –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ</li><li>[ ] –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã</li><li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å TrainingArguments</li><li>[ ] Fine-tune –º–æ–¥–µ–ª—å</li><li>[ ] –û—Ü–µ–Ω–∏—Ç—å –Ω–∞ test</li><li>[ ] –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å</li></ul><blockquote>¬´BERT —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª NLP. Fine-tuning –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ—â–Ω—É—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –ø–æ–¥ –≤–∞—à—É –∑–∞–¥–∞—á—É –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤ –æ–±—É—á–µ–Ω–∏—è¬ª.  </blockquote></div><div class="block"><h2>üîó –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏</h2><ul><li><a href="https://huggingface.co/docs/transformers/" target="_blank">üìö Transformers –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è</a></li><li><a href="https://huggingface.co/bert-base-uncased" target="_blank">ü§ó BERT –Ω–∞ Hugging Face</a></li><li><a href="https://arxiv.org/abs/1810.04805" target="_blank">üìÑ BERT –Ω–∞—É—á–Ω–∞—è —Å—Ç–∞—Ç—å—è</a></li><li><a href="https://www.youtube.com/watch?v=xI0HHN5XKDo" target="_blank">üé• BERT explained</a></li></ul></div></div></body></html>

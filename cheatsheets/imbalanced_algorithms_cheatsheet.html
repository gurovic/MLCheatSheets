<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–ê–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üõ†Ô∏è –ê–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º</h1>
  <div class="subtitle">–ú–µ—Ç–æ–¥—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ ‚Ä¢ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ñ–æ–∫—É—Å<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û–±–∑–æ—Ä –ø–æ–¥—Ö–æ–¥–æ–≤</h2>
    <p>–¢—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–±–æ—Ç—ã —Å imbalanced data:</p>
    <ul>
      <li><strong>Data-level</strong>: —Ä–µ—Å–µ–º–ø–ª–∏–Ω–≥ (SMOTE, undersampling)</li>
      <li><strong>Algorithm-level</strong>: –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤</li>
      <li><strong>Hybrid</strong>: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –æ–±–æ–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤</li>
    </ul>
    <p>–≠—Ç–æ—Ç cheatsheet ‚Äî –ø—Ä–æ algorithm-level –ø–æ–¥—Ö–æ–¥—ã</p>
  </div>

  <div class="block">
    <h2>üî∑ 2. Class Weights (–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤)</h2>
    <p><strong>–ò–¥–µ—è</strong>: –Ω–∞–∑–Ω–∞—á–∏—Ç—å –±–æ–ª—å—à–∏–π —à—Ç—Ä–∞—Ñ –∑–∞ –æ—à–∏–±–∫–∏ –Ω–∞ –º–∏–Ω–æ—Ä–Ω–æ–º –∫–ª–∞—Å—Å–µ</p>
    <pre><code>from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# Auto-–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –≤–µ—Å–æ–≤
lr = LogisticRegression(class_weight='balanced')
rf = RandomForestClassifier(class_weight='balanced')
svm = SVC(class_weight='balanced')

# –†—É—á–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–µ—Å–æ–≤
# class_weight={0: 1, 1: 10} 
# –æ–∑–Ω–∞—á–∞–µ—Ç –æ—à–∏–±–∫–∞ –Ω–∞ –∫–ª–∞—Å—Å–µ 1 –≤ 10 —Ä–∞–∑ –¥–æ—Ä–æ–∂–µ
lr = LogisticRegression(class_weight={0: 1, 1: 10})
rf.fit(X_train, y_train)

# –î–ª—è XGBoost
import xgboost as xgb
scale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1])
model = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤</h2>
    <pre><code>from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ
classes = np.unique(y_train)
weights = compute_class_weight(
    class_weight='balanced',
    classes=classes,
    y=y_train
)
class_weights = dict(zip(classes, weights))
print(f"Class weights: {class_weights}")

# –§–æ—Ä–º—É–ª–∞: n_samples / (n_classes * n_samples_class)
# –ü—Ä–∏–º–µ—Ä: 1000 samples, 900 –∫–ª–∞—Å—Å 0, 100 –∫–ª–∞—Å—Å 1
# weight_0 = 1000 / (2 * 900) = 0.56
# weight_1 = 1000 / (2 * 100) = 5.0

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
model = LogisticRegression(class_weight=class_weights)
model.fit(X_train, y_train)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Sample Weights</h2>
    <p>–í–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ:</p>
    <pre><code>from sklearn.ensemble import RandomForestClassifier
import numpy as np

# –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ—Å–æ–≤
sample_weights = np.ones(len(y_train))
sample_weights[y_train == 1] = 10  # –ú–∏–Ω–æ—Ä–Ω—ã–π –∫–ª–∞—Å—Å –≤–∞–∂–Ω–µ–µ

# –û–±—É—á–µ–Ω–∏–µ —Å –≤–µ—Å–∞–º–∏
model = RandomForestClassifier()
model.fit(X_train, y_train, sample_weight=sample_weights)

# –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏)
# –ß–µ–º —Å—Ç–∞—Ä—à–µ –¥–∞–Ω–Ω—ã–µ, —Ç–µ–º –º–µ–Ω—å—à–µ –≤–µ—Å
time_decay = np.exp(-0.01 * np.arange(len(y_train)))
combined_weights = sample_weights * time_decay
model.fit(X_train, y_train, sample_weight=combined_weights)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Threshold Moving</h2>
    <p>–°–º–µ—â–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:</p>
    <pre><code>from sklearn.metrics import precision_recall_curve, f1_score
import numpy as np

# –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
y_proba = model.predict_proba(X_val)[:, 1]

# –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
precisions, recalls, thresholds = precision_recall_curve(y_val, y_proba)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)

# –ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è F1
optimal_idx = np.argmax(f1_scores)
optimal_threshold = thresholds[optimal_idx]
print(f"Optimal threshold: {optimal_threshold:.3f}")

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞
y_pred = (y_proba >= optimal_threshold).astype(int)

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –¥—Ä—É–≥–∏—Ö –º–µ—Ç—Ä–∏–∫
# –ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è recall –ø—Ä–∏ precision >= 0.8
valid_idx = precisions >= 0.8
if np.any(valid_idx):
    best_idx = np.argmax(recalls[valid_idx])
    threshold = thresholds[valid_idx][best_idx]</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Ensemble of Balanced Bagging</h2>
    <p>–ê–Ω—Å–∞–º–±–ª—å –Ω–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞—Ö:</p>
    <pre><code>from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# Balanced Random Forest
from imblearn.ensemble import BalancedRandomForestClassifier

brf = BalancedRandomForestClassifier(
    n_estimators=100,
    random_state=42,
    sampling_strategy='auto',  # –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –∫–∞–∂–¥—ã–π bootstrap
    replacement=True
)
brf.fit(X_train, y_train)

# –ò–ª–∏ —Ä—É—á–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ Bagging
from imblearn.under_sampling import RandomUnderSampler

base_estimator = DecisionTreeClassifier()
bagging = BaggingClassifier(
    base_estimator=base_estimator,
    n_estimators=50,
    max_samples=0.8,
    bootstrap=True
)
bagging.fit(X_train, y_train)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Easy Ensemble</h2>
    <p>–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π undersampling + –∞–Ω—Å–∞–º–±–ª—å:</p>
    <pre><code>from imblearn.ensemble import EasyEnsembleClassifier

# –°–æ–∑–¥–∞–µ—Ç N —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥–≤—ã–±–æ—Ä–æ–∫
# –û–±—É—á–∞–µ—Ç –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –∫–∞–∂–¥–æ–π
# –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
eec = EasyEnsembleClassifier(
    n_estimators=10,
    random_state=42,
    sampling_strategy='auto',  # –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å
    replacement=False  # –ë–µ–∑ –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–∏ undersampling
)
eec.fit(X_train, y_train)
y_pred = eec.predict(X_test)

# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
# - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –º–∏–Ω–æ—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
# - Diversity —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω—ã–µ undersampled –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏
# - Reduce overfitting</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Balanced Bagging Classifier</h2>
    <pre><code>from imblearn.ensemble import BalancedBaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bbc = BalancedBaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=50,
    sampling_strategy='auto',  # –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –∫–∞–∂–¥—ã–π bootstrap
    replacement=False,
    random_state=42
)
bbc.fit(X_train, y_train)

# –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç:
# - Bagging –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è variance
# - Random undersampling –¥–ª—è –±–∞–ª–∞–Ω—Å–∞
# - –†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º base estimator</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. RUSBoost</h2>
    <p>Random Undersampling + Boosting:</p>
    <pre><code>from imblearn.ensemble import RUSBoostClassifier

rusboost = RUSBoostClassifier(
    n_estimators=50,
    learning_rate=1.0,
    algorithm='SAMME.R',  # Real AdaBoost
    random_state=42
)
rusboost.fit(X_train, y_train)

# –ù–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ boosting:
# 1. Random undersampling –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
# 2. –û–±—É—á–µ–Ω–∏–µ —Å–ª–∞–±–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
# 3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
# Combine boosting —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. One-Class Classification</h2>
    <p>–ö–æ–≥–¥–∞ –º–∏–Ω–æ—Ä–Ω—ã–π –∫–ª–∞—Å—Å –∫—Ä–∞–π–Ω–µ —Ä–µ–¥–∫–∏–π (< 1%):</p>
    <pre><code>from sklearn.svm import OneClassSVM
from sklearn.ensemble import IsolationForest

# One-Class SVM: –æ–±—É—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Å–µ
# –û–±—ã—á–Ω–æ –Ω–∞ –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–º (normal)
X_normal = X_train[y_train == 0]

ocsvm = OneClassSVM(
    kernel='rbf',
    gamma='auto',
    nu=0.05  # Expected fraction of outliers
)
ocsvm.fit(X_normal)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: 1 = normal, -1 = anomaly
y_pred = ocsvm.predict(X_test)
y_pred = (y_pred == -1).astype(int)  # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ 0/1

# Isolation Forest (–ª—É—á—à–µ –¥–ª—è –≤—ã—Å–æ–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π)
iso_forest = IsolationForest(
    contamination=0.01,  # –û–∂–∏–¥–∞–µ–º–∞—è –¥–æ–ª—è –∞–Ω–æ–º–∞–ª–∏–π
    random_state=42
)
iso_forest.fit(X_normal)
y_pred = iso_forest.predict(X_test)
y_pred = (y_pred == -1).astype(int)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Focal Loss</h2>
    <p>–î–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π ‚Äî —Ñ–æ–∫—É—Å –Ω–∞ hard examples:</p>
    <pre><code>import tensorflow as tf

def focal_loss(gamma=2.0, alpha=0.25):
    """
    Focal Loss –¥–ª—è –±–æ—Ä—å–±—ã —Å class imbalance
    gamma: —Ñ–æ–∫—É—Å –Ω–∞ hard examples (–æ–±—ã—á–Ω–æ 2)
    alpha: –≤–µ—Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
    """
    def loss(y_true, y_pred):
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
        
        # Binary focal loss
        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
        alpha_t = tf.where(tf.equal(y_true, 1), alpha, 1 - alpha)
        
        focal_weight = alpha_t * tf.pow((1 - pt), gamma)
        loss = -focal_weight * tf.log(pt)
        
        return tf.reduce_mean(loss)
    return loss

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ Keras
model.compile(
    optimizer='adam',
    loss=focal_loss(gamma=2.0, alpha=0.25),
    metrics=['accuracy']
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Cost-Sensitive Neural Networks</h2>
    <pre><code>import torch
import torch.nn as nn

# PyTorch: –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –≤ loss
class_weights = torch.tensor([1.0, 10.0])  # –ö–ª–∞—Å—Å 1 –≤–∞–∂–Ω–µ–µ
criterion = nn.CrossEntropyLoss(weight=class_weights)

# Keras/TensorFlow
from tensorflow.keras.losses import BinaryCrossentropy

# –í–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ sample
loss = BinaryCrossentropy()
model.compile(optimizer='adam', loss=loss)

# –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
class_weight = {0: 1.0, 1: 10.0}
model.fit(
    X_train, y_train,
    class_weight=class_weight,
    epochs=50
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. Anomaly Detection –ø–æ–¥—Ö–æ–¥—ã</h2>
    <pre><code>from sklearn.covariance import EllipticEnvelope
from sklearn.neighbors import LocalOutlierFactor

# Elliptic Envelope (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç Gaussian —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
ee = EllipticEnvelope(contamination=0.05)
ee.fit(X_train[y_train == 0])  # –¢–æ–ª—å–∫–æ normal class
y_pred = ee.predict(X_test)
y_pred = (y_pred == -1).astype(int)

# Local Outlier Factor
lof = LocalOutlierFactor(
    n_neighbors=20,
    contamination=0.05,
    novelty=True  # –î–ª—è predict –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
)
lof.fit(X_train[y_train == 0])
y_pred = lof.predict(X_test)
y_pred = (y_pred == -1).astype(int)

# –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
# - –ö—Ä–∞–π–Ω–∏–π –¥–∏—Å–±–∞–ª–∞–Ω—Å (< 1% minority)
# - Anomaly detection –∑–∞–¥–∞—á–∏
# - Fraud detection, intrusion detection</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. Cascade/Rejection Classifier</h2>
    <p>–î–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:</p>
    <pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

# –≠—Ç–∞–ø 1: –≥—Ä—É–±—ã–π —Ñ–∏–ª—å—Ç—Ä (high recall)
filter_model = RandomForestClassifier(class_weight={0: 1, 1: 20})
filter_model.fit(X_train, y_train)

# –ü–æ–ª—É—á–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
y_proba_filter = filter_model.predict_proba(X_test)[:, 1]

# –í—ã–±—Ä–∞—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (–Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è high recall)
candidates_idx = y_proba_filter >= 0.1
X_candidates = X_test[candidates_idx]

# –≠—Ç–∞–ø 2: —Ç–æ—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (high precision)
if len(X_candidates) > 0:
    refiner_model = LogisticRegression(class_weight={0: 1, 1: 5})
    refiner_model.fit(X_train, y_train)
    y_pred_refined = refiner_model.predict(X_candidates)
    
# –ò—Ç–æ–≥–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred = np.zeros(len(X_test))
y_pred[candidates_idx] = y_pred_refined

# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
# - –°–Ω–∏–∂–µ–Ω–∏–µ false positives
# - Efficiency (–Ω–µ –≤—Å–µ –ø—Ä–æ—Ö–æ–¥—è—Ç stage 2)
# - –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ—Ä–æ–≥–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ stage 2</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</th><th>–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</th></tr>
      <tr><td><strong>Class Weights</strong></td><td>–ü—Ä–æ—Å—Ç–æ—Ç–∞, –±—ã—Å—Ç—Ä–æ</td><td>–ù–µ –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ</td></tr>
      <tr><td><strong>Threshold Moving</strong></td><td>–ì–∏–±–∫–æ—Å—Ç—å, –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏</td><td>–¢—Ä–µ–±—É–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏</td></tr>
      <tr><td><strong>Balanced Bagging</strong></td><td>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ</td><td>–í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –¥–æ—Ä–æ–≥–æ</td></tr>
      <tr><td><strong>Easy Ensemble</strong></td><td>Diversity, robustness</td><td>–ú–Ω–æ–≥–æ –º–æ–¥–µ–ª–µ–π</td></tr>
      <tr><td><strong>One-Class</strong></td><td>–î–ª—è –∫—Ä–∞–π–Ω–µ–≥–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞</td><td>–¢–µ—Ä—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é</td></tr>
      <tr><td><strong>Focal Loss</strong></td><td>–î–ª—è NN, hard examples</td><td>–¢–æ–ª—å–∫–æ –¥–ª—è NN</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –î–µ–ª–∞—Ç—å</h3>
        <ul>
          <li>–ù–∞—á–∏–Ω–∞—Ç—å —Å class_weight='balanced'</li>
          <li>–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Å data-level –º–µ—Ç–æ–¥–∞–º–∏</li>
          <li>–ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å threshold –¥–ª—è –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫</li>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–Ω—Å–∞–º–±–ª–∏ –¥–ª—è robustness</li>
          <li>–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫–∞—Ö</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ù–µ –¥–µ–ª–∞—Ç—å</h3>
        <ul>
          <li>–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å</li>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ accuracy</li>
          <li>–ü—Ä–∏–º–µ–Ω—è—Ç—å –æ–¥–∏–Ω –º–µ—Ç–æ–¥ –±–µ–∑ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤</li>
          <li>–ó–∞–±—ã–≤–∞—Ç—å –ø—Ä–æ computational cost</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 17. –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω</h2>
    <pre><code>from sklearn.model_selection import cross_val_score
from sklearn.metrics import f1_score, make_scorer

def train_imbalanced_model(X_train, y_train, X_val, y_val):
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥"""
    
    # 1. Class weights
    from sklearn.utils.class_weight import compute_class_weight
    classes = np.unique(y_train)
    weights = compute_class_weight('balanced', classes=classes, y=y_train)
    class_weights = dict(zip(classes, weights))
    
    # 2. Balanced ensemble
    from imblearn.ensemble import BalancedRandomForestClassifier
    model = BalancedRandomForestClassifier(
        n_estimators=100,
        class_weight=class_weights,
        random_state=42
    )
    
    # 3. Train
    model.fit(X_train, y_train)
    
    # 4. Threshold tuning
    y_proba = model.predict_proba(X_val)[:, 1]
    from sklearn.metrics import precision_recall_curve
    precisions, recalls, thresholds = precision_recall_curve(y_val, y_proba)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    
    # 5. Final predictions
    y_pred = (y_proba >= optimal_threshold).astype(int)
    
    print(f"Optimal threshold: {optimal_threshold:.3f}")
    print(f"F1-Score: {f1_score(y_val, y_pred):.3f}")
    
    return model, optimal_threshold

model, threshold = train_imbalanced_model(X_train, y_train, X_val, y_val)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ò–∑–º–µ—Ä–∏—Ç—å —Å—Ç–µ–ø–µ–Ω—å –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤</li>
      <li>[ ] –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å class_weight='balanced'</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å threshold –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏</li>
      <li>[ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å ensemble –º–µ—Ç–æ–¥—ã</li>
      <li>[ ] –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Å SMOTE –∏–ª–∏ undersampling</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (F1, PR-AUC)</li>
      <li>[ ] Cross-validation —Å–æ stratification</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ hold-out test set</li>
      <li>[ ] –°—Ä–∞–≤–Ω–∏—Ç—å —Å baseline</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å confusion matrix</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–ú—ã –Ω–µ –ø—Ä–æ—Å—Ç–æ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú—ã –¥–µ–ª–∞–µ–º —Ç–∞–∫, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –ø—Ä–∏–¥–∞–≤–∞–ª–∞ –±–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è —Ä–µ–¥–∫–∏–º, –Ω–æ –≤–∞–∂–Ω—ã–º —Å–ª—É—á–∞—è–º ‚Äî –Ω–∞–ø—Ä–∏–º–µ—Ä, –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤—É –∏–ª–∏ –±–æ–ª–µ–∑–Ω—è–º. –≠—Ç–æ –∫–∞–∫ –æ–ø—ã—Ç–Ω—ã–π –≤—Ä–∞—á, –∫–æ—Ç–æ—Ä—ã–π –æ—Å–æ–±–µ–Ω–Ω–æ –≤–Ω–∏–º–∞—Ç–µ–ª–µ–Ω –∫ —Ä–µ–¥–∫–∏–º, –Ω–æ –æ–ø–∞—Å–Ω—ã–º —Å–∏–º–ø—Ç–æ–º–∞–º¬ª.
    </blockquote>
  </div>

</div>

</body>
</html>

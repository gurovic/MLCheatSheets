<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>PyTorch –ü–æ–ª–Ω—ã–π –ì–∞–π–¥ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üî• PyTorch –ü–æ–ª–Ω—ã–π –ì–∞–π–¥</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã PyTorch</h2>
    <p><strong>PyTorch</strong> ‚Äî –ø–æ–ø—É–ª—è—Ä–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –æ—Ç Meta (Facebook).</p>
    <ul>
      <li><strong>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å</strong>: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ—ã</li>
      <li><strong>Pythonic</strong>: –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–ª—è Python-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤</li>
      <li><strong>–ê–≤—Ç–æ–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ</strong>: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á—ë—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</li>
      <li><strong>GPU-—É—Å–∫–æ—Ä–µ–Ω–∏–µ</strong>: –ø—Ä–æ—Å—Ç–∞—è —Ä–∞–±–æ—Ç–∞ —Å CUDA</li>
      <li><strong>–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è</strong>: –ø–æ–ø—É–ª—è—Ä–µ–Ω –≤ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–π —Å—Ä–µ–¥–µ</li>
    </ul>
    <pre><code># –£—Å—Ç–∞–Ω–æ–≤–∫–∞
pip install torch torchvision torchaudio

# –ò–º–ø–æ—Ä—Ç
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä—Å–∏–∏
print(torch.__version__)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
print(torch.cuda.is_available())
print(torch.cuda.device_count())</code></pre>

  <div class="block">
    <h2>üî∑ 2. –¢–µ–Ω–∑–æ—Ä—ã (Tensors)</h2>
    <p>–¢–µ–Ω–∑–æ—Ä—ã ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –≤ PyTorch, –∞–Ω–∞–ª–æ–≥ NumPy –º–∞—Å—Å–∏–≤–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU.</p>
    <pre><code># –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤
x = torch.tensor([1, 2, 3])
y = torch.zeros(3, 4)
z = torch.ones(2, 3, 4)
rand_t = torch.randn(3, 3)  # –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
rand_u = torch.rand(3, 3)   # Uniform [0, 1]

# –ò–∑ numpy
import numpy as np
arr = np.array([1, 2, 3])
t = torch.from_numpy(arr)

# –í numpy
arr_back = t.numpy()

# –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
float_t = torch.tensor([1.0, 2.0], dtype=torch.float32)
int_t = torch.tensor([1, 2], dtype=torch.int64)
bool_t = torch.tensor([True, False], dtype=torch.bool)

# –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
print(x.shape)  # torch.Size([3])
print(x.size()) # torch.Size([3])
print(x.dim())  # 1 (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–º–µ—Ä–µ–Ω–∏–π)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –û–ø–µ—Ä–∞—Ü–∏–∏ —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏</h2>
    <pre><code># –ê—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

c = a + b      # –°–ª–æ–∂–µ–Ω–∏–µ
c = a - b      # –í—ã—á–∏—Ç–∞–Ω–∏–µ
c = a * b      # –ü–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ
c = a / b      # –î–µ–ª–µ–Ω–∏–µ

# –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ
A = torch.randn(3, 4)
B = torch.randn(4, 5)
C = torch.matmul(A, B)  # –∏–ª–∏ A @ B

# –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
A_t = A.t()            # 2D
A_t = A.transpose(0, 1)  # –û–±—â–∏–π —Å–ª—É—á–∞–π

# –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã
x = torch.randn(12)
y = x.view(3, 4)       # reshape (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å contiguous)
z = x.reshape(3, 4)    # reshape (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤—Å–µ–≥–¥–∞)

# –°—Ä–µ–∑—ã –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
x = torch.randn(4, 5)
print(x[0, :])     # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞
print(x[:, 1])     # –í—Ç–æ—Ä–æ–π —Å—Ç–æ–ª–±–µ—Ü
print(x[1:3, :])   # –°—Ç—Ä–æ–∫–∏ 1 –∏ 2</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. GPU –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞</h2>
    <pre><code># –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# –ü–µ—Ä–µ–Ω–æ—Å —Ç–µ–Ω–∑–æ—Ä–∞ –Ω–∞ GPU
x = torch.randn(3, 3)
x_gpu = x.to(device)
# –∏–ª–∏
x_gpu = x.cuda()

# –ü–µ—Ä–µ–Ω–æ—Å –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ CPU
x_cpu = x_gpu.to('cpu')
# –∏–ª–∏
x_cpu = x_gpu.cpu()

# –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–∞–∑—É –Ω–∞ –Ω—É–∂–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
x = torch.randn(3, 3, device=device)

# –î–ª—è –º–æ–¥–µ–ª–∏
model = MyModel()
model.to(device)

# –ù–µ—Å–∫–æ–ª—å–∫–æ GPU
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)
    
# –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU
torch.cuda.empty_cache()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ê–≤—Ç–æ–≥—Ä–∞–¥–∏–µ–Ω—Ç (Autograd)</h2>
    <p>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è backpropagation.</p>
    <pre><code># –í–∫–ª—é—á–µ–Ω–∏–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
x = torch.tensor([2.0], requires_grad=True)
y = torch.tensor([3.0], requires_grad=True)

# –í—ã—á–∏—Å–ª–µ–Ω–∏—è
z = x**2 + y**3
loss = z.mean()

# Backpropagation
loss.backward()

# –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã
print(x.grad)  # dLoss/dx
print(y.grad)  # dLoss/dy

# –û—á–∏—Å—Ç–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
x.grad.zero_()
y.grad.zero_()

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –∞–≤—Ç–æ–≥—Ä–∞–¥–∞ (inference)
with torch.no_grad():
    z = x**2 + y**3  # –ù–µ –≤—ã—á–∏—Å–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
    
# –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
x_detached = x.detach()  # –ù–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä –±–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏</h2>
    <pre><code>import torch.nn as nn
import torch.nn.functional as F

# –°–ø–æ—Å–æ–± 1: –ù–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ç nn.Module
class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = SimpleNet(784, 128, 10)

# –°–ø–æ—Å–æ–± 2: Sequential
model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(128, 10)
)

# –ü—Ä–æ—Å–º–æ—Ç—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
print(model)

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –°–ª–æ–∏ (Layers)</h2>
    <table>
      <tr><th>–°–ª–æ–π</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–ü—Ä–∏–º–µ—Ä</th></tr>
      <tr><td><code>nn.Linear</code></td><td>Fully connected</td><td><code>nn.Linear(128, 64)</code></td></tr>
      <tr><td><code>nn.Conv2d</code></td><td>2D —Å–≤—ë—Ä—Ç–∫–∞</td><td><code>nn.Conv2d(3, 64, 3)</code></td></tr>
      <tr><td><code>nn.MaxPool2d</code></td><td>Max pooling</td><td><code>nn.MaxPool2d(2, 2)</code></td></tr>
      <tr><td><code>nn.Dropout</code></td><td>Dropout</td><td><code>nn.Dropout(0.5)</code></td></tr>
      <tr><td><code>nn.BatchNorm2d</code></td><td>Batch norm</td><td><code>nn.BatchNorm2d(64)</code></td></tr>
      <tr><td><code>nn.RNN</code></td><td>RNN</td><td><code>nn.RNN(128, 64)</code></td></tr>
      <tr><td><code>nn.LSTM</code></td><td>LSTM</td><td><code>nn.LSTM(128, 64)</code></td></tr>
      <tr><td><code>nn.GRU</code></td><td>GRU</td><td><code>nn.GRU(128, 64)</code></td></tr>
      <tr><td><code>nn.Embedding</code></td><td>Embeddings</td><td><code>nn.Embedding(10000, 300)</code></td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 8. –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏</h2>
    <pre><code># –î–æ—Å—Ç—É–ø–Ω—ã –∫–∞–∫ nn.Module –∏ F.functional

# ReLU
x = F.relu(x)
# –∏–ª–∏
relu = nn.ReLU()
x = relu(x)

# –î—Ä—É–≥–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
x = F.sigmoid(x)
x = F.tanh(x)
x = F.leaky_relu(x, negative_slope=0.01)
x = F.elu(x)
x = F.gelu(x)  # –ü–æ–ø—É–ª—è—Ä–Ω–∞ –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö
x = F.softmax(x, dim=1)  # dim –≤–∞–∂–µ–Ω!
x = F.log_softmax(x, dim=1)

# Swish/SiLU (–Ω–æ–≤–∞—è, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è)
x = F.silu(x)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å (Loss Functions)</h2>
    <pre><code># –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–±–∏–Ω–∞—Ä–Ω–∞—è)
criterion = nn.BCELoss()  # Binary Cross Entropy
# –ù—É–∂–µ–Ω sigmoid –Ω–∞ –≤—ã—Ö–æ–¥–µ

criterion = nn.BCEWithLogitsLoss()  # BCE + sigmoid
# –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —á–∏—Å–ª–µ–Ω–Ω–æ

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è)
criterion = nn.CrossEntropyLoss()
# –í–∫–ª—é—á–∞–µ—Ç softmax! –ù–µ –ø—Ä–∏–º–µ–Ω—è–π—Ç–µ softmax –Ω–∞ –≤—ã—Ö–æ–¥–µ

# –†–µ–≥—Ä–µ—Å—Å–∏—è
criterion = nn.MSELoss()  # Mean Squared Error
criterion = nn.L1Loss()   # Mean Absolute Error
criterion = nn.SmoothL1Loss()  # Huber loss

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
outputs = model(inputs)
loss = criterion(outputs, targets)

# –ü—Ä–∏–º–µ—Ä –¥–ª—è CrossEntropyLoss
# outputs: [batch_size, num_classes]
# targets: [batch_size] (–∏–Ω–¥–µ–∫—Å—ã –∫–ª–∞—Å—Å–æ–≤, –Ω–µ one-hot!)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã</h2>
    <pre><code>import torch.optim as optim

# SGD
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# Adam (–ø–æ–ø—É–ª—è—Ä–Ω—ã–π)
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))

# AdamW (Adam —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º weight decay)
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)

# RMSprop
optimizer = optim.RMSprop(model.parameters(), lr=0.01)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
for epoch in range(num_epochs):
    optimizer.zero_grad()  # –û—á–∏—Å—Ç–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    optimizer.step()       # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è</h2>
    <pre><code># –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SimpleNet(784, 128, 10).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# –û–±—É—á–µ–Ω–∏–µ
num_epochs = 10
for epoch in range(num_epochs):
    model.train()  # –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
    
    running_loss = 0.0
    for i, (inputs, labels) in enumerate(train_loader):
        # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
        inputs = inputs.to(device)
        labels = labels.to(device)
        
        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        # Backward pass –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
    
    epoch_loss = running_loss / len(train_loader)
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    model.eval()  # –†–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    accuracy = 100 * correct / total
    print(f'Validation Accuracy: {accuracy:.2f}%')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. DataLoader –∏ Dataset</h2>
    <pre><code>from torch.utils.data import Dataset, DataLoader

# –ö–∞—Å—Ç–æ–º–Ω—ã–π Dataset
class CustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
dataset = CustomDataset(X_train, y_train)

# DataLoader
train_loader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,      # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å
    num_workers=4,     # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
    pin_memory=True    # –£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–ª—è GPU
)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
for batch_idx, (inputs, labels) in enumerate(train_loader):
    # inputs: [batch_size, ...]
    # labels: [batch_size]
    pass</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π</h2>
    <pre><code># –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ–π –º–æ–¥–µ–ª–∏
torch.save(model, 'model.pth')
model = torch.load('model.pth')

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –≤–µ—Å–æ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
torch.save(model.state_dict(), 'model_weights.pth')
model.load_state_dict(torch.load('model_weights.pth'))

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
}
torch.save(checkpoint, 'checkpoint.pth')

# –ó–∞–≥—Ä—É–∑–∫–∞ checkpoint
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']

# –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ –¥—Ä—É–≥–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
model.load_state_dict(torch.load('model.pth', map_location='cpu'))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. Learning Rate Scheduler</h2>
    <pre><code>from torch.optim.lr_scheduler import *

# StepLR: —Å–Ω–∏–∂–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ N —ç–ø–æ—Ö
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)

# ExponentialLR: —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ
scheduler = ExponentialLR(optimizer, gamma=0.9)

# ReduceLROnPlateau: —Å–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ —É–ª—É—á—à–µ–Ω–∏—è
scheduler = ReduceLROnPlateau(
    optimizer, mode='min', factor=0.1, patience=5
)

# CosineAnnealingLR: –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ
scheduler = CosineAnnealingLR(optimizer, T_max=50)

# OneCycleLR: One Cycle Policy
scheduler = OneCycleLR(
    optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=10
)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
for epoch in range(num_epochs):
    train(...)
    val_loss = validate(...)
    
    # StepLR, ExponentialLR, CosineAnnealing
    scheduler.step()
    
    # ReduceLROnPlateau
    scheduler.step(val_loss)
    
    # OneCycleLR (–≤—ã–∑—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞!)
    # scheduler.step()  # –≤ —Ü–∏–∫–ª–µ –ø–æ –±–∞—Ç—á–∞–º</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. –°–≤—ë—Ä—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å (CNN)</h2>
    <pre><code>class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        # Convolutional layers
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        
        # Pooling
        self.pool = nn.MaxPool2d(2, 2)
        
        # Batch normalization
        self.bn1 = nn.BatchNorm2d(32)
        self.bn2 = nn.BatchNorm2d(64)
        self.bn3 = nn.BatchNorm2d(128)
        
        # Fully connected
        self.fc1 = nn.Linear(128 * 4 * 4, 256)
        self.fc2 = nn.Linear(256, 10)
        
        # Dropout
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x):
        # Conv block 1
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        
        # Conv block 2
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        
        # Conv block 3
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        
        # Flatten
        x = x.view(x.size(0), -1)
        
        # FC layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è —Å–µ—Ç—å (RNN/LSTM)</h2>
    <pre><code>class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            dropout=0.2
        )
        self.fc = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥
        out = out[:, -1, :]
        
        # FC layer
        out = self.fc(out)
        return out

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
model = LSTMModel(input_size=10, hidden_size=128, num_layers=2, num_classes=5)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. Transfer Learning</h2>
    <pre><code>import torchvision.models as models

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
model = models.resnet50(pretrained=True)

# –ó–∞–º–æ—Ä–æ–∑–∫–∞ –≤—Å–µ—Ö —Å–ª–æ—ë–≤
for param in model.parameters():
    param.requires_grad = False

# –ó–∞–º–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 10)  # 10 –∫–ª–∞—Å—Å–æ–≤

# –û–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π
optimizer = optim.Adam(model.fc.parameters(), lr=0.001)

# –ò–ª–∏: Fine-tuning –≤—Å–µ–π –º–æ–¥–µ–ª–∏
# 1. –û–±—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π
# 2. –†–∞–∑–º–æ—Ä–æ–∑–∏—Ç—å –≤—Å–µ —Å–ª–æ–∏
for param in model.parameters():
    param.requires_grad = True
    
# 3. –û–±—É—á–∏—Ç—å —Å –º–∞–ª–µ–Ω—å–∫–∏–º learning rate
optimizer = optim.Adam(model.parameters(), lr=0.0001)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ß–µ–∫-–ª–∏—Å—Ç –æ–±—É—á–µ–Ω–∏—è</h2>
    <ul>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU: <code>torch.cuda.is_available()</code></li>
      <li>[ ] –ü–µ—Ä–µ–Ω–µ—Å—Ç–∏ –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: <code>model.to(device)</code></li>
      <li>[ ] –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å <code>model.train()</code> –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º</li>
      <li>[ ] –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å <code>model.eval()</code> –ø–µ—Ä–µ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π</li>
      <li>[ ] –û—á–∏—â–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã: <code>optimizer.zero_grad()</code></li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å <code>with torch.no_grad():</code> –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏</li>
      <li>[ ] –ü—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã–±—Ä–∞—Ç—å loss –¥–ª—è –∑–∞–¥–∞—á–∏</li>
      <li>[ ] –ù–µ –ø—Ä–∏–º–µ–Ω—è—Ç—å softmax –ø–µ—Ä–µ–¥ CrossEntropyLoss</li>
      <li>[ ] –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å learning rate scheduler</li>
      <li>[ ] –û—Å–≤–æ–±–æ–∂–¥–∞—Ç—å GPU –ø–∞–º—è—Ç—å: <code>torch.cuda.empty_cache()</code></li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 19. –ü–æ–ª–µ–∑–Ω—ã–µ —Å–æ–≤–µ—Ç—ã</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ Best Practices</h3>
        <ul>
          <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ <code>DataLoader</code> —Å <code>num_workers > 0</code></li>
          <li>–ü—Ä–∏–º–µ–Ω—è–π—Ç–µ <code>pin_memory=True</code> –¥–ª—è GPU</li>
          <li>–ù–æ—Ä–º–∞–ª–∏–∑—É–π—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ</li>
          <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ batch normalization</li>
          <li>AdamW –≤–º–µ—Å—Ç–æ Adam –¥–ª—è weight decay</li>
          <li>Gradient clipping –¥–ª—è RNN</li>
          <li>Mixed precision training (AMP) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏</h3>
        <ul>
          <li>–ó–∞–±—ã—Ç—å <code>optimizer.zero_grad()</code></li>
          <li>–ù–µ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ GPU</li>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å softmax –ø–µ—Ä–µ–¥ CrossEntropyLoss</li>
          <li>–ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å <code>with torch.no_grad()</code> –ø—Ä–∏ inference</li>
          <li>–£—Ç–µ—á–∫–∏ –ø–∞–º—è—Ç–∏ (—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–Ω–∑–æ—Ä—ã —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏)</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 20. –û—Ç–ª–∞–¥–∫–∞ –∏ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ</h2>
    <pre><code># –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
assert not torch.isnan(loss).any(), "Loss is NaN!"
assert not torch.isinf(loss).any(), "Loss is Inf!"

# Gradient clipping (–¥–ª—è RNN)
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# –ü–∞–º—è—Ç—å GPU
print(torch.cuda.memory_allocated() / 1024**2, "MB")
print(torch.cuda.memory_reserved() / 1024**2, "MB")

# –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
with torch.autograd.profiler.profile(use_cuda=True) as prof:
    output = model(input)
print(prof.key_averages().table(sort_by="cuda_time_total"))

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
from torchviz import make_dot
make_dot(loss, params=dict(model.named_parameters())).render("model_graph")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 21. –ò—Ç–æ–≥–æ–≤—ã–π —á–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å PyTorch –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å GPU</li>
      <li>[ ] –°–æ–∑–¥–∞—Ç—å Dataset –∏ DataLoader</li>
      <li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ (nn.Module)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å learning rate scheduler</li>
      <li>[ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è</li>
      <li>[ ] –î–æ–±–∞–≤–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é</li>
      <li>[ ] –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å</li>
      <li>[ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´PyTorch ‚Äî —ç—Ç–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è "—É–º–Ω—ã—Ö" –ø—Ä–æ–≥—Ä–∞–º–º, –∫–æ—Ç–æ—Ä—ã–µ —É—á–∞—Ç—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä Lego –¥–ª—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –≤—ã —Å–æ–±–∏—Ä–∞–µ—Ç–µ –º–æ–¥–µ–ª—å –∏–∑ –±–ª–æ–∫–æ–≤ (—Å–ª–æ—ë–≤), –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç–µ –µ–π –ø—Ä–∏–º–µ—Ä—ã, –∏ –æ–Ω–∞ —É—á–∏—Ç—Å—è —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏ ‚Äî —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –ø–æ–Ω–∏–º–∞—Ç—å —Ç–µ–∫—Å—Ç, –¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

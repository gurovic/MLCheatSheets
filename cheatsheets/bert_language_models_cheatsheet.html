<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>BERT –∏ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ BERT –∏ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. BERT: –æ—Å–Ω–æ–≤—ã</h2>
    <p><strong>BERT</strong> (Bidirectional Encoder Representations from Transformers) ‚Äî —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ—Ç Google (2018)</p>
    <ul>
      <li><strong>–î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è</strong>: —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–µ–≤–∞ –∏ —Å–ø—Ä–∞–≤–∞</li>
      <li><strong>–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ</strong>: –Ω–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–∞—Ö —Ç–µ–∫—Å—Ç–æ–≤</li>
      <li><strong>Fine-tuning</strong>: –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–µ</li>
      <li><strong>Transfer learning</strong>: –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –∑–Ω–∞–Ω–∏—è –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ BERT</h2>
    <p>–û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ <strong>Transformer Encoder</strong></p>
    <ul>
      <li><strong>BERT-Base</strong>: 12 —Å–ª–æ–µ–≤, 768 hidden units, 12 attention heads (110M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)</li>
      <li><strong>BERT-Large</strong>: 24 —Å–ª–æ—è, 1024 hidden units, 16 attention heads (340M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)</li>
      <li><strong>–í—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã</strong>: [CLS] + —Ç–µ–∫—Å—Ç + [SEP]</li>
      <li><strong>–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏</strong>: –¥–æ 512 —Ç–æ–∫–µ–Ω–æ–≤</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ BERT</h2>
    <p>–î–≤–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è:</p>
    <table>
      <tr><th>–ó–∞–¥–∞—á–∞</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th></tr>
      <tr><td><strong>MLM</strong> (Masked Language Model)</td><td>–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å 15% –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤</td></tr>
      <tr><td><strong>NSP</strong> (Next Sentence Prediction)</td><td>–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∏–¥–µ—Ç –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ B –ø–æ—Å–ª–µ A</td></tr>
    </table>
    <blockquote>
      MLM –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö LM
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ —Å BERT</h2>
    <pre><code>from transformers import BertTokenizer, BertModel
import torch

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
tokenizer = BertTokenizer.from_pretrained(
    'bert-base-uncased'
)
model = BertModel.from_pretrained(
    'bert-base-uncased'
)

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
text = "Hello, how are you?"
inputs = tokenizer(
    text, 
    return_tensors="pt",
    padding=True,
    truncation=True
)

# –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
with torch.no_grad():
    outputs = model(**inputs)
    
# –≠–º–±–µ–¥–¥–∏–Ω–≥ [CLS] —Ç–æ–∫–µ–Ω–∞ (–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤—Å–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
cls_embedding = outputs.last_hidden_state[:, 0, :]

# –í—Å–µ —Ç–æ–∫–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
token_embeddings = outputs.last_hidden_state</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Fine-tuning BERT –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</h2>
    <pre><code>from transformers import BertForSequenceClassification
from transformers import Trainer, TrainingArguments

# –ú–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2  # –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    learning_rate=2e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_steps=10
)

# –û–±—É—á–µ–Ω–∏–µ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

trainer.train()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –í–∞—Ä–∏–∞–Ω—Ç—ã BERT</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>RoBERTa</strong></td><td>–ë–µ–∑ NSP, –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö</td><td>–õ—É—á—à–µ BERT</td></tr>
      <tr><td><strong>ALBERT</strong></td><td>–§–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</td><td>–ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</td></tr>
      <tr><td><strong>DistilBERT</strong></td><td>Knowledge distillation</td><td>60% –±—ã—Å—Ç—Ä–µ–µ, 97% –∫–∞—á–µ—Å—Ç–≤–∞</td></tr>
      <tr><td><strong>ELECTRA</strong></td><td>Discriminator –ø–æ–¥—Ö–æ–¥</td><td>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ</td></tr>
      <tr><td><strong>DeBERTa</strong></td><td>Disentangled attention</td><td>SOTA —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏</h2>
    <ul>
      <li><strong>mBERT</strong>: multilingual BERT –Ω–∞ 104 —è–∑—ã–∫–∞—Ö</li>
      <li><strong>XLM-RoBERTa</strong>: cross-lingual, 100 —è–∑—ã–∫–æ–≤</li>
      <li><strong>mT5</strong>: multilingual T5</li>
      <li><strong>ruBERT</strong>: —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞</li>
    </ul>
    <pre><code># –ó–∞–≥—Ä—É–∑–∫–∞ ruBERT
from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained(
    'DeepPavlov/rubert-base-cased'
)
model = BertModel.from_pretrained(
    'DeepPavlov/rubert-base-cased'
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ó–∞–¥–∞—á–∏ —Å BERT</h2>
    <table>
      <tr><th>–ó–∞–¥–∞—á–∞</th><th>–ú–æ–¥–µ–ª—å HuggingFace</th></tr>
      <tr><td>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞</td><td><code>BertForSequenceClassification</code></td></tr>
      <tr><td>NER (Named Entity Recognition)</td><td><code>BertForTokenClassification</code></td></tr>
      <tr><td>Question Answering</td><td><code>BertForQuestionAnswering</code></td></tr>
      <tr><td>Multiple Choice</td><td><code>BertForMultipleChoice</code></td></tr>
      <tr><td>Sentence Similarity</td><td><code>Sentence-BERT</code></td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 9. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</h2>
    <ul>
      <li><strong>DistilBERT</strong>: 40% –º–µ–Ω—å—à–µ, 60% –±—ã—Å—Ç—Ä–µ–µ</li>
      <li><strong>Quantization</strong>: –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤ (int8)</li>
      <li><strong>ONNX</strong>: –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π runtime</li>
      <li><strong>TensorRT</strong>: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞ GPU</li>
      <li><strong>Pruning</strong>: —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–≤–∞–∂–Ω—ã—Ö –≤–µ—Å–æ–≤</li>
    </ul>
    <pre><code># –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
from transformers import BertForSequenceClassification
from torch.quantization import quantize_dynamic

model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased'
)

quantized_model = quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. GPT vs BERT</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>BERT</th><th>GPT</th></tr>
      <tr><td><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong></td><td>Encoder-only</td><td>Decoder-only</td></tr>
      <tr><td><strong>–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ</strong></td><td>–î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è</td><td>–û–¥–Ω–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è (–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è)</td></tr>
      <tr><td><strong>–ó–∞–¥–∞—á–∞ –æ–±—É—á–µ–Ω–∏—è</strong></td><td>MLM + NSP</td><td>Next token prediction</td></tr>
      <tr><td><strong>–õ—É—á—à–µ –¥–ª—è</strong></td><td>–ü–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞</td><td>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞</td></tr>
      <tr><td><strong>Fine-tuning</strong></td><td>–¢—Ä–µ–±—É–µ—Ç—Å—è</td><td>–ú–æ–∂–Ω–æ zero-shot</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ò—Å—Ç–æ—Ä–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π</h2>
    <ol>
      <li><strong>Word2Vec, GloVe</strong> (2013): —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏</li>
      <li><strong>ELMo</strong> (2018): –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏</li>
      <li><strong>GPT</strong> (2018): —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä decoder</li>
      <li><strong>BERT</strong> (2018): –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä</li>
      <li><strong>GPT-2</strong> (2019): –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ GPT</li>
      <li><strong>T5, BART</strong> (2019): encoder-decoder</li>
      <li><strong>GPT-3</strong> (2020): 175B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
      <li><strong>ChatGPT, GPT-4</strong> (2022-2023): RLHF</li>
    </ol>
  </div>

  <div class="block">
    <h2>üî∑ 12. Sentence-BERT</h2>
    <p>–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è BERT –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è <strong>sentence embeddings</strong></p>
    <pre><code>from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = SentenceTransformer(
    'paraphrase-multilingual-MiniLM-L12-v2'
)

# –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
sentences = [
    "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ‚Äî —ç—Ç–æ –∫—Ä—É—Ç–æ",
    "Deep Learning –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–µ–Ω",
    "–Ø –ª—é–±–ª—é –ø–∏—Ü—Ü—É"
]

embeddings = model.encode(sentences)

# –°—Ö–æ–¥—Å—Ç–≤–æ
similarity = cosine_similarity(
    [embeddings[0]], [embeddings[1]]
)
print(f"Similarity: {similarity[0][0]:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è BERT</h2>
    <ul>
      <li><strong>Sentiment Analysis</strong>: –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏</li>
      <li><strong>Text Classification</strong>: –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è</li>
      <li><strong>NER</strong>: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π</li>
      <li><strong>Question Answering</strong>: SQuAD</li>
      <li><strong>Semantic Search</strong>: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫</li>
      <li><strong>Summarization</strong>: —Å encoder-decoder</li>
      <li><strong>Chatbots</strong>: –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤</li>
      <li><strong>Recommendation</strong>: –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 14. Best Practices</h2>
    <ol>
      <li><strong>–í—ã–±–æ—Ä —Ä–∞–∑–º–µ—Ä–∞</strong>: Base –¥–ª—è –Ω–∞—á–∞–ª–∞, Large –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞</li>
      <li><strong>Learning rate</strong>: 2e-5, 3e-5, 5e-5</li>
      <li><strong>Batch size</strong>: 16 –∏–ª–∏ 32 (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç GPU)</li>
      <li><strong>Epochs</strong>: 3-4 –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ</li>
      <li><strong>Warmup</strong>: 10% –æ—Ç –æ–±—â–∏—Ö —à–∞–≥–æ–≤</li>
      <li><strong>Max length</strong>: –æ–±—Ä–µ–∑–∞—Ç—å –¥–æ 128-512 —Ç–æ–∫–µ–Ω–æ–≤</li>
      <li><strong>Gradient clipping</strong>: max_grad_norm=1.0</li>
    </ol>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</h3>
        <ul>
          <li>SOTA —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –º–Ω–æ–≥–∏—Ö –∑–∞–¥–∞—á–∞—Ö NLP</li>
          <li>Transfer learning —ç–∫–æ–Ω–æ–º–∏—Ç –≤—Ä–µ–º—è</li>
          <li>–ü–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç</li>
          <li>–ú–Ω–æ–≥–æ –≥–æ—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π</li>
        </ul>
      </div>
      <div class="bad">
        <h3>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</h3>
        <ul>
          <li>–¢—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤</li>
          <li>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ 512 —Ç–æ–∫–µ–Ω–æ–≤</li>
          <li>–ù–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç (–Ω—É–∂–µ–Ω GPT)</li>
          <li>Fine-tuning —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ß–µ–∫-–ª–∏—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è</h2>
    <ol>
      <li>‚úì –í—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â—É—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å</li>
      <li>‚úì –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ</li>
      <li>‚úì –ù–∞—Å—Ç—Ä–æ–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å padding/truncation</li>
      <li>‚úì –í—ã–±—Ä–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (lr, batch_size, epochs)</li>
      <li>‚úì –î–æ–±–∞–≤–∏—Ç—å warmup –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏</li>
      <li>‚úì –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å overfitting –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏</li>
      <li>‚úì –û—Ü–µ–Ω–∏—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ</li>
      <li>‚úì –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è production (quantization, ONNX)</li>
    </ol>
  </div>

</div>

</body>
</html>

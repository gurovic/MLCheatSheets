<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Federated Learning Cheatsheet — 3 колонки</title>
  <style>
    @media screen{body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Helvetica,Arial,sans-serif;color:#333;background:#fafcff;padding:10px}}
    @media print{body{background:white;padding:0}@page{size:A4 landscape;margin:10mm}}
    .container{column-count:3;column-gap:20px;max-width:100%}
    .block{break-inside:avoid;margin-bottom:1.2em;padding:12px;background:white;border-radius:6px;box-shadow:0 1px 3px rgba(0,0,0,0.05)}
    h1{font-size:1.6em;font-weight:700;color:#1a5fb4;text-align:center;margin:0 0 8px;column-span:all}
    .subtitle{text-align:center;color:#666;font-size:0.9em;margin-bottom:12px;column-span:all}
    h2{font-size:1.15em;font-weight:700;color:#1a5fb4;margin:0 0 8px;padding-bottom:4px;border-bottom:1px solid #e0e7ff}
    p,ul,ol{font-size:0.92em;margin:0.6em 0}
    ul,ol{padding-left:18px}
    li{margin-bottom:4px}
    code{font-family:'Consolas','Courier New',monospace;background-color:#f0f4ff;padding:1px 4px;border-radius:3px;font-size:0.88em}
    pre{background-color:#f0f4ff;padding:8px;border-radius:4px;overflow-x:auto;font-size:0.84em;margin:6px 0}
    pre code{padding:0;background:none;white-space:pre-wrap}
    strong{color:#1a5fb4}
  </style>
</head>
<body>
  <h1>Federated Learning (Федеративное обучение)</h1>
  <div class="subtitle"></div>
  <div class="container">
    <div class="block">
      <h2>1. Основная концепция</h2>
      <p><strong>Federated Learning (FL)</strong> — парадигма машинного обучения, где модель обучается на распределённых устройствах/серверах без централизованного сбора данных</p>
      <p><strong>Ключевой принцип:</strong> "Bring the code to the data, not data to the code"</p>
      <p><strong>Мотивация:</strong></p>
      <ul>
        <li><strong>Приватность</strong>: данные остаются у владельцев</li>
        <li><strong>Безопасность</strong>: меньше рисков утечки</li>
        <li><strong>Законодательство</strong>: GDPR, HIPAA compliance</li>
        <li><strong>Bandwidth</strong>: экономия трафика</li>
        <li><strong>Latency</strong>: edge computing</li>
      </ul>
    </div>

    <div class="block">
      <h2>2. Архитектура FL</h2>
      <p><strong>Компоненты:</strong></p>
      <ul>
        <li><strong>Central Server</strong>: координатор, aggregator</li>
        <li><strong>Clients</strong>: устройства с локальными данными (телефоны, больницы и т.д.)</li>
        <li><strong>Global Model</strong>: общая модель на сервере</li>
        <li><strong>Local Models</strong>: копии модели на клиентах</li>
      </ul>
      <p><strong>Типы топологий:</strong></p>
      <ul>
        <li><strong>Cross-device</strong>: миллионы мобильных устройств</li>
        <li><strong>Cross-silo</strong>: несколько организаций/дата-центров</li>
        <li><strong>Peer-to-peer</strong>: децентрализованная сеть без центрального сервера</li>
      </ul>
    </div>

    <div class="block">
      <h2>3. Federated Averaging (FedAvg)</h2>
      <p><strong>Базовый алгоритм FL от Google:</strong></p>
      <pre><code>Server side:
1. Инициализировать w_global
2. Для каждого раунда t:
   a) Выбрать подмножество клиентов C_t
   b) Отправить w_global клиентам
   c) Получить обновления от клиентов
   d) Агрегировать:
      w_global ← Σ_k (n_k/n) · w_k
      где n_k — размер данных клиента k

Client k side:
1. Получить w_global
2. Локальное обучение E эпох на своих данных
3. Отправить обновлённые веса w_k серверу</code></pre>
      <p><strong>Параметры:</strong> E (local epochs), B (batch size), C (fraction of clients)</p>
    </div>

    <div class="block">
      <h2>4. Вызовы FL</h2>
      <p><strong>Statistical heterogeneity:</strong> non-IID данные у разных клиентов</p>
      <ul>
        <li>Разные распределения классов</li>
        <li>Concept drift между клиентами</li>
        <li>Несбалансированность данных</li>
      </ul>
      <p><strong>System heterogeneity:</strong></p>
      <ul>
        <li>Разная вычислительная мощность</li>
        <li>Нестабильные сетевые соединения</li>
        <li>Частые выпадения клиентов</li>
      </ul>
      <p><strong>Communication efficiency:</strong> огромные communication costs</p>
      <p><strong>Privacy concerns:</strong> модель может leak информацию о данных</p>
    </div>

    <div class="block">
      <h2>5. Улучшения FedAvg</h2>
      <p><strong>FedProx:</strong> добавляет proximal term для стабильности</p>
      <pre><code>L_k(w) = F_k(w) + (μ/2)||w - w_global||²</code></pre>
      <p><strong>FedOpt:</strong> использует адаптивные оптимизаторы (Adam, Yogi) на сервере</p>
      <p><strong>FedNova:</strong> нормализация по числу локальных шагов</p>
      <p><strong>SCAFFOLD:</strong> control variates для коррекции client drift</p>
      <p><strong>FedBN:</strong> не агрегировать Batch Normalization statistics</p>
      <p><strong>Personalized FL:</strong> каждый клиент адаптирует модель под себя</p>
    </div>

    <div class="block">
      <h2>6. Differential Privacy в FL</h2>
      <p><strong>Цель:</strong> гарантировать, что отдельные примеры не влияют на модель заметно</p>
      <p><strong>DP-SGD (Differential Private SGD):</strong></p>
      <ol>
        <li>Clip градиенты по норме C</li>
        <li>Добавить Gaussian шум σ·N(0,C²I)</li>
        <li>Обновить веса</li>
      </ol>
      <p><strong>Privacy budget ε:</strong></p>
      <pre><code>ε = O(q·T·σ⁻¹)
где q — sampling rate, T — iterations</code></pre>
      <p><strong>Trade-off:</strong> больше приватности (меньше ε) → хуже accuracy</p>
      <p><strong>Local vs Central DP:</strong> в FL обычно применяют local DP на клиентах</p>
    </div>

    <div class="block">
      <h2>7. Secure Aggregation</h2>
      <p><strong>Проблема:</strong> сервер может увидеть обновления отдельных клиентов</p>
      <p><strong>Решение:</strong> криптографические протоколы для приватной агрегации</p>
      <p><strong>Secure Aggregation Protocol:</strong></p>
      <ul>
        <li>Клиенты маскируют свои обновления случайными значениями</li>
        <li>Сервер видит только сумму</li>
        <li>Маски взаимно компенсируются</li>
      </ul>
      <p><strong>Схема:</strong></p>
      <pre><code>Клиент k: отправляет w_k + mask_k
Сервер: Σ_k (w_k + mask_k) = Σ_k w_k
(маски сумма = 0 по дизайну)</code></pre>
      <p><strong>Устойчивость к dropout:</strong> протокол работает даже если часть клиентов выпадает</p>
    </div>

    <div class="block">
      <h2>8. Communication Efficiency</h2>
      <p><strong>Проблема:</strong> передача полных моделей затратна</p>
      <p><strong>Gradient Compression:</strong></p>
      <ul>
        <li><strong>Quantization</strong>: использовать меньше бит</li>
        <li><strong>Sparsification</strong>: передавать только top-k градиентов</li>
        <li><strong>Sketching</strong>: low-rank approximations</li>
      </ul>
      <p><strong>Local Computation:</strong></p>
      <ul>
        <li>Больше локальных эпох (E > 1)</li>
        <li>Меньше частота коммуникации</li>
      </ul>
      <p><strong>Structured Updates:</strong></p>
      <ul>
        <li>Low-rank updates: ΔW = UV^T</li>
        <li>Factorized models</li>
      </ul>
      <p><strong>Достижение:</strong> 100-1000x сжатие без потери accuracy</p>
    </div>

    <div class="block">
      <h2>9. Реализация</h2>
      <pre><code># Пример с Flower framework
import flwr as fl
import torch

class FlowerClient(fl.client.NumPyClient):
    def __init__(self, model, train_loader):
        self.model = model
        self.train_loader = train_loader
    
    def get_parameters(self, config):
        return [val.cpu().numpy() 
                for _, val in self.model.state_dict().items()]
    
    def set_parameters(self, parameters):
        params_dict = zip(
            self.model.state_dict().keys(),
            parameters
        )
        state_dict = {
            k: torch.tensor(v) 
            for k, v in params_dict
        }
        self.model.load_state_dict(state_dict)
    
    def fit(self, parameters, config):
        self.set_parameters(parameters)
        # Local training
        train(self.model, self.train_loader, 
              epochs=config["local_epochs"])
        return self.get_parameters(config={}), len(self.train_loader), {}
    
    def evaluate(self, parameters, config):
        self.set_parameters(parameters)
        loss, accuracy = test(self.model, self.test_loader)
        return loss, len(self.test_loader), {"accuracy": accuracy}

# Start client
fl.client.start_numpy_client(
    server_address="localhost:8080",
    client=FlowerClient(model, train_loader)
)

# Server strategy
strategy = fl.server.strategy.FedAvg(
    fraction_fit=0.1,  # 10% клиентов на раунд
    min_available_clients=10
)

fl.server.start_server(
    server_address="localhost:8080",
    config=fl.server.ServerConfig(num_rounds=50),
    strategy=strategy
)</code></pre>
    </div>

    <div class="block">
      <h2>10. Варианты FL</h2>
      <p><strong>Vertical FL:</strong> разные признаки у разных сторон, одни и те же примеры</p>
      <p><strong>Horizontal FL:</strong> одни признаки, разные примеры (стандартный FL)</p>
      <p><strong>Federated Transfer Learning:</strong> transfer learning в FL setting</p>
      <p><strong>Split Learning:</strong> модель разделена между клиентом и сервером</p>
      <p><strong>Asynchronous FL:</strong> клиенты обновляют модель асинхронно</p>
      <p><strong>Hierarchical FL:</strong> многоуровневая иерархия aggregation</p>
    </div>

    <div class="block">
      <h2>11. Применения</h2>
      <p><strong>Mobile devices:</strong></p>
      <ul>
        <li>Клавиатурные предсказания (Gboard)</li>
        <li>Voice assistants</li>
        <li>Персонализация приложений</li>
      </ul>
      <p><strong>Healthcare:</strong></p>
      <ul>
        <li>Обучение на данных больниц без sharing PHI</li>
        <li>Drug discovery</li>
        <li>Medical imaging</li>
      </ul>
      <p><strong>Finance:</strong></p>
      <ul>
        <li>Fraud detection между банками</li>
        <li>Credit scoring</li>
      </ul>
      <p><strong>IoT:</strong> edge devices, smart homes</p>
    </div>

    <div class="block">
      <h2>12. Best Practices</h2>
      <p><strong>Дизайн модели:</strong></p>
      <ul>
        <li>Компактные модели для мобильных устройств</li>
        <li>Избегать очень глубоких сетей</li>
        <li>Batch Normalization → Group/Layer Normalization</li>
      </ul>
      <p><strong>Параметры FedAvg:</strong></p>
      <ul>
        <li><strong>E</strong>: 1-5 локальных эпох</li>
        <li><strong>C</strong>: 0.01-0.1 (fraction of clients)</li>
        <li><strong>Learning rate</strong>: меньше чем централизованное обучение</li>
      </ul>
      <p><strong>Мониторинг:</strong></p>
      <ul>
        <li>Отслеживать convergence rate</li>
        <li>Детектировать stragglers</li>
        <li>Проверять fairness между клиентами</li>
      </ul>
      <p><strong>Privacy:</strong> всегда использовать Differential Privacy + Secure Aggregation в продакшене</p>
    </div>
  </div>
</body>
</html>

<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>AdaBoost Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.88em;
      margin: 6px 0;
    }

    th, td {
      padding: 6px 8px;
      text-align: left;
      border: 1px solid #e0e7ff;
    }

    th {
      background-color: #1a5fb4;
      color: white;
      font-weight: 700;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ AdaBoost</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å AdaBoost</h2>
    <p><strong>Adaptive Boosting</strong>: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å–ª–∞–±—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤</p>
    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –∫–∞–∂–¥—ã–π —Å–ª–µ–¥—É—é—â–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö</li>
      <li><strong>–í–µ—Å–∞ –ø—Ä–∏–º–µ—Ä–æ–≤</strong>: —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç—Å—è –¥–ª—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö</li>
      <li><strong>–í–µ—Å–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤</strong>: –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏</li>
      <li><strong>–§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ</strong>: –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤</li>
      <li><strong>–°–ª–∞–±—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã</strong>: –æ–±—ã—á–Ω–æ decision stumps (–¥–µ—Ä–µ–≤—å—è –≥–ª—É–±–∏–Ω—ã 1)</li>
    </ul>
    <blockquote>
      üí° AdaBoost –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å–ª–∞–±—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤ –æ–¥–∏–Ω —Å–∏–ª—å–Ω—ã–π
    </blockquote>

  <div class="block">
    <h2>üî∑ 2. –ê–ª–≥–æ—Ä–∏—Ç–º AdaBoost</h2>
    <pre><code><strong>–®–∞–≥–∏:</strong>
1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: w_i = 1/N –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
2. –î–ª—è t = 1 –¥–æ T:
   a. –û–±—É—á–∏—Ç—å —Å–ª–∞–±—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä h_t –Ω–∞ –≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
   b. –í—ã—á–∏—Å–ª–∏—Ç—å –æ—à–∏–±–∫—É: Œµ_t = Œ£(w_i * I(y_i ‚â† h_t(x_i)))
   c. –í—ã—á–∏—Å–ª–∏—Ç—å –≤–µ—Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞:
      Œ±_t = 0.5 * ln((1 - Œµ_t) / Œµ_t)
   d. –û–±–Ω–æ–≤–∏—Ç—å –≤–µ—Å–∞ –ø—Ä–∏–º–µ—Ä–æ–≤:
      w_i = w_i * exp(-Œ±_t * y_i * h_t(x_i))
   e. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤–µ—Å–∞: w_i = w_i / Œ£w_i
3. –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä:
   H(x) = sign(Œ£ Œ±_t * h_t(x))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä (–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)</h2>
    <pre><code>from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
X, y = make_classification(n_samples=1000, n_features=20,
                          n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
ada = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=50,
    learning_rate=1.0,
    random_state=42
)

# –û–±—É—á–µ–Ω–∏–µ
ada.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
y_pred = ada.predict(X_test)
y_proba = ada.predict_proba(X_test)

# –û—Ü–µ–Ω–∫–∞
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã AdaBoost</h2>
    <table>
      <tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</th></tr>
      <tr><td><code>n_estimators</code></td><td>–ß–∏—Å–ª–æ —Å–ª–∞–±—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤</td><td>50-500</td></tr>
      <tr><td><code>learning_rate</code></td><td>–í–µ—Å –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞</td><td>0.1-1.0</td></tr>
      <tr><td><code>base_estimator</code></td><td>–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä</td><td>DecisionTreeClassifier(max_depth=1)</td></tr>
      <tr><td><code>algorithm</code></td><td>'SAMME' –∏–ª–∏ 'SAMME.R'</td><td>'SAMME.R' (–¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π)</td></tr>
      <tr><td><code>random_state</code></td><td>Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏</td><td>–õ—é–±–æ–µ —Ü–µ–ª–æ–µ</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 5. –†–µ–≥—Ä–µ—Å—Å–∏—è —Å AdaBoost</h2>
    <pre><code>from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
import numpy as np

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
X, y = make_regression(n_samples=1000, n_features=10,
                      noise=10, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –ú–æ–¥–µ–ª—å
ada_reg = AdaBoostRegressor(
    base_estimator=DecisionTreeRegressor(max_depth=4),
    n_estimators=100,
    learning_rate=1.0,
    loss='linear',  # 'linear', 'square', 'exponential'
    random_state=42
)

# –û–±—É—á–µ–Ω–∏–µ
ada_reg.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
y_pred = ada_reg.predict(X_test)

# –û—Ü–µ–Ω–∫–∞
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print(f"RMSE: {rmse:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</h2>
    <pre><code>from sklearn.model_selection import GridSearchCV

# –°–µ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.5, 1.0],
    'base_estimator__max_depth': [1, 2, 3]
}

# Grid Search
ada = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(),
    random_state=42
)

grid_search = GridSearchCV(
    ada, param_grid,
    cv=5, scoring='accuracy',
    n_jobs=-1, verbose=1
)

grid_search.fit(X_train, y_train)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV score: {grid_search.best_score_:.3f}")

# –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
best_ada = grid_search.best_estimator_
test_score = best_ada.score(X_test, y_test)
print(f"Test score: {test_score:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</h2>
    <pre><code>import matplotlib.pyplot as plt

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
ada = AdaBoostClassifier(n_estimators=100, random_state=42)
ada.fit(X_train, y_train)

# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
importances = ada.feature_importances_
indices = np.argsort(importances)[::-1]

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(12, 6))
plt.title('Feature Importances (AdaBoost)')
plt.bar(range(X_train.shape[1]), importances[indices])
plt.xticks(range(X_train.shape[1]), indices)
plt.xlabel('Feature Index')
plt.ylabel('Importance')
plt.grid(axis='y', alpha=0.3)
plt.show()

# –¢–æ–ø-N –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
top_n = 10
top_features = indices[:top_n]
print(f"Top {top_n} features: {top_features}")
print(f"Importances: {importances[top_features]}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Staged predictions</h2>
    <pre><code># Staged predictions –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
import matplotlib.pyplot as plt

ada = AdaBoostClassifier(n_estimators=100, random_state=42)
ada.fit(X_train, y_train)

# –û—Ü–µ–Ω–∫–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ
train_scores = []
test_scores = []

for y_train_pred in ada.staged_predict(X_train):
    train_scores.append(accuracy_score(y_train, y_train_pred))

for y_test_pred in ada.staged_predict(X_test):
    test_scores.append(accuracy_score(y_test, y_test_pred))

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(10, 6))
plt.plot(train_scores, label='Training Accuracy', linewidth=2)
plt.plot(test_scores, label='Test Accuracy', linewidth=2)
plt.xlabel('Number of Estimators')
plt.ylabel('Accuracy')
plt.title('AdaBoost: Training vs Test Accuracy')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
optimal_n = np.argmax(test_scores) + 1
print(f"Optimal n_estimators: {optimal_n}")
print(f"Best test accuracy: {test_scores[optimal_n-1]:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏</h2>
    <pre><code>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier

# –ú–æ–¥–µ–ª–∏
models = {
    'Decision Tree': DecisionTreeClassifier(max_depth=5),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    'AdaBoost': AdaBoostClassifier(n_estimators=100),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100)
}

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    train_score = model.score(X_train, y_train)
    test_score = model.score(X_test, y_test)
    results[name] = {'train': train_score, 'test': test_score}

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import pandas as pd

df_results = pd.DataFrame(results).T
df_results.plot(kind='bar', figsize=(10, 6))
plt.title('Model Comparison')
plt.ylabel('Accuracy')
plt.xlabel('Model')
plt.xticks(rotation=15)
plt.legend(['Train', 'Test'])
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

print(df_results)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h2>
    <ul>
      <li><strong>‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:</strong>
        <ul>
          <li>–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è –º–Ω–æ–≥–∏—Ö –∑–∞–¥–∞—á</li>
          <li>–ú–∞–ª–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏</li>
          <li>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π feature selection</li>
          <li>–ù–µ —Ç—Ä–µ–±—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
          <li>–†–∞–±–æ—Ç–∞–µ—Ç —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –∏ —á–∏—Å–ª–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏</li>
        </ul>
      </li>
      <li><strong>‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:</strong>
        <ul>
          <li>–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ —à—É–º—É –∏ –≤—ã–±—Ä–æ—Å–∞–º</li>
          <li>–°–∫–ª–æ–Ω–µ–Ω –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é</li>
          <li>–ú–µ–¥–ª–µ–Ω–Ω–µ–µ Random Forest (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)</li>
          <li>–ü–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏</li>
          <li>–¢—Ä–µ–±—É–µ—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–¥–±–æ—Ä–∞ n_estimators</li>
        </ul>
      </li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 11. –†–∞–±–æ—Ç–∞ —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏</h2>
    <pre><code>from sklearn.utils import class_weight

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤
class_weights = class_weight.compute_class_weight(
    'balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weight_dict = dict(enumerate(class_weights))

# AdaBoost —Å –≤–µ—Å–∞–º–∏
from sklearn.tree import DecisionTreeClassifier

base_clf = DecisionTreeClassifier(
    max_depth=1,
    class_weight=class_weight_dict
)

ada = AdaBoostClassifier(
    base_estimator=base_clf,
    n_estimators=100,
    random_state=42
)

ada.fit(X_train, y_train)

# SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

smote = SMOTE(random_state=42)
pipeline = ImbPipeline([
    ('smote', smote),
    ('adaboost', AdaBoostClassifier(n_estimators=100))
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Early Stopping</h2>
    <pre><code>from sklearn.model_selection import train_test_split

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train –∏ validation
X_tr, X_val, y_tr, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

# –û–±—É—á–µ–Ω–∏–µ —Å —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π
best_score = 0
best_n = 0
patience = 10
no_improvement = 0

for n in range(10, 501, 10):
    ada = AdaBoostClassifier(n_estimators=n, random_state=42)
    ada.fit(X_tr, y_tr)
    val_score = ada.score(X_val, y_val)
    
    if val_score > best_score:
        best_score = val_score
        best_n = n
        no_improvement = 0
    else:
        no_improvement += 1
    
    if no_improvement >= patience:
        print(f"Early stopping at {n} estimators")
        break

print(f"Best n_estimators: {best_n}")
print(f"Best validation score: {best_score:.3f}")

# –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
final_ada = AdaBoostClassifier(n_estimators=best_n, 
                               random_state=42)
final_ada.fit(X_train, y_train)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AdaBoost</h2>
    <blockquote>
      <strong>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ AdaBoost –∫–æ–≥–¥–∞:</strong>
      <ul>
        <li>–î–∞–Ω–Ω—ã–µ —Å –Ω–µ–±–æ–ª—å—à–∏–º —à—É–º–æ–º</li>
        <li>–ù—É–∂–Ω–∞ –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å</li>
        <li>–í–∞–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å (—Å decision stumps)</li>
        <li>–ï—Å—Ç—å –≤—Ä–µ–º—è –Ω–∞ –ø–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
        <li>–î–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã</li>
      </ul>
      <strong>–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–≥–¥–∞:</strong>
      <ul>
        <li>–ú–Ω–æ–≥–æ —à—É–º–∞ –∏ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö</li>
        <li>–î–∞–Ω–Ω—ã–µ —Å–∏–ª—å–Ω–æ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã</li>
        <li>–ù—É–∂–Ω–∞ –≤—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è</li>
        <li>–î–∞–Ω–Ω—ã–µ –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
      </ul>
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ß–µ–∫-–ª–∏—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è</h2>
    <ol>
      <li>‚úÖ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ –≤—ã–±—Ä–æ—Å—ã –∏ —à—É–º</li>
      <li>‚úÖ –í—ã–±—Ä–∞—Ç—å –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä (–æ–±—ã—á–Ω–æ decision stump)</li>
      <li>‚úÖ –ù–∞—á–∞—Ç—å —Å n_estimators=50-100</li>
      <li>‚úÖ –ü–æ–¥–æ–±—Ä–∞—Ç—å learning_rate (0.1-1.0)</li>
      <li>‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å cross-validation –¥–ª—è –æ—Ü–µ–Ω–∫–∏</li>
      <li>‚úÖ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (staged predictions)</li>
      <li>‚úÖ –ù–∞—Å—Ç—Ä–æ–∏—Ç—å n_estimators —Å early stopping</li>
      <li>‚úÖ –û—Ü–µ–Ω–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
      <li>‚úÖ –°—Ä–∞–≤–Ω–∏—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ –∞–Ω—Å–∞–º–±–ª—è–º–∏</li>
      <li>‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ</li>
    </ol>
  </div>

</div>

</div>
</body>
</html>

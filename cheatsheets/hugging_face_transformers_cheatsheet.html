<!DOCTYPE html>
<html lang="ru">
<head>
<meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>ü§ó Hugging Face Transformers Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
<style>
@media screen{body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Helvetica,Arial,sans-serif;color:#333;background:#fafcff;padding:10px
        min-width: 900px;
      }}@media print{body{background:white;padding:0}@page{size:A4 landscape;margin:10mm}}    .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }.block{break-inside:avoid;margin-bottom:1.2em;padding:12px;background:white;border-radius:6px;box-shadow:0 1px 3px rgba(0,0,0,0.05)}h1{font-size:1.6em;font-weight:700;color:#1a5fb4;text-align:center;margin:0 0 8px;column-span:all}.subtitle{text-align:center;color:#666;font-size:0.9em;margin-bottom:12px;column-span:all}h2{font-size:1.15em;font-weight:700;color:#1a5fb4;margin:0 0 8px;padding-bottom:4px;border-bottom:1px solid #e0e7ff}p,ul,ol{font-size:0.92em;margin:0.6em 0}ul,ol{padding-left:18px}li{margin-bottom:4px}code{font-family:'Consolas','Courier New',monospace;background-color:#f0f4ff;padding:1px 4px;border-radius:3px;font-size:0.88em}pre{background-color:#f0f4ff;padding:8px;border-radius:4px;overflow-x:auto;font-size:0.84em;margin:6px 0}pre code{padding:0;background:none;white-space:pre-wrap}table{width:100%;border-collapse:collapse;font-size:0.82em;margin:6px 0}th{background-color:#e6f0ff;text-align:left;padding:4px 6px;font-weight:600}td{padding:4px 6px;border-bottom:1px solid #f0f4ff}tr:nth-child(even){background-color:#f8fbff}.good-vs-bad{display:flex;flex-direction:column;gap:8px}.good-vs-bad div{flex:1;padding:6px 8px;border-radius:4px}.good{background-color:#f0f9f4;border-left:3px solid #2e8b57}.bad{background-color:#fdf0f2;border-left:3px solid #d32f2f}.good h3,.bad h3{margin:0 0 4px;font-size:1em;font-weight:700}.good ul,.bad ul{padding-left:20px;margin:0}.good li::before{content:"‚úÖ ";font-weight:bold}.bad li::before{content:"‚ùå ";font-weight:bold}blockquote{font-style:italic;margin:8px 0;padding:6px 10px;background:#f8fbff;border-left:2px solid #1a5fb4;font-size:0.88em}
</style>
</head>
<body>
<div class="container">
<h1>ü§ó Hugging Face Transformers</h1>
<div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

<div class="block">
<h2>üî∑ 1. –í–≤–µ–¥–µ–Ω–∏–µ</h2>
<p><strong>Hugging Face Transformers</strong> ‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è state-of-the-art –º–æ–¥–µ–ª–µ–π NLP, computer vision, audio.</p>
<ul>
<li><strong>–ë–æ–ª–µ–µ 100,000 –º–æ–¥–µ–ª–µ–π</strong> –≤ Model Hub</li>
<li><strong>–ï–¥–∏–Ω—ã–π API</strong> –¥–ª—è PyTorch, TensorFlow, JAX</li>
<li><strong>–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏</strong>: BERT, GPT, T5, CLIP –∏ –¥—Ä.</li>
<li><strong>Pipeline API</strong>: –ø—Ä–æ—Å—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–µ–∑ –∫–æ–¥–∞</li>
<li><strong>Trainer API</strong>: —É–¥–æ–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π</li>
</ul>
<pre><code># –£—Å—Ç–∞–Ω–æ–≤–∫–∞
pip install transformers
pip install transformers[torch]  # –° PyTorch
pip install transformers[tf]     # –° TensorFlow

# –ò–º–ø–æ—Ä—Ç
from transformers import pipeline, AutoModel, AutoTokenizer</code></pre>

  </div>
<div class="block">
<h2>üî∑ 2. Pipeline API</h2>
<p>–°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π.</p>
<pre><code># –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
classifier = pipeline("sentiment-analysis")
result = classifier("I love this product!")
# [{'label': 'POSITIVE', 'score': 0.9998}]

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
generator = pipeline("text-generation", model="gpt2")
text = generator("Once upon a time", max_length=50)

# Named Entity Recognition
ner = pipeline("ner")
entities = ner("My name is John and I live in New York")

# Question Answering
qa = pipeline("question-answering")
result = qa(question="What is AI?", context="AI is artificial intelligence...")

# Summarization
summarizer = pipeline("summarization")
summary = summarizer("Long text here...", max_length=50)

# Translation
translator = pipeline("translation_en_to_fr")
result = translator("Hello, how are you?")

# Zero-shot classification
classifier = pipeline("zero-shot-classification")
result = classifier(
    "This is about sports",
    candidate_labels=["politics", "sports", "technology"]
)</code></pre>
</div>

<div class="block">
<h2>üî∑ 3. AutoClasses</h2>
<p>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –Ω—É–∂–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏.</p>
<pre><code>from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# –î–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3
)

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
text = "Hello, how are you?"
inputs = tokenizer(text, return_tensors="pt")
# {'input_ids': tensor(...), 'attention_mask': tensor(...)}

# Forward pass
outputs = model(**inputs)
logits = outputs.logits</code></pre>
</div>

<div class="block">
<h2>üî∑ 4. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è</h2>
<pre><code># –ë–∞–∑–æ–≤–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
tokens = tokenizer.tokenize("Hello world")
# ['hello', 'world']

# –í ID
input_ids = tokenizer.encode("Hello world")
# [101, 7592, 2088, 102]

# –û–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç
text = tokenizer.decode(input_ids)
# "[CLS] hello world [SEP]"

# –ü–æ–ª–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
encoding = tokenizer(
    "Hello world",
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt"
)

# –ë–∞—Ç—á —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
texts = ["Text 1", "Text 2", "Text 3"]
batch_encoding = tokenizer(
    texts,
    padding=True,
    truncation=True,
    return_tensors="pt"
)

# Special tokens
print(tokenizer.cls_token)  # [CLS]
print(tokenizer.sep_token)  # [SEP]
print(tokenizer.pad_token)  # [PAD]
print(tokenizer.mask_token) # [MASK]</code></pre>
</div>

<div class="block">
<h2>üî∑ 5. Fine-tuning —Å Trainer</h2>
<pre><code>from transformers import Trainer, TrainingArguments
from datasets import load_dataset

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
dataset = load_dataset("glue", "mrpc")

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
def tokenize_function(examples):
    return tokenizer(
        examples["sentence1"],
        examples["sentence2"],
        padding="max_length",
        truncation=True
    )

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    compute_metrics=compute_metrics
)

# –û–±—É—á–µ–Ω–∏–µ
trainer.train()

# –û—Ü–µ–Ω–∫–∞
metrics = trainer.evaluate()

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
predictions = trainer.predict(test_dataset)</code></pre>
</div>

<div class="block">
<h2>üî∑ 6. –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏</h2>
<table>
<tr><th>–ú–æ–¥–µ–ª—å</th><th>–ó–∞–¥–∞—á–∏</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏</th></tr>
<tr><td><strong>BERT</strong></td><td>Classification, NER, QA</td><td>Bidirectional, masked LM</td></tr>
<tr><td><strong>GPT-2/3</strong></td><td>Text generation</td><td>Autoregressive</td></tr>
<tr><td><strong>T5</strong></td><td>All NLP tasks</td><td>Text-to-text framework</td></tr>
<tr><td><strong>RoBERTa</strong></td><td>Same as BERT</td><td>Improved BERT</td></tr>
<tr><td><strong>DistilBERT</strong></td><td>Same as BERT</td><td>40% smaller, 60% faster</td></tr>
<tr><td><strong>ELECTRA</strong></td><td>Classification</td><td>Efficient pre-training</td></tr>
<tr><td><strong>XLNet</strong></td><td>Classification</td><td>Permutation LM</td></tr>
<tr><td><strong>BART</strong></td><td>Summarization</td><td>Seq2seq with denoising</td></tr>
</table>
</div>

<div class="block">
<h2>üî∑ 7. Datasets –±–∏–±–ª–∏–æ—Ç–µ–∫–∞</h2>
<pre><code>from datasets import load_dataset, load_metric

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
dataset = load_dataset("imdb")
dataset = load_dataset("squad")
dataset = load_dataset("glue", "mrpc")

# –°—Ç—Ä—É–∫—Ç—É—Ä–∞
print(dataset)
# DatasetDict({
#     train: Dataset
#     test: Dataset
# })

# –î–æ—Å—Ç—É–ø –∫ –¥–∞–Ω–Ω—ã–º
train_data = dataset["train"]
print(train_data[0])

# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
filtered = dataset.filter(lambda x: x["label"] == 1)

# –ú–∞–ø–ø–∏–Ω–≥
processed = dataset.map(preprocess_function, batched=True)

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
train_test = dataset["train"].train_test_split(test_size=0.2)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
dataset.save_to_disk("./my_dataset")

# –ó–∞–≥—Ä—É–∑–∫–∞
dataset = load_dataset("./my_dataset")</code></pre>
</div>

<div class="block">
<h2>üî∑ 8. Metrics</h2>
<pre><code>from datasets import load_metric
import numpy as np

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç—Ä–∏–∫
accuracy_metric = load_metric("accuracy")
f1_metric = load_metric("f1")
rouge_metric = load_metric("rouge")
bleu_metric = load_metric("bleu")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    accuracy = accuracy_metric.compute(
        predictions=predictions,
        references=labels
    )
    f1 = f1_metric.compute(
        predictions=predictions,
        references=labels,
        average="weighted"
    )
    
    return {
        "accuracy": accuracy["accuracy"],
        "f1": f1["f1"]
    }

# –í Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics
)</code></pre>
</div>

<div class="block">
<h2>üî∑ 9. Model Hub</h2>
<p>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏ –ø—É–±–ª–∏–∫–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π.</p>
<pre><code># –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ Hub
model = AutoModel.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

# –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π
from huggingface_hub import list_models

models = list_models(filter="text-classification")

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ä–µ–≤–∏–∑–∏–∏
model = AutoModel.from_pretrained(
    "bert-base-uncased",
    revision="main"  # –∏–ª–∏ commit hash
)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ
model.save_pretrained("./my_model")
tokenizer.save_pretrained("./my_model")

# –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
model = AutoModel.from_pretrained("./my_model")

# –ü—É–±–ª–∏–∫–∞—Ü–∏—è –≤ Hub (—Ç—Ä–µ–±—É–µ—Ç –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏)
from huggingface_hub import HfApi

api = HfApi()
api.upload_folder(
    folder_path="./my_model",
    repo_id="username/model-name"
)

# –ò–ª–∏ —á–µ—Ä–µ–∑ push_to_hub
model.push_to_hub("username/model-name")
tokenizer.push_to_hub("username/model-name")</code></pre>
</div>

<div class="block">
<h2>üî∑ 10. Generation</h2>
<pre><code># Text generation
from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

input_ids = tokenizer.encode("Once upon a time", return_tensors="pt")

# Greedy decoding
output = model.generate(input_ids, max_length=50)

# Beam search
output = model.generate(
    input_ids,
    max_length=50,
    num_beams=5,
    early_stopping=True
)

# Sampling
output = model.generate(
    input_ids,
    max_length=50,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    temperature=0.7
)

# Nucleus sampling (top-p)
output = model.generate(
    input_ids,
    max_length=50,
    do_sample=True,
    top_p=0.92,
    top_k=0
)

# –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)</code></pre>
</div>

<div class="block">
<h2>üî∑ 11. –ß–µ–∫-–ª–∏—Å—Ç</h2>
<ul>
<li>[ ] –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å transformers –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</li>
<li>[ ] –í—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â—É—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å</li>
<li>[ ] –ó–∞–≥—Ä—É–∑–∏—Ç—å tokenizer –∏ model</li>
<li>[ ] –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ</li>
<li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å TrainingArguments</li>
<li>[ ] –°–æ–∑–¥–∞—Ç—å Trainer —Å compute_metrics</li>
<li>[ ] Fine-tune –º–æ–¥–µ–ª—å</li>
<li>[ ] –û—Ü–µ–Ω–∏—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏</li>
<li>[ ] –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å</li>
<li>[ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
</ul>

<h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
<blockquote>
¬´Hugging Face Transformers ‚Äî —ç—Ç–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å —Ç—ã—Å—è—á–∞–º–∏ –≥–æ—Ç–æ–≤—ã—Ö AI-–º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –∑–≤—É–∫–æ–º. –í–º–µ—Å—Ç–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è (—á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–¥–µ–ª–∏ –∏ –æ–≥—Ä–æ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤), –º—ã –±–µ—Ä—ë–º –≥–æ—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ –¥–æ–æ–±—É—á–∞–µ–º –µ—ë –ø–æ–¥ —Å–≤–æ—é –∑–∞–¥–∞—á—É –∑–∞ —á–∞—Å—ã. –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–π –¥–≤–∏–≥–∞—Ç–µ–ª—å –≤–º–µ—Å—Ç–æ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è –∫–æ–ª–µ—Å–∞¬ª.
</blockquote>
</div>


</div>
</body>
</html>

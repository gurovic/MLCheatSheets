<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –≤ Machine Learning Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üé≤ –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –≤ Machine Learning</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞</h2>
    <p><strong>–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥</strong> —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∫–∞–∫ —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ–ª–∏—á–∏–Ω—ã —Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π.</p>
    <ul>
      <li><strong>–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è</strong>: –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É–±–µ–∂–¥–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö</li>
      <li><strong>–û—Ç–ª–∏—á–∏–µ –æ—Ç —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞</strong>: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ‚Äî —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ–ª–∏—á–∏–Ω—ã</li>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</strong>:
        <ul>
          <li>–ö–≤–∞–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏</li>
          <li>–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∞–ø—Ä–∏–æ—Ä–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è</li>
          <li>–†–∞–±–æ—Ç–∞ —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö</li>
        </ul>
      </li>
    </ul>
    <blockquote>üí° "–ß–∞—Å—Ç–æ—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç: –∫–∞–∫–æ–≤–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö? –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π: –∫–∞–∫–æ–≤–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–∏ –¥–∞–Ω–Ω—ã—Ö?"</blockquote>

  <div class="block">
    <h2>üî∑ 2. –¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞</h2>
    <p><strong>–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞</strong>:</p>
    <p style="text-align: center; font-size: 1.1em;">P(Œ∏|D) = P(D|Œ∏) √ó P(Œ∏) / P(D)</p>
    
    <table>
      <tr><th>–¢–µ—Ä–º–∏–Ω</th><th>–ù–∞–∑–≤–∞–Ω–∏–µ</th><th>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</th></tr>
      <tr>
        <td>P(Œ∏|D)</td>
        <td><strong>Posterior</strong><br>–ê–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–µ</td>
        <td>–£–±–µ–∂–¥–µ–Ω–∏–µ –æ Œ∏ –ø–æ—Å–ª–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö D</td>
      </tr>
      <tr>
        <td>P(D|Œ∏)</td>
        <td><strong>Likelihood</strong><br>–ü—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ</td>
        <td>–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö Œ∏</td>
      </tr>
      <tr>
        <td>P(Œ∏)</td>
        <td><strong>Prior</strong><br>–ê–ø—Ä–∏–æ—Ä–Ω–æ–µ</td>
        <td>–ù–∞—á–∞–ª—å–Ω–æ–µ —É–±–µ–∂–¥–µ–Ω–∏–µ –æ Œ∏ –¥–æ –¥–∞–Ω–Ω—ã—Ö</td>
      </tr>
      <tr>
        <td>P(D)</td>
        <td><strong>Evidence</strong><br>–°–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ</td>
        <td>–ù–æ—Ä–º–∞–ª–∏–∑—É—é—â–∞—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞</td>
      </tr>
    </table>
    
    <p><strong>–ò–Ω—Ç—É–∏—Ü–∏—è</strong>:</p>
    <p>Posterior ‚àù Likelihood √ó Prior</p>
    <p>"–ù–æ–≤–æ–µ —É–±–µ–∂–¥–µ–Ω–∏–µ = –î–∞–Ω–Ω—ã–µ √ó –°—Ç–∞—Ä–æ–µ —É–±–µ–∂–¥–µ–Ω–∏–µ"</p>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ü—Ä–∏–º–µ—Ä: –º–æ–Ω–µ—Ç–∞</h2>
    <p><strong>–ó–∞–¥–∞—á–∞</strong>: –æ—Ü–µ–Ω–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–ø–∞–¥–µ–Ω–∏—è –æ—Ä–ª–∞ Œ∏</p>
    <pre><code>import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# –î–∞–Ω–Ω—ã–µ: 7 –æ—Ä–ª–æ–≤ –∏–∑ 10 –±—Ä–æ—Å–∫–æ–≤
heads = 7
tosses = 10

# Prior: Beta(2, 2) - —Å–ª–µ–≥–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π
prior_alpha, prior_beta = 2, 2

# Likelihood: Binomial(heads | Œ∏, tosses)
# Posterior: Beta(alpha + heads, beta + tails)
posterior_alpha = prior_alpha + heads
posterior_beta = prior_beta + (tosses - heads)

# Posterior —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
theta = np.linspace(0, 1, 1000)
prior = stats.beta.pdf(theta, prior_alpha, prior_beta)
posterior = stats.beta.pdf(theta, posterior_alpha, posterior_beta)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(10, 5))
plt.plot(theta, prior, label='Prior', linestyle='--')
plt.plot(theta, posterior, label='Posterior')
plt.axvline(heads/tosses, color='red', 
            linestyle=':', label='MLE')
plt.xlabel('Œ∏ (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ä–ª–∞)')
plt.ylabel('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏')
plt.legend()
plt.title('–ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –º–æ–Ω–µ—Ç—ã')
plt.show()

# Posterior —Å—Ä–µ–¥–Ω–µ–µ –∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª
post_mean = posterior_alpha / (posterior_alpha + posterior_beta)
post_std = np.sqrt(posterior_alpha * posterior_beta / 
                   ((posterior_alpha + posterior_beta)**2 * 
                    (posterior_alpha + posterior_beta + 1)))

print(f"Posterior —Å—Ä–µ–¥–Ω–µ–µ: {post_mean:.3f}")
print(f"Posterior std: {post_std:.3f}")

# 95% credible interval
ci_low, ci_high = stats.beta.ppf([0.025, 0.975], 
                                  posterior_alpha, 
                                  posterior_beta)
print(f"95% CI: [{ci_low:.3f}, {ci_high:.3f}]")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –í—ã–±–æ—Ä –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (Prior)</h2>
    <p><strong>–¢–∏–ø—ã –ø—Ä–∏–æ—Ä–æ–≤</strong>:</p>
    <table>
      <tr><th>–¢–∏–ø</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr>
      <tr>
        <td><strong>Uninformative</strong><br>(–Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π)</td>
        <td>–†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ –∏–ª–∏ —à–∏—Ä–æ–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</td>
        <td>–ù–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π</td>
      </tr>
      <tr>
        <td><strong>Informative</strong><br>(–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π)</td>
        <td>–£–∑–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∫—Ä—É–≥ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è</td>
        <td>–ï—Å—Ç—å —Å–∏–ª—å–Ω—ã–µ —É–±–µ–∂–¥–µ–Ω–∏—è</td>
      </tr>
      <tr>
        <td><strong>Conjugate</strong><br>(—Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã–π)</td>
        <td>Prior –∏ posterior –æ–¥–Ω–æ–≥–æ —Å–µ–º–µ–π—Å—Ç–≤–∞</td>
        <td>–î–ª—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞</td>
      </tr>
      <tr>
        <td><strong>Empirical</strong><br>(—ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π)</td>
        <td>–û—Ü–µ–Ω–µ–Ω –∏–∑ –¥–∞–Ω–Ω—ã—Ö</td>
        <td>–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏</td>
      </tr>
    </table>

    <p><strong>–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã</strong>:</p>
    <ul>
      <li>Binomial likelihood ‚Üí Beta prior</li>
      <li>Normal likelihood (–∏–∑–≤–µ—Å—Ç–Ω–∞—è œÉ) ‚Üí Normal prior</li>
      <li>Poisson likelihood ‚Üí Gamma prior</li>
      <li>Categorical likelihood ‚Üí Dirichlet prior</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è</h2>
    <p><strong>–ú–æ–¥–µ–ª—å</strong>: y = Xw + Œµ, –≥–¥–µ w ~ N(Œº‚ÇÄ, Œ£‚ÇÄ)</p>
    <pre><code>import numpy as np
from scipy import stats

class BayesianLinearRegression:
    def __init__(self, alpha=1.0, beta=1.0):
        """
        alpha: precision of prior (1/variance)
        beta: precision of noise (1/noise_variance)
        """
        self.alpha = alpha
        self.beta = beta
        self.w_mean = None
        self.w_cov = None
    
    def fit(self, X, y):
        # Prior: w ~ N(0, (1/alpha)I)
        n_features = X.shape[1]
        
        # Posterior parameters
        S_N_inv = self.alpha * np.eye(n_features) + \
                  self.beta * X.T @ X
        self.w_cov = np.linalg.inv(S_N_inv)
        self.w_mean = self.beta * self.w_cov @ X.T @ y
        
        return self
    
    def predict(self, X, return_std=False):
        y_mean = X @ self.w_mean
        
        if return_std:
            # Predictive variance
            y_var = 1/self.beta + np.sum(X @ self.w_cov * X, axis=1)
            y_std = np.sqrt(y_var)
            return y_mean, y_std
        
        return y_mean

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

X, y = make_regression(n_samples=100, n_features=5, noise=0.5)
X_train, X_test, y_train, y_test = train_test_split(X, y)

# –û–±—É—á–µ–Ω–∏–µ
model = BayesianLinearRegression(alpha=1.0, beta=25.0)
model.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å—é
y_pred, y_std = model.predict(X_test, return_std=True)

print(f"Posterior mean weights:
{model.w_mean}")
print(f"Posterior covariance:
{model.w_cov}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
plt.figure(figsize=(10, 5))
plt.errorbar(range(len(y_test)), y_pred, yerr=2*y_std, 
             fmt='o', capsize=5, alpha=0.7, label='Predictions')
plt.plot(y_test, 'r*', label='True values')
plt.xlabel('Sample')
plt.ylabel('Value')
plt.legend()
plt.title('–ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å—é')
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ù–∞–∏–≤–Ω—ã–π –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä</h2>
    <p><strong>–ü—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ</strong>: –ø—Ä–∏–∑–Ω–∞–∫–∏ —É—Å–ª–æ–≤–Ω–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã –ø—Ä–∏ –¥–∞–Ω–Ω–æ–º –∫–ª–∞—Å—Å–µ</p>
    <p>P(y|x‚ÇÅ,...,x‚Çô) ‚àù P(y) √ó ‚àèP(x·µ¢|y)</p>
    
    <pre><code>from sklearn.naive_bayes import GaussianNB, MultinomialNB

# Gaussian Naive Bayes (–¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
gnb = GaussianNB()
gnb.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred = gnb.predict(X_test)

# –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤
y_proba = gnb.predict_proba(X_test)

# Multinomial NB (–¥–ª—è —Ç–µ–∫—Å—Ç–∞, —Å—á–µ—Ç–æ–≤)
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
X_train_counts = vectorizer.fit_transform(texts_train)

mnb = MultinomialNB(alpha=1.0)  # alpha - —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –õ–∞–ø–ª–∞—Å–∞
mnb.fit(X_train_counts, y_train)

# Bernoulli NB (–¥–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
from sklearn.naive_bayes import BernoulliNB

bnb = BernoulliNB()
bnb.fit(X_train_binary, y_train)</code></pre>

    <p><strong>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</strong>:</p>
    <ul>
      <li>‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ (–æ—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ)</li>
      <li>‚úÖ –ë–∞–∑–æ–≤—ã–π baseline</li>
      <li>‚úÖ –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å</li>
      <li>‚úÖ –ú–∞–ª–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö</li>
      <li>‚ùå –°–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</h2>
    <p>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:</p>
    <pre><code>from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern
from scipy.stats import norm
from scipy.optimize import minimize

def bayesian_optimization(func, bounds, n_iter=20):
    """
    –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    func: —Ü–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)
    bounds: –≥—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    """
    # –ù–∞—á–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–π–Ω—ã–µ —Ç–æ—á–∫–∏
    X_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], 
                                 size=(5, bounds.shape[0]))
    y_sample = np.array([func(x) for x in X_sample])
    
    # Gaussian Process
    gp = GaussianProcessRegressor(
        kernel=Matern(nu=2.5),
        alpha=1e-6,
        n_restarts_optimizer=10
    )
    
    for i in range(n_iter):
        # –û–±–Ω–æ–≤–∏—Ç—å GP
        gp.fit(X_sample, y_sample)
        
        # Acquisition function (Expected Improvement)
        def acquisition(x):
            mu, sigma = gp.predict(x.reshape(1, -1), 
                                   return_std=True)
            best_y = np.max(y_sample)
            
            # Expected Improvement
            with np.errstate(divide='ignore'):
                Z = (mu - best_y) / sigma
                ei = (mu - best_y) * norm.cdf(Z) + \
                     sigma * norm.pdf(Z)
            
            return -ei  # –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ EI
        
        # –ù–∞–π—Ç–∏ —Å–ª–µ–¥—É—é—â—É—é —Ç–æ—á–∫—É
        x_next = None
        best_acq = np.inf
        
        for x0 in np.random.uniform(bounds[:, 0], bounds[:, 1],
                                    size=(10, bounds.shape[0])):
            result = minimize(acquisition, x0, bounds=bounds,
                            method='L-BFGS-B')
            if result.fun < best_acq:
                best_acq = result.fun
                x_next = result.x
        
        # –û—Ü–µ–Ω–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –≤ –Ω–æ–≤–æ–π —Ç–æ—á–∫–µ
        y_next = func(x_next)
        
        # –î–æ–±–∞–≤–∏—Ç—å –∫ —Å—ç–º–ø–ª–∞–º
        X_sample = np.vstack([X_sample, x_next])
        y_sample = np.append(y_sample, y_next)
    
    # –õ—É—á—à–∞—è –Ω–∞–π–¥–µ–Ω–Ω–∞—è —Ç–æ—á–∫–∞
    best_idx = np.argmax(y_sample)
    return X_sample[best_idx], y_sample[best_idx]

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def objective(params):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å –¥–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    from sklearn.ensemble import RandomForestClassifier
    model = RandomForestClassifier(
        n_estimators=int(params[0]),
        max_depth=int(params[1]),
        min_samples_split=int(params[2])
    )
    model.fit(X_train, y_train)
    return model.score(X_val, y_val)

bounds = np.array([
    [50, 200],    # n_estimators
    [3, 20],      # max_depth
    [2, 20]       # min_samples_split
])

best_params, best_score = bayesian_optimization(
    objective, bounds, n_iter=30
)

print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_params}")
print(f"–õ—É—á—à–∏–π score: {best_score:.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏</h2>
    <p>–í–º–µ—Å—Ç–æ —Ç–æ—á–µ—á–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –≤–µ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:</p>
    <pre><code># PyTorch + Blitz (Bayesian NN library)
import torch
import torch.nn as nn
from blitz.modules import BayesianLinear
from blitz.utils import variational_estimator

@variational_estimator
class BayesianNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.blinear1 = BayesianLinear(input_dim, hidden_dim)
        self.blinear2 = BayesianLinear(hidden_dim, output_dim)
    
    def forward(self, x):
        x = torch.relu(self.blinear1(x))
        x = self.blinear2(x)
        return x

# –û–±—É—á–µ–Ω–∏–µ
model = BayesianNN(input_dim=10, hidden_dim=50, output_dim=1)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

for epoch in range(100):
    loss = model.sample_elbo(
        inputs=X_train,
        labels=y_train,
        criterion=criterion,
        sample_nbr=3  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤ –≤–µ—Å–æ–≤
    )
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å—é
predictions = []
for _ in range(100):  # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—ç–º–ø–ª—ã
    with torch.no_grad():
        pred = model(X_test)
        predictions.append(pred)

predictions = torch.stack(predictions)
mean = predictions.mean(dim=0)
std = predictions.std(dim=0)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ú–∞—Ä–∫–æ–≤—Å–∫–∏–µ —Ü–µ–ø–∏ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (MCMC)</h2>
    <p>–î–ª—è —Å–ª–æ–∂–Ω—ã—Ö posterior —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ:</p>
    <pre><code># PyMC –¥–ª—è –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
import pymc as pm

with pm.Model() as model:
    # Prior
    alpha = pm.Normal('alpha', mu=0, sigma=10)
    beta = pm.Normal('beta', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=1)
    
    # Likelihood
    mu = alpha + beta * X
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, 
                      observed=y)
    
    # MCMC —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
    trace = pm.sample(
        2000,  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤
        tune=1000,  # burn-in –ø–µ—Ä–∏–æ–¥
        return_inferencedata=True,
        chains=4  # –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Ü–µ–ø–∏
    )

# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
import arviz as az

# Summary —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
print(az.summary(trace))

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è posterior
az.plot_posterior(trace)

# Trace plots
az.plot_trace(trace)

# –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
print(f"R-hat: {az.rhat(trace)}")  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å ~1.0</code></pre>

    <p><strong>–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ MCMC –∞–ª–≥–æ—Ä–∏—Ç–º—ã</strong>:</p>
    <ul>
      <li><strong>Metropolis-Hastings</strong>: –±–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º</li>
      <li><strong>Gibbs Sampling</strong>: –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —É—Å–ª–æ–≤–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π</li>
      <li><strong>Hamiltonian Monte Carlo (HMC)</strong>: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –¥–ª—è –≤—ã—Å–æ–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π</li>
      <li><strong>NUTS</strong>: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ HMC</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 10. –í–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –≤—ã–≤–æ–¥</h2>
    <p>–ê–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è posterior –ø—Ä–æ—Å—Ç—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º:</p>
    <p>–ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º KL(q(Œ∏) || p(Œ∏|D))</p>
    <p>–≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ ELBO (Evidence Lower Bound)</p>
    
    <pre><code># Variational Inference —Å PyMC
with pm.Model() as model:
    # Prior –∏ likelihood –∫–∞–∫ —Ä–∞–Ω—å—à–µ
    alpha = pm.Normal('alpha', mu=0, sigma=10)
    beta = pm.Normal('beta', mu=0, sigma=10, shape=X.shape[1])
    sigma = pm.HalfNormal('sigma', sigma=1)
    
    mu = alpha + pm.math.dot(X, beta)
    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, 
                      observed=y)
    
    # Variational inference (–±—ã—Å—Ç—Ä–µ–µ MCMC)
    approx = pm.fit(
        n=10000,  # –∏—Ç–µ—Ä–∞—Ü–∏–π
        method='advi'  # Automatic Differentiation VI
    )
    
    # –ü–æ–ª—É—á–∏—Ç—å posterior —Å—ç–º–ø–ª—ã
    trace = approx.sample(2000)

# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ VI:
# ‚úÖ –ë—ã—Å—Ç—Ä–µ–µ MCMC
# ‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ
# ‚úÖ –£–¥–æ–±–Ω–æ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π (VAE)
# ‚ùå –ê–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è (–Ω–µ —Ç–æ—á–Ω—ã–π posterior)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ì–∞—É—Å—Å–æ–≤—Å–∫–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã</h2>
    <p>–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–º–µ—Ä–Ω—ã–º–∏ –ø—Ä–∏–æ—Ä–∞–º–∏:</p>
    <pre><code>from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel

# –Ø–¥—Ä–æ (kernel) –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç prior
kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)

gp = GaussianProcessRegressor(
    kernel=kernel,
    n_restarts_optimizer=10,
    alpha=1e-10
)

# –û–±—É—á–µ–Ω–∏–µ
gp.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å—é
y_pred, y_std = gp.predict(X_test, return_std=True)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(12, 5))
plt.plot(X_test, y_test, 'r.', label='True')
plt.plot(X_test, y_pred, 'b-', label='Prediction')
plt.fill_between(
    X_test.ravel(),
    y_pred - 2*y_std,
    y_pred + 2*y_std,
    alpha=0.3,
    label='95% confidence'
)
plt.legend()
plt.title('Gaussian Process Regression')
plt.show()

# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —è–¥—Ä–∞
print(f"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —è–¥—Ä–æ:
{gp.kernel_}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π A/B —Ç–µ—Å—Ç</h2>
    <p>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏:</p>
    <pre><code>import pymc as pm

# –î–∞–Ω–Ω—ã–µ A/B —Ç–µ—Å—Ç–∞
n_A, conversions_A = 1000, 100  # –≤–∞—Ä–∏–∞–Ω—Ç A
n_B, conversions_B = 1000, 120  # –≤–∞—Ä–∏–∞–Ω—Ç B

with pm.Model() as model:
    # Prior (Beta —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
    p_A = pm.Beta('p_A', alpha=1, beta=1)
    p_B = pm.Beta('p_B', alpha=1, beta=1)
    
    # Likelihood (Binomial)
    obs_A = pm.Binomial('obs_A', n=n_A, p=p_A, 
                        observed=conversions_A)
    obs_B = pm.Binomial('obs_B', n=n_B, p=p_B, 
                        observed=conversions_B)
    
    # –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏
    delta = pm.Deterministic('delta', p_B - p_A)
    
    # –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
    trace = pm.sample(5000, tune=1000, return_inferencedata=True)

# –ê–Ω–∞–ª–∏–∑
import arviz as az

print(az.summary(trace, var_names=['p_A', 'p_B', 'delta']))

# –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —á—Ç–æ B –ª—É—á—à–µ A
prob_B_better = (trace.posterior['delta'] > 0).mean()
print(f"P(B > A) = {prob_B_better:.1%}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
az.plot_posterior(trace, var_names=['delta'])

# Expected loss (–æ–∂–∏–¥–∞–µ–º—ã–µ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞)
delta_samples = trace.posterior['delta'].values.flatten()
loss_choose_A = np.maximum(delta_samples, 0).mean()
loss_choose_B = np.maximum(-delta_samples, 0).mean()

print(f"Expected loss –µ—Å–ª–∏ –≤—ã–±—Ä–∞—Ç—å A: {loss_choose_A:.4f}")
print(f"Expected loss –µ—Å–ª–∏ –≤—ã–±—Ä–∞—Ç—å B: {loss_choose_B:.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –±–∞–π–µ—Å–æ–≤—Å–∫–∏–µ –º–æ–¥–µ–ª–∏</h2>
    <p>–ú–æ–¥–µ–ª–∏ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:</p>
    <pre><code># –ü—Ä–∏–º–µ—Ä: —Å—Ç—É–¥–µ–Ω—Ç—ã –≤ —Ä–∞–∑–Ω—ã—Ö —à–∫–æ–ª–∞—Ö
import pymc as pm

# –î–∞–Ω–Ω—ã–µ
n_schools = 8
n_students_per_school = 20

with pm.Model() as hierarchical_model:
    # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–ø–æ–ø—É–ª—è—Ü–∏–æ–Ω–Ω—ã–µ)
    mu_global = pm.Normal('mu_global', mu=0, sigma=10)
    sigma_global = pm.HalfNormal('sigma_global', sigma=5)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —à–∫–æ–ª (–≥—Ä—É–ø–ø–æ–≤—ã–µ)
    mu_school = pm.Normal('mu_school', 
                          mu=mu_global, 
                          sigma=sigma_global,
                          shape=n_schools)
    
    sigma_school = pm.HalfNormal('sigma_school', 
                                  sigma=2, 
                                  shape=n_schools)
    
    # –ù–∞–±–ª—é–¥–µ–Ω–∏—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤
    for i in range(n_schools):
        pm.Normal(f'obs_school_{i}',
                  mu=mu_school[i],
                  sigma=sigma_school[i],
                  observed=student_scores[i])
    
    # Inference
    trace = pm.sample(2000, tune=1000, return_inferencedata=True)

# Shrinkage effect: —à–∫–æ–ª—ã —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö
# "–ø—Ä–∏—Ç—è–≥–∏–≤–∞—é—Ç—Å—è" –∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É —Å—Ä–µ–¥–Ω–µ–º—É</code></pre>

    <p><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</strong>:</p>
    <ul>
      <li>–ß–∞—Å—Ç–∏—á–Ω–æ–µ pooling –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏</li>
      <li>–†–∞–±–æ—Ç–∞ —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö –≤ –≥—Ä—É–ø–ø–∞—Ö</li>
      <li>–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 14. Bayesian Model Averaging (BMA)</h2>
    <p>–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π:</p>
    <pre><code># BMA: –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ posterior –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
def bayesian_model_averaging(models, X_train, y_train, X_test):
    """
    models: —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π [(model1, prior1), (model2, prior2), ...]
    """
    from scipy.special import logsumexp
    
    # –í—ã—á–∏—Å–ª–∏—Ç—å evidence –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    log_evidences = []
    predictions = []
    
    for model, prior in models:
        # –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å
        model.fit(X_train, y_train)
        
        # Log likelihood (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ)
        y_pred = model.predict(X_train)
        log_likelihood = -0.5 * np.sum((y_train - y_pred)**2)
        
        # Log evidence = log likelihood + log prior
        log_evidence = log_likelihood + np.log(prior)
        log_evidences.append(log_evidence)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predictions.append(model.predict(X_test))
    
    # Posterior model probabilities
    log_evidences = np.array(log_evidences)
    log_posterior_probs = log_evidences - logsumexp(log_evidences)
    posterior_probs = np.exp(log_posterior_probs)
    
    # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    predictions = np.array(predictions)
    bma_prediction = np.average(predictions, axis=0, 
                                 weights=posterior_probs)
    
    return bma_prediction, posterior_probs

# –ü—Ä–∏–º–µ—Ä
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor

models = [
    (Ridge(alpha=1.0), 0.3),
    (Ridge(alpha=10.0), 0.3),
    (RandomForestRegressor(n_estimators=100), 0.4)
]

bma_pred, model_probs = bayesian_model_averaging(
    models, X_train, y_train, X_test
)

print("Posterior model probabilities:")
for i, prob in enumerate(model_probs):
    print(f"Model {i+1}: {prob:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. Best Practices</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –î–µ–ª–∞—Ç—å</h3>
        <ul>
          <li>–í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å prior –∏ posterior</li>
          <li>–ü—Ä–æ–≤–µ—Ä—è—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å MCMC (R-hat, trace plots)</li>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã –∫–æ–≥–¥–∞ –µ—Å—Ç—å –∑–Ω–∞–Ω–∏—è</li>
          <li>–ö–≤–∞–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å</li>
          <li>–ü—Ä–æ–≤–æ–¥–∏—Ç—å posterior predictive checks</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ò–∑–±–µ–≥–∞—Ç—å</h3>
        <ul>
          <li>–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã–±–æ—Ä –ø—Ä–∏–æ—Ä–∞</li>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MCMC –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏</li>
          <li>–ó–∞–±—ã–≤–∞—Ç—å –ø—Ä–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
          <li>–ü–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—ç–º–ø–ª–æ–≤</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ Python</h2>
    <table>
      <tr><th>–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr>
        <td><strong>PyMC</strong></td>
        <td>–ú–æ—â–Ω–∞—è, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è</td>
        <td>–û–±—â–µ–µ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ</td>
      </tr>
      <tr>
        <td><strong>Stan (PyStan)</strong></td>
        <td>HMC/NUTS, –±—ã—Å—Ç—Ä—ã–π</td>
        <td>–°–ª–æ–∂–Ω—ã–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏</td>
      </tr>
      <tr>
        <td><strong>TensorFlow Probability</strong></td>
        <td>–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å TF</td>
        <td>–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏</td>
      </tr>
      <tr>
        <td><strong>Pyro</strong></td>
        <td>–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ PyTorch</td>
        <td>–ì–ª—É–±–æ–∫–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏</td>
      </tr>
      <tr>
        <td><strong>scikit-learn</strong></td>
        <td>–ü—Ä–æ—Å—Ç—ã–µ –º–µ—Ç–æ–¥—ã</td>
        <td>Naive Bayes, GP</td>
      </tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 17. –ß–∞—Å—Ç–æ—Ç–Ω—ã–π vs –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>–ß–∞—Å—Ç–æ—Ç–Ω—ã–π</th><th>–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π</th></tr>
      <tr>
        <td>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</td>
        <td>–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ</td>
        <td>–°–ª—É—á–∞–π–Ω—ã–µ –≤–µ–ª–∏—á–∏–Ω—ã</td>
      </tr>
      <tr>
        <td>–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å</td>
        <td>–ß–∞—Å—Ç–æ—Ç–∞ —Å–æ–±—ã—Ç–∏–π</td>
        <td>–°—Ç–µ–ø–µ–Ω—å —É–±–µ–∂–¥–µ–Ω–∏—è</td>
      </tr>
      <tr>
        <td>Prior knowledge</td>
        <td>–ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è</td>
        <td>–ß–µ—Ä–µ–∑ prior</td>
      </tr>
      <tr>
        <td>–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å</td>
        <td>Confidence intervals</td>
        <td>Credible intervals</td>
      </tr>
      <tr>
        <td>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</td>
        <td>–°–ª–æ–∂–Ω–µ–µ</td>
        <td>–ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–µ–µ</td>
      </tr>
      <tr>
        <td>–í—ã—á–∏—Å–ª–µ–Ω–∏—è</td>
        <td>–û–±—ã—á–Ω–æ –±—ã—Å—Ç—Ä–µ–µ</td>
        <td>–ú–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ—Ä–æ–≥–æ</td>
      </tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥</h2>
    <ul>
      <li>‚úÖ <strong>–ú–∞–ª–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö</strong>: prior –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</li>
      <li>‚úÖ <strong>–ù—É–∂–Ω–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å</strong>: –ø–æ–ª—É—á–∞–µ–º credible intervals</li>
      <li>‚úÖ <strong>–ï—Å—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è</strong>: –º–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å —á–µ—Ä–µ–∑ prior</li>
      <li>‚úÖ <strong>–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã</strong>: –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ</li>
      <li>‚úÖ <strong>–û–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ</strong>: –ª–µ–≥–∫–æ –æ–±–Ω–æ–≤–ª—è—Ç—å posterior</li>
      <li>‚úÖ <strong>A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ</strong>: —Ç–æ—á–Ω–∞—è –∫–≤–∞–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–∏—Å–∫–æ–≤</li>
      <li>‚ùå <strong>–û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ + –Ω—É–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å</strong>: —á–∞—Å—Ç–æ—Ç–Ω—ã–π –±—ã—Å—Ç—Ä–µ–µ</li>
      <li>‚ùå <strong>–ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏</strong>: –º–æ–∂–µ—Ç –±—ã—Ç—å overkill</li>
    </ul>
  </div>

</div>

</div>
</body>
</html>

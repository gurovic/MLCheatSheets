<!DOCTYPE html><html lang="ru"><head><meta charset="UTF-8"><title>Bayesian Optimization Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title><style>@media screen{body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Helvetica,Arial,sans-serif;color:#333;background:#fafcff;padding:10px}}@media print{body{background:white;padding:0}@page{size:A4 landscape;margin:10mm}}.container{column-count:3;column-gap:20px;max-width:100%}.block{break-inside:avoid;margin-bottom:1.2em;padding:12px;background:white;border-radius:6px;box-shadow:0 1px 3px rgba(0,0,0,0.05)}h1{font-size:1.6em;font-weight:700;color:#1a5fb4;text-align:center;margin:0 0 8px;column-span:all}.subtitle{text-align:center;color:#666;font-size:0.9em;margin-bottom:12px;column-span:all}h2{font-size:1.15em;font-weight:700;color:#1a5fb4;margin:0 0 8px;padding-bottom:4px;border-bottom:1px solid #e0e7ff}p,ul,ol{font-size:0.92em;margin:0.6em 0}ul,ol{padding-left:18px}li{margin-bottom:4px}code{font-family:'Consolas','Courier New',monospace;background-color:#f0f4ff;padding:1px 4px;border-radius:3px;font-size:0.88em}pre{background-color:#f0f4ff;padding:8px;border-radius:4px;overflow-x:auto;font-size:0.84em;margin:6px 0}pre code{padding:0;background:none;white-space:pre-wrap}table{width:100%;border-collapse:collapse;font-size:0.82em;margin:6px 0}th{background-color:#e6f0ff;text-align:left;padding:4px 6px;font-weight:600}td{padding:4px 6px;border-bottom:1px solid #f0f4ff}tr:nth-child(even){background-color:#f8fbff}.good-vs-bad{display:flex;flex-direction:column;gap:8px}.good-vs-bad div{flex:1;padding:6px 8px;border-radius:4px}.good{background-color:#f0f9f4;border-left:3px solid #2e8b57}.bad{background-color:#fdf0f2;border-left:3px solid #d32f2f}.good h3,.bad h3{margin:0 0 4px;font-size:1em;font-weight:700}.good ul,.bad ul{padding-left:20px;margin:0}.good li::before{content:"‚úÖ ";font-weight:bold}.bad li::before{content:"‚ùå ";font-weight:bold}blockquote{font-style:italic;margin:8px 0;padding:6px 10px;background:#f8fbff;border-left:2px solid #1a5fb4;font-size:0.88em}.formula{text-align:center;font-family:'Cambria Math',serif;font-size:1em;margin:10px 0;padding:8px;background:#f8fbff;border-radius:4px}@media print{.container{column-gap:12px}.block{box-shadow:none}code,pre,table{font-size:0.78em}h1{font-size:1.4em}h2{font-size:1em}}</style></head><body><div class="container"><h1>üé≤ Bayesian Optimization</h1><div class="subtitle">üìÖ 3 —è–Ω–≤–∞—Ä—è 2026</div><div class="block"><h2>üî∑ 1. –°—É—Ç—å</h2><ul><li><strong>–î–ª—è –¥–æ—Ä–æ–≥–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π</strong>: –∫–æ–≥–¥–∞ –∫–∞–∂–¥–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–æ—Ä–æ–≥–∞—è</li><li><strong>–°—É—Ä—Ä–æ–≥–∞—Ç–Ω–∞—è –º–æ–¥–µ–ª—å</strong>: Gaussian Process (GP) –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏</li><li><strong>Acquisition function</strong>: –±–∞–ª–∞–Ω—Å exploration/exploitation</li><li><strong>–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å</strong>: –≤—ã–±–æ—Ä —Å–ª–µ–¥—É—é—â–µ–π —Ç–æ—á–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏</li></ul><div class="formula">–ú–∞–∫—Å: f(x), –≥–¥–µ –æ—Ü–µ–Ω–∫–∞ f(x) –¥–æ—Ä–æ–≥–∞—è</div></div><div class="block"><h2>üî∑ 2. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ (scikit-optimize)</h2><pre><code>from skopt import gp_minimize
from skopt.space import Real, Integer

# –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞
space = [
    Real(1e-6, 1e-1, prior='log-uniform', name='learning_rate'),
    Integer(10, 100, name='n_estimators'),
    Integer(3, 10, name='max_depth')
]

# –¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, -accuracy)
def objective(params):
    lr, n_est, depth = params
    
    model = RandomForestClassifier(
        n_estimators=n_est,
        max_depth=depth,
        random_state=42
    )
    model.fit(X_train, y_train)
    score = model.score(X_val, y_val)
    
    return -score  # –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º, –ø–æ—ç—Ç–æ–º—É –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π

# Bayesian Optimization
result = gp_minimize(
    objective,
    space,
    n_calls=50,       # —á–∏—Å–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
    random_state=42,
    verbose=True
)

print(f"Best params: {result.x}")
print(f"Best score: {-result.fun}")</code></pre></div><div class="block"><h2>üî∑ 3. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ (Optuna)</h2><pre><code>import optuna

def objective(trial):
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    lr = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)
    n_est = trial.suggest_int('n_estimators', 10, 100)
    depth = trial.suggest_int('max_depth', 3, 10)
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    model = RandomForestClassifier(
        n_estimators=n_est,
        max_depth=depth,
        random_state=42
    )
    model.fit(X_train, y_train)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–µ—Ç—Ä–∏–∫—É (–º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º)
    return model.score(X_val, y_val)

# –°–æ–∑–¥–∞—Ç—å study
study = optuna.create_study(
    direction='maximize',
    sampler=optuna.samplers.TPESampler()
)

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
study.optimize(objective, n_trials=50)

print(f"Best params: {study.best_params}")
print(f"Best score: {study.best_value}")</code></pre></div><div class="block"><h2>üî∑ 4. –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç</h2><ol><li><strong>–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è</strong>: –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ç–æ—á–µ–∫</li><li><strong>Surrogate Model</strong>: –æ–±—É—á–∏—Ç—å GP –Ω–∞ —Ç–µ–∫—É—â–∏—Ö —Ç–æ—á–∫–∞—Ö</li><li><strong>Acquisition Function</strong>: –Ω–∞–π—Ç–∏ —Å–ª–µ–¥—É—é—â—É—é —Ç–æ—á–∫—É</li><li><strong>–û—Ü–µ–Ω–∫–∞</strong>: –≤—ã—á–∏—Å–ª–∏—Ç—å f(x) –≤ –Ω–æ–≤–æ–π —Ç–æ—á–∫–µ</li><li><strong>–ü–æ–≤—Ç–æ—Ä–∏—Ç—å</strong>: –æ–±–Ω–æ–≤–∏—Ç—å GP –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å</li></ol><p><strong>GP (Gaussian Process)</strong>: –¥–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ + uncertainty</p><p><strong>Acquisition Function</strong>: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞</p></div><div class="block"><h2>üî∑ 5. Acquisition Functions</h2><table><tr><th>–§—É–Ω–∫—Ü–∏—è</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr><tr><td><b>EI (Expected Improvement)</b></td><td>–û–∂–∏–¥–∞–µ–º–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ</td><td>–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é, –±–∞–ª–∞–Ω—Å</td></tr><tr><td><b>PI (Probability of Improvement)</b></td><td>–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è</td><td>–ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π</td></tr><tr><td><b>UCB (Upper Confidence Bound)</b></td><td>–í–µ—Ä—Ö–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ –¥–æ–≤–µ—Ä–∏—è</td><td>–ë–æ–ª—å—à–µ exploration</td></tr><tr><td><b>LCB (Lower Confidence Bound)</b></td><td>–ù–∏–∂–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞</td><td>–ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è</td></tr></table></div><div class="block"><h2>üî∑ 6. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏</h2><table><tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–°–æ–≤–µ—Ç</th></tr><tr><td><code>n_calls</code></td><td>–ß–∏—Å–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π</td><td>50-200, –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –±—é–¥–∂–µ—Ç–∞</td></tr><tr><td><code>n_initial_points</code></td><td>–°–ª—É—á–∞–π–Ω—ã—Ö —Ç–æ—á–µ–∫ –≤–Ω–∞—á–∞–ª–µ</td><td>10-20</td></tr><tr><td><code>acq_func</code></td><td>Acquisition function</td><td>'EI', 'PI', 'gp_hedge'</td></tr><tr><td><code>kappa</code></td><td>Exploration –¥–ª—è UCB</td><td>1.96 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é</td></tr><tr><td><code>xi</code></td><td>Exploration –¥–ª—è EI/PI</td><td>0.01 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é</td></tr></table></div><div class="block"><h2>üî∑ 7. –¢–∏–ø—ã –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ –ø–æ–∏—Å–∫–∞</h2><pre><code>from skopt.space import Real, Integer, Categorical

space = [
    # –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    Real(0.001, 0.1, prior='log-uniform', name='lr'),
    Real(0.1, 0.9, name='dropout'),
    
    # –¶–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    Integer(10, 200, name='n_estimators'),
    Integer(2, 10, name='max_depth'),
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    Categorical(['adam', 'sgd', 'rmsprop'], name='optimizer'),
    Categorical([32, 64, 128, 256], name='batch_size')
]</code></pre></div><div class="block"><h2>üî∑ 8. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–∏–º–µ—Ä —Å CV</h2><pre><code>from skopt import gp_minimize
from sklearn.model_selection import cross_val_score

def objective(params):
    lr, n_est, depth = params
    
    model = RandomForestClassifier(
        n_estimators=n_est,
        max_depth=depth,
        random_state=42
    )
    
    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
    scores = cross_val_score(
        model, X_train, y_train,
        cv=5, scoring='accuracy'
    )
    
    # –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π score
    return -scores.mean()

result = gp_minimize(
    objective,
    space,
    n_calls=50,
    n_initial_points=10,
    acq_func='EI',
    random_state=42
)

print(f"Best: {result.x}, Score: {-result.fun:.4f}")</code></pre></div><div class="block"><h2>üî∑ 9. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤</h2><pre><code>from skopt.plots import plot_convergence, plot_objective
import matplotlib.pyplot as plt

# Convergence plot
plot_convergence(result)
plt.title('Convergence Plot')
plt.show()

# Objective function plot
plot_objective(result)
plt.tight_layout()
plt.show()

# –ò—Å—Ç–æ—Ä–∏—è –ø–æ–∏—Å–∫–∞
import pandas as pd
history = pd.DataFrame({
    'iteration': range(len(result.func_vals)),
    'score': -result.func_vals
})
history['best_so_far'] = history['score'].cummax()

plt.figure(figsize=(10, 6))
plt.plot(history['iteration'], history['score'], 'o', label='Score')
plt.plot(history['iteration'], history['best_so_far'], '-', label='Best so far')
plt.xlabel('Iteration')
plt.ylabel('Score')
plt.legend()
plt.title('Optimization Progress')
plt.show()</code></pre></div><div class="block"><h2>üî∑ 10. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h2><div class="good-vs-bad"><div class="good"><h3>‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</h3><ul><li>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ Grid/Random Search</li><li>–£—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ—à–ª—ã–µ –æ—Ü–µ–Ω–∫–∏</li><li>–•–æ—Ä–æ—à–æ –¥–ª—è –¥–æ—Ä–æ–≥–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π</li><li>–ë–∞–ª–∞–Ω—Å exploration/exploitation</li><li>–ú–µ–Ω—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —Ö–æ—Ä–æ—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞</li></ul></div><div class="bad"><h3>‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h3><ul><li>–ú–µ–¥–ª–µ–Ω–Ω–µ–µ –Ω–∞ –æ–¥–Ω—É –∏—Ç–µ—Ä–∞—Ü–∏—é</li><li>–ü–ª–æ—Ö–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è (>20 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)</li><li>–¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏</li><li>GP –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏</li><li>–ù–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑—É–µ—Ç—Å—è –ª–µ–≥–∫–æ</li></ul></div></div></div><div class="block"><h2>üî∑ 11. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2><div class="good-vs-bad"><div class="good"><h3>‚úÖ –•–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥–∏—Ç</h3><ul><li>–î–æ—Ä–æ–≥–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è (–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏)</li><li>–ú–∞–ª—ã–π –±—é–¥–∂–µ—Ç –∏—Ç–µ—Ä–∞—Ü–∏–π (<200)</li><li>–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ</li><li>2-20 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li><li>–ù—É–∂–µ–Ω –æ–ø—Ç–∏–º—É–º, –Ω–µ –ø—Ä–æ—Å—Ç–æ "—Ö–æ—Ä–æ—à–æ"</li></ul></div><div class="bad"><h3>‚ùå –ü–ª–æ—Ö–æ –ø–æ–¥—Ö–æ–¥–∏—Ç</h3><ul><li>–î–µ—à–µ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Grid Search)</li><li>–ú–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (>20)</li><li>–ù—É–∂–Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è</li><li>–î–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º –∑–Ω–∞—á–µ–Ω–∏–π</li></ul></div></div></div><div class="block"><h2>üî∑ 12. BO vs Grid vs Random Search</h2><table><tr><th>–ú–µ—Ç–æ–¥</th><th>–ò—Ç–µ—Ä–∞—Ü–∏–π</th><th>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr><tr><td>Grid Search</td><td>–ú–Ω–æ–≥–æ</td><td>–ù–∏–∑–∫–∞—è</td><td>–ú–∞–ª–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, –¥–µ—à–µ–≤–æ</td></tr><tr><td>Random Search</td><td>–°—Ä–µ–¥–Ω–µ</td><td>–°—Ä–µ–¥–Ω—è—è</td><td>Baseline, –≤—ã—Å–æ–∫–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏</td></tr><tr><td>Bayesian Opt</td><td>–ú–∞–ª–æ</td><td>–í—ã—Å–æ–∫–∞—è</td><td>–î–æ—Ä–æ–≥–æ, –º–∞–ª–æ–µ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</td></tr></table></div><div class="block"><h2>üî∑ 13. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å XGBoost</h2><pre><code>import xgboost as xgb
from skopt import gp_minimize
from skopt.space import Real, Integer

def objective(params):
    max_depth, learning_rate, n_estimators, subsample = params
    
    model = xgb.XGBClassifier(
        max_depth=max_depth,
        learning_rate=learning_rate,
        n_estimators=n_estimators,
        subsample=subsample,
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss'
    )
    
    model.fit(X_train, y_train)
    score = model.score(X_val, y_val)
    
    return -score

space = [
    Integer(3, 10, name='max_depth'),
    Real(0.01, 0.3, prior='log-uniform', name='learning_rate'),
    Integer(50, 300, name='n_estimators'),
    Real(0.5, 1.0, name='subsample')
]

result = gp_minimize(objective, space, n_calls=50)
print(f"Best params: {result.x}")</code></pre></div><div class="block"><h2>üî∑ 14. –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è</h2><pre><code># Optuna –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—é
import optuna
from joblib import Parallel, delayed

def objective(trial):
    lr = trial.suggest_loguniform('lr', 1e-6, 1e-1)
    # ... –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    return score

# –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
study = optuna.create_study(direction='maximize')

# n_jobs=-1 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ —è–¥—Ä–∞
study.optimize(objective, n_trials=100, n_jobs=-1)

# –ò–ª–∏ —Ä—É—á–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è (–¥–ª—è skopt)
from skopt import Optimizer

opt = Optimizer(space, base_estimator='GP', acq_func='EI')

# –ó–∞–ø—Ä–æ—Å–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ—á–µ–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
x = opt.ask(n_points=4)

# –û—Ü–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
y = Parallel(n_jobs=4)(delayed(objective)(xi) for xi in x)

# –û–±–Ω–æ–≤–∏—Ç—å optimizer
opt.tell(x, y)</code></pre></div><div class="block"><h2>üî∑ 15. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2><ul><li><strong>–ù–∞—á–Ω–∏—Ç–µ —Å –º–∞–ª–æ–≥–æ</strong>: 50-100 –∏—Ç–µ—Ä–∞—Ü–∏–π –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ</li><li><strong>Log-scale –¥–ª—è LR</strong>: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ prior='log-uniform'</li><li><strong>–ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ convergence</strong>: –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫</li><li><strong>–°—Ä–∞–≤–Ω–∏—Ç–µ —Å Random</strong>: baseline –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª—å–∑—ã</li><li><strong>Warmstart</strong>: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–æ—à–ª—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã</li><li><strong>Optuna –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞</strong>: –ª—É—á—à–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∏ DB</li></ul></div><div class="block"><h2>üî∑ 16. –ß–µ–∫-–ª–∏—Å—Ç</h2><ul><li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ü–µ–ª–µ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é (objective)</li><li>[ ] –ó–∞–¥–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞</li><li>[ ] –í—ã–±—Ä–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É (skopt/Optuna)</li><li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –±—é–¥–∂–µ—Ç –∏—Ç–µ—Ä–∞—Ü–∏–π</li><li>[ ] –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é</li><li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å convergence</li><li>[ ] –û—Ü–µ–Ω–∏—Ç—å –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ test</li><li>[ ] –°—Ä–∞–≤–Ω–∏—Ç—å —Å Random Search</li></ul><blockquote>¬´Bayesian Optimization ‚Äî –∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–æ–≥–¥–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –¥–æ—Ä–æ–≥–∏. –ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º—É–º –≤ 5-10 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ —á–µ–º Random Search¬ª.</blockquote></div><div class="block"><h2>üîó –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏</h2><ul><li><a href="https://scikit-optimize.github.io/" target="_blank">üìö scikit-optimize Documentation</a></li><li><a href="https://optuna.org/" target="_blank">üìö Optuna Documentation</a></li><li><a href="https://arxiv.org/abs/1807.02811" target="_blank">üìÑ Practical Bayesian Optimization Paper</a></li><li><a href="https://distill.pub/2020/bayesian-optimization/" target="_blank">üìñ Visual Guide to Bayesian Optimization</a></li></ul></div></div></body></html>

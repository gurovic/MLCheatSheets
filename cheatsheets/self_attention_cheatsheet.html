<!DOCTYPE html>
<html lang="ru">
<head><meta charset="UTF-8"><title>Attention Mechanism Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
<style>@media screen{body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Helvetica,Arial,sans-serif;color:#333;background:#fafcff;padding:10px}}@media print{body{background:white;padding:0}@page{size:A4 landscape;margin:10mm}}.container{column-count:3;column-gap:20px;max-width:100%}.block{break-inside:avoid;margin-bottom:1.2em;padding:12px;background:white;border-radius:6px;box-shadow:0 1px 3px rgba(0,0,0,0.05)}h1{font-size:1.6em;font-weight:700;color:#1a5fb4;text-align:center;margin:0 0 8px;column-span:all}.subtitle{text-align:center;color:#666;font-size:0.9em;margin-bottom:12px;column-span:all}h2{font-size:1.15em;font-weight:700;color:#1a5fb4;margin:0 0 8px;padding-bottom:4px;border-bottom:1px solid #e0e7ff}p,ul,ol{font-size:0.92em;margin:0.6em 0}ul,ol{padding-left:18px}li{margin-bottom:4px}code{font-family:'Consolas','Courier New',monospace;background-color:#f0f4ff;padding:1px 4px;border-radius:3px;font-size:0.88em}pre{background-color:#f0f4ff;padding:8px;border-radius:4px;overflow-x:auto;font-size:0.84em;margin:6px 0}pre code{padding:0;background:none;white-space:pre-wrap}table{width:100%;border-collapse:collapse;font-size:0.82em;margin:6px 0}th{background-color:#e6f0ff;text-align:left;padding:4px 6px;font-weight:600}td{padding:4px 6px;border-bottom:1px solid #f0f4ff}tr:nth-child(even){background-color:#f8fbff}blockquote{font-style:italic;margin:8px 0;padding:6px 10px;background:#f8fbff;border-left:2px solid #1a5fb4;font-size:0.88em}</style>
</head>
<body>
<div class="container">
<h1>üéØ Attention Mechanism Cheatsheet</h1>
<div class="subtitle">–î–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö ‚Ä¢ –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è ‚Ä¢ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ñ–æ–∫—É—Å<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

<div class="block"><h2>üî∑ 1. –°—É—Ç—å Attention</h2><ul><li><strong>–ü—Ä–æ–±–ª–µ–º–∞ RNN</strong>: —Å–ª–æ–∂–Ω–æ –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</li><li><strong>–ò–¥–µ—è</strong>: –º–æ–¥–µ–ª—å "–æ–±—Ä–∞—â–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ" –Ω–∞ –≤–∞–∂–Ω—ã–µ —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–∞</li><li><strong>–ú–µ—Ö–∞–Ω–∏–∑–º</strong>: –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –≤—Ö–æ–¥–æ–≤</li><li><strong>–í–µ—Å–∞</strong>: –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏</li><li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: NLP, CV, Speech</li><li><strong>–û—Å–Ω–æ–≤–∞</strong>: Transformers (BERT, GPT)</li></ul></div>

<div class="block"><h2>üî∑ 2. –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)</h2><p><strong>Query, Key, Value</strong>:</p><ul><li><strong>Query (Q)</strong>: —á—Ç–æ –∏—â–µ–º</li><li><strong>Key (K)</strong>: —Å —á–µ–º —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º</li><li><strong>Value (V)</strong>: —á—Ç–æ –∏–∑–≤–ª–µ–∫–∞–µ–º</li></ul><p><strong>–§–æ—Ä–º—É–ª–∞</strong>:</p><p>Attention(Q, K, V) = softmax(QK·µÄ / ‚àöd‚Çñ) ¬∑ V</p></div>

<div class="block"><h2>üî∑ 3. –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è</h2><pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k):
        super().__init__()
        self.d_k = d_k
    
    def forward(self, Q, K, V, mask=None):
        # Q, K, V: (batch, seq_len, d_k)
        
        # 1. Dot product
        scores = torch.matmul(Q, K.transpose(-2, -1))
        
        # 2. Scale
        scores = scores / (self.d_k ** 0.5)
        
        # 3. Mask (optional)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # 4. Softmax
        attention_weights = F.softmax(scores, dim=-1)
        
        # 5. Multiply by V
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights</code></pre></div>

<div class="block"><h2>üî∑ 4. Self-Attention</h2><p><strong>–ö–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –≤–Ω–∏–º–∞–Ω–∏—è –∫ –æ—Å—Ç–∞–ª—å–Ω—ã–º</strong></p><pre><code>class SelfAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.d_model = d_model
        
        # –ü—Ä–æ–µ–∫—Ü–∏–∏ –¥–ª—è Q, K, V
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        
        self.attention = ScaledDotProductAttention(d_model)
    
    def forward(self, x):
        # x: (batch, seq_len, d_model)
        
        Q = self.W_q(x)
        K = self.W_k(x)
        V = self.W_v(x)
        
        output, weights = self.attention(Q, K, V)
        
        return output, weights</code></pre></div>

<div class="block"><h2>üî∑ 5. Multi-Head Attention</h2><p><strong>–ù–µ—Å–∫–æ–ª—å–∫–æ attention –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ</strong></p><pre><code>class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def split_heads(self, x):
        batch_size = x.size(0)
        x = x.view(batch_size, -1, self.num_heads, self.d_k)
        return x.transpose(1, 2)
    
    def forward(self, Q, K, V, mask=None):
        Q = self.split_heads(self.W_q(Q))
        K = self.split_heads(self.W_k(K))
        V = self.split_heads(self.W_v(V))
        
        # Attention –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention = F.softmax(scores, dim=-1)
        output = torch.matmul(attention, V)
        
        # Concat heads
        output = output.transpose(1, 2).contiguous()
        output = output.view(output.size(0), -1, self.d_model)
        
        return self.W_o(output)</code></pre></div>

<div class="block"><h2>üî∑ 6. –¢–∏–ø—ã Attention</h2><table><tr><th>–¢–∏–ø</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr><tr><td><strong>Self-Attention</strong></td><td>–≠–ª–µ–º–µ–Ω—Ç—ã –≤–Ω–∏–º–∞—é—Ç –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É</td><td>Transformers, BERT</td></tr><tr><td><strong>Cross-Attention</strong></td><td>–î–≤–µ —Ä–∞–∑–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</td><td>Encoder-Decoder, Translation</td></tr><tr><td><strong>Causal/Masked</strong></td><td>–í–Ω–∏–º–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–æ—à–ª–æ–µ</td><td>GPT, —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏</td></tr><tr><td><strong>Global</strong></td><td>–í–Ω–∏–º–∞–Ω–∏–µ –∫–æ –≤—Å–µ–º —ç–ª–µ–º–µ–Ω—Ç–∞–º</td><td>–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ Transformers</td></tr><tr><td><strong>Local</strong></td><td>–¢–æ–ª—å–∫–æ –∫ –±–ª–∏–∂–∞–π—à–∏–º</td><td>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ Transformers</td></tr></table></div>

<div class="block"><h2>üî∑ 7. Masked Attention (GPT)</h2><pre><code># –ú–∞—Å–∫–∞ –¥–ª—è causal attention
def create_causal_mask(seq_len):
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    mask = mask.masked_fill(mask == 1, -float('inf'))
    return mask

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
mask = create_causal_mask(seq_len)
scores = scores + mask  # Broadcasting</code></pre></div>

<div class="block"><h2>üî∑ 8. –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ</h2><p><strong>Attention –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–∏</strong></p><pre><code>class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * 
            -(math.log(10000.0) / d_model)
        )
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + self.pe[:x.size(1), :]
        return x</code></pre></div>

<div class="block"><h2>üî∑ 9. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Attention</h2><ul><li><strong>–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è</strong>: –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç RNN</li><li><strong>–î–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</strong>: –ª—é–±–∞—è –ø–æ–∑–∏—Ü–∏—è –∫ –ª—é–±–æ–π</li><li><strong>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å</strong>: –≤–µ—Å–∞ attention –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å</li><li><strong>–ì–∏–±–∫–æ—Å—Ç—å</strong>: —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π</li><li><strong>–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å</strong>: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π</li></ul></div>

<div class="block"><h2>üî∑ 10. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2><ul><li><strong>NLP</strong>: BERT, GPT, T5 ‚Äî –≤—Å–µ –Ω–∞ Transformers</li><li><strong>Machine Translation</strong>: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</li><li><strong>Computer Vision</strong>: Vision Transformers (ViT)</li><li><strong>Speech Recognition</strong>: Whisper, Wav2Vec2</li><li><strong>Multimodal</strong>: CLIP (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è + —Ç–µ–∫—Å—Ç)</li></ul></div>

<div class="block"><h2>üî∑ 11. Cross-Attention –≤ Encoder-Decoder</h2><pre><code># Encoder output ‚Üí Keys & Values
# Decoder state ‚Üí Query

class EncoderDecoderAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.mha = MultiHeadAttention(d_model, num_heads=8)
    
    def forward(self, decoder_state, encoder_output):
        # Query from decoder
        Q = decoder_state
        
        # Keys and Values from encoder
        K = encoder_output
        V = encoder_output
        
        output = self.mha(Q, K, V)
        return output</code></pre></div>

<div class="block"><h2>üî∑ 12. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã</h2><ul><li><strong>Linear Attention</strong>: O(n) –≤–º–µ—Å—Ç–æ O(n¬≤)</li><li><strong>Sparse Attention</strong>: –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤—É</li><li><strong>Longformer</strong>: sliding window + global</li><li><strong>Reformer</strong>: LSH attention</li><li><strong>Performer</strong>: kernel approximation</li></ul></div>

<div class="block"><h2>üî∑ 13. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏</h2><ul><li><strong>–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ</strong>: –¥–µ–ª–∏—Ç—å –Ω–∞ ‚àöd_k –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!</li><li><strong>Multi-head</strong>: 8 –∏–ª–∏ 16 –≥–æ–ª–æ–≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç</li><li><strong>Dropout</strong>: –Ω–∞ attention weights (0.1)</li><li><strong>Residual</strong>: Add & Norm –ø–æ—Å–ª–µ attention</li><li><strong>–ü–æ–∑–∏—Ü–∏–∏</strong>: –¥–æ–±–∞–≤–ª—è—Ç—å positional encoding</li></ul></div>

<div class="block"><h2>üî∑ 14. –ß–µ–∫-–ª–∏—Å—Ç</h2><ul><li>[ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å Q, K, V –ø—Ä–æ–µ–∫—Ü–∏–∏</li><li>[ ] –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å scores –Ω–∞ ‚àöd_k</li><li>[ ] –ü—Ä–∏–º–µ–Ω–∏—Ç—å softmax</li><li>[ ] –î–ª—è decoder ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Å–∫—É</li><li>[ ] Multi-head –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞</li><li>[ ] –î–æ–±–∞–≤–∏—Ç—å positional encoding</li><li>[ ] Residual connections + LayerNorm</li><li>[ ] Dropout –Ω–∞ attention weights</li></ul><blockquote>¬´Attention ‚Äî —ç—Ç–æ –º–µ—Ö–∞–Ω–∏–∑–º –≤—ã–±–æ—Ä–æ—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è: –º–æ–¥–µ–ª—å —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏ —Ä–µ—à–∞–µ—Ç, –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –≤–∞–∂–Ω—ã –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–æ–≤–∞. –ö–∞–∫ –∫–æ–≥–¥–∞ –º—ã —á–∏—Ç–∞–µ–º ‚Äî —Ñ–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö, –∞ –Ω–µ –Ω–∞ –∫–∞–∂–¥–æ–º –æ–¥–∏–Ω–∞–∫–æ–≤–æ¬ª.</blockquote></div>

</div>
</body>
</html>

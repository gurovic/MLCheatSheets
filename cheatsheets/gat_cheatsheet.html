<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Graph Attention Networks (GAT) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen { body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; color: #333; background: #fafcff; padding: 10px; } }
    @media print { body { background: white; padding: 0; } @page { size: A4 landscape; margin: 10mm; } }
    .container { column-count: 3; column-gap: 20px; max-width: 100%; }
    .block { break-inside: avoid; margin-bottom: 1.2em; padding: 12px; background: white; border-radius: 6px; box-shadow: 0 1px 3px rgba(0,0,0,0.05); }
    h1 { font-size: 1.6em; font-weight: 700; color: #1a5fb4; text-align: center; margin: 0 0 8px; column-span: all; }
    .subtitle { text-align: center; color: #666; font-size: 0.9em; margin-bottom: 12px; column-span: all; }
    h2 { font-size: 1.15em; font-weight: 700; color: #1a5fb4; margin: 0 0 8px; padding-bottom: 4px; border-bottom: 1px solid #e0e7ff; }
    p, ul, ol { font-size: 0.92em; margin: 0.6em 0; }
    ul, ol { padding-left: 18px; }
    li { margin-bottom: 4px; }
    code { font-family: 'Consolas', 'Courier New', monospace; background-color: #f0f4ff; padding: 1px 4px; border-radius: 3px; font-size: 0.88em; }
    pre { background-color: #f0f4ff; padding: 8px; border-radius: 4px; overflow-x: auto; font-size: 0.84em; margin: 6px 0; }
    pre code { padding: 0; background: none; white-space: pre-wrap; }
    table { width: 100%; border-collapse: collapse; font-size: 0.82em; margin: 6px 0; }
    th { background-color: #e6f0ff; text-align: left; padding: 4px 6px; font-weight: 600; }
    td { padding: 4px 6px; border-bottom: 1px solid #f0f4ff; }
    tr:nth-child(even) { background-color: #f8fbff; }
    .good-vs-bad { display: flex; flex-direction: column; gap: 8px; }
    .good-vs-bad div { flex: 1; padding: 6px 8px; border-radius: 4px; }
    .good { background-color: #f0f9f4; border-left: 3px solid #2e8b57; }
    .bad { background-color: #fdf0f2; border-left: 3px solid #d32f2f; }
    .good h3, .bad h3 { margin: 0 0 4px; font-size: 1em; font-weight: 700; }
    .good ul, .bad ul { padding-left: 20px; margin: 0; }
    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }
    blockquote { font-style: italic; margin: 8px 0; padding: 6px 10px; background: #f8fbff; border-left: 2px solid #1a5fb4; font-size: 0.88em; }
    @media print { .container { column-gap: 12px; } .block { box-shadow: none; } code, pre, table { font-size: 0.78em; } h1 { font-size: 1.4em; } h2 { font-size: 1em; } }
  </style>
</head>
<body>
<div class="container">
  <h1>üî≠ Graph Attention Networks (GAT)</h1>
  <div class="subtitle">üìÖ 4 —è–Ω–≤–∞—Ä—è 2026</div>
  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å GAT</h2>
    <ul>
      <li><strong>–ü—Ä–æ–±–ª–µ–º–∞ GCN</strong>: –≤—Å–µ —Å–æ—Å–µ–¥–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ –≤–∞–∂–Ω—ã</li>
      <li><strong>–†–µ—à–µ–Ω–∏–µ</strong>: attention –º–µ—Ö–∞–Ω–∏–∑–º ‚Äî –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Å–æ—Å–µ–¥–µ–π</li>
      <li><strong>Adaptive</strong>: –≤–µ—Å–∞ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —É–∑–ª–æ–≤</li>
      <li><strong>Multi-head attention</strong>: –Ω–µ—Å–∫–æ–ª—å–∫–æ attention heads</li>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ</strong>: —Ä–∞–∑–Ω—ã–µ —Å–æ—Å–µ–¥–∏ –ø–æ–ª—É—á–∞—é—Ç —Ä–∞–∑–Ω—ã–µ –≤–µ—Å–∞</li>
    </ul>
  <div class="block">
    <h2>üî∑ 2. –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ GAT</h2>
    <p><strong>Attention coefficients:</strong></p>
    <p>e<sub>ij</sub> = LeakyReLU(a<sup>T</sup>[W h<sub>i</sub> || W h<sub>j</sub>])</p>
    <p><strong>Normalization (softmax):</strong></p>
    <p>Œ±<sub>ij</sub> = exp(e<sub>ij</sub>) / Œ£<sub>k‚ààN<sub>i</sub></sub> exp(e<sub>ik</sub>)</p>
    <p><strong>Aggregation:</strong></p>
    <p>h'<sub>i</sub> = œÉ(Œ£<sub>j‚ààN<sub>i</sub></sub> Œ±<sub>ij</sub> W h<sub>j</sub>)</p>
    <p>–≥–¥–µ:</p>
    <ul>
      <li>h<sub>i</sub> - –ø—Ä–∏–∑–Ω–∞–∫–∏ —É–∑–ª–∞ i</li>
      <li>W - –≤–µ—Å–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è</li>
      <li>a - attention –≤–µ–∫—Ç–æ—Ä</li>
      <li>|| - –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è</li>
    </ul>
  </div>
  <div class="block">
    <h2>üî∑ 3. Intuition</h2>
    <p><strong>–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:</strong></p>
    <ol>
      <li>–õ–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: Wh</li>
      <li>–í—ã—á–∏—Å–ª–∏—Ç—å attention –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã (i,j)</li>
      <li>–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å attention –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã</li>
      <li>–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å —Å –≤–µ—Å–∞–º–∏ attention</li>
    </ol>
    <p><strong>–û—Ç–ª–∏—á–∏–µ –æ—Ç GCN:</strong></p>
    <ul>
      <li>GCN: —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ (1/‚àö(d<sub>i</sub>d<sub>j</sub>))</li>
      <li>GAT: –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ (–∑–∞–≤–∏—Å—è—Ç –æ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)</li>
    </ul>
  </div>
  <div class="block">
    <h2>üî∑ 4. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ PyTorch</h2>
    <pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

class GATLayer(nn.Module):
    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2):
        super().__init__()
        self.W = nn.Parameter(torch.zeros(in_features, out_features))
        self.a = nn.Parameter(torch.zeros(2*out_features, 1))
        self.dropout = dropout
        self.leakyrelu = nn.LeakyReLU(alpha)
        nn.init.xavier_uniform_(self.W.data)
        nn.init.xavier_uniform_(self.a.data)
    
    def forward(self, h, adj):
        # h: [N, in_features], adj: [N, N]
        Wh = torch.mm(h, self.W)  # [N, out_features]
        
        # Attention mechanism
        N = Wh.size(0)
        a_input = torch.cat([
            Wh.repeat(1, N).view(N*N, -1),
            Wh.repeat(N, 1)
        ], dim=1).view(N, N, -1)  # [N, N, 2*out_features]
        
        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))
        
        # Mask with adjacency
        zero_vec = -9e15*torch.ones_like(e)
        attention = torch.where(adj > 0, e, zero_vec)
        attention = F.softmax(attention, dim=1)
        attention = F.dropout(attention, self.dropout, training=self.training)
        
        # Aggregation
        h_prime = torch.matmul(attention, Wh)
        return h_prime</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 5. Multi-head Attention</h2>
    <pre><code>class MultiHeadGATLayer(nn.Module):
    def __init__(self, in_features, out_features, num_heads, merge='cat'):
        super().__init__()
        self.heads = nn.ModuleList([
            GATLayer(in_features, out_features)
            for _ in range(num_heads)
        ])
        self.merge = merge
    
    def forward(self, h, adj):
        head_outs = [head(h, adj) for head in self.heads]
        
        if self.merge == 'cat':
            return torch.cat(head_outs, dim=1)
        else:  # mean
            return torch.mean(torch.stack(head_outs), dim=0)

# GAT —Å multi-head
class GAT(nn.Module):
    def __init__(self, nfeat, nhid, nclass, nheads):
        super().__init__()
        self.gat1 = MultiHeadGATLayer(nfeat, nhid, nheads, merge='cat')
        self.gat2 = MultiHeadGATLayer(nhid*nheads, nclass, 1, merge='mean')
    
    def forward(self, x, adj):
        x = F.elu(self.gat1(x, adj))
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.gat2(x, adj)
        return F.log_softmax(x, dim=1)</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 6. PyTorch Geometric</h2>
    <pre><code>from torch_geometric.nn import GATConv

class GAT(torch.nn.Module):
    def __init__(self, num_features, num_classes, hidden=8, heads=8):
        super().__init__()
        self.conv1 = GATConv(
            num_features,
            hidden,
            heads=heads,
            dropout=0.6
        )
        self.conv2 = GATConv(
            hidden * heads,
            num_classes,
            heads=1,
            concat=False,
            dropout=0.6
        )
    
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        
        x = F.dropout(x, p=0.6, training=self.training)
        x = F.elu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.conv2(x, edge_index)
        
        return F.log_softmax(x, dim=1)</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 7. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏</h2>
    <table>
      <tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–ó–Ω–∞—á–µ–Ω–∏–µ</th><th>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π</th></tr>
      <tr><td><code>num_heads</code></td><td>8</td><td>–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ</td></tr>
      <tr><td><code>hidden_size</code></td><td>8</td><td>–ù–∞ head, –∏—Ç–æ–≥–æ 64</td></tr>
      <tr><td><code>dropout</code></td><td>0.6</td><td>–í—ã—Å–æ–∫–∏–π dropout –≤–∞–∂–µ–Ω</td></tr>
      <tr><td><code>alpha</code></td><td>0.2</td><td>LeakyReLU slope</td></tr>
      <tr><td><code>num_layers</code></td><td>2</td><td>–û–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ</td></tr>
      <tr><td><code>merge</code></td><td>'cat' / 'mean'</td><td>–ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –∏–ª–∏ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ heads</td></tr>
    </table>
  </div>
  <div class="block">
    <h2>üî∑ 8. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ GAT</h2>
    <ul>
      <li><strong>–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å</strong>: —Ä–∞–∑–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–æ—Å–µ–¥–µ–π</li>
      <li><strong>Interpretable</strong>: attention –≤–µ—Å–∞ –º–æ–∂–Ω–æ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å</li>
      <li><strong>Inductive</strong>: —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ unseen graphs</li>
      <li><strong>–ù–µ—Ç —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</strong>: –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø–æ–ª–Ω—ã–π –≥—Ä–∞—Ñ</li>
      <li><strong>–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è</strong>: attention –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ</li>
    </ul>
  </div>
  <div class="block">
    <h2>üî∑ 9. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è attention</h2>
    <pre><code>import networkx as nx
import matplotlib.pyplot as plt

def visualize_attention(G, attention_weights, node_features):
    pos = nx.spring_layout(G)
    
    # –†–∞–∑–º–µ—Ä —É–∑–ª–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
    node_sizes = [attention_weights[i].sum() * 300 for i in G.nodes()]
    
    # –†–∏—Å–æ–≤–∞—Ç—å —Ä–µ–±—Ä–∞ —Å —Ç–æ–ª—â–∏–Ω–æ–π –ø–æ attention
    for (u, v) in G.edges():
        alpha = attention_weights[u, v].item()
        nx.draw_networkx_edges(
            G, pos,
            [(u, v)],
            width=alpha*5,
            alpha=alpha,
            edge_color='gray'
        )
    
    nx.draw_networkx_nodes(
        G, pos,
        node_size=node_sizes,
        node_color=range(len(G.nodes())),
        cmap='viridis'
    )
    nx.draw_networkx_labels(G, pos)
    plt.show()</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 10. GAT vs GCN</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>GCN</th><th>GAT</th></tr>
      <tr><td>–í–µ—Å–∞</td><td>–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ (–ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ)</td><td>–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ (learned)</td></tr>
      <tr><td>Complexity</td><td>O(|E|d¬≤)</td><td>O(|E|d¬≤ + |V|d)</td></tr>
      <tr><td>Inductive</td><td>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ</td><td>–î–∞</td></tr>
      <tr><td>Interpretability</td><td>–ù–∏–∑–∫–∞—è</td><td>–í—ã—Å–æ–∫–∞—è (attention)</td></tr>
      <tr><td>Training time</td><td>–ë—ã—Å—Ç—Ä–µ–µ</td><td>–ú–µ–¥–ª–µ–Ω–Ω–µ–µ (attention)</td></tr>
      <tr><td>Performance</td><td>–•–æ—Ä–æ—à–æ</td><td>–ß–∞—Å—Ç–æ –ª—É—á—à–µ</td></tr>
    </table>
  </div>
  <div class="block">
    <h2>üî∑ 11. DGL —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è</h2>
    <pre><code>import dgl
import dgl.nn as dglnn

class GAT(nn.Module):
    def __init__(self, in_feats, h_feats, num_classes, num_heads=8):
        super().__init__()
        self.conv1 = dglnn.GATConv(in_feats, h_feats, num_heads)
        self.conv2 = dglnn.GATConv(h_feats*num_heads, num_classes, 1)
    
    def forward(self, g, features):
        h = self.conv1(g, features)
        h = h.flatten(1)  # concatenate heads
        h = F.elu(h)
        h = self.conv2(g, h)
        h = h.mean(1)  # average heads
        return h

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
g = dgl.graph(([0, 1, 2, 3], [1, 2, 3, 0]))
g.ndata['feat'] = torch.randn(4, 10)
model = GAT(10, 8, 3)
logits = model(g, g.ndata['feat'])</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 12. –í–∞—Ä–∏–∞–Ω—Ç—ã GAT</h2>
    <ul>
      <li><strong>GATv2</strong>: —É–ª—É—á—à–µ–Ω–Ω–∞—è attention —Ñ—É–Ω–∫—Ü–∏—è</li>
      <li><strong>SuperGAT</strong>: attention supervision</li>
      <li><strong>HAN</strong>: Heterogeneous attention (—Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã —É–∑–ª–æ–≤/—Ä–µ–±–µ—Ä)</li>
      <li><strong>Transformer-like GAT</strong>: –¥–æ–±–∞–≤–∏—Ç—å key-query-value</li>
    </ul>
    <pre><code># GATv2 (PyTorch Geometric)
from torch_geometric.nn import GATv2Conv

conv = GATv2Conv(
    in_channels,
    out_channels,
    heads=8,
    concat=True
)</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 13. Tricks –¥–ª—è –æ–±—É—á–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>High dropout</strong>: 0.6 –Ω–∞ —Å–ª–æ—è—Ö –∏ attention</li>
      <li><strong>Layer normalization</strong>: –ø–µ—Ä–µ–¥ attention</li>
      <li><strong>Residual connections</strong>: –¥–ª—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π</li>
      <li><strong>Attention dropout</strong>: –æ—Ç–¥–µ–ª—å–Ω—ã–π dropout –¥–ª—è attention</li>
      <li><strong>Edge features</strong>: –¥–æ–±–∞–≤–∏—Ç—å –≤ attention computation</li>
    </ul>
    <pre><code>class GATWithResidual(nn.Module):
    def forward(self, x, adj):
        h = self.gat1(x, adj)
        h = h + x  # residual
        h = F.layer_norm(h, h.shape)
        return h</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 14. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <table>
      <tr><th>–û–±–ª–∞—Å—Ç—å</th><th>–ó–∞–¥–∞—á–∞</th></tr>
      <tr><td><strong>NLP</strong></td><td>–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥, co-reference resolution</td></tr>
      <tr><td><strong>Chemistry</strong></td><td>–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–≤–æ–π—Å—Ç–≤ –º–æ–ª–µ–∫—É–ª</td></tr>
      <tr><td><strong>Social Networks</strong></td><td>Influence propagation, community detection</td></tr>
      <tr><td><strong>Recommendation</strong></td><td>User-item –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏–∏</td></tr>
      <tr><td><strong>Traffic</strong></td><td>–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç–∏ –¥–æ—Ä–æ–≥</td></tr>
    </table>
  </div>
  <div class="block">
    <h2>üî∑ 15. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GAT</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –•–æ—Ä–æ—à–æ –¥–ª—è</h3>
        <ul>
          <li>–ù—É–∂–Ω–∞ interpretability (attention –≤–µ—Å–∞)</li>
          <li>–†–∞–∑–Ω—ã–µ —Å–æ—Å–µ–¥–∏ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—É—é –≤–∞–∂–Ω–æ—Å—Ç—å</li>
          <li>Inductive learning (–Ω–æ–≤—ã–µ —É–∑–ª—ã)</li>
          <li>Heterogeneous graphs</li>
          <li>–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞—Ñ—ã</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –û—Å—Ç–æ—Ä–æ–∂–Ω–æ</h3>
        <ul>
          <li>–û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –≥—Ä–∞—Ñ—ã (–º–µ–¥–ª–µ–Ω–Ω–µ–µ GCN)</li>
          <li>–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö (overfit —Ä–∏—Å–∫)</li>
          <li>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã</li>
          <li>–ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏ (GCN –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)</li>
        </ul>
      </div>
    </div>
  </div>
  <div class="block">
    <h2>üî∑ 16. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å —á–∏—Å–ª–æ attention heads (8 —Å—Ç–∞–Ω–¥–∞—Ä—Ç)</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å high dropout (0.6)</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ELU –∏–ª–∏ LeakyReLU –∞–∫—Ç–∏–≤–∞—Ü–∏—é</li>
      <li>[ ] Concat heads –Ω–∞ –ø–µ—Ä–≤–æ–º —Å–ª–æ–µ, mean –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–º</li>
      <li>[ ] –î–æ–±–∞–≤–∏—Ç—å residual connections –¥–ª—è >2 —Å–ª–æ–µ–≤</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å attention –≤–µ—Å–∞</li>
      <li>[ ] –°—Ä–∞–≤–Ω–∏—Ç—å —Å GCN baseline</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ overfitting</li>
    </ul>
    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´GAT ‚Äî —ç—Ç–æ —É–ª—É—á—à–µ–Ω–∏–µ GCN —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è. –ú–æ–¥–µ–ª—å —Å–∞–º–∞ —Ä–µ—à–∞–µ—Ç, –∫–∞–∫–∏–µ —Å–æ—Å–µ–¥–∏ –≤–∞–∂–Ω–µ–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–∑–ª–∞, –≤–º–µ—Å—Ç–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –≤–µ—Å–æ–≤ –¥–ª—è –≤—Å–µ—Ö. –ö–∞–∫ –≤ –∫–æ–º–∞–Ω–¥–µ: –Ω–µ –≤—Å–µ –∫–æ–ª–ª–µ–≥–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ –≤–ª–∏—è—é—Ç –Ω–∞ –≤–∞—à–∏ —Ä–µ—à–µ–Ω–∏—è, GAT —ç—Ç–æ —É—á–∏—Ç—ã–≤–∞–µ—Ç¬ª.
    </blockquote>
  </div>
</div>
</div>
</body>
</html>
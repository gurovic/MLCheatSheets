<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–õ–∏–Ω–µ–π–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üîÑ –õ–∏–Ω–µ–π–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞</h2>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: –Ω–∞—É—á–∏—Ç—å—Å—è —Å–∂–∏–º–∞—Ç—å –∏ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ</li>
      <li><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong>: —ç–Ω–∫–æ–¥–µ—Ä + –¥–µ–∫–æ–¥–µ—Ä</li>
      <li><strong>–û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è</strong>: –≤—ã—Ö–æ–¥ = –≤—Ö–æ–¥</li>
      <li><strong>–õ–∏–Ω–µ–π–Ω—ã–π</strong>: —Ç–æ–ª—å–∫–æ –ª–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, feature extraction</li>
    </ul>
    <p><strong>–ü—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏</strong>: –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä —É—á–∏—Ç—Å—è "—Å–∂–∏–º–∞—Ç—å" –¥–∞–Ω–Ω—ã–µ –≤ –º–µ–Ω—å—à–µ–µ —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∞ –∑–∞—Ç–µ–º –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –∏—Ö –æ–±—Ä–∞—Ç–Ω–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ç–µ—Ä—è–º–∏.</p>

  <div class="block">
    <h2>üî∑ 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</h2>
    <p><strong>–°—Ç—Ä—É–∫—Ç—É—Ä–∞:</strong></p>
    <ol>
      <li><strong>–≠–Ω–∫–æ–¥–µ—Ä</strong>: X (n –ø—Ä–∏–∑–Ω–∞–∫–æ–≤) ‚Üí Z (k –ø—Ä–∏–∑–Ω–∞–∫–æ–≤), –≥–¥–µ k &lt; n</li>
      <li><strong>–õ–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ</strong>: —Å–∂–∞—Ç–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ Z</li>
      <li><strong>–î–µ–∫–æ–¥–µ—Ä</strong>: Z (k –ø—Ä–∏–∑–Ω–∞–∫–æ–≤) ‚Üí X' (n –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)</li>
    </ol>
    <p><strong>–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å:</strong></p>
    <ul>
      <li>MSE = Mean((X - X')¬≤)</li>
      <li>–ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –≤—Ö–æ–¥–æ–º –∏ –≤—ã—Ö–æ–¥–æ–º</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è</h2>
    <pre><code>import numpy as np
from sklearn.preprocessing import StandardScaler

class LinearAutoencoder:
    def __init__(self, n_components):
        self.n_components = n_components
        self.encoder_weights = None
        self.decoder_weights = None
        
    def fit(self, X, epochs=100, lr=0.01):
        n_features = X.shape[1]
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self.encoder_weights = np.random.randn(
            n_features, self.n_components
        ) * 0.01
        self.decoder_weights = np.random.randn(
            self.n_components, n_features
        ) * 0.01
        
        # –û–±—É—á–µ–Ω–∏–µ
        for epoch in range(epochs):
            # Forward pass
            encoded = X @ self.encoder_weights
            decoded = encoded @ self.decoder_weights
            
            # Loss
            loss = np.mean((X - decoded) ** 2)
            
            # Backward pass
            d_decoded = 2 * (decoded - X) / X.shape[0]
            d_decoder = encoded.T @ d_decoded
            d_encoded = d_decoded @ self.decoder_weights.T
            d_encoder = X.T @ d_encoded
            
            # Update
            self.encoder_weights -= lr * d_encoder
            self.decoder_weights -= lr * d_decoder
            
            if epoch % 20 == 0:
                print(f"Epoch {epoch}: Loss = {loss:.4f}")
    
    def encode(self, X):
        return X @ self.encoder_weights
    
    def decode(self, Z):
        return Z @ self.decoder_weights
    
    def reconstruct(self, X):
        return self.decode(self.encode(X))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å PyTorch</h2>
    <pre><code>import torch
import torch.nn as nn
import torch.optim as optim

class LinearAutoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.encoder = nn.Linear(input_dim, latent_dim)
        self.decoder = nn.Linear(latent_dim, input_dim)
        
    def forward(self, x):
        z = self.encoder(x)
        x_reconstructed = self.decoder(z)
        return x_reconstructed
    
    def encode(self, x):
        return self.encoder(x)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
model = LinearAutoencoder(input_dim=784, latent_dim=32)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# –û–±—É—á–µ–Ω–∏–µ
for epoch in range(100):
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, X_train)
    loss.backward()
    optimizer.step()
    
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å Keras</h2>
    <pre><code>from tensorflow import keras
from tensorflow.keras import layers

def build_linear_autoencoder(input_dim, latent_dim):
    # –≠–Ω–∫–æ–¥–µ—Ä
    encoder_input = layers.Input(shape=(input_dim,))
    encoded = layers.Dense(latent_dim)(encoder_input)
    
    encoder = keras.Model(encoder_input, encoded, 
                         name='encoder')
    
    # –î–µ–∫–æ–¥–µ—Ä
    decoder_input = layers.Input(shape=(latent_dim,))
    decoded = layers.Dense(input_dim)(decoder_input)
    
    decoder = keras.Model(decoder_input, decoded, 
                         name='decoder')
    
    # –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å
    autoencoder_input = layers.Input(shape=(input_dim,))
    encoded_repr = encoder(autoencoder_input)
    reconstructed = decoder(encoded_repr)
    
    autoencoder = keras.Model(autoencoder_input, 
                             reconstructed, 
                             name='autoencoder')
    
    return autoencoder, encoder, decoder

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
input_dim = 784  # –Ω–∞–ø—Ä–∏–º–µ—Ä, MNIST
latent_dim = 32

autoencoder, encoder, decoder = build_linear_autoencoder(
    input_dim, latent_dim
)

autoencoder.compile(
    optimizer='adam',
    loss='mse',
    metrics=['mae']
)

history = autoencoder.fit(
    X_train, X_train,
    epochs=50,
    batch_size=256,
    validation_split=0.2,
    verbose=1
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –°–≤—è–∑—å —Å PCA</h2>
    <p><strong>–í–∞–∂–Ω—ã–π —Ñ–∞–∫—Ç:</strong> –õ–∏–Ω–µ–π–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä —Å MSE-—Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ—Ç–µ—Ä—å —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–µ–Ω PCA!</p>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>–õ–∏–Ω–µ–π–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä</th><th>PCA</th></tr>
      <tr><td>–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞</td><td>–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫</td><td>–°–∏–Ω–≥—É–ª—è—Ä–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ (SVD)</td></tr>
      <tr><td>–†–µ–∑—É–ª—å—Ç–∞—Ç</td><td>–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –ª–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è</td><td>–ì–ª–∞–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã</td></tr>
      <tr><td>–°–∫–æ—Ä–æ—Å—Ç—å</td><td>–ú–µ–¥–ª–µ–Ω–Ω–µ–µ (–∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ)</td><td>–ë—ã—Å—Ç—Ä–µ–µ (–ø—Ä—è–º–æ–µ —Ä–µ—à–µ–Ω–∏–µ)</td></tr>
      <tr><td>–ì–∏–±–∫–æ—Å—Ç—å</td><td>–õ–µ–≥–∫–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–æ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–≥–æ</td><td>–¢–æ–ª—å–∫–æ –ª–∏–Ω–µ–π–Ω—ã–π</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å PCA</h2>
    <pre><code>from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA
pca = PCA(n_components=10)
X_pca = pca.fit_transform(X_scaled)
X_pca_reconstructed = pca.inverse_transform(X_pca)
pca_error = np.mean((X_scaled - X_pca_reconstructed) ** 2)

# –õ–∏–Ω–µ–π–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä
ae = LinearAutoencoder(n_components=10)
ae.fit(X_scaled, epochs=100)
X_ae_reconstructed = ae.reconstruct(X_scaled)
ae_error = np.mean((X_scaled - X_ae_reconstructed) ** 2)

print(f"PCA reconstruction error: {pca_error:.4f}")
print(f"AE reconstruction error: {ae_error:.4f}")
# –î–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ!</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏</strong>: –∞–Ω–∞–ª–æ–≥ PCA</li>
      <li><strong>–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</strong>: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è</li>
      <li><strong>–°–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö</strong>: —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –ø—Ä–∏ —Ö—Ä–∞–Ω–µ–Ω–∏–∏</li>
      <li><strong>–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π</strong>: –≤—ã—Å–æ–∫–∞—è –æ—à–∏–±–∫–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏</li>
      <li><strong>–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞</strong>: –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –º–æ–¥–µ–ª–µ–π</li>
      <li><strong>–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è</strong>: –ø—Ä–æ–µ–∫—Ü–∏—è –≤ 2D/3D</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 9. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π</h2>
    <pre><code># –û–±—É—á–∞–µ–º –Ω–∞ "–Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö" –¥–∞–Ω–Ω—ã—Ö
ae = LinearAutoencoder(n_components=10)
ae.fit(X_normal, epochs=100)

# –í—ã—á–∏—Å–ª—è–µ–º –æ—à–∏–±–∫–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
X_reconstructed = ae.reconstruct(X_test)
reconstruction_errors = np.mean(
    (X_test - X_reconstructed) ** 2, 
    axis=1
)

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä–æ–≥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 95-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å)
threshold = np.percentile(reconstruction_errors, 95)

# –ê–Ω–æ–º–∞–ª–∏–∏
anomalies = reconstruction_errors > threshold

print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {anomalies.sum()}")
print(f"–ü–æ—Ä–æ–≥: {threshold:.4f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 4))
plt.hist(reconstruction_errors, bins=50, alpha=0.7)
plt.axvline(threshold, color='red', linestyle='--', 
           label='–ü–æ—Ä–æ–≥')
plt.xlabel('–û—à–∏–±–∫–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏')
plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
plt.legend()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</h2>
    <p>–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è:</p>
    <pre><code># L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤ PyTorch
class RegularizedAutoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.encoder = nn.Linear(input_dim, latent_dim)
        self.decoder = nn.Linear(latent_dim, input_dim)
        
    def forward(self, x):
        z = self.encoder(x)
        x_reconstructed = self.decoder(z)
        return x_reconstructed, z

# –û–±—É—á–µ–Ω–∏–µ —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
model = RegularizedAutoencoder(784, 32)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), 
                      weight_decay=1e-5)  # L2

for epoch in range(100):
    optimizer.zero_grad()
    outputs, latent = model(X_train)
    
    # Reconstruction loss
    recon_loss = criterion(outputs, X_train)
    
    # Sparsity constraint
    sparsity_loss = torch.mean(torch.abs(latent))
    
    # Total loss
    loss = recon_loss + 0.001 * sparsity_loss
    
    loss.backward()
    optimizer.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã</h2>
    <table>
      <tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</th></tr>
      <tr><td><code>latent_dim</code></td><td>–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞</td><td>–ù–∞—á–∞—Ç—å —Å 10-50</td></tr>
      <tr><td><code>learning_rate</code></td><td>–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è</td><td>0.001-0.01</td></tr>
      <tr><td><code>epochs</code></td><td>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö</td><td>50-200</td></tr>
      <tr><td><code>batch_size</code></td><td>–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞</td><td>32-256</td></tr>
      <tr><td><code>weight_decay</code></td><td>L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</td><td>1e-5 - 1e-3</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 12. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</h2>
    <pre><code>import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error

# –û—Ü–µ–Ω–∫–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
X_reconstructed = ae.reconstruct(X_test)

# MSE
mse = mean_squared_error(X_test, X_reconstructed)
print(f"MSE: {mse:.4f}")

# MAE
mae = np.mean(np.abs(X_test - X_reconstructed))
print(f"MAE: {mae:.4f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è MNIST
n_samples = 10
fig, axes = plt.subplots(2, n_samples, 
                         figsize=(15, 3))

for i in range(n_samples):
    # –û—Ä–∏–≥–∏–Ω–∞–ª
    axes[0, i].imshow(X_test[i].reshape(28, 28), 
                     cmap='gray')
    axes[0, i].axis('off')
    
    # –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
    axes[1, i].imshow(
        X_reconstructed[i].reshape(28, 28), 
        cmap='gray'
    )
    axes[1, i].axis('off')

axes[0, 0].set_ylabel('–û—Ä–∏–≥–∏–Ω–∞–ª', size=12)
axes[1, 0].set_ylabel('–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', size=12)
plt.tight_layout()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –•–æ—Ä–æ—à–æ</h3>
        <ul>
          <li>–õ–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ –¥–∞–Ω–Ω—ã—Ö</li>
          <li>–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ</li>
          <li>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –≤–∞–∂–Ω–∞</li>
          <li>–ù–µ–±–æ–ª—å—à–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã</li>
          <li>–ö–∞–∫ baseline –ø–µ—Ä–µ–¥ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º–∏</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ü–ª–æ—Ö–æ</h3>
        <ul>
          <li>–°–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ</li>
          <li>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ª—É—á—à–µ CNN autoencoder)</li>
          <li>–¢–µ–∫—Å—Ç (–ª—É—á—à–µ RNN/Transformer)</li>
          <li>–ù—É–∂–Ω–∞ –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ (StandardScaler)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å —Ä–∞–∑–º–µ—Ä –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—à–∏–±–∫—É —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã</li>
      <li>[ ] –°—Ä–∞–≤–Ω–∏—Ç—å —Å PCA</li>
      <li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é</li>
      <li>[ ] –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ learning rates</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å early stopping</li>
      <li>[ ] –ï—Å–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç ‚Äî –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–π</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–õ–∏–Ω–µ–π–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä ‚Äî —ç—Ç–æ –∫–∞–∫ —É–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö: –æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ "–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è" –≤ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç —á–µ—Ä–µ–∑ –Ω–µ–±–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ –∫–ª—é—á–µ–≤—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫. –ü–æ—Ç–æ–º –æ–Ω –º–æ–∂–µ—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ç–µ—Ä—è–º–∏. –≠—Ç–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–µ–æ–±—ã—á–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

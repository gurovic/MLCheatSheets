<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Distributed Training Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen { body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; color: #333; background: #fafcff; padding: 10px; } }
    @media print { body { background: white; padding: 0; } @page { size: A4 landscape; margin: 10mm; } }
        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }
    .block { break-inside: avoid; margin-bottom: 1.2em; padding: 12px; background: white; border-radius: 6px; box-shadow: 0 1px 3px rgba(0,0,0,0.05); }
    h1 { font-size: 1.6em; font-weight: 700; color: #1a5fb4; text-align: center; margin: 0 0 8px; column-span: all; }
    .subtitle { text-align: center; color: #666; font-size: 0.9em; margin-bottom: 12px; column-span: all; }
    h2 { font-size: 1.15em; font-weight: 700; color: #1a5fb4; margin: 0 0 8px; padding-bottom: 4px; border-bottom: 1px solid #e0e7ff; }
    p, ul, ol { font-size: 0.92em; margin: 0.6em 0; }
    ul, ol { padding-left: 18px; }
    li { margin-bottom: 4px; }
    code { font-family: 'Consolas', 'Courier New', monospace; background-color: #f0f4ff; padding: 1px 4px; border-radius: 3px; font-size: 0.88em; }
    pre { background-color: #f0f4ff; padding: 8px; border-radius: 4px; overflow-x: auto; font-size: 0.84em; margin: 6px 0; }
    pre code { padding: 0; background: none; white-space: pre-wrap; }
    table { width: 100%; border-collapse: collapse; font-size: 0.82em; margin: 6px 0; }
    th { background-color: #e6f0ff; text-align: left; padding: 4px 6px; font-weight: 600; }
    td { padding: 4px 6px; border-bottom: 1px solid #f0f4ff; }
    tr:nth-child(even) { background-color: #f8fbff; }
    .good-vs-bad { display: flex; flex-direction: column; gap: 8px; }
    .good-vs-bad div { flex: 1; padding: 6px 8px; border-radius: 4px; }
    .good { background-color: #f0f9f4; border-left: 3px solid #2e8b57; }
    .bad { background-color: #fdf0f2; border-left: 3px solid #d32f2f; }
    .good h3, .bad h3 { margin: 0 0 4px; font-size: 1em; font-weight: 700; }
    .good ul, .bad ul { padding-left: 20px; margin: 0; }
    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }
    blockquote { font-style: italic; margin: 8px 0; padding: 6px 10px; background: #f8fbff; border-left: 2px solid #1a5fb4; font-size: 0.88em; }
    @media print { .container { column-gap: 12px; } .block { box-shadow: none; } code, pre, table { font-size: 0.78em; } h1 { font-size: 1.4em; } h2 { font-size: 1em; } }
  </style>
</head>
<body>
<div class="container">
  <h1>üöÄ Distributed Training</h1>
  <div class="subtitle">üìÖ 4 —è–Ω–≤–∞—Ä—è 2026</div>
  <div class="block">
    <h2>üî∑ 1. –¢–∏–ø—ã –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞</h2>
    <ul>
      <li><strong>Data Parallelism</strong>: –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å —Ä–µ–ø–ª–∏—Ü–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –≤—Å–µ GPU, –¥–∞–Ω–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è</li>
      <li><strong>Model Parallelism</strong>: –º–æ–¥–µ–ª—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∞ –Ω–∞ —á–∞—Å—Ç–∏, –∫–∞–∂–¥–∞—è –Ω–∞ —Å–≤–æ–µ–º GPU</li>
      <li><strong>Pipeline Parallelism</strong>: —Å–ª–æ–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö GPU, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –ø–∞–π–ø–ª–∞–π–Ω—É</li>
      <li><strong>Tensor Parallelism</strong>: –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –º–µ–∂–¥—É GPU</li>
      <li><strong>Hybrid</strong>: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤</li>
    </ul>
  <div class="block">
    <h2>üî∑ 2. Data Parallelism - PyTorch</h2>
    <pre><code>import torch
import torch.nn as nn
from torch.nn.parallel import DistributedDataParallel as DDP

# 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
torch.distributed.init_process_group(backend='nccl')
local_rank = torch.distributed.get_rank()
torch.cuda.set_device(local_rank)

# 2. –ú–æ–¥–µ–ª—å –Ω–∞ GPU
model = MyModel().cuda()
model = DDP(model, device_ids=[local_rank])

# 3. DataLoader —Å DistributedSampler
sampler = torch.utils.data.distributed.DistributedSampler(
    dataset, 
    num_replicas=world_size,
    rank=local_rank
)
dataloader = DataLoader(dataset, sampler=sampler)

# 4. –û–±—É—á–µ–Ω–∏–µ
for epoch in range(num_epochs):
    sampler.set_epoch(epoch)  # –≤–∞–∂–Ω–æ!
    for batch in dataloader:
        output = model(batch)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 3. –ó–∞–ø—É—Å–∫ distributed training</h2>
    <pre><code># torchrun (PyTorch >= 1.9)
torchrun --nproc_per_node=4 train.py

# –° –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —É–∑–ª–∞–º–∏
torchrun     --nnodes=2     --nproc_per_node=4     --node_rank=0     --master_addr="192.168.1.1"     --master_port=29500     train.py

# –°—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–± (torch.distributed.launch)
python -m torch.distributed.launch     --nproc_per_node=4     train.py</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 4. –ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏</h2>
    <table>
      <tr><th>–û–ø–µ—Ä–∞—Ü–∏—è</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th></tr>
      <tr><td><strong>all_reduce</strong></td><td>–°—É–º–º–∞/—Å—Ä–µ–¥–Ω–µ–µ —Å –≤—Å–µ—Ö GPU ‚Üí –≤—Å–µ–º GPU</td></tr>
      <tr><td><strong>broadcast</strong></td><td>–û–¥–∏–Ω GPU ‚Üí –≤—Å–µ GPU</td></tr>
      <tr><td><strong>gather</strong></td><td>–í—Å–µ GPU ‚Üí –æ–¥–∏–Ω GPU</td></tr>
      <tr><td><strong>scatter</strong></td><td>–û–¥–∏–Ω GPU ‚Üí –≤—Å–µ GPU (—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å)</td></tr>
      <tr><td><strong>all_gather</strong></td><td>–í—Å–µ GPU ‚Üí –≤—Å–µ GPU</td></tr>
      <tr><td><strong>reduce</strong></td><td>–í—Å–µ GPU ‚Üí –æ–¥–∏–Ω GPU (–∞–≥—Ä–µ–≥–∞—Ü–∏—è)</td></tr>
    </table>
    <pre><code># –ü—Ä–∏–º–µ—Ä all_reduce –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.SUM)
tensor /= world_size  # —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 5. Backends —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ</h2>
    <table>
      <tr><th>Backend</th><th>CPU</th><th>GPU</th><th>–°–∫–æ—Ä–æ—Å—Ç—å</th></tr>
      <tr><td><strong>nccl</strong></td><td>‚ùå</td><td>‚úÖ</td><td>–°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –¥–ª—è GPU</td></tr>
      <tr><td><strong>gloo</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π</td></tr>
      <tr><td><strong>mpi</strong></td><td>‚úÖ</td><td>‚úÖ</td><td>–î–ª—è HPC –∫–ª–∞—Å—Ç–µ—Ä–æ–≤</td></tr>
    </table>
    <p><strong>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è</strong>: nccl –¥–ª—è GPU, gloo –¥–ª—è CPU</p>
  </div>
  <div class="block">
    <h2>üî∑ 6. DeepSpeed ZeRO</h2>
    <p><strong>ZeRO (Zero Redundancy Optimizer)</strong>: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏</p>
    <ul>
      <li><strong>Stage 1</strong>: optimizer states –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω—ã</li>
      <li><strong>Stage 2</strong>: + gradients –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω—ã</li>
      <li><strong>Stage 3</strong>: + model parameters –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω—ã</li>
    </ul>
    <pre><code>import deepspeed

ds_config = {
    "train_batch_size": 64,
    "fp16": {"enabled": True},
    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {"device": "cpu"}
    }
}

model, optimizer, _, _ = deepspeed.initialize(
    model=model,
    config=ds_config
)

for batch in dataloader:
    loss = model(batch)
    model.backward(loss)
    model.step()</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 7. Model Parallelism</h2>
    <pre><code>class ModelParallelResNet(nn.Module):
    def __init__(self):
        super().__init__()
        # –ü–µ—Ä–≤–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞ –Ω–∞ GPU 0
        self.seq1 = nn.Sequential(...).to('cuda:0')
        # –í—Ç–æ—Ä–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞ –Ω–∞ GPU 1
        self.seq2 = nn.Sequential(...).to('cuda:1')
    
    def forward(self, x):
        x = x.to('cuda:0')
        x = self.seq1(x)
        # –ü–µ—Ä–µ–Ω–æ—Å –º–µ–∂–¥—É GPU
        x = x.to('cuda:1')
        x = self.seq2(x)
        return x</code></pre>
    <p><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: GPU –ø—Ä–æ—Å—Ç–∞–∏–≤–∞—é—Ç (–æ–¥–∏–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç, –¥—Ä—É–≥–æ–π –∂–¥–µ—Ç)</p>
    <p><strong>–†–µ—à–µ–Ω–∏–µ</strong>: Pipeline Parallelism</p>
  </div>
  <div class="block">
    <h2>üî∑ 8. Pipeline Parallelism</h2>
    <ul>
      <li>–†–∞–∑–¥–µ–ª–∏—Ç—å batch –Ω–∞ micro-batches</li>
      <li>–ö–∞–∂–¥—ã–π micro-batch –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ pipeline</li>
      <li>GPU —Ä–∞–±–æ—Ç–∞—é—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –Ω–∞–¥ —Ä–∞–∑–Ω—ã–º–∏ micro-batches</li>
    </ul>
    <pre><code>from torch.distributed.pipeline.sync import Pipe

model = nn.Sequential(
    nn.Linear(100, 100),  # GPU 0
    nn.ReLU(),
    nn.Linear(100, 100),  # GPU 1
    nn.ReLU(),
    nn.Linear(100, 10)    # GPU 2
)

model = Pipe(model, chunks=8)  # 8 micro-batches
output = model(input)</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 9. Tensor Parallelism (Megatron)</h2>
    <p>–î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π (GPT, BERT)</p>
    <ul>
      <li>Attention heads —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –º–µ–∂–¥—É GPU</li>
      <li>FFN layers —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –º–µ–∂–¥—É GPU</li>
      <li>–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è</li>
    </ul>
    <pre><code># –ü—Å–µ–≤–¥–æ–∫–æ–¥
class TensorParallelAttention:
    def __init__(self, hidden_size, num_heads, tp_size):
        self.heads_per_gpu = num_heads // tp_size
        self.qkv = ColumnParallelLinear(hidden_size, 3*hidden_size)
        self.output = RowParallelLinear(hidden_size, hidden_size)
    
    def forward(self, x):
        qkv = self.qkv(x)  # —Ä–∞–∑–¥–µ–ª–µ–Ω–æ –ø–æ GPU
        attention = self.attention(qkv)
        output = self.output(attention)  # all-reduce –∑–¥–µ—Å—å
        return output</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 10. Gradient Accumulation</h2>
    <pre><code># –°–∏–º—É–ª—è—Ü–∏—è –±–æ–ª—å—à–µ–≥–æ batch size
accumulation_steps = 4
effective_batch_size = batch_size * accumulation_steps * world_size

for i, batch in enumerate(dataloader):
    output = model(batch)
    loss = criterion(output, target) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 11. Hugging Face Accelerate</h2>
    <pre><code>from accelerate import Accelerator

accelerator = Accelerator()

model, optimizer, dataloader = accelerator.prepare(
    model, optimizer, dataloader
)

for batch in dataloader:
    output = model(batch)
    loss = criterion(output, target)
    accelerator.backward(loss)
    optimizer.step()
    optimizer.zero_grad()

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
# - Multi-GPU (DDP)
# - Mixed precision
# - DeepSpeed
# - FSDP</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 12. Fully Sharded Data Parallel (FSDP)</h2>
    <pre><code>from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

model = MyModel()
model = FSDP(
    model,
    auto_wrap_policy=default_auto_wrap_policy,
    mixed_precision=mixed_precision_policy,
    sharding_strategy=ShardingStrategy.FULL_SHARD
)

# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –Ω–∞–¥ DDP:
# - –ú–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ (–∫–∞–∫ DeepSpeed ZeRO-3)
# - –í—Å—Ç—Ä–æ–µ–Ω–æ –≤ PyTorch
# - –•–æ—Ä–æ—à–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 13. –û—Ç–ª–∞–¥–∫–∞ distributed training</h2>
    <table>
      <tr><th>–ü—Ä–æ–±–ª–µ–º–∞</th><th>–†–µ—à–µ–Ω–∏–µ</th></tr>
      <tr><td>–ó–∞–≤–∏—Å–∞–Ω–∏–µ</td><td>–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é, –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –¥–æ–ª–∂–Ω—ã –¥–µ–ª–∞—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ collective operations</td></tr>
      <tr><td>–†–∞–∑–Ω—ã–µ loss –Ω–∞ GPU</td><td>–ü—Ä–æ–≤–µ—Ä–∏—Ç—å seeds, batch size, learning rate scaling</td></tr>
      <tr><td>OOM –Ω–∞ –æ–¥–Ω–æ–º GPU</td><td>–ù–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DistributedSampler</td></tr>
      <tr><td>–ú–µ–¥–ª–µ–Ω–Ω–∞—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è</td><td>–ü—Ä–æ–≤–µ—Ä–∏—Ç—å network, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å gradient accumulation</td></tr>
    </table>
  </div>
  <div class="block">
    <h2>üî∑ 14. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏</h2>
    <ul>
      <li><strong>Gradient bucketing</strong>: –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–±–æ–ª—å—à–∏–µ —Ç–µ–Ω–∑–æ—Ä—ã</li>
      <li><strong>Overlap communication</strong>: –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º–∏</li>
      <li><strong>Compression</strong>: —Å–∂–∏–º–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (PowerSGD)</li>
      <li><strong>Local SGD</strong>: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑ –≤ N —à–∞–≥–æ–≤</li>
    </ul>
    <pre><code># PyTorch DDP —Å gradient bucketing
model = DDP(
    model,
    bucket_cap_mb=25,  # —Ä–∞–∑–º–µ—Ä bucket
    gradient_as_bucket_view=True  # –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
)</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 15. Scaling efficiency</h2>
    <p><strong>Ideal scaling</strong>: 8 GPU = 8x —Å–∫–æ—Ä–æ—Å—Ç—å</p>
    <p><strong>–†–µ–∞–ª—å–Ω–æ—Å—Ç—å</strong>:</p>
    <ul>
      <li>2 GPU: 1.9x</li>
      <li>4 GPU: 3.6x</li>
      <li>8 GPU: 6.8x</li>
      <li>16 GPU: 12x</li>
    </ul>
    <p><strong>–ü—Ä–∏—á–∏–Ω—ã</strong>: communication overhead, synchronization</p>
    <p><strong>–£–ª—É—á—à–∏—Ç—å</strong>: –±–æ–ª—å—à–µ computation per iteration, gradient accumulation</p>
  </div>
  <div class="block">
    <h2>üî∑ 16. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á—Ç–æ</h2>
    <table>
      <tr><th>–°—Ü–µ–Ω–∞—Ä–∏–π</th><th>–ú–µ—Ç–æ–¥</th></tr>
      <tr><td>–ú–æ–¥–µ–ª—å –ø–æ–º–µ—â–∞–µ—Ç—Å—è –Ω–∞ 1 GPU</td><td>Data Parallelism (DDP)</td></tr>
      <tr><td>–ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è</td><td>Model/Pipeline Parallelism</td></tr>
      <tr><td>–û—á–µ–Ω—å –±–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å (GPT)</td><td>Tensor Parallelism + Pipeline</td></tr>
      <tr><td>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ –ø–∞–º—è—Ç—å</td><td>FSDP –∏–ª–∏ DeepSpeed ZeRO-3</td></tr>
      <tr><td>–ú–Ω–æ–≥–æ —É–∑–ª–æ–≤</td><td>Data Parallel + gradient accumulation</td></tr>
    </table>
  </div>
  <div class="block">
    <h2>üî∑ 17. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å —Ç–∏–ø –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å backend (nccl –¥–ª—è GPU)</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DistributedSampler</li>
      <li>[ ] –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å seeds</li>
      <li>[ ] –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å learning rate (lr * world_size)</li>
      <li>[ ] –í–∫–ª—é—á–∏—Ç—å gradient accumulation –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å GPU utilization</li>
      <li>[ ] –ò–∑–º–µ—Ä–∏—Ç—å scaling efficiency</li>
    </ul>
    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´Distributed Training ‚Äî —ç—Ç–æ –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –±–æ–ª—å—à—É—é –∑–∞–¥–∞—á—É –º–µ–∂–¥—É –∫–æ–º–∞–Ω–¥–æ–π. –ö–∞–∂–¥—ã–π GPU —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–¥ —Å–≤–æ–µ–π —á–∞—Å—Ç—å—é –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –º–æ–¥–µ–ª–∏, –ø–æ—Ç–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è. 8 GPU –º–æ–≥—É—Ç –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 6-7 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ –æ–¥–Ω–æ–≥–æ GPU¬ª.
    </blockquote>
  </div>
</div>
</div>
</body>
</html>

<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üé≤ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ</h2>
    <ul>
      <li><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ‚Üí –ø–ª–æ—Ö–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</li>
      <li><strong>Vanishing gradients</strong>: —Å–ª–∏—à–∫–æ–º –º–∞–ª—ã–µ –≤–µ—Å–∞</li>
      <li><strong>Exploding gradients</strong>: —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –≤–µ—Å–∞</li>
      <li><strong>–°–∏–º–º–µ—Ç—Ä–∏—è</strong>: –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –≤–µ—Å–∞ ‚Üí –Ω–µ–π—Ä–æ–Ω—ã —É—á–∞—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ</li>
    </ul>
    <blockquote>
      –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ ‚Äî —ç—Ç–æ –∫–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å—Ç–∞—Ä—Ç –≤ –±–µ–≥–µ: –ø–ª–æ—Ö–æ–π —Å—Ç–∞—Ä—Ç –º–æ–∂–µ—Ç —Å–¥–µ–ª–∞—Ç—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–º –ø–æ–±–µ–¥—É.
    </blockquote>

    </div>
<div class="block">
    <h2>üî∑ 2. –ü–ª–æ—Ö–∏–µ –º–µ—Ç–æ–¥—ã</h2>
    <div class="good-vs-bad">
      <div class="bad">
        <h3>‚ùå –ù—É–ª–µ–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è</h3>
        <ul>
          <li>–í—Å–µ –≤–µ—Å–∞ = 0</li>
          <li>–í—Å–µ –Ω–µ–π—Ä–æ–Ω—ã —É—á–∞—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ</li>
          <li>–ù–µ—Ç –æ–±—É—á–µ–Ω–∏—è, —Å–∏–º–º–µ—Ç—Ä–∏—è</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è</h3>
        <ul>
          <li>w = np.random.randn(n) * 10</li>
          <li>Exploding gradients</li>
          <li>–†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –°–ª–∏—à–∫–æ–º –º–∞–ª—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è</h3>
        <ul>
          <li>w = np.random.randn(n) * 0.0001</li>
          <li>Vanishing gradients</li>
          <li>–ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 3. Xavier/Glorot Initialization</h2>
    <p><strong>–î–ª—è sigmoid –∏ tanh –∞–∫—Ç–∏–≤–∞—Ü–∏–π:</strong></p>
    <pre><code>import numpy as np

# Xavier Uniform
def xavier_uniform(n_in, n_out):
    limit = np.sqrt(6.0 / (n_in + n_out))
    return np.random.uniform(-limit, limit, (n_in, n_out))

# Xavier Normal
def xavier_normal(n_in, n_out):
    std = np.sqrt(2.0 / (n_in + n_out))
    return np.random.randn(n_in, n_out) * std

# Keras/TensorFlow
from tensorflow.keras import initializers

model.add(Dense(
    64,
    kernel_initializer='glorot_uniform',  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    bias_initializer='zeros'
))

# PyTorch
import torch.nn as nn

nn.Linear(in_features, out_features)  # Xavier –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
# –∏–ª–∏ —è–≤–Ω–æ:
nn.init.xavier_uniform_(layer.weight)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. He Initialization</h2>
    <p><strong>–î–ª—è ReLU –∏ –µ–≥–æ –≤–∞—Ä–∏–∞—Ü–∏–π:</strong></p>
    <pre><code># He Uniform
def he_uniform(n_in, n_out):
    limit = np.sqrt(6.0 / n_in)
    return np.random.uniform(-limit, limit, (n_in, n_out))

# He Normal (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
def he_normal(n_in, n_out):
    std = np.sqrt(2.0 / n_in)
    return np.random.randn(n_in, n_out) * std

# Keras/TensorFlow
model.add(Dense(
    64,
    activation='relu',
    kernel_initializer='he_normal'
))

# PyTorch
nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
# –∏–ª–∏
nn.init.kaiming_uniform_(layer.weight)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–§–æ—Ä–º—É–ª–∞</th><th>–ê–∫—Ç–∏–≤–∞—Ü–∏—è</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr>
      <tr><td><strong>Xavier Uniform</strong></td><td>U(-‚àö(6/(n_in+n_out)), ‚àö(6/(n_in+n_out)))</td><td>sigmoid, tanh</td><td>–ú–µ–ª–∫–∏–µ —Å–µ—Ç–∏</td></tr>
      <tr><td><strong>Xavier Normal</strong></td><td>N(0, ‚àö(2/(n_in+n_out)))</td><td>sigmoid, tanh</td><td>–õ—é–±—ã–µ —Å–µ—Ç–∏</td></tr>
      <tr><td><strong>He Uniform</strong></td><td>U(-‚àö(6/n_in), ‚àö(6/n_in))</td><td>ReLU, Leaky ReLU</td><td>CNN</td></tr>
      <tr><td><strong>He Normal</strong></td><td>N(0, ‚àö(2/n_in))</td><td>ReLU, Leaky ReLU</td><td>–ì–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)</td></tr>
      <tr><td><strong>LeCun</strong></td><td>N(0, ‚àö(1/n_in))</td><td>SELU</td><td>Self-normalizing NN</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è bias</h2>
    <pre><code># –û–±—ã—á–Ω–æ bias = 0
bias = np.zeros(n_out)

# Keras
model.add(Dense(64, bias_initializer='zeros'))

# –ò–Ω–æ–≥–¥–∞ –Ω–µ–±–æ–ª—å—à–æ–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è ReLU
bias = np.ones(n_out) * 0.01

# –î–ª—è LSTM forget gate ‚Äî –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –µ–¥–∏–Ω–∏—Ü–∞–º–∏
model.add(LSTM(64, recurrent_initializer='glorot_uniform'))

# PyTorch
nn.init.zeros_(layer.bias)
# –∏–ª–∏
nn.init.constant_(layer.bias, 0.01)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä –Ω–∞ Keras</h2>
    <pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation

model = Sequential()

# –ü–µ—Ä–≤—ã–π —Å–ª–æ–π —Å ReLU
model.add(Dense(
    128,
    input_dim=784,
    kernel_initializer='he_normal',
    bias_initializer='zeros',
    activation='relu'
))

# –í—Ç–æ—Ä–æ–π —Å–ª–æ–π —Å ReLU
model.add(Dense(
    64,
    kernel_initializer='he_normal',
    bias_initializer='zeros',
    activation='relu'
))

# –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π —Å sigmoid
model.add(Dense(
    10,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    activation='softmax'
))

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä –Ω–∞ PyTorch</h2>
    <pre><code>import torch
import torch.nn as nn

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                # He initialization –¥–ª—è ReLU
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.softmax(self.fc3(x), dim=1)
        return x

model = Net()

# –ò–ª–∏ –≤—Ä—É—á–Ω—É—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è
nn.init.kaiming_normal_(model.fc1.weight)
nn.init.zeros_(model.fc1.bias)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏</h2>
    <table>
      <tr><th>–°–ª—É—á–∞–π</th><th>–ú–µ—Ç–æ–¥</th><th>–ö–æ–¥</th></tr>
      <tr><td><strong>LSTM/GRU</strong></td><td>Xavier</td><td>glorot_uniform</td></tr>
      <tr><td><strong>CNN —Ñ–∏–ª—å—Ç—Ä—ã</strong></td><td>He</td><td>he_normal</td></tr>
      <tr><td><strong>BatchNorm –ø–æ—Å–ª–µ</strong></td><td>–õ—é–±–æ–π</td><td>BN –∫–æ–º–ø–µ–Ω—Å–∏—Ä—É–µ—Ç</td></tr>
      <tr><td><strong>Residual connections</strong></td><td>He</td><td>he_normal</td></tr>
      <tr><td><strong>Embedding —Å–ª–æ–∏</strong></td><td>Uniform</td><td>[-0.05, 0.05]</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏</h2>
    <pre><code>import matplotlib.pyplot as plt

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤
def check_weight_distribution(model):
    for layer in model.layers:
        if hasattr(layer, 'kernel'):
            weights = layer.get_weights()[0]
            
            plt.figure(figsize=(12, 4))
            
            plt.subplot(1, 2, 1)
            plt.hist(weights.flatten(), bins=50)
            plt.title(f'{layer.name} - Histogram')
            plt.xlabel('Weight value')
            
            plt.subplot(1, 2, 2)
            plt.hist(weights.flatten(), bins=50, cumulative=True, density=True)
            plt.title(f'{layer.name} - CDF')
            
            print(f"{layer.name}:")
            print(f"  Mean: {weights.mean():.6f}")
            print(f"  Std: {weights.std():.6f}")
            print(f"  Min: {weights.min():.6f}")
            print(f"  Max: {weights.max():.6f}")
            
            plt.tight_layout()
            plt.show()

check_weight_distribution(model)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∞–∫—Ç–∏–≤–∞—Ü–∏–π</h2>
    <pre><code># –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
def check_activations(model, X_sample):
    from tensorflow.keras.models import Model
    
    layer_outputs = [layer.output for layer in model.layers]
    activation_model = Model(inputs=model.input, outputs=layer_outputs)
    
    activations = activation_model.predict(X_sample[:1])
    
    for i, activation in enumerate(activations):
        print(f"\nLayer {i}: {model.layers[i].name}")
        print(f"  Shape: {activation.shape}")
        print(f"  Mean: {activation.mean():.6f}")
        print(f"  Std: {activation.std():.6f}")
        print(f"  % zeros: {(activation == 0).mean() * 100:.2f}%")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        plt.figure(figsize=(10, 3))
        plt.hist(activation.flatten(), bins=50)
        plt.title(f'Layer {i} - Activations')
        plt.show()

check_activations(model, X_train)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Transfer Learning</h2>
    <pre><code># –ü—Ä–∏ transfer learning —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç:
# 1. –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è base –º–æ–¥–µ–ª–∏
# 2. –°–ª—É—á–∞–π–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è –Ω–æ–≤—ã—Ö —Å–ª–æ—ë–≤

from tensorflow.keras.applications import VGG16

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
base_model = VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# –ó–∞–º–æ—Ä–æ–∑–∫–∞ –≤–µ—Å–æ–≤
base_model.trainable = False

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–ª–æ—ë–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π
model = Sequential([
    base_model,
    Flatten(),
    Dense(256, kernel_initializer='he_normal', activation='relu'),
    Dropout(0.5),
    Dense(10, kernel_initializer='glorot_uniform', activation='softmax')
])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. Custom –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è</h2>
    <pre><code># Keras - custom initializer
from tensorflow.keras.initializers import Initializer

class MyInitializer(Initializer):
    def __call__(self, shape, dtype=None):
        # –°–≤–æ—è –ª–æ–≥–∏–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        return np.random.normal(0, 0.01, shape)

model.add(Dense(64, kernel_initializer=MyInitializer()))

# PyTorch - custom
def custom_init(m):
    if isinstance(m, nn.Linear):
        # –°–≤–æ—è –ª–æ–≥–∏–∫–∞
        nn.init.normal_(m.weight, mean=0, std=0.01)
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)

model.apply(custom_init)

# –ò–ª–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥
def my_init(tensor):
    with torch.no_grad():
        tensor.uniform_(-0.1, 0.1)
    return tensor

weight = my_init(torch.empty(3, 5))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ He –¥–ª—è ReLU/Leaky ReLU</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Xavier –¥–ª—è sigmoid/tanh</li>
      <li>[ ] Bias –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –Ω—É–ª—è–º–∏</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–µ—Ä–≤—ã—Ö –±–∞—Ç—á–µ–π</li>
      <li>[ ] –ò–∑–±–µ–≥–∞–π—Ç–µ –Ω—É–ª–µ–≤–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>[ ] –î–ª—è CNN –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ He Normal</li>
      <li>[ ] –î–ª—è LSTM –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Xavier</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞ vanishing/exploding gradients</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ ‚Äî —ç—Ç–æ –∫–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º—É–∑—ã–∫–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –ø–µ—Ä–µ–¥ –∫–æ–Ω—Ü–µ—Ä—Ç–æ–º: –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–¥–µ–ª–∞–µ—Ç –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–º —Ö–æ—Ä–æ—à–µ–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ, –¥–∞–∂–µ –µ—Å–ª–∏ –º—É–∑—ã–∫–∞–Ω—Ç —Ç–∞–ª–∞–Ω—Ç–ª–∏–≤¬ª.
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</h2>
    <table>
      <tr><th>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</th><th>–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è</th></tr>
      <tr><td>–ü—Ä–æ—Å—Ç–æ–π MLP —Å ReLU</td><td>He Normal</td></tr>
      <tr><td>MLP —Å sigmoid/tanh</td><td>Xavier Normal</td></tr>
      <tr><td>CNN</td><td>He Normal</td></tr>
      <tr><td>ResNet</td><td>He Normal</td></tr>
      <tr><td>LSTM/GRU</td><td>Xavier Uniform</td></tr>
      <tr><td>Transformer</td><td>Xavier Uniform</td></tr>
      <tr><td>GAN Generator</td><td>Normal(0, 0.02)</td></tr>
      <tr><td>GAN Discriminator</td><td>He Normal</td></tr>
    </table>
    <pre><code># –ë—ã—Å—Ç—Ä—ã–π —Ä–µ—Ü–µ–ø—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–ª—É—á–∞–µ–≤:
# 1. ReLU ‚Üí He Normal
# 2. Sigmoid/Tanh ‚Üí Xavier Normal
# 3. Bias ‚Üí Zeros
# 4. –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –ø–µ—Ä–≤—ã–µ —ç–ø–æ—Ö–∏</code></pre>
  </div>



</div>
</body>
</html>

<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Meta-learning (MAML, Prototypical Networks) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üß† Meta-learning (MAML, Prototypical Networks)</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –Ω–∞—É—á–∏—Ç—å—Å—è –±—ã—Å—Ç—Ä–æ —É—á–∏—Ç—å—Å—è –Ω–∞ –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö</li>
      <li><strong>–¶–µ–ª—å</strong>: –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–µ —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤</li>
      <li><strong>–ö–ª—é—á</strong>: –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ –∑–∞–¥–∞—á (meta-training)</li>
      <li><strong>Few-shot</strong>: 1-shot, 5-shot, 10-shot learning</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 2. –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã</h2>
    <table>
      <tr><th>–ü–æ–¥—Ö–æ–¥</th><th>–ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–∏</th><th>–ò–¥–µ—è</th></tr>
      <tr><td><strong>Metric-based</strong></td><td>Prototypical Networks, Matching Networks, Siamese Networks</td><td>–û–±—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏</td></tr>
      <tr><td><strong>Optimization-based</strong></td><td>MAML, Reptile, MetaSGD</td><td>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏</td></tr>
      <tr><td><strong>Model-based</strong></td><td>Meta Networks, Memory-augmented NN</td><td>–ú–æ–¥–µ–ª—å —Å –ø–∞–º—è—Ç—å—é</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. MAML (Model-Agnostic Meta-Learning)</h2>
    <p><strong>–ü—Ä–∏–Ω—Ü–∏–ø</strong>: –Ω–∞–π—Ç–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ª–µ–≥–∫–æ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è</p>
    <pre><code>import torch
import torch.nn as nn
import torch.optim as optim

def maml_inner_loop(model, support_x, support_y, inner_lr=0.01, steps=5):
    """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–µ"""
    # –ö–æ–ø–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
    fast_weights = {name: p.clone() for name, p in model.named_parameters()}
    
    for step in range(steps):
        # Forward pass
        logits = model(support_x, weights=fast_weights)
        loss = F.cross_entropy(logits, support_y)
        
        # Gradient descent –≤ inner loop
        grads = torch.autograd.grad(loss, fast_weights.values(), create_graph=True)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ fast weights
        fast_weights = {
            name: p - inner_lr * g 
            for (name, p), g in zip(fast_weights.items(), grads)
        }
    
    return fast_weights

def maml_outer_loop(model, tasks, meta_lr=0.001, inner_steps=5):
    """Meta-learning —à–∞–≥"""
    meta_optimizer = optim.Adam(model.parameters(), lr=meta_lr)
    
    for task in tasks:
        support_x, support_y, query_x, query_y = task
        
        # Inner loop: –∞–¥–∞–ø—Ç–∞—Ü–∏—è
        fast_weights = maml_inner_loop(model, support_x, support_y, steps=inner_steps)
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ query set
        query_logits = model(query_x, weights=fast_weights)
        meta_loss = F.cross_entropy(query_logits, query_y)
        
        # Outer loop: –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ Œ∏
        meta_optimizer.zero_grad()
        meta_loss.backward()
        meta_optimizer.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Prototypical Networks</h2>
    <p><strong>–ò–¥–µ—è</strong>: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é –¥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤ –∫–ª–∞—Å—Å–æ–≤</p>
    <pre><code>import torch.nn.functional as F

class PrototypicalNetwork(nn.Module):
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder  # CNN –∏–ª–∏ –¥—Ä—É–≥–æ–π —ç–Ω–∫–æ–¥–µ—Ä
    
    def forward(self, support_x, support_y, query_x, n_classes, n_support):
        # –≠–Ω–∫–æ–¥–∏–Ω–≥ support –∏ query
        support_embeddings = self.encoder(support_x)
        query_embeddings = self.encoder(query_x)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤ (—Å—Ä–µ–¥–Ω–µ–µ –ø–æ –∫–ª–∞—Å—Å—É)
        prototypes = torch.zeros(n_classes, support_embeddings.size(-1))
        for c in range(n_classes):
            mask = (support_y == c)
            prototypes[c] = support_embeddings[mask].mean(dim=0)
        
        # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –æ—Ç query –¥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤
        distances = torch.cdist(query_embeddings, prototypes)
        
        # –õ–æ–≥–∏—Ç—ã (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ)
        logits = -distances
        return logits

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
encoder = ConvEncoder()
proto_net = PrototypicalNetwork(encoder)

# N-way K-shot task
logits = proto_net(support_x, support_y, query_x, n_classes=5, n_support=5)
loss = F.cross_entropy(logits, query_y)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Matching Networks</h2>
    <p><strong>Attention-based –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</strong></p>
    <pre><code>def attention_kernel(query, support, temperature=1.0):
    """Cosine similarity attention"""
    similarities = F.cosine_similarity(
        query.unsqueeze(1),  # [Q, 1, D]
        support.unsqueeze(0), # [1, S, D]
        dim=-1
    ) / temperature
    return F.softmax(similarities, dim=-1)  # [Q, S]

class MatchingNetwork(nn.Module):
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder
        self.temperature = nn.Parameter(torch.tensor(1.0))
    
    def forward(self, support_x, support_y, query_x):
        # –≠–Ω–∫–æ–¥–∏–Ω–≥
        support_emb = self.encoder(support_x)
        query_emb = self.encoder(query_x)
        
        # Attention weights
        attn = attention_kernel(query_emb, support_emb, self.temperature)
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –ø–æ –º–µ—Ç–∫–∞–º
        support_y_onehot = F.one_hot(support_y)
        predictions = attn @ support_y_onehot.float()
        
        return predictions</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Reptile (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è MAML)</h2>
    <pre><code>def reptile(model, tasks, meta_lr=0.1, inner_lr=0.01, inner_steps=10):
    """Reptile: –ø—Ä–æ—â–µ MAML, –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≤—Ç–æ—Ä–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π"""
    
    for task in tasks:
        support_x, support_y = task
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–µ –≤–µ—Å–∞
        old_weights = {name: p.clone() for name, p in model.named_parameters()}
        
        # Inner loop: –æ–±—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∑–∞–¥–∞—á–µ
        optimizer = optim.SGD(model.parameters(), lr=inner_lr)
        for step in range(inner_steps):
            logits = model(support_x)
            loss = F.cross_entropy(logits, support_y)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        # Meta update: –¥–≤–∏–≥–∞–µ–º—Å—è –∫ –Ω–æ–≤—ã–º –≤–µ—Å–∞–º
        with torch.no_grad():
            for name, p in model.named_parameters():
                p.copy_(
                    old_weights[name] + meta_lr * (p - old_weights[name])
                )</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Episode-based Training</h2>
    <p><strong>–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —ç–ø–∏–∑–æ–¥–∞</strong></p>
    <ul>
      <li><strong>N-way</strong>: N –∫–ª–∞—Å—Å–æ–≤ –≤ —ç–ø–∏–∑–æ–¥–µ</li>
      <li><strong>K-shot</strong>: K –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å –¥–ª—è support</li>
      <li><strong>Q query</strong>: Q –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è</li>
    </ul>
    <pre><code>def sample_episode(dataset, n_way=5, k_shot=5, q_query=15):
    """–°–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∞ –¥–ª—è meta-training"""
    # –í—ã–±–∏—Ä–∞–µ–º N —Å–ª—É—á–∞–π–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
    classes = np.random.choice(len(dataset.classes), n_way, replace=False)
    
    support_x, support_y = [], []
    query_x, query_y = [], []
    
    for i, cls in enumerate(classes):
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∫–ª–∞—Å—Å–∞
        cls_examples = dataset.get_class_examples(cls)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ support –∏ query
        indices = np.random.choice(len(cls_examples), k_shot + q_query, replace=False)
        
        support_x.append(cls_examples[indices[:k_shot]])
        support_y.extend([i] * k_shot)
        
        query_x.append(cls_examples[indices[k_shot:]])
        query_y.extend([i] * q_query)
    
    support_x = torch.cat(support_x)
    query_x = torch.cat(query_x)
    support_y = torch.tensor(support_y)
    query_y = torch.tensor(query_y)
    
    return support_x, support_y, query_x, query_y</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Relation Networks</h2>
    <p><strong>–û–±—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å</strong></p>
    <pre><code>class RelationNetwork(nn.Module):
    def __init__(self, encoder, relation_module):
        super().__init__()
        self.encoder = encoder
        # –ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ similarity
        self.relation_module = relation_module
    
    def forward(self, support_x, support_y, query_x, n_classes, n_support):
        # –≠–Ω–∫–æ–¥–∏–Ω–≥
        support_emb = self.encoder(support_x)
        query_emb = self.encoder(query_x)
        
        # –ü—Ä–æ—Ç–æ—Ç–∏–ø—ã –∫–ª–∞—Å—Å–æ–≤
        prototypes = torch.zeros(n_classes, support_emb.size(-1))
        for c in range(n_classes):
            prototypes[c] = support_emb[support_y == c].mean(dim=0)
        
        # Concatenate query —Å –∫–∞–∂–¥—ã–º –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–º
        n_query = query_emb.size(0)
        query_emb_repeated = query_emb.unsqueeze(1).repeat(1, n_classes, 1)
        prototypes_repeated = prototypes.unsqueeze(0).repeat(n_query, 1, 1)
        
        pairs = torch.cat([query_emb_repeated, prototypes_repeated], dim=-1)
        
        # Relation scores —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å
        relations = self.relation_module(pairs).squeeze(-1)
        
        return relations

# Relation module
relation_module = nn.Sequential(
    nn.Linear(embedding_dim * 2, 256),
    nn.ReLU(),
    nn.Linear(256, 1),
    nn.Sigmoid()
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Meta-training vs Meta-testing</h2>
    <p><strong>Meta-training</strong></p>
    <ul>
      <li>–ú–Ω–æ–∂–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –∏–∑ train –∫–ª–∞—Å—Å–æ–≤</li>
      <li>–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ "—É—á–∏—Ç—å—Å—è"</li>
      <li>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è meta-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
    </ul>
    <p><strong>Meta-testing</strong></p>
    <ul>
      <li>–ù–æ–≤—ã–µ unseen –∫–ª–∞—Å—Å—ã</li>
      <li>–ê–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞ support set</li>
      <li>–û—Ü–µ–Ω–∫–∞ –Ω–∞ query set</li>
    </ul>
    <pre><code># Meta-training
for epoch in range(num_epochs):
    for episode in range(episodes_per_epoch):
        # Sample —ç–ø–∏–∑–æ–¥ –∏–∑ train –∫–ª–∞—Å—Å–æ–≤
        support_x, support_y, query_x, query_y = sample_episode(
            train_dataset, n_way=5, k_shot=5
        )
        
        # MAML/Prototypical/etc. —à–∞–≥
        loss = meta_learn_step(model, support_x, support_y, query_x, query_y)

# Meta-testing
test_accuracies = []
for episode in range(test_episodes):
    # Sample –∏–∑ test –∫–ª–∞—Å—Å–æ–≤ (unseen!)
    support_x, support_y, query_x, query_y = sample_episode(
        test_dataset, n_way=5, k_shot=5
    )
    
    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞
    acc = evaluate_episode(model, support_x, support_y, query_x, query_y)
    test_accuracies.append(acc)

print(f"Meta-test accuracy: {np.mean(test_accuracies):.2f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è</h3>
        <ul>
          <li>–ù–∞—á–∏–Ω–∞—Ç—å —Å Prototypical Networks (–ø—Ä–æ—Å—Ç–æ)</li>
          <li>Augmentation –¥–∞–Ω–Ω—ã—Ö –≤ support –∏ query</li>
          <li>–ö–æ—Å–∏–Ω—É—Å–Ω–∞—è similarity —á–∞—Å—Ç–æ –ª—É—á—à–µ Euclidean</li>
          <li>Transductive inference (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å query unlabeled)</li>
          <li>Episodic training —Å —Ä–∞–∑–Ω—ã–º N –∏ K</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ò–∑–±–µ–≥–∞—Ç—å</h3>
        <ul>
          <li>–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π inner learning rate –≤ MAML</li>
          <li>–û–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ N-way K-shot –Ω–∞ train –∏ test</li>
          <li>–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ data augmentation</li>
          <li>–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞–ª–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∑–∞–¥–∞—á</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 11. –î–∞—Ç–∞—Å–µ—Ç—ã</h2>
    <table>
      <tr><th>–î–∞—Ç–∞—Å–µ—Ç</th><th>–î–æ–º–µ–Ω</th><th>–ö–ª–∞—Å—Å—ã</th></tr>
      <tr><td><strong>Omniglot</strong></td><td>–°–∏–º–≤–æ–ª—ã</td><td>1623 –∫–ª–∞—Å—Å–∞</td></tr>
      <tr><td><strong>miniImageNet</strong></td><td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è</td><td>100 –∫–ª–∞—Å—Å–æ–≤</td></tr>
      <tr><td><strong>tieredImageNet</strong></td><td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è</td><td>608 –∫–ª–∞—Å—Å–æ–≤</td></tr>
      <tr><td><strong>Meta-Dataset</strong></td><td>Multi-domain</td><td>–†–∞–∑–Ω—ã–µ –¥–æ–º–µ–Ω—ã</td></tr>
      <tr><td><strong>CUB</strong></td><td>–ü—Ç–∏—Ü—ã</td><td>200 –≤–∏–¥–æ–≤</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ú–µ—Ç—Ä–∏–∫–∏</h2>
    <p><strong>N-way K-shot accuracy</strong></p>
    <ul>
      <li>–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ query set –ø–æ—Å–ª–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏</li>
      <li>–£—Å—Ä–µ–¥–Ω—è–µ—Ç—Å—è –ø–æ —ç–ø–∏–∑–æ–¥–∞–º</li>
      <li>–û–±—ã—á–Ω–æ: 5-way 1-shot, 5-way 5-shot</li>
    </ul>
    <p><strong>Confidence interval</strong></p>
    <pre><code>import scipy.stats as st

def compute_confidence_interval(accuracies, confidence=0.95):
    """95% –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª"""
    mean = np.mean(accuracies)
    std_err = st.sem(accuracies)
    interval = st.t.interval(
        confidence,
        len(accuracies) - 1,
        loc=mean,
        scale=std_err
    )
    return mean, interval

# –ü—Ä–∏–º–µ—Ä
test_accs = [0.85, 0.87, 0.84, 0.86, 0.88]  # –ø–æ —ç–ø–∏–∑–æ–¥–∞–º
mean, (low, high) = compute_confidence_interval(test_accs)
print(f"Accuracy: {mean:.2f} ¬± {(high-low)/2:.2f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏</h2>
    <p><strong>Task-conditioned adaptation</strong></p>
    <ul>
      <li>–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å learning rate –Ω–∞ –∑–∞–¥–∞—á—É</li>
      <li>Meta-SGD, Alpha MAML</li>
    </ul>
    <p><strong>Transductive inference</strong></p>
    <ul>
      <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å query set (unlabeled) –ø—Ä–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏</li>
      <li>TPN (Transductive Propagation Network)</li>
    </ul>
    <p><strong>Meta-learning + Self-supervised</strong></p>
    <ul>
      <li>Pre-training —Å rotation, jigsaw</li>
      <li>–õ—É—á—à–µ features –¥–ª—è meta-learning</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>–ú–µ–¥–∏—Ü–∏–Ω–∞</strong>: –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–µ–¥–∫–∏—Ö –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π (–º–∞–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤)</li>
      <li><strong>–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞</strong>: –±—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤—ã–º –æ–±—ä–µ–∫—Ç–∞–º</li>
      <li><strong>–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è</strong>: –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º</li>
      <li><strong>Drug Discovery</strong>: –ø–æ–∏—Å–∫ –º–æ–ª–µ–∫—É–ª —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö</li>
      <li><strong>NLP</strong>: –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤—ã–º —è–∑—ã–∫–∞–º/–¥–æ–º–µ–Ω–∞–º</li>
      <li><strong>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</strong>: —Ö–æ–ª–æ–¥–Ω—ã–π —Å—Ç–∞—Ä—Ç</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —á–µ–ª–æ–≤–µ–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—É—á–∏–ª—Å—è —É—á–∏—Ç—å—Å—è. –û–Ω –≤–∏–¥–µ–ª –º–Ω–æ–≥–æ —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á –∏ —Ç–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç 
      –±—ã—Å—Ç—Ä–æ –æ—Å–≤–æ–∏—Ç—å –Ω–æ–≤—É—é, –¥–∞–∂–µ –µ—Å–ª–∏ –ø—Ä–∏–º–µ—Ä–æ–≤ –º–∞–ª–æ. –ù–∞—à–∞ –º–æ–¥–µ–ª—å —Ç–∞–∫ –∂–µ: –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ –∑–∞–¥–∞—á, 
      —á—Ç–æ–±—ã –Ω–∞—É—á–∏—Ç—å—Å—è –±—ã—Å—Ç—Ä–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –Ω–æ–≤—ã–º –∫–ª–∞—Å—Å–∞–º —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏¬ª.
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥ (metric/optimization/model-based)</li>
      <li>[ ] –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ episodic —Ñ–æ—Ä–º–∞—Ç–µ</li>
      <li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å N-way K-shot –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é</li>
      <li>[ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å episodic sampling</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å inner –∏ outer learning rates</li>
      <li>[ ] –ü—Ä–∏–º–µ–Ω–∏—Ç—å data augmentation</li>
      <li>[ ] Meta-train –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ –∑–∞–¥–∞—á</li>
      <li>[ ] Meta-test –Ω–∞ unseen –∫–ª–∞—Å—Å–∞—Ö</li>
      <li>[ ] –í—ã—á–∏—Å–ª–∏—Ç—å confidence intervals</li>
      <li>[ ] –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ N –∏ K –Ω–∞ —Ç–µ—Å—Ç–µ</li>
    </ul>
  </div>

</div>

</body>
</html>

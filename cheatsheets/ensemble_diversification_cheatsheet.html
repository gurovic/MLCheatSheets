<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>–î–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∞–Ω—Å–∞–º–±–ª–µ–π Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.88em;
      margin: 6px 0;
    }

    th, td {
      padding: 6px 8px;
      text-align: left;
      border: 1px solid #e0e7ff;
    }

    th {
      background-color: #1a5fb4;
      color: white;
      font-weight: 700;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üé≤ –î–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∞–Ω—Å–∞–º–±–ª–µ–π</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</h2>
    <p><strong>–î–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</strong>: —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –æ—à–∏–±–æ–∫</p>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –æ—à–∏–±–∞—Ç—å—Å—è –ø–æ-—Ä–∞–∑–Ω–æ–º—É</li>
      <li><strong>–ü—Ä–∏–Ω—Ü–∏–ø</strong>: –Ω–µ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ –∫–æ–º–ø–µ–Ω—Å–∏—Ä—É—é—Ç –¥—Ä—É–≥ –¥—Ä—É–≥–∞</li>
      <li><strong>–≠—Ñ—Ñ–µ–∫—Ç</strong>: —Å–Ω–∏–∂–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –∏ —É–ª—É—á—à–µ–Ω–∏–µ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏</li>
      <li><strong>–ë–∞–ª–∞–Ω—Å</strong>: –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è vs. —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π</li>
    </ul>
    <blockquote>
      üí° –ß–µ–º –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã –º–æ–¥–µ–ª–∏ –≤ –∞–Ω—Å–∞–º–±–ª–µ, —Ç–µ–º –≤—ã—à–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
    </blockquote>

  <div class="block">
    <h2>üî∑ 2. –¢–∏–ø—ã –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</h2>
    <table>
      <tr><th>–¢–∏–ø</th><th>–ú–µ—Ç–æ–¥</th><th>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å</th></tr>
      <tr><td>–î–∞–Ω–Ω—ã–µ</td><td>Bootstrap, –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏</td><td>‚≠ê‚≠ê‚≠ê‚≠ê</td></tr>
      <tr><td>–ü—Ä–∏–∑–Ω–∞–∫–∏</td><td>Random subspaces</td><td>‚≠ê‚≠ê‚≠ê‚≠ê</td></tr>
      <tr><td>–ê–ª–≥–æ—Ä–∏—Ç–º—ã</td><td>–†–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏</td><td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td></tr>
      <tr><td>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</td><td>–†–∞–∑–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã</td><td>‚≠ê‚≠ê‚≠ê</td></tr>
      <tr><td>–¶–µ–ª–µ–≤–∞—è</td><td>–†–∞–∑–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏</td><td>‚≠ê‚≠ê‚≠ê</td></tr>
      <tr><td>–í—Ä–µ–º–µ–Ω–Ω–∞—è</td><td>–†–∞–∑–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã</td><td>‚≠ê‚≠ê‚≠ê</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. –î–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ –¥–∞–Ω–Ω—ã–µ</h2>
    <pre><code>from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# Bootstrap aggregating (Bagging)
bagging = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=10,
    max_samples=0.8,      # 80% –¥–∞–Ω–Ω—ã—Ö
    bootstrap=True,        # –° –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º
    bootstrap_features=False,
    random_state=42
)

# Pasting (–±–µ–∑ –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏—è)
pasting = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=10,
    max_samples=0.8,
    bootstrap=False,       # –ë–µ–∑ –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏—è
    random_state=42
)

# Random patches (–¥–∞–Ω–Ω—ã–µ + –ø—Ä–∏–∑–Ω–∞–∫–∏)
patches = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=10,
    max_samples=0.8,
    max_features=0.8,      # 80% –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    bootstrap=True,
    bootstrap_features=True,
    random_state=42
)

bagging.fit(X_train, y_train)
y_pred = bagging.predict(X_test)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –î–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–Ω–∞–∫–∏</h2>
    <pre><code>from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Random subspaces
class RandomSubspaceEnsemble:
    def __init__(self, base_model, n_models=10, feature_fraction=0.7):
        self.base_model = base_model
        self.n_models = n_models
        self.feature_fraction = feature_fraction
        self.models = []
        self.feature_indices = []
    
    def fit(self, X, y):
        n_features = X.shape[1]
        n_select = max(1, int(n_features * self.feature_fraction))
        
        for i in range(self.n_models):
            # –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            indices = np.random.choice(
                n_features, n_select, replace=False
            )
            self.feature_indices.append(indices)
            
            # –ö–æ–ø–∏—Ä—É–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            from sklearn.base import clone
            model = clone(self.base_model)
            model.fit(X[:, indices], y)
            self.models.append(model)
        
        return self
    
    def predict(self, X):
        predictions = []
        for model, indices in zip(self.models, self.feature_indices):
            pred = model.predict(X[:, indices])
            predictions.append(pred)
        
        # –ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ–º
        from scipy import stats
        return stats.mode(predictions, axis=0)[0].ravel()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
from sklearn.tree import DecisionTreeClassifier
ensemble = RandomSubspaceEnsemble(
    DecisionTreeClassifier(max_depth=5),
    n_models=10,
    feature_fraction=0.7
)
ensemble.fit(X_train, y_train)
y_pred = ensemble.predict(X_test)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –î–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∞–ª–≥–æ—Ä–∏—Ç–º—ã</h2>
    <pre><code>from sklearn.ensemble import VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

# –ì–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å —Ä–∞–∑–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
voting = VotingClassifier(
    estimators=[
        ('dt', DecisionTreeClassifier(max_depth=5)),
        ('lr', LogisticRegression(max_iter=1000)),
        ('svm', SVC(probability=True, kernel='rbf')),
        ('nb', GaussianNB()),
        ('knn', KNeighborsClassifier(n_neighbors=5))
    ],
    voting='soft',  # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    weights=[2, 1, 2, 1, 1]  # –í–µ—Å–∞ –º–æ–¥–µ–ª–µ–π
)

voting.fit(X_train, y_train)
y_pred = voting.predict(X_test)
y_proba = voting.predict_proba(X_test)

# –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR

voting_reg = VotingRegressor(
    estimators=[
        ('ridge', Ridge(alpha=1.0)),
        ('lasso', Lasso(alpha=1.0)),
        ('dt', DecisionTreeRegressor(max_depth=5)),
        ('svr', SVR(kernel='rbf'))
    ],
    weights=[1, 1, 2, 1]
)

voting_reg.fit(X_train, y_train)
y_pred = voting_reg.predict(X_test)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –î–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã</h2>
    <pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
models = []
for max_depth in [3, 5, 7, 10, None]:
    for min_samples_split in [2, 5, 10]:
        model = DecisionTreeClassifier(
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            random_state=42
        )
        models.append(model)

# –û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
for model in models:
    model.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
def ensemble_predict(models, X):
    predictions = np.array([m.predict(X) for m in models])
    from scipy import stats
    return stats.mode(predictions, axis=0)[0].ravel()

y_pred = ensemble_predict(models, X_test)

# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—è VotingClassifier
from sklearn.ensemble import VotingClassifier
estimators = [(f'model_{i}', m) for i, m in enumerate(models)]
voting = VotingClassifier(estimators=estimators, voting='hard')
voting.fit(X_train, y_train)
y_pred = voting.predict(X_test)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ò–∑–º–µ—Ä–µ–Ω–∏–µ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</h2>
    <pre><code>import numpy as np
from sklearn.metrics import accuracy_score

def measure_diversity(models, X, y):
    """–ò–∑–º–µ—Ä–µ–Ω–∏–µ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–Ω—Å–∞–º–±–ª—è"""
    n_models = len(models)
    predictions = np.array([m.predict(X) for m in models])
    
    # 1. Pairwise disagreement
    disagreements = []
    for i in range(n_models):
        for j in range(i+1, n_models):
            disagree = np.mean(predictions[i] != predictions[j])
            disagreements.append(disagree)
    
    avg_disagreement = np.mean(disagreements)
    
    # 2. Q-statistic (–¥–ª—è –ø–∞—Ä –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤)
    q_stats = []
    for i in range(n_models):
        for j in range(i+1, n_models):
            N11 = np.sum((predictions[i] == y) & (predictions[j] == y))
            N00 = np.sum((predictions[i] != y) & (predictions[j] != y))
            N10 = np.sum((predictions[i] == y) & (predictions[j] != y))
            N01 = np.sum((predictions[i] != y) & (predictions[j] == y))
            
            denominator = (N11 * N00 + N01 * N10)
            if denominator > 0:
                Q = (N11 * N00 - N01 * N10) / denominator
                q_stats.append(Q)
    
    avg_q = np.mean(q_stats) if q_stats else 0
    
    # 3. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    from scipy.stats import pearsonr
    correlations = []
    for i in range(n_models):
        for j in range(i+1, n_models):
            corr, _ = pearsonr(predictions[i], predictions[j])
            correlations.append(corr)
    
    avg_correlation = np.mean(correlations)
    
    return {
        'disagreement': avg_disagreement,
        'q_statistic': avg_q,
        'correlation': avg_correlation
    }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
diversity = measure_diversity(models, X_test, y_test)
print(f"Disagreement: {diversity['disagreement']:.3f}")
print(f"Q-statistic: {diversity['q_statistic']:.3f}")
print(f"Correlation: {diversity['correlation']:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è</h2>
    <pre><code>import numpy as np

class NegativeCorrelationEnsemble:
    """–ê–Ω—Å–∞–º–±–ª—å —Å —è–≤–Ω–æ–π –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π"""
    
    def __init__(self, n_models=5, lambda_param=0.5):
        self.n_models = n_models
        self.lambda_param = lambda_param  # –°–∏–ª–∞ penalty
        self.models = []
    
    def fit(self, X, y, epochs=10):
        from sklearn.neural_network import MLPRegressor
        
        for i in range(self.n_models):
            model = MLPRegressor(
                hidden_layer_sizes=(50,),
                max_iter=epochs,
                random_state=i
            )
            
            # –û–±—É—á–µ–Ω–∏–µ —Å —É—á—ë—Ç–æ–º –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
            if len(self.models) > 0:
                # –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
                other_preds = np.mean([
                    m.predict(X) for m in self.models
                ], axis=0)
                
                # –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ü–µ–ª–µ–≤–∞—è –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
                y_modified = y + self.lambda_param * (y - other_preds)
                model.fit(X, y_modified)
            else:
                model.fit(X, y)
            
            self.models.append(model)
        
        return self
    
    def predict(self, X):
        predictions = np.array([m.predict(X) for m in self.models])
        return np.mean(predictions, axis=0)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
nc_ensemble = NegativeCorrelationEnsemble(
    n_models=5,
    lambda_param=0.3
)
nc_ensemble.fit(X_train, y_train, epochs=20)
y_pred = nc_ensemble.predict(X_test)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</h2>
    <ul>
      <li><strong>–ü—Ä–∞–≤–∏–ª–æ 1</strong>: –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π</li>
      <li><strong>–ü—Ä–∞–≤–∏–ª–æ 2</strong>: –°–ª–∞–±–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è < 0.7)</li>
      <li><strong>–ü—Ä–∞–≤–∏–ª–æ 3</strong>: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π > 0.5</li>
      <li><strong>–ü—Ä–∞–≤–∏–ª–æ 4</strong>: –†–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –æ—à–∏–±–æ–∫ —É –º–æ–¥–µ–ª–µ–π</li>
      <li><strong>–ü—Ä–∞–≤–∏–ª–æ 5</strong>: 5-20 –º–æ–¥–µ–ª–µ–π –æ–±—ã—á–Ω–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ</li>
    </ul>
    <blockquote>
      üí° –û–ø—Ç–∏–º—É–º: Accuracy √ó (1 - Correlation)
    </blockquote>
    <pre><code># –û—Ü–µ–Ω–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω—Å–∞–º–±–ª—è
def ensemble_quality(models, X, y):
    predictions = np.array([m.predict(X) for m in models])
    
    # –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å
    accuracies = [accuracy_score(y, pred) for pred in predictions]
    avg_accuracy = np.mean(accuracies)
    
    # –°—Ä–µ–¥–Ω—è—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
    correlations = []
    for i in range(len(models)):
        for j in range(i+1, len(models)):
            from scipy.stats import pearsonr
            corr, _ = pearsonr(predictions[i], predictions[j])
            correlations.append(abs(corr))
    
    avg_correlation = np.mean(correlations)
    
    # –ö–∞—á–µ—Å—Ç–≤–æ –∞–Ω—Å–∞–º–±–ª—è
    quality = avg_accuracy * (1 - avg_correlation)
    
    return {
        'accuracy': avg_accuracy,
        'correlation': avg_correlation,
        'quality': quality
    }

quality = ensemble_quality(models, X_test, y_test)
print(f"Quality score: {quality['quality']:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</h2>
    <table>
      <tr><th>–°—Ç—Ä–∞—Ç–µ–≥–∏—è</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr>
      <tr><td>Bootstrap</td><td>–ú–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö, –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏</td></tr>
      <tr><td>Random subspaces</td><td>–ú–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</td></tr>
      <tr><td>–ì–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω—ã–µ –∞–Ω—Å–∞–º–±–ª–∏</td><td>–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö</td></tr>
      <tr><td>–ü–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è</td><td>–û–≥—Ä–∞–Ω–∏—á–µ–Ω—ã —Ç–∏–ø—ã –º–æ–¥–µ–ª–µ–π</td></tr>
      <tr><td>–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è</td><td>–ù—É–∂–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</td></tr>
      <tr><td>–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è</td><td>–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ, –º–Ω–æ–≥–æ —Ä–µ—Å—É—Ä—Å–æ–≤</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</h2>
    <pre><code>from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# AutoML –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
class AutoDiverseEnsemble:
    def __init__(self, base_models, n_select=5):
        self.base_models = base_models
        self.n_select = n_select
        self.selected_models = []
    
    def fit(self, X, y):
        # –û–±—É—á–∏—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏
        trained_models = []
        for name, model in self.base_models:
            model.fit(X, y)
            score = model.score(X, y)
            trained_models.append((name, model, score))
        
        # –ñ–∞–¥–Ω—ã–π –æ—Ç–±–æ—Ä —Å —É—á—ë—Ç–æ–º –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        self.selected_models = [trained_models[0]]
        
        for _ in range(self.n_select - 1):
            best_score = -np.inf
            best_model = None
            
            for name, model, acc in trained_models:
                if (name, model, acc) in self.selected_models:
                    continue
                
                # –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
                temp_models = [m[1] for m in self.selected_models] + [model]
                quality = ensemble_quality(temp_models, X, y)['quality']
                
                if quality > best_score:
                    best_score = quality
                    best_model = (name, model, acc)
            
            if best_model:
                self.selected_models.append(best_model)
        
        return self
    
    def predict(self, X):
        predictions = [m[1].predict(X) for m in self.selected_models]
        from scipy import stats
        return stats.mode(predictions, axis=0)[0].ravel()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
base_models = [
    ('dt', DecisionTreeClassifier(max_depth=5)),
    ('rf', RandomForestClassifier(n_estimators=50)),
    ('lr', LogisticRegression()),
    ('svm', SVC(probability=True)),
    ('nb', GaussianNB())
]

auto_ensemble = AutoDiverseEnsemble(base_models, n_select=3)
auto_ensemble.fit(X_train, y_train)
y_pred = auto_ensemble.predict(X_test)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏</h2>
    <ul>
      <li>‚úÖ <strong>–ò–∑–º–µ—Ä—è–π—Ç–µ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—é</strong>: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏</li>
      <li>‚úÖ <strong>–ë–∞–ª–∞–Ω—Å</strong>: —Ç–æ—á–Ω–æ—Å—Ç—å vs. –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</li>
      <li>‚úÖ <strong>–ö–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ –ø–æ–¥—Ö–æ–¥—ã</strong>: –¥–∞–Ω–Ω—ã–µ + –∞–ª–≥–æ—Ä–∏—Ç–º—ã + –ø–∞—Ä–∞–º–µ—Ç—Ä—ã</li>
      <li>‚úÖ <strong>–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥</strong>: —Å–ª–µ–¥–∏—Ç–µ –∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π –º–æ–¥–µ–ª–µ–π</li>
      <li>‚úÖ <strong>–í–∞–ª–∏–¥–∞—Ü–∏—è</strong>: –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li>‚úÖ <strong>–ù–∞—á–∏–Ω–∞–π—Ç–µ –ø—Ä–æ—Å—Ç–æ</strong>: bagging/random subspaces</li>
      <li>‚úÖ <strong>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç</strong>: —Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏</li>
      <li>‚úÖ <strong>–£–¥–∞–ª—è–π—Ç–µ –ª–∏—à–Ω–µ–µ</strong>: –∏—Å–∫–ª—é—á–∞–π—Ç–µ —Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–∏–µ –º–æ–¥–µ–ª–∏</li>
    </ul>
  </div>

</div>

</div>
</body>
</html>

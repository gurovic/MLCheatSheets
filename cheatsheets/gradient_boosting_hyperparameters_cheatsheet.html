<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Гиперпараметры градиентного бустинга Cheatsheet — 3 колонки</title>
  <style>
    @media screen {body {font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;color: #333;background: #fafcff;padding: 10px;}}
    @media print {body {background: white;padding: 0;}@page {size: A4 landscape;margin: 10mm;}}
    .container {column-count: 3;column-gap: 20px;max-width: 100%;}
    .block {break-inside: avoid;margin-bottom: 1.2em;padding: 12px;background: white;border-radius: 6px;box-shadow: 0 1px 3px rgba(0,0,0,0.05);}
    h1 {font-size: 1.6em;font-weight: 700;color: #1a5fb4;text-align: center;margin: 0 0 8px;column-span: all;}
    .subtitle {text-align: center;color: #666;font-size: 0.9em;margin-bottom: 12px;column-span: all;}
    h2 {font-size: 1.15em;font-weight: 700;color: #1a5fb4;margin: 0 0 8px;padding-bottom: 4px;border-bottom: 1px solid #e0e7ff;}
    p, ul, ol {font-size: 0.92em;margin: 0.6em 0;}
    ul, ol {padding-left: 18px;}
    li {margin-bottom: 4px;}
    code {font-family: 'Consolas', 'Courier New', monospace;background-color: #f0f4ff;padding: 1px 4px;border-radius: 3px;font-size: 0.88em;}
    pre {background-color: #f0f4ff;padding: 8px;border-radius: 4px;overflow-x: auto;font-size: 0.84em;margin: 6px 0;}
    pre code {padding: 0;background: none;white-space: pre-wrap;}
    strong {color: #1a5fb4;font-weight: 600;}
    .formula {background: #fff9e6;padding: 6px;border-left: 3px solid #ffcc00;margin: 8px 0;font-style: italic;}
    table {width: 100%;border-collapse: collapse;font-size: 0.88em;margin: 8px 0;}
    table th {background-color: #e0e7ff;padding: 6px;text-align: left;font-weight: 600;}
    table td {padding: 5px 6px;border-bottom: 1px solid #e0e7ff;}
  </style>
</head>
<body>

<h1>Гиперпараметры градиентного бустинга</h1>
<div class="subtitle">Полное руководство по настройке XGBoost, LightGBM, CatBoost</div>

<div class="container">

  <div class="block">
    <h2>1. Базовые параметры деревьев</h2>
    <p><strong>max_depth</strong> — максимальная глубина дерева.</p>
    <ul>
      <li><strong>Диапазон:</strong> 3-10</li>
      <li><strong>Значение:</strong> 6 (по умолчанию)</li>
      <li><strong>Эффект:</strong> больше → переобучение, меньше → недообучение</li>
      <li><strong>XGBoost:</strong> max_depth=6</li>
      <li><strong>LightGBM:</strong> max_depth=-1 (без ограничений)</li>
      <li><strong>CatBoost:</strong> depth=6</li>
    </ul>
    
    <pre><code># XGBoost
xgb_params = {'max_depth': 6}

# LightGBM
lgb_params = {'max_depth': -1}

# CatBoost
cat_params = {'depth': 6}</code></pre>
    
    <p><strong>Рекомендации:</strong></p>
    <ul>
      <li>Начните с 6-8</li>
      <li>Уменьшайте при переобучении</li>
      <li>Увеличивайте если недообучение</li>
    </ul>
  </div>

  <div class="block">
    <h2>2. Learning rate</h2>
    <p><strong>learning_rate (eta, eta)</strong> — скорость обучения.</p>
    
    <div class="formula">
      prediction = prediction + learning_rate × tree_prediction
    </div>
    
    <ul>
      <li><strong>Диапазон:</strong> 0.01-0.3</li>
      <li><strong>Значение:</strong> 0.1 (по умолчанию)</li>
      <li><strong>Правило:</strong> меньше lr → больше n_estimators</li>
    </ul>
    
    <pre><code># XGBoost
xgb.XGBClassifier(
    learning_rate=0.05,
    n_estimators=1000
)

# LightGBM
lgb.LGBMClassifier(
    learning_rate=0.05,
    n_estimators=1000
)

# CatBoost
CatBoostClassifier(
    learning_rate=0.05,
    iterations=1000
)</code></pre>
    
    <p><strong>Стратегии:</strong></p>
    <ul>
      <li>0.1: быстрое обучение, хорошо для экспериментов</li>
      <li>0.05: баланс скорости и качества</li>
      <li>0.01-0.03: лучшее качество, долгое обучение</li>
      <li>0.3+: риск переобучения</li>
    </ul>
  </div>

  <div class="block">
    <h2>3. Количество деревьев</h2>
    <p><strong>n_estimators / num_boost_round</strong> — число деревьев в ансамбле.</p>
    
    <ul>
      <li><strong>Диапазон:</strong> 100-10000+</li>
      <li><strong>Значение:</strong> 100 (по умолчанию, обычно мало)</li>
      <li><strong>Связь:</strong> зависит от learning_rate</li>
    </ul>
    
    <table>
      <tr>
        <th>learning_rate</th>
        <th>n_estimators</th>
      </tr>
      <tr>
        <td>0.1</td>
        <td>100-500</td>
      </tr>
      <tr>
        <td>0.05</td>
        <td>500-1500</td>
      </tr>
      <tr>
        <td>0.01</td>
        <td>2000-10000</td>
      </tr>
    </table>
    
    <pre><code># С early stopping
model = xgb.XGBClassifier(
    n_estimators=10000,
    early_stopping_rounds=50,
    learning_rate=0.01
)

model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    verbose=100
)

print(f"Best iteration: {model.best_iteration}")</code></pre>
  </div>

  <div class="block">
    <h2>4. Подвыборка данных</h2>
    <p><strong>subsample</strong> — доля строк для каждого дерева.</p>
    
    <ul>
      <li><strong>Диапазон:</strong> 0.5-1.0</li>
      <li><strong>Значение:</strong> 1.0 (все данные)</li>
      <li><strong>Эффект:</strong> < 1.0 → регуляризация, быстрее обучение</li>
    </ul>
    
    <pre><code># XGBoost
xgb_params = {
    'subsample': 0.8,  # 80% строк
    'colsample_bytree': 0.8  # 80% признаков
}

# LightGBM
lgb_params = {
    'bagging_fraction': 0.8,
    'bagging_freq': 5,  # каждые 5 итераций
    'feature_fraction': 0.8
}

# CatBoost
cat_params = {
    'subsample': 0.8,
    'rsm': 0.8  # random subspace method
}</code></pre>
    
    <p><strong>Рекомендации:</strong></p>
    <ul>
      <li>0.8-0.9: хорошая регуляризация</li>
      <li>0.5-0.7: сильная регуляризация</li>
      <li>1.0: если данных мало</li>
    </ul>
  </div>

  <div class="block">
    <h2>5. Подвыборка признаков</h2>
    <p><strong>colsample_by*</strong> — доля признаков.</p>
    
    <p><strong>XGBoost варианты:</strong></p>
    <ul>
      <li><strong>colsample_bytree:</strong> для каждого дерева</li>
      <li><strong>colsample_bylevel:</strong> для каждого уровня</li>
      <li><strong>colsample_bynode:</strong> для каждого разбиения</li>
    </ul>
    
    <pre><code># Комбинированная стратегия
xgb_params = {
    'colsample_bytree': 0.8,    # 80% при построении дерева
    'colsample_bylevel': 0.8,   # 80% на каждом уровне
    'colsample_bynode': 0.8     # 80% в каждом узле
}

# Эффективная доля: 0.8 × 0.8 × 0.8 = 0.512 (51.2%)

# LightGBM
lgb_params = {
    'feature_fraction': 0.8,
    'feature_fraction_bynode': 0.8
}

# CatBoost
cat_params = {
    'rsm': 0.8  # Random Subspace Method
}</code></pre>
  </div>

  <div class="block">
    <h2>6. Регуляризация</h2>
    <p><strong>L1 и L2 регуляризация весов.</strong></p>
    
    <p><strong>XGBoost:</strong></p>
    <ul>
      <li><strong>reg_alpha:</strong> L1 (lasso)</li>
      <li><strong>reg_lambda:</strong> L2 (ridge)</li>
    </ul>
    
    <pre><code># XGBoost
xgb_params = {
    'reg_alpha': 0.1,   # L1
    'reg_lambda': 1.0   # L2
}

# LightGBM
lgb_params = {
    'lambda_l1': 0.1,
    'lambda_l2': 1.0
}

# CatBoost
cat_params = {
    'l2_leaf_reg': 3.0  # L2 регуляризация листьев
}</code></pre>
    
    <p><strong>Когда использовать:</strong></p>
    <ul>
      <li>L1: для feature selection</li>
      <li>L2: общая регуляризация</li>
      <li>Увеличивайте при переобучении</li>
      <li>Начинайте с L2 = 1.0</li>
    </ul>
  </div>

  <div class="block">
    <h2>7. Минимальные размеры</h2>
    <p><strong>Ограничения на листья и разбиения.</strong></p>
    
    <p><strong>min_child_weight (XGBoost):</strong></p>
    <ul>
      <li>Минимальная сумма весов в листе</li>
      <li>Диапазон: 0-100</li>
      <li>По умолчанию: 1</li>
      <li>Больше → консервативнее модель</li>
    </ul>
    
    <pre><code># XGBoost
xgb_params = {
    'min_child_weight': 3,  # минимум 3 в leaf
    'gamma': 0.1  # минимальный gain для split
}

# LightGBM
lgb_params = {
    'min_child_samples': 20,  # минимум сэмплов в leaf
    'min_child_weight': 0.001,
    'min_split_gain': 0.1  # минимальный gain
}

# CatBoost
cat_params = {
    'min_data_in_leaf': 1,
    'max_leaves': 31  # максимум листьев
}</code></pre>
  </div>

  <div class="block">
    <h2>8. Специфичные для LightGBM</h2>
    <p><strong>num_leaves</strong> — максимальное число листьев.</p>
    
    <ul>
      <li><strong>Диапазон:</strong> 20-255</li>
      <li><strong>Правило:</strong> num_leaves < 2^max_depth</li>
      <li><strong>Важно:</strong> ключевой параметр LightGBM</li>
    </ul>
    
    <pre><code># Оптимальная настройка LightGBM
lgb_params = {
    'num_leaves': 31,  # 2^5 - 1
    'max_depth': -1,
    'min_child_samples': 20,
    'min_child_weight': 0.001,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'lambda_l1': 0.1,
    'lambda_l2': 0.1,
    'min_split_gain': 0.0
}

# Больше листьев = сложнее модель
# 31 листьев — хороший старт
# 255 листьев — для больших датасетов</code></pre>
    
    <p><strong>boosting_type:</strong></p>
    <ul>
      <li><strong>gbdt:</strong> традиционный (по умолчанию)</li>
      <li><strong>dart:</strong> Dropout для деревьев</li>
      <li><strong>goss:</strong> Gradient-based One-Side Sampling</li>
    </ul>
  </div>

  <div class="block">
    <h2>9. Специфичные для CatBoost</h2>
    <p><strong>Параметры для категориальных признаков.</strong></p>
    
    <pre><code>cat_params = {
    # Основные
    'iterations': 1000,
    'learning_rate': 0.05,
    'depth': 6,
    
    # Регуляризация
    'l2_leaf_reg': 3.0,
    'bagging_temperature': 1.0,
    'random_strength': 1.0,
    
    # Категориальные
    'one_hot_max_size': 2,  # one-hot до 2 уникальных значений
    'cat_features': [0, 1, 2],  # индексы категориальных
    
    # Бордеры
    'border_count': 128,  # квантили для числовых
    
    # Ранняя остановка
    'early_stopping_rounds': 50,
    
    # GPU
    'task_type': 'GPU',
    'devices': '0'
}

model = CatBoostClassifier(**cat_params)
model.fit(
    X_train, y_train,
    eval_set=(X_val, y_val),
    cat_features=['category', 'color'],
    verbose=100
)</code></pre>
  </div>

  <div class="block">
    <h2>10. Стратегии настройки</h2>
    <p><strong>Пошаговый подход:</strong></p>
    
    <p><strong>Шаг 1: Базовая модель</strong></p>
    <pre><code>base_params = {
    'learning_rate': 0.1,
    'n_estimators': 100,
    'max_depth': 6
}

model = xgb.XGBClassifier(**base_params)
score = cross_val_score(model, X, y, cv=5).mean()
print(f"Baseline: {score:.4f}")</code></pre>
    
    <p><strong>Шаг 2: Настройка структуры дерева</strong></p>
    <pre><code>param_grid = {
    'max_depth': [3, 5, 7, 9],
    'min_child_weight': [1, 3, 5],
    'gamma': [0, 0.1, 0.2]
}

grid = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc')
grid.fit(X_train, y_train)</code></pre>
    
    <p><strong>Шаг 3: Настройка подвыборки</strong></p>
    <pre><code>param_grid = {
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}</code></pre>
    
    <p><strong>Шаг 4: Регуляризация</strong></p>
    <pre><code>param_grid = {
    'reg_alpha': [0, 0.1, 1, 10],
    'reg_lambda': [0.1, 1, 10, 100]
}</code></pre>
    
    <p><strong>Шаг 5: Точная настройка learning_rate</strong></p>
    <pre><code># Уменьшить lr, увеличить n_estimators
final_params = {
    **best_params,
    'learning_rate': 0.01,
    'n_estimators': 5000
}</code></pre>
  </div>

  <div class="block">
    <h2>11. Bayesian Optimization</h2>
    <pre><code>from hyperopt import hp, fmin, tpe, Trials

space = {
    'max_depth': hp.quniform('max_depth', 3, 10, 1),
    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.3)),
    'n_estimators': hp.quniform('n_estimators', 100, 1000, 50),
    'subsample': hp.uniform('subsample', 0.5, 1.0),
    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),
    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.01), np.log(10)),
    'reg_lambda': hp.loguniform('reg_lambda', np.log(0.01), np.log(10))
}

def objective(params):
    params = {
        'max_depth': int(params['max_depth']),
        'n_estimators': int(params['n_estimators']),
        **{k: v for k, v in params.items() if k not in ['max_depth', 'n_estimators']}
    }
    
    model = xgb.XGBClassifier(**params, random_state=42)
    score = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc').mean()
    return -score  # минимизируем

trials = Trials()
best = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,
    max_evals=100,
    trials=trials
)</code></pre>
  </div>

  <div class="block">
    <h2>12. Практические рекомендации</h2>
    <p><strong>Начальные значения:</strong></p>
    <pre><code># Хорошие стартовые параметры
xgb_starter = {
    'learning_rate': 0.05,
    'n_estimators': 1000,
    'max_depth': 6,
    'min_child_weight': 3,
    'gamma': 0.1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_alpha': 0.1,
    'reg_lambda': 1.0,
    'early_stopping_rounds': 50
}

lgb_starter = {
    'learning_rate': 0.05,
    'n_estimators': 1000,
    'num_leaves': 31,
    'max_depth': -1,
    'min_child_samples': 20,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'lambda_l1': 0.1,
    'lambda_l2': 0.1
}

cat_starter = {
    'learning_rate': 0.05,
    'iterations': 1000,
    'depth': 6,
    'l2_leaf_reg': 3.0,
    'subsample': 0.8,
    'random_strength': 1.0,
    'bagging_temperature': 1.0
}</code></pre>
    
    <p><strong>Общие советы:</strong></p>
    <ul>
      <li>Используйте early stopping всегда</li>
      <li>Мониторьте train vs validation loss</li>
      <li>Начинайте с маленького lr + много деревьев</li>
      <li>Регуляризируйте через sampling, а не только L1/L2</li>
      <li>Сохраняйте лучшие параметры</li>
      <li>Cross-validation для финальной оценки</li>
    </ul>
  </div>

</div>

</body>
</html>

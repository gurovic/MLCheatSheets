<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>RNN/LSTM –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üìà RNN/LSTM –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã</h2>
    <ul>
      <li><strong>RNN</strong>: Recurrent Neural Networks</li>
      <li><strong>Sequence</strong>: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π</li>
      <li><strong>Memory</strong>: —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ</li>
      <li><strong>–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã</strong>: —Ü–µ–Ω—ã, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, –ø—Ä–æ–¥–∞–∂–∏</li>
    </ul>
    <p><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–ª—è TS:</strong></p>
    <ul>
      <li>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
      <li>–†–∞–±–æ—Ç–∞ —Å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω–æ–π</li>
      <li>–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</li>
      <li>–ú—É–ª—å—Ç–∏–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã–µ —Ä—è–¥—ã</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. RNN vs LSTM vs GRU</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</th><th>–°–∫–æ—Ä–æ—Å—Ç—å</th><th>–ü–∞–º—è—Ç—å</th></tr>
      <tr><td><strong>Simple RNN</strong></td><td>–ú–∞–ª–æ</td><td>–ë—ã—Å—Ç—Ä–æ</td><td>–ö–æ—Ä–æ—Ç–∫–∞—è</td></tr>
      <tr><td><strong>LSTM</strong></td><td>–ú–Ω–æ–≥–æ</td><td>–ú–µ–¥–ª–µ–Ω–Ω–æ</td><td>–î–ª–∏–Ω–Ω–∞—è</td></tr>
      <tr><td><strong>GRU</strong></td><td>–°—Ä–µ–¥–Ω–µ</td><td>–°—Ä–µ–¥–Ω–µ</td><td>–°—Ä–µ–¥–Ω—è—è</td></tr>
    </table>
    <p><strong>–í—ã–±–æ—Ä:</strong></p>
    <ul>
      <li>Short sequences (< 50): Simple RNN</li>
      <li>Long dependencies: LSTM</li>
      <li>–û–≥—Ä–∞–Ω–∏—á–µ–Ω—ã —Ä–µ—Å—É—Ä—Å—ã: GRU</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö</h2>
    <pre><code>import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = pd.read_csv('data.csv', parse_dates=['date'])
values = df['value'].values

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values.reshape(-1, 1))

# –°–æ–∑–¥–∞–Ω–∏–µ sequences
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    return np.array(X), np.array(y)

seq_length = 10
X, y = create_sequences(scaled, seq_length)

# Train/test split
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. LSTM –º–æ–¥–µ–ª—å –≤ PyTorch</h2>
    <pre><code>import torch
import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, num_layers=2):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            dropout=0.2
        )
        self.fc = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        # x shape: (batch, seq_len, features)
        h0 = torch.zeros(self.num_layers, x.size(0), 
                        self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), 
                        self.hidden_size).to(x.device)
        
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

model = LSTMModel(input_size=1, hidden_size=50, num_layers=2)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏</h2>
    <pre><code>criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

num_epochs = 100
batch_size = 32

for epoch in range(num_epochs):
    model.train()
    for i in range(0, len(X_train), batch_size):
        batch_X = torch.FloatTensor(X_train[i:i+batch_size])
        batch_y = torch.FloatTensor(y_train[i:i+batch_size])
        
        # Forward pass
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ</h2>
    <pre><code># –û–¥–Ω–æ—à–∞–≥–æ–≤—ã–π –ø—Ä–æ–≥–Ω–æ–∑
model.eval()
with torch.no_grad():
    test_inputs = torch.FloatTensor(X_test)
    predictions = model(test_inputs)
    predictions = scaler.inverse_transform(predictions.numpy())

# –ú–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–π –ø—Ä–æ–≥–Ω–æ–∑ (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π)
def forecast_recursive(model, last_sequence, steps):
    forecasts = []
    current_seq = last_sequence.copy()
    
    for _ in range(steps):
        # –ü—Ä–æ–≥–Ω–æ–∑
        with torch.no_grad():
            x = torch.FloatTensor(current_seq).unsqueeze(0)
            pred = model(x).item()
        
        forecasts.append(pred)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        current_seq = np.append(current_seq[1:], [[pred]], axis=0)
    
    return scaler.inverse_transform(np.array(forecasts).reshape(-1, 1))

# –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 30 —à–∞–≥–æ–≤ –≤–ø–µ—Ä–µ–¥
last_seq = X_test[-1]
forecast = forecast_recursive(model, last_seq, steps=30)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Keras/TensorFlow –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞</h2>
    <pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(seq_length, 1)),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(25),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.1,
    verbose=1
)

# –ü—Ä–æ–≥–Ω–æ–∑
predictions = model.predict(X_test)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã</h2>
    <p><strong>Stacked LSTM:</strong></p>
    <pre><code>model = Sequential([
    LSTM(64, return_sequences=True),
    LSTM(32, return_sequences=True),
    LSTM(16, return_sequences=False),
    Dense(1)
])</code></pre>

    <p><strong>Bidirectional LSTM:</strong></p>
    <pre><code>from tensorflow.keras.layers import Bidirectional

model = Sequential([
    Bidirectional(LSTM(50, return_sequences=True)),
    Bidirectional(LSTM(50)),
    Dense(1)
])</code></pre>

    <p><strong>Attention LSTM:</strong></p>
    <pre><code>from tensorflow.keras.layers import Attention

lstm_out = LSTM(50, return_sequences=True)(inputs)
attention_out = Attention()([lstm_out, lstm_out])
output = Dense(1)(attention_out)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ú—É–ª—å—Ç–∏–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã–µ —Ä—è–¥—ã</h2>
    <pre><code># –ù–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
df = pd.read_csv('multivariate.csv')
features = ['price', 'volume', 'volatility']
values = df[features].values

scaler = MinMaxScaler()
scaled = scaler.fit_transform(values)

# X shape: (samples, seq_length, num_features)
X, y = create_sequences(scaled, seq_length=10)

model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(10, 3)),
    LSTM(50),
    Dense(1)  # –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ price
])

model.compile(optimizer='adam', loss='mse')
model.fit(X_train, y_train, epochs=100)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –•–æ—Ä–æ—à–æ</h3>
        <ul>
          <li>–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã</li>
          <li>–î–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (> 50 —à–∞–≥–æ–≤)</li>
          <li>–ú—É–ª—å—Ç–∏–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã–µ —Ä—è–¥—ã</li>
          <li>–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ (> 10k —Ç–æ—á–µ–∫)</li>
          <li>–°–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ü–ª–æ—Ö–æ</h3>
        <ul>
          <li>–ú–∞–ª—ã–π –¥–∞—Ç–∞—Å–µ—Ç (< 1000)</li>
          <li>–ü—Ä–æ—Å—Ç—ã–µ –ª–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã</li>
          <li>–ù—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å</li>
          <li>–û–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –≤—ã—á–∏—Å–ª–µ–Ω–∏—è</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ (MinMaxScaler)</li>
      <li>[ ] –°–æ–∑–¥–∞—Ç—å sequences –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã</li>
      <li>[ ] Train/val/test split –ø–æ –≤—Ä–µ–º–µ–Ω–∏</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å LSTM –∏–ª–∏ GRU</li>
      <li>[ ] –î–æ–±–∞–≤–∏—Ç—å Dropout (0.2-0.3)</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å hidden_size (32-128)</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Adam optimizer</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å validation loss</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´LSTM –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ ‚Äî —ç—Ç–æ –∫–∞–∫ –æ–ø—ã—Ç–Ω—ã–π —Ç—Ä–µ–π–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–Ω–∏—Ç –∏—Å—Ç–æ—Ä–∏—é —Ü–µ–Ω –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, —á—Ç–æ–±—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –±—É–¥—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π, LSTM "–≤–∏–¥–∏—Ç" —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

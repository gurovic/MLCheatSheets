<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Constrained Optimization –≤ ML Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üìò Constrained Optimization –≤ ML</h1>
  <div class="subtitle">üìÖ 5 —è–Ω–≤–∞—Ä—è 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å –º–µ—Ç–æ–¥–æ–≤</h2>
    <p><strong>Constrained optimization</strong> ‚Äî –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏</p>
    <pre><code>minimize    f(x)
subject to  g(x) ‚â§ 0  (–Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞)
            h(x) = 0  (—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞)</code></pre>
    <ul>
      <li><strong>f(x)</strong> ‚Äî —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (loss function)</li>
      <li><strong>g(x), h(x)</strong> ‚Äî –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è (constraints)</li>
      <li><strong>x</strong> ‚Äî –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏–ª–∏ —Ä–µ—à–µ–Ω–∏—è</li>
    </ul>
    <blockquote>
      –í ML —á–∞—Å—Ç–æ –Ω—É–∂–Ω–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—à–∏–±–∫—É, –Ω–æ –∏ —Å–æ–±–ª—é–¥–∞—Ç—å –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
    </blockquote>

    </div>
<div class="block">
    <h2>üî∑ 2. –ó–∞—á–µ–º –Ω—É–∂–Ω—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ ML</h2>
    <ul>
      <li><strong>Fairness</strong>: —Ä–∞–≤–Ω—ã–µ –æ—à–∏–±–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø</li>
      <li><strong>Privacy</strong>: differential privacy bounds</li>
      <li><strong>Monotonicity</strong>: –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å –ø–æ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º</li>
      <li><strong>Shape constraints</strong>: –≤—ã–ø—É–∫–ª–æ—Å—Ç—å, –≥–ª–∞–¥–∫–æ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏</li>
      <li><strong>Sparsity</strong>: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ —á–∏—Å–ª–æ –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
      <li><strong>Resource limits</strong>: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞–º—è—Ç–∏, –≤—Ä–µ–º–µ–Ω–∏</li>
      <li><strong>Domain knowledge</strong>: —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ/–±–∏–∑–Ω–µ—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. –¢–∏–ø—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π</h2>
    <table>
      <tr><th>–¢–∏–ø</th><th>–ü—Ä–∏–º–µ—Ä</th></tr>
      <tr><td><strong>Box constraints</strong></td><td>0 ‚â§ w_i ‚â§ 1</td></tr>
      <tr><td><strong>Linear equality</strong></td><td>Œ£w_i = 1</td></tr>
      <tr><td><strong>Linear inequality</strong></td><td>Aw ‚â§ b</td></tr>
      <tr><td><strong>Quadratic</strong></td><td>||w||¬≤ ‚â§ C</td></tr>
      <tr><td><strong>Convex</strong></td><td>f(w) ‚â§ 0, f –≤—ã–ø—É–∫–ª–∞—è</td></tr>
      <tr><td><strong>Nonlinear</strong></td><td>g(w) = 0, g –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–∞—è</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ú–µ—Ç–æ–¥ –º–Ω–æ–∂–∏—Ç–µ–ª–µ–π –õ–∞–≥—Ä–∞–Ω–∂–∞</h2>
    <p><strong>–û—Å–Ω–æ–≤–∞ —Ç–µ–æ—Ä–∏–∏:</strong> –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ —à—Ç—Ä–∞—Ñ—ã</p>
    <pre><code>L(x, Œª, Œº) = f(x) + Œ£Œª_i¬∑g_i(x) + Œ£Œº_j¬∑h_j(x)</code></pre>
    <ul>
      <li><strong>Œª, Œº</strong> ‚Äî –º–Ω–æ–∂–∏—Ç–µ–ª–∏ –õ–∞–≥—Ä–∞–Ω–∂–∞ (Lagrange multipliers)</li>
      <li><strong>KKT —É—Å–ª–æ–≤–∏—è</strong>: –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —É—Å–ª–æ–≤–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏</li>
    </ul>
    <p><strong>Dual problem:</strong> –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –ø–æ Œª, Œº (—á–∞—Å—Ç–æ –ø—Ä–æ—â–µ —Ä–µ—à–∏—Ç—å)</p>
    <blockquote>
      SVM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç dual —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ú–µ—Ç–æ–¥—ã —Ä–µ—à–µ–Ω–∏—è</h2>
    <p><strong>1. Penalty methods (—à—Ç—Ä–∞—Ñ–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏):</strong></p>
    <pre><code># –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º constraint –≤ penalty
minimize f(x) + œÅ¬∑Œ£max(0, g_i(x))¬≤</code></pre>
    <p><strong>2. Barrier methods (–±–∞—Ä—å–µ—Ä–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏):</strong></p>
    <pre><code># –ë–∞—Ä—å–µ—Ä –≤–Ω—É—Ç—Ä–∏ –¥–æ–ø—É—Å—Ç–∏–º–æ–π –æ–±–ª–∞—Å—Ç–∏
minimize f(x) - Œº¬∑Œ£log(-g_i(x))</code></pre>
    <p><strong>3. Projected gradient descent:</strong></p>
    <pre><code># –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —à–∞–≥ + –ø—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ –¥–æ–ø—É—Å—Ç–∏–º—É—é –æ–±–ª–∞—Å—Ç—å
x_{t+1} = project(x_t - Œ±¬∑‚àáf(x_t))</code></pre>
    <p><strong>4. Augmented Lagrangian (ADMM):</strong></p>
    <ul>
      <li>–ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç Lagrange multipliers –∏ penalty</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 6. Projected Gradient Descent</h2>
    <pre><code>import numpy as np

def project_simplex(x):
    """–ü—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ —Å–∏–º–ø–ª–µ–∫—Å: x >= 0, sum(x) = 1"""
    n = len(x)
    x_sorted = np.sort(x)[::-1]
    cumsum = np.cumsum(x_sorted)
    idx = np.arange(n) + 1
    rho = np.where(x_sorted > (cumsum - 1) / idx)[0][-1]
    theta = (cumsum[rho] - 1) / (rho + 1)
    return np.maximum(x - theta, 0)

def pgd(f, grad_f, x0, n_iter=1000, lr=0.01):
    """Projected Gradient Descent"""
    x = x0.copy()
    for i in range(n_iter):
        # Gradient step
        x = x - lr * grad_f(x)
        # Projection
        x = project_simplex(x)
    return x</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ü—Ä–∞–∫—Ç–∏–∫–∞: scipy.optimize</h2>
    <pre><code>from scipy.optimize import minimize

# –§—É–Ω–∫—Ü–∏—è –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
def objective(x):
    return x[0]**2 + x[1]**2

def constraint_eq(x):
    return x[0] + x[1] - 1

def constraint_ineq(x):
    return x[0] - x[1]

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
constraints = [
    {'type': 'eq', 'fun': constraint_eq},
    {'type': 'ineq', 'fun': constraint_ineq}
]

# Bounds: 0 <= x_i <= 1
bounds = [(0, 1), (0, 1)]

# –†–µ—à–µ–Ω–∏–µ
result = minimize(
    objective, 
    x0=[0.5, 0.5],
    method='SLSQP',  # Sequential Least SQuares Programming
    bounds=bounds,
    constraints=constraints
)
print(result.x, result.fun)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Constrained ML: –ø—Ä–∏–º–µ—Ä—ã</h2>
    <p><strong>–ü—Ä–∏–º–µ—Ä 1: Portfolio Optimization</strong></p>
    <pre><code># min risk, subject to: sum(w) = 1, w >= 0
from scipy.optimize import minimize
import numpy as np

def portfolio_variance(w, Sigma):
    return w @ Sigma @ w

def optimize_portfolio(returns, Sigma):
    n = len(returns)
    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
    constraints = [
        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
    ]
    bounds = [(0, 1)] * n
    
    result = minimize(
        lambda w: portfolio_variance(w, Sigma),
        x0=np.ones(n)/n,
        method='SLSQP',
        bounds=bounds,
        constraints=constraints
    )
    return result.x</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Fairness Constraints</h2>
    <p><strong>–ó–∞–¥–∞—á–∞:</strong> –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å —Ä–∞–≤–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø</p>
    <pre><code># Constraint: |TPR_group1 - TPR_group2| <= Œµ
# TPR = True Positive Rate

from sklearn.linear_model import LogisticRegression

# Fairness-aware –æ–±—É—á–µ–Ω–∏–µ
def fair_logistic_regression(X, y, sensitive_attr, epsilon=0.05):
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –≥—Ä—É–ø–ø—ã
    group1 = sensitive_attr == 0
    group2 = sensitive_attr == 1
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–æ–¥–µ–ª—å
    model = LogisticRegression()
    
    # –î–æ–±–∞–≤–ª—è–µ–º fairness constraint —á–µ—Ä–µ–∑
    # post-processing –∏–ª–∏ regularization
    # (—É–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä)
    
    return model</code></pre>
    <p><strong>–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏:</strong></p>
    <ul>
      <li><code>fairlearn</code> ‚Äî fairness constraints –≤ sklearn</li>
      <li><code>AIF360</code> ‚Äî IBM AI Fairness 360</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 10. ADMM (Alternating Direction Method of Multipliers)</h2>
    <p><strong>–ú–æ—â–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –±–æ–ª—å—à–∏—Ö –∑–∞–¥–∞—á:</strong></p>
    <pre><code># –†–µ—à–∞–µ–º: min f(x) + g(z), subject to Ax + Bz = c
# ADMM –∏—Ç–µ—Ä–∞—Ü–∏–∏:
x_{k+1} = argmin_x L_œÅ(x, z_k, u_k)
z_{k+1} = argmin_z L_œÅ(x_{k+1}, z, u_k)
u_{k+1} = u_k + œÅ(Ax_{k+1} + Bz_{k+1} - c)</code></pre>
    <p><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:</strong></p>
    <ul>
      <li>Lasso —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏</li>
      <li>–†–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (–∫–∞–∂–¥–∞—è –º–∞—à–∏–Ω–∞ —Ä–µ—à–∞–µ—Ç —Å–≤–æ—é —á–∞—Å—Ç—å)</li>
      <li>Consensus optimization</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 11. Constrained Neural Networks</h2>
    <p><strong>–°–ø–æ—Å–æ–±—ã –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π:</strong></p>
    <ul>
      <li><strong>Penalty –≤ loss</strong>: <code>loss = mse + Œª¬∑penalty</code></li>
      <li><strong>Projection layers</strong>: –ø—Ä–æ–µ–∫—Ü–∏—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è</li>
      <li><strong>Constrained architectures</strong>: –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç constraint</li>
      <li><strong>Augmented Lagrangian</strong>: –¥–æ–±–∞–≤–ª—è–µ–º Lagrange multipliers</li>
    </ul>
    <pre><code># PyTorch: monotonicity constraint
class MonotonicNN(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, 1)
    
    def forward(self, x):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏
        with torch.no_grad():
            self.fc1.weight.clamp_(min=0)
            self.fc2.weight.clamp_(min=0)
        
        h = F.relu(self.fc1(x))
        return self.fc2(h)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td>SLSQP</td><td>scipy.optimize</td><td>–û–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è</td></tr>
      <tr><td>Interior Point</td><td>scipy, cvxpy</td><td>–í—ã–ø—É–∫–ª—ã–µ –∑–∞–¥–∞—á–∏</td></tr>
      <tr><td>Active Set</td><td>scipy, quadprog</td><td>QP –∑–∞–¥–∞—á–∏</td></tr>
      <tr><td>ADMM</td><td>cvxpy</td><td>–ë–æ–ª—å—à–∏–µ, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ</td></tr>
      <tr><td>Projected GD</td><td>Custom</td><td>–ü—Ä–æ—Å—Ç—ã–µ constraints</td></tr>
      <tr><td>Frank-Wolfe</td><td>Custom</td><td>–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ constraints</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 13. –í—ã–ø—É–∫–ª–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (CVXPY)</h2>
    <pre><code>import cvxpy as cp

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
x = cp.Variable(n)

# Objective
objective = cp.Minimize(cp.sum_squares(A @ x - b))

# Constraints
constraints = [
    x >= 0,           # Non-negative
    cp.sum(x) == 1,   # Sum to 1
    cp.norm(x, 1) <= C  # L1 constraint
]

# –†–µ—à–µ–Ω–∏–µ
problem = cp.Problem(objective, constraints)
problem.solve()

print("Optimal x:", x.value)
print("Optimal value:", problem.value)</code></pre>
    <p><strong>CVXPY</strong> –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç solver (ECOS, SCS, OSQP)</p>
  </div>

  <div class="block">
    <h2>üî∑ 14. –¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏</h2>
    <ul>
      <li>‚ùå <strong>–ù–µ—Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</strong> ‚Äî –ø—Ä–æ–≤–µ—Ä—å—Ç–µ feasibility</li>
      <li>‚ùå <strong>–°–ª–∏—à–∫–æ–º –∂—ë—Å—Ç–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</strong> ‚Äî –º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç –æ–±—É—á–∏—Ç—å—Å—è</li>
      <li>‚ùå <strong>–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–ø—É–∫–ª–æ—Å—Ç–∏</strong> ‚Äî –Ω–µ–≤—ã–ø—É–∫–ª—ã–µ –∑–∞–¥–∞—á–∏ —Å–ª–æ–∂–Ω–µ–µ</li>
      <li>‚ùå <strong>–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è</strong> constraints –∏ objective</li>
      <li>‚ùå <strong>–ó–∞–±—ã—Ç—å –ø—Ä–æ —á–∏—Å–ª–µ–Ω–Ω—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</strong></li>
      <li>‚úÖ <strong>–ü—Ä–æ–≤–µ—Ä—è—Ç—å KKT —É—Å–ª–æ–≤–∏—è</strong> –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏</li>
      <li>‚úÖ <strong>–ù–∞—á–∏–Ω–∞—Ç—å —Å feasible point</strong></li>
      <li>‚úÖ <strong>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏</strong> (cvxpy, scipy)</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ Constrained optimization –∫–æ–≥–¥–∞:</h3>
        <ul>
          <li>–ï—Å—Ç—å —á—ë—Ç–∫–∏–µ –±–∏–∑–Ω–µ—Å/—Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
          <li>–ù—É–∂–Ω–∞ fairness –∏–ª–∏ privacy</li>
          <li>–¢—Ä–µ–±—É–µ—Ç—Å—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å (monotonicity)</li>
          <li>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–Ω—ã –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
          <li>–ó–∞–¥–∞—á–∞ –≤—ã–ø—É–∫–ª–∞—è</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ù–µ –Ω—É–∂–Ω–æ –µ—Å–ª–∏:</h3>
        <ul>
          <li>–ù–µ—Ç —á—ë—Ç–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π</li>
          <li>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ loss</li>
          <li>–ó–∞–¥–∞—á–∞ —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞—è (deep NN)</li>
          <li>Constraints –º–µ–Ω—è—é—Ç—Å—è —á–∞—Å—Ç–æ</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –§–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å feasibility (–µ—Å—Ç—å –ª–∏ –¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Ç–æ—á–∫–∏)</li>
      <li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –∑–∞–¥–∞—á–∏ (–≤—ã–ø—É–∫–ª–∞—è/–Ω–µ–≤—ã–ø—É–∫–ª–∞—è)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –º–µ—Ç–æ–¥ —Ä–µ—à–µ–Ω–∏—è</li>
      <li>[ ] –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å objective –∏ constraints</li>
      <li>[ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π (cvxpy, scipy)</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å KKT —É—Å–ª–æ–≤–∏—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è</li>
      <li>[ ] –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å constraints –Ω–∞ test set</li>
      <li>[ ] –°—Ä–∞–≤–Ω–∏—Ç—å —Å unconstrained baseline</li>
    </ul>
    <blockquote>
      <strong>–ó–æ–ª–æ—Ç–æ–µ –ø—Ä–∞–≤–∏–ª–æ:</strong> –µ—Å–ª–∏ –∑–∞–¥–∞—á–∞ –≤—ã–ø—É–∫–ª–∞—è —Å –ª–∏–Ω–µ–π–Ω—ã–º–∏/–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≥–æ—Ç–æ–≤—ã–µ —Å–æ–ª–≤–µ—Ä—ã (cvxpy), –∏–Ω–∞—á–µ ‚Äî projected gradient –∏–ª–∏ ADMM.
    </blockquote>
  </div>



</div>
</body>
</html>

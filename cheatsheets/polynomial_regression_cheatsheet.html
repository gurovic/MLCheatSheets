<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Polynomial Regression Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 600;
    }

    .good p, .bad p {
      margin: 0;
      font-size: 0.88em;
    }

    em {
      color: #d32f2f;
      font-style: normal;
      font-weight: 600;
    }

    strong {
      color: #1a5fb4;
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üìà Polynomial Regression</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏</h2>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π</li>
      <li><strong>–ò–¥–µ—è</strong>: –¥–æ–±–∞–≤–∏—Ç—å —Å—Ç–µ–ø–µ–Ω–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (x, x¬≤, x¬≥, ...)</li>
      <li><strong>–û—Å—Ç–∞–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω–æ–π</strong>: –ø–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º!</li>
      <li><strong>–§–æ—Ä–º—É–ª–∞</strong>: y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + ... + Œ≤‚Çôx‚Åø</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: –∫–æ–≥–¥–∞ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç –∫—Ä–∏–≤–∏–∑–Ω—É</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–ø–∏—Å—å</h2>
    <p><strong>–î–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞:</strong></p>
    <p>y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + Œ≤‚ÇÉx¬≥ + ... + Œ≤‚Çôx‚Åø + Œµ</p>
    <p><strong>–î–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:</strong></p>
    <p>y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + Œ≤‚ÇÉx‚ÇÅ¬≤ + Œ≤‚ÇÑx‚ÇÅx‚ÇÇ + Œ≤‚ÇÖx‚ÇÇ¬≤ + ...</p>
    <ul>
      <li><strong>n</strong> - —Å—Ç–µ–ø–µ–Ω—å –ø–æ–ª–∏–Ω–æ–º–∞</li>
      <li><strong>–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è</strong>: x‚ÇÅx‚ÇÇ, x‚ÇÅx‚ÇÉ, ...</li>
      <li><strong>Œµ</strong> - –æ—à–∏–±–∫–∞</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥</h2>
    <pre><code>from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
import numpy as np

# –î–∞–Ω–Ω—ã–µ
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 7, 18, 35, 58])

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
model = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('linear', LinearRegression())
])

# –û–±—É—á–µ–Ω–∏–µ
model.fit(X, y)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
y_pred = model.predict(X)

# R¬≤ score
print(f"R¬≤: {model.score(X, y):.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –†—É—á–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</h2>
    <pre><code>from sklearn.preprocessing import PolynomialFeatures

# –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
X = np.array([[2, 3]])

# –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å—Ç–µ–ø–µ–Ω–∏ 2
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# –†–µ–∑—É–ª—å—Ç–∞—Ç: [1, 2, 3, 4, 6, 9]
# 1 - bias (–∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞)
# 2, 3 - –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
# 4 - x‚ÇÅ¬≤
# 6 - x‚ÇÅx‚ÇÇ (–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ)
# 9 - x‚ÇÇ¬≤

print(f"–ù–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {X_poly}")
print(f"–ù–∞–∑–≤–∞–Ω–∏—è: {poly.get_feature_names_out()}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã PolynomialFeatures</h2>
    <table>
      <tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–ó–Ω–∞—á–µ–Ω–∏–µ</th></tr>
      <tr><td><code>degree</code></td><td>–°—Ç–µ–ø–µ–Ω—å –ø–æ–ª–∏–Ω–æ–º–∞</td><td>2-4 (–æ–±—ã—á–Ω–æ)</td></tr>
      <tr><td><code>interaction_only</code></td><td>–¢–æ–ª—å–∫–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è</td><td>False (—É–º–æ–ª—á.)</td></tr>
      <tr><td><code>include_bias</code></td><td>–î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É</td><td>True (—É–º–æ–ª—á.)</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 6. –í—ã–±–æ—Ä —Å—Ç–µ–ø–µ–Ω–∏ –ø–æ–ª–∏–Ω–æ–º–∞</h2>
    <pre><code>from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

degrees = range(1, 11)
train_scores = []
cv_scores = []

for degree in degrees:
    model = Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('linear', LinearRegression())
    ])
    
    # Train score
    model.fit(X_train, y_train)
    train_scores.append(model.score(X_train, y_train))
    
    # CV score
    cv_score = cross_val_score(
        model, X_train, y_train, 
        cv=5, scoring='r2'
    ).mean()
    cv_scores.append(cv_score)

plt.plot(degrees, train_scores, label='Train')
plt.plot(degrees, cv_scores, label='CV')
plt.xlabel('–°—Ç–µ–ø–µ–Ω—å –ø–æ–ª–∏–Ω–æ–º–∞')
plt.ylabel('R¬≤ Score')
plt.legend()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π</h2>
    <pre><code>import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
np.random.seed(42)
X = np.linspace(0, 10, 100).reshape(-1, 1)
y = 2 + 3*X + 0.5*X**2 + np.random.randn(100, 1)*2

# –ú–æ–¥–µ–ª–∏ —Ä–∞–∑–Ω—ã—Ö —Å—Ç–µ–ø–µ–Ω–µ–π
degrees = [1, 2, 3, 5]
plt.figure(figsize=(12, 8))

for i, degree in enumerate(degrees, 1):
    plt.subplot(2, 2, i)
    
    # –û–±—É—á–µ–Ω–∏–µ
    poly = PolynomialFeatures(degree=degree)
    X_poly = poly.fit_transform(X)
    model = LinearRegression()
    model.fit(X_poly, y)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    X_test = np.linspace(0, 10, 300).reshape(-1, 1)
    X_test_poly = poly.transform(X_test)
    y_pred = model.predict(X_test_poly)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.scatter(X, y, alpha=0.5)
    plt.plot(X_test, y_pred, 'r-', linewidth=2)
    plt.title(f'–°—Ç–µ–ø–µ–Ω—å {degree}')
    plt.xlabel('X')
    plt.ylabel('y')

plt.tight_layout()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∏ –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ</h2>
    <table>
      <tr><th>–°—Ç–µ–ø–µ–Ω—å</th><th>–ü—Ä–æ–±–ª–µ–º–∞</th><th>–ü—Ä–∏–∑–Ω–∞–∫–∏</th></tr>
      <tr><td><strong>1</strong></td><td>–ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ</td><td>–ù–∏–∑–∫–∏–π R¬≤ –Ω–∞ train –∏ test</td></tr>
      <tr><td><strong>2-3</strong></td><td>–û–ø—Ç–∏–º–∞–ª—å–Ω–æ</td><td>–•–æ—Ä–æ—à–∏–π R¬≤ –Ω–∞ –æ–±–æ–∏—Ö</td></tr>
      <tr><td><strong>4-6</strong></td><td>–†–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è</td><td>High train, low test R¬≤</td></tr>
      <tr><td><strong>>7</strong></td><td>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</td><td>–û–≥—Ä–æ–º–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ train/test</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 9. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏</h2>
    <pre><code>from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import StandardScaler

# Ridge —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –ø–æ–ª–∏–Ω–æ–º–∞–º–∏
model = Pipeline([
    ('poly', PolynomialFeatures(degree=5)),
    ('scaler', StandardScaler()),
    ('ridge', Ridge(alpha=1.0))
])

model.fit(X_train, y_train)

# –ò–ª–∏ Lasso
model_lasso = Pipeline([
    ('poly', PolynomialFeatures(degree=5)),
    ('scaler', StandardScaler()),
    ('lasso', Lasso(alpha=0.1))
])

model_lasso.fit(X_train, y_train)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
print(f"Ridge R¬≤: {model.score(X_test, y_test):.4f}")
print(f"Lasso R¬≤: {model_lasso.score(X_test, y_test):.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –¢–æ–ª—å–∫–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è</h2>
    <pre><code># –ë–µ–∑ —Å—Ç–µ–ø–µ–Ω–µ–π, —Ç–æ–ª—å–∫–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
poly = PolynomialFeatures(
    degree=2, 
    interaction_only=True,
    include_bias=False
)

X = np.array([[1, 2, 3]])
X_inter = poly.fit_transform(X)

# [1, 2, 3, 2, 3, 6]
# x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, x‚ÇÅx‚ÇÇ, x‚ÇÅx‚ÇÉ, x‚ÇÇx‚ÇÉ
# –ë–µ–∑ x‚ÇÅ¬≤, x‚ÇÇ¬≤, x‚ÇÉ¬≤

print(poly.get_feature_names_out())</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</h2>
    <p><strong>–í–∞–∂–Ω–æ!</strong> –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –º–æ–≥—É—Ç –∏–º–µ—Ç—å –æ—á–µ–Ω—å —Ä–∞–∑–Ω—ã–µ –º–∞—Å—à—Ç–∞–±—ã:</p>
    <pre><code>from sklearn.preprocessing import StandardScaler

# –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω
model = Pipeline([
    ('poly', PolynomialFeatures(degree=3)),
    ('scaler', StandardScaler()),  # ‚Üê –í–∞–∂–Ω–æ!
    ('linear', LinearRegression())
])

# –ë–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç–∏–ø–∞ x¬≥ 
# –±—É–¥—É—Ç –Ω–∞–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ x, —á—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ 
# –∫ —á–∏—Å–ª–µ–Ω–Ω–æ–π –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</h2>
    <pre><code>from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# –û–±—É—á–µ–Ω–∏–µ
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
model = LinearRegression()
model.fit(X_poly, y)

# –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
feature_names = poly.get_feature_names_out()
coefficients = model.coef_[0]

# –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
importance = sorted(
    zip(feature_names, np.abs(coefficients)), 
    key=lambda x: x[1], 
    reverse=True
)

for name, coef in importance[:5]:
    print(f"{name}: {coef:.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ú–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–µ –ø–æ–ª–∏–Ω–æ–º—ã</h2>
    <pre><code># –î–ª—è 3 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Å—Ç–µ–ø–µ–Ω—å 2
X = np.array([[1, 2, 3]])
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:
# (n + d)! / (n! √ó d!)
# –≥–¥–µ n - –∏—Å—Ö–æ–¥–Ω–æ–µ —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# d - —Å—Ç–µ–ø–µ–Ω—å –ø–æ–ª–∏–Ω–æ–º–∞

# –î–ª—è 3 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Å—Ç–µ–ø–µ–Ω—å 2: 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# –î–ª—è 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Å—Ç–µ–ø–µ–Ω—å 2: 66 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# –î–ª—è 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Å—Ç–µ–ø–µ–Ω—å 3: 286 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤!

print(f"–ù–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_poly.shape[1]}")
print(f"–ù–∞–∑–≤–∞–Ω–∏—è:\n{poly.get_feature_names_out()}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <table>
      <tr><th>–û–±–ª–∞—Å—Ç—å</th><th>–ó–∞–¥–∞—á–∞</th></tr>
      <tr><td><strong>–§–∏–∑–∏–∫–∞</strong></td><td>–¢—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, —É—Å–∫–æ—Ä–µ–Ω–∏—è</td></tr>
      <tr><td><strong>–≠–∫–æ–Ω–æ–º–∏–∫–∞</strong></td><td>–ö—Ä–∏–≤—ã–µ —Å–ø—Ä–æ—Å–∞/–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è</td></tr>
      <tr><td><strong>–ë–∏–æ–ª–æ–≥–∏—è</strong></td><td>–†–æ—Å—Ç –ø–æ–ø—É–ª—è—Ü–∏–π</td></tr>
      <tr><td><strong>–ò–Ω–∂–µ–Ω–µ—Ä–∏—è</strong></td><td>–ö–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω—ã–µ –∫—Ä–∏–≤—ã–µ</td></tr>
      <tr><td><strong>–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥</strong></td><td>–ö—Ä–∏–≤—ã–µ –æ—Ç–∫–ª–∏–∫–∞</td></tr>
      <tr><td><strong>–§–∏–Ω–∞–Ω—Å—ã</strong></td><td>–î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 15. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</th><th>–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</th></tr>
      <tr><td><strong>–õ–∏–Ω–µ–π–Ω–∞—è</strong></td><td>–ü—Ä–æ—Å—Ç–∞—è, –±—ã—Å—Ç—Ä–∞—è</td><td>–¢–æ–ª—å–∫–æ –ª–∏–Ω–µ–π–Ω—ã–µ —Å–≤—è–∑–∏</td></tr>
      <tr><td><strong>–ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è</strong></td><td>–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∫—Ä–∏–≤—ã–µ</td><td>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ, –º–∞—Å—à—Ç–∞–±</td></tr>
      <tr><td><strong>–î–µ—Ä–µ–≤–æ</strong></td><td>–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ, –∞–≤—Ç–æ–º–∞—Å—à—Ç–∞–±</td><td>–°—Ç—É–ø–µ–Ω—á–∞—Ç—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è</td></tr>
      <tr><td><strong>SVM (RBF)</strong></td><td>–ì–∏–±–∫–∞—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å</td><td>–ú–µ–¥–ª–µ–Ω–Ω–µ–µ, —Å–ª–æ–∂–Ω–µ–µ</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å—Ç–µ–ø–µ–Ω–∏</h2>
    <pre><code>from sklearn.model_selection import GridSearchCV

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞–π–ø–ª–∞–π–Ω
pipeline = Pipeline([
    ('poly', PolynomialFeatures()),
    ('scaler', StandardScaler()),
    ('linear', LinearRegression())
])

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
param_grid = {
    'poly__degree': [1, 2, 3, 4, 5]
}

# Grid search —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
grid_search = GridSearchCV(
    pipeline, 
    param_grid, 
    cv=5,
    scoring='r2',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print(f"–õ—É—á—à–∞—è —Å—Ç–µ–ø–µ–Ω—å: {grid_search.best_params_}")
print(f"–õ—É—á—à–∏–π R¬≤: {grid_search.best_score_:.4f}")

# –¢–µ—Å—Ç–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
test_score = grid_search.score(X_test, y_test)
print(f"–¢–µ—Å—Ç R¬≤: {test_score:.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. –≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è</h2>
    <p><strong>–û—Å—Ç–æ—Ä–æ–∂–Ω–æ!</strong> –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –ø–ª–æ—Ö–æ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö:</p>
    <pre><code># –û–±—É—á–µ–Ω–∏–µ –Ω–∞ X ‚àà [0, 10]
X_train = np.linspace(0, 10, 100).reshape(-1, 1)
y_train = 2*X_train + noise

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ X ‚àà [10, 20]
X_test = np.linspace(10, 20, 50).reshape(-1, 1)

# –í—ã—Å–æ–∫–∏–µ —Å—Ç–µ–ø–µ–Ω–∏ –¥–∞—é—Ç –ø–ª–æ—Ö–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã!
# –û—Å–æ–±–µ–Ω–Ω–æ —Å—Ç–µ–ø–µ–Ω–∏ > 3

# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è 
# –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ü—Ä–∏–º–µ—Ä: –ø—Ä–æ–≥–Ω–æ–∑ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã</h2>
    <pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# –î–∞–Ω–Ω—ã–µ: –¥–µ–Ω—å –≥–æ–¥–∞ ‚Üí —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
days = np.arange(1, 366)
# –°–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å (–≥–æ–¥–∏—á–Ω—ã–π —Ü–∏–∫–ª)
temps = 15 + 10*np.sin(2*np.pi*days/365) + np.random.randn(365)*2

X = days.reshape(-1, 1)
y = temps

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –ú–æ–¥–µ–ª—å
model = Pipeline([
    ('poly', PolynomialFeatures(degree=5)),
    ('scaler', StandardScaler()),
    ('ridge', Ridge(alpha=10.0))
])

model.fit(X_train, y_train)
print(f"R¬≤: {model.score(X_test, y_test):.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 19. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</h2>
    <pre><code>from sklearn.metrics import mean_squared_error

degrees = [1, 2, 3, 5, 10]

for degree in degrees:
    model = Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('scaler', StandardScaler()),
        ('linear', LinearRegression())
    ])
    
    model.fit(X_train, y_train)
    
    train_rmse = np.sqrt(mean_squared_error(
        y_train, model.predict(X_train)
    ))
    test_rmse = np.sqrt(mean_squared_error(
        y_test, model.predict(X_test)
    ))
    
    print(f"–°—Ç–µ–ø–µ–Ω—å {degree}:")
    print(f"  Train RMSE: {train_rmse:.3f}")
    print(f"  Test RMSE: {test_rmse:.3f}")
    print(f"  –†–∞–∑–Ω–∏—Ü–∞: {test_rmse - train_rmse:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 20. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –î–µ–ª–∞—Ç—å</h3>
        <p>‚Ä¢ –ù–∞—á–∏–Ω–∞—Ç—å —Å–æ —Å—Ç–µ–ø–µ–Ω–∏ 2-3<br>
        ‚Ä¢ –í—Å–µ–≥–¥–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏<br>
        ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é<br>
        ‚Ä¢ –ü—Ä–∏–º–µ–Ω—è—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è –≤—ã—Å–æ–∫–∏—Ö —Å—Ç–µ–ø–µ–Ω–µ–π<br>
        ‚Ä¢ –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è</p>
      </div>
      <div class="bad">
        <h3>‚ùå –ù–µ –¥–µ–ª–∞—Ç—å</h3>
        <p>‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–µ–ø–µ–Ω—å > 5 –±–µ–∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏<br>
        ‚Ä¢ –ó–∞–±—ã–≤–∞—Ç—å –ø—Ä–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ<br>
        ‚Ä¢ –≠–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –¥–∞–Ω–Ω—ã—Ö<br>
        ‚Ä¢ –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ<br>
        ‚Ä¢ –ü—Ä–∏–º–µ–Ω—è—Ç—å –±–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞</p>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 21. –¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏</h2>
    <table>
      <tr><th>–û—à–∏–±–∫–∞</th><th>–†–µ—à–µ–Ω–∏–µ</th></tr>
      <tr><td>–°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∞—è —Å—Ç–µ–ø–µ–Ω—å</td><td>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CV –¥–ª—è –≤—ã–±–æ—Ä–∞</td></tr>
      <tr><td>–ù–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å</td><td>–î–æ–±–∞–≤–∏—Ç—å StandardScaler</td></tr>
      <tr><td>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</td><td>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Ridge/Lasso)</td></tr>
      <tr><td>–ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</td><td>–£–º–µ–Ω—å—à–∏—Ç—å —Å—Ç–µ–ø–µ–Ω—å –∏–ª–∏ —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</td></tr>
      <tr><td>–ü–ª–æ—Ö–∞—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è</td><td>–ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 22. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤</h2>
    <pre><code># –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
model = LinearRegression()
model.fit(X_poly, y)

# –£—Ä–∞–≤–Ω–µ–Ω–∏–µ
features = poly.get_feature_names_out()
coefs = model.coef_[0]
intercept = model.intercept_

print(f"y = {intercept:.2f}")
for feat, coef in zip(features[1:], coefs[1:]):
    if coef >= 0:
        print(f"  + {coef:.2f}*{feat}")
    else:
        print(f"  - {abs(coef):.2f}*{feat}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 23. –†–µ—Å—É—Ä—Å—ã</h2>
    <ul>
      <li><strong>–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è</strong>: sklearn.preprocessing.PolynomialFeatures</li>
      <li><strong>–°—Ç–∞—Ç—å—è</strong>: "Polynomial Regression" –Ω–∞ StatQuest</li>
      <li><strong>–ö–Ω–∏–≥–∞</strong>: "Introduction to Statistical Learning"</li>
      <li><strong>–í–∏–¥–µ–æ</strong>: "Polynomial Regression" - StatQuest</li>
      <li><strong>–ü—Ä–∞–∫—Ç–∏–∫–∞</strong>: Kaggle - Polynomial Regression Tutorial</li>
    </ul>
  </div>

</div>

</div>
</body>
</html>

<!DOCTYPE html><html lang="ru"><head><meta charset="UTF-8"><title>Bias-Variance Trade-off Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title><style>@media screen{body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Helvetica,Arial,sans-serif;color:#333;background:#fafcff;padding:10px}}@media print{body{background:white;padding:0}@page{size:A4 landscape;margin:10mm}}.container{column-count:3;column-gap:20px;max-width:100%}.block{break-inside:avoid;margin-bottom:1.2em;padding:12px;background:white;border-radius:6px;box-shadow:0 1px 3px rgba(0,0,0,0.05)}h1{font-size:1.6em;font-weight:700;color:#1a5fb4;text-align:center;margin:0 0 8px;column-span:all}.subtitle{text-align:center;color:#666;font-size:0.9em;margin-bottom:12px;column-span:all}h2{font-size:1.15em;font-weight:700;color:#1a5fb4;margin:0 0 8px;padding-bottom:4px;border-bottom:1px solid #e0e7ff}p,ul,ol{font-size:0.92em;margin:0.6em 0}ul,ol{padding-left:18px}li{margin-bottom:4px}code{font-family:'Consolas','Courier New',monospace;background-color:#f0f4ff;padding:1px 4px;border-radius:3px;font-size:0.88em}pre{background-color:#f0f4ff;padding:8px;border-radius:4px;overflow-x:auto;font-size:0.84em;margin:6px 0}pre code{padding:0;background:none;white-space:pre-wrap}table{width:100%;border-collapse:collapse;font-size:0.82em;margin:6px 0}th{background-color:#e6f0ff;text-align:left;padding:4px 6px;font-weight:600}td{padding:4px 6px;border-bottom:1px solid #f0f4ff}tr:nth-child(even){background-color:#f8fbff}.good-vs-bad{display:flex;flex-direction:column;gap:8px}.good-vs-bad div{flex:1;padding:6px 8px;border-radius:4px}.good{background-color:#f0f9f4;border-left:3px solid #2e8b57}.bad{background-color:#fdf0f2;border-left:3px solid #d32f2f}.good h3,.bad h3{margin:0 0 4px;font-size:1em;font-weight:700}.good ul,.bad ul{padding-left:20px;margin:0}.good li::before{content:"‚úÖ ";font-weight:bold}.bad li::before{content:"‚ùå ";font-weight:bold}blockquote{font-style:italic;margin:8px 0;padding:6px 10px;background:#f8fbff;border-left:2px solid #1a5fb4;font-size:0.88em}.formula{text-align:center;font-family:'Cambria Math',serif;font-size:1em;margin:10px 0;padding:8px;background:#f8fbff;border-radius:4px}@media print{.container{column-gap:12px}.block{box-shadow:none}code,pre,table{font-size:0.78em}h1{font-size:1.4em}h2{font-size:1em}}</style></head><body><div class="container"><h1>‚öñÔ∏è Bias-Variance Trade-off Cheatsheet</h1><div class="subtitle">–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ ‚Ä¢ –ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ ‚Ä¢ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏<br>üìÖ 3 —è–Ω–≤–∞—Ä—è 2026</div><div class="block"><h2>üî∑ 1. –°—É—Ç—å</h2><ul><li><strong>–û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏</strong> = Bias¬≤ + Variance + Irreducible Error</li><li><strong>Bias (—Å–º–µ—â–µ–Ω–∏–µ)</strong>: –æ—à–∏–±–∫–∞ –∏–∑-–∑–∞ —É–ø—Ä–æ—â–µ–Ω–∏–π –º–æ–¥–µ–ª–∏</li><li><strong>Variance (–¥–∏—Å–ø–µ—Ä—Å–∏—è)</strong>: —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –æ–±—É—á–∞—é—â–∏–º –¥–∞–Ω–Ω—ã–º</li><li><strong>Trade-off</strong>: —É–º–µ–Ω—å—à–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –¥—Ä—É–≥–æ–µ</li></ul><div class="formula">Error = Bias¬≤ + Variance + œÉ¬≤</div></div><div class="block"><h2>üî∑ 2. Bias (–°–º–µ—â–µ–Ω–∏–µ)</h2><ul><li><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: –Ω–∞—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—à–∏–±–∞–µ—Ç—Å—è</li><li><strong>–í—ã—Å–æ–∫–∏–π bias</strong>: –º–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–∞—è (–Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ)</li><li><strong>–ü—Ä–∏–º–µ—Ä—ã</strong>: –ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li><li><strong>–°–∏–º–ø—Ç–æ–º—ã</strong>: –ø–ª–æ—Ö–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train –∏ test</li></ul><pre><code># –ü—Ä–∏–º–µ—Ä: –≤—ã—Å–æ–∫–∏–π bias
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
# –û—à–∏–±–∫–∞ –Ω–∞ train –≤—ã—Å–æ–∫–∞—è ‚Üí –≤—ã—Å–æ–∫–∏–π bias</code></pre></div><div class="block"><h2>üî∑ 3. Variance (–î–∏—Å–ø–µ—Ä—Å–∏—è)</h2><ul><li><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–µ–Ω—è—é—Ç—Å—è –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö train –¥–∞–Ω–Ω—ã—Ö</li><li><strong>–í—ã—Å–æ–∫–∞—è variance</strong>: –º–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞—è (–ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)</li><li><strong>–ü—Ä–∏–º–µ—Ä—ã</strong>: –≥–ª—É–±–æ–∫–æ–µ –¥–µ—Ä–µ–≤–æ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π</li><li><strong>–°–∏–º–ø—Ç–æ–º—ã</strong>: –æ—Ç–ª–∏—á–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train, –ø–ª–æ—Ö–∞—è –Ω–∞ test</li></ul><pre><code># –ü—Ä–∏–º–µ—Ä: –≤—ã—Å–æ–∫–∞—è variance
from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()  # –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
model.fit(X_train, y_train)
# Train –æ—Ç–ª–∏—á–Ω–æ, test –ø–ª–æ—Ö–æ ‚Üí –≤—ã—Å–æ–∫–∞—è variance</code></pre></div><div class="block"><h2>üî∑ 4. –í–∏–∑—É–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ</h2><table><tr><th>–ú–æ–¥–µ–ª—å</th><th>Bias</th><th>Variance</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th></tr><tr><td>–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è</td><td>–í—ã—Å–æ–∫–∏–π</td><td>–ù–∏–∑–∫–∞—è</td><td>–ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ</td></tr><tr><td>–î–µ—Ä–µ–≤–æ (depth=3)</td><td>–°—Ä–µ–¥–Ω–∏–π</td><td>–°—Ä–µ–¥–Ω—è—è</td><td>–ë–∞–ª–∞–Ω—Å ‚úì</td></tr><tr><td>–î–µ—Ä–µ–≤–æ (–±–µ–∑ –ª–∏–º–∏—Ç–æ–≤)</td><td>–ù–∏–∑–∫–∏–π</td><td>–í—ã—Å–æ–∫–∞—è</td><td>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</td></tr><tr><td>Random Forest</td><td>–ù–∏–∑–∫–∏–π</td><td>–°—Ä–µ–¥–Ω—è—è</td><td>–•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å</td></tr></table></div><div class="block"><h2>üî∑ 5. –ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è</h2><pre><code>from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import numpy as np

train_sizes, train_scores, test_scores = learning_curve(
    model, X, y,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5, scoring='r2'
)

plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores.mean(axis=1), label='Train')
plt.plot(train_sizes, test_scores.mean(axis=1), label='Test')
plt.xlabel('Training examples')
plt.ylabel('Score')
plt.legend()
plt.title('Learning Curves')
plt.show()</code></pre></div><div class="block"><h2>üî∑ 6. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è</h2><ul><li><strong>–í—ã—Å–æ–∫–∏–π bias</strong>: train –∏ test –æ—à–∏–±–∫–∏ –±–ª–∏–∑–∫–∏, –Ω–æ –≤—ã—Å–æ–∫–∏</li><li><strong>–í—ã—Å–æ–∫–∞—è variance</strong>: –±–æ–ª—å—à–æ–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É train –∏ test</li><li><strong>–•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å</strong>: train –∏ test –±–ª–∏–∑–∫–∏, –æ–±–µ –Ω–∏–∑–∫–∏–µ</li></ul><p>üìä <strong>–í—ã—Å–æ–∫–∏–π bias:</strong> train ‚âà test, –æ–±–µ –ø–ª–æ—Ö–∏–µ</p><p>üìä <strong>–í—ã—Å–æ–∫–∞—è variance:</strong> train –æ—Ç–ª–∏—á–Ω–æ, test –ø–ª–æ—Ö–æ</p><p>üìä <strong>–ë–∞–ª–∞–Ω—Å:</strong> train ‚âà test, –æ–±–µ —Ö–æ—Ä–æ—à–∏–µ</p></div><div class="block"><h2>üî∑ 7. –°–ø–æ—Å–æ–±—ã —É–º–µ–Ω—å—à–∏—Ç—å Bias</h2><ul><li>‚úÖ –£—Å–ª–æ–∂–Ω–∏—Ç—å –º–æ–¥–µ–ª—å (–±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)</li><li>‚úÖ –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li><li>‚úÖ –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏</li><li>‚úÖ –£–º–µ–Ω—å—à–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é</li><li>‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º</li></ul><pre><code># –£–º–µ–Ω—å—à–∏—Ç—å bias
from sklearn.preprocessing import PolynomialFeatures

# –î–æ–±–∞–≤–∏—Ç—å –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

model = LinearRegression()
model.fit(X_poly, y)</code></pre></div><div class="block"><h2>üî∑ 8. –°–ø–æ—Å–æ–±—ã —É–º–µ–Ω—å—à–∏—Ç—å Variance</h2><ul><li>‚úÖ –£–ø—Ä–æ—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å (–º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)</li><li>‚úÖ –ë–æ–ª—å—à–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö</li><li>‚úÖ –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é</li><li>‚úÖ –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è</li><li>‚úÖ –ê–Ω—Å–∞–º–±–ª–∏ (Random Forest, Bagging)</li><li>‚úÖ Early stopping</li><li>‚úÖ Dropout (–¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π)</li></ul><pre><code># –£–º–µ–Ω—å—à–∏—Ç—å variance
from sklearn.ensemble import RandomForestRegressor

# –ê–Ω—Å–∞–º–±–ª—å —É–º–µ–Ω—å—à–∞–µ—Ç variance
model = RandomForestRegressor(
    n_estimators=100,
    max_depth=5  # –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å
)
model.fit(X_train, y_train)</code></pre></div><div class="block"><h2>üî∑ 9. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</h2><pre><code>from sklearn.linear_model import Ridge, Lasso

# Ridge (L2) —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Lasso (L1) —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
lasso = Lasso(alpha=1.0)
lasso.fit(X_train, y_train)

# alpha ‚Üë ‚Üí bias ‚Üë, variance ‚Üì
# alpha ‚Üì ‚Üí bias ‚Üì, variance ‚Üë</code></pre></div><div class="block"><h2>üî∑ 10. Validation Curves</h2><pre><code>from sklearn.model_selection import validation_curve

param_range = [1, 2, 3, 5, 7, 10, 15, 20]
train_scores, test_scores = validation_curve(
    DecisionTreeRegressor(random_state=42),
    X, y,
    param_name='max_depth',
    param_range=param_range,
    cv=5, scoring='r2'
)

plt.figure(figsize=(10, 6))
plt.plot(param_range, train_scores.mean(axis=1), label='Train')
plt.plot(param_range, test_scores.mean(axis=1), label='Test')
plt.xlabel('max_depth')
plt.ylabel('R¬≤ Score')
plt.legend()
plt.title('Validation Curve')
plt.show()</code></pre></div><div class="block"><h2>üî∑ 11. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã</h2><table><tr><th>–°–∏–º–ø—Ç–æ–º—ã</th><th>–ü—Ä–æ–±–ª–µ–º–∞</th><th>–†–µ—à–µ–Ω–∏–µ</th></tr><tr><td>Train ‚Üì, Test ‚Üì</td><td>–í—ã—Å–æ–∫–∏–π bias</td><td>–£—Å–ª–æ–∂–Ω–∏—Ç—å –º–æ–¥–µ–ª—å</td></tr><tr><td>Train ‚Üë, Test ‚Üì</td><td>–í—ã—Å–æ–∫–∞—è variance</td><td>–£–ø—Ä–æ—Å—Ç–∏—Ç—å –∏–ª–∏ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö</td></tr><tr><td>Train ‚Üë, Test ‚Üë</td><td>–•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å</td><td>–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é</td></tr></table></div><div class="block"><h2>üî∑ 12. –°–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏</h2><div class="good-vs-bad"><div class="bad"><h3>‚ùå –°–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–∞—è (High Bias)</h3><ul><li>–õ–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li><li>–ú–∞–ª–æ–µ —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li><li>–°–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</li><li>–ú–µ–ª–∫–æ–µ –¥–µ—Ä–µ–≤–æ (max_depth=1)</li></ul></div><div class="good"><h3>‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å</h3><ul><li>–ë–∞–ª–∞–Ω—Å bias-variance</li><li>–•–æ—Ä–æ—à–æ –Ω–∞ train –∏ test</li><li>–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è</li><li>–ü–æ–¥–æ–±—Ä–∞–Ω–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</li></ul></div></div></div><div class="block"><h2>üî∑ 13. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä</h2><pre><code>import numpy as np
from sklearn.model_selection import cross_val_score

# –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å (–≤—ã—Å–æ–∫–∏–π bias)
simple = LinearRegression()
simple_scores = cross_val_score(simple, X, y, cv=5, scoring='r2')

# –°–ª–æ–∂–Ω–∞—è –º–æ–¥–µ–ª—å (–≤—ã—Å–æ–∫–∞—è variance)
complex = DecisionTreeRegressor()
complex_scores = cross_val_score(complex, X, y, cv=5, scoring='r2')

# –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
balanced = DecisionTreeRegressor(max_depth=5, min_samples_split=20)
balanced_scores = cross_val_score(balanced, X, y, cv=5, scoring='r2')

print(f"Simple: {simple_scores.mean():.3f}")
print(f"Complex: {complex_scores.mean():.3f}")
print(f"Balanced: {balanced_scores.mean():.3f}")</code></pre></div><div class="block"><h2>üî∑ 14. –†–æ–ª—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö</h2><ul><li><strong>–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö</strong>: –≤—ã—Å–æ–∫–∞—è variance (–ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –ª–µ–≥–∫–æ)</li><li><strong>–ú–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö</strong>: variance ‚Üì, –Ω–æ bias –æ—Å—Ç–∞–µ—Ç—Å—è</li><li><strong>Bias –Ω–µ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è</strong> —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö</li><li><strong>Variance —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è</strong> —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö</li></ul><blockquote>¬´–ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ–º–æ–≥–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π variance. –ü—Ä–∏ –≤—ã—Å–æ–∫–æ–º bias –Ω—É–∂–Ω–∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –º–æ–¥–µ–ª—å¬ª</blockquote></div><div class="block"><h2>üî∑ 15. –ß–µ–∫-–ª–∏—Å—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏</h2><ul><li>[ ] –û—Ü–µ–Ω–∏—Ç—å –æ—à–∏–±–∫—É –Ω–∞ train –∏ test</li><li>[ ] –ü–æ—Å—Ç—Ä–æ–∏—Ç—å learning curves</li><li>[ ] –ü–æ—Å—Ç—Ä–æ–∏—Ç—å validation curves</li><li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É (bias/variance)</li><li>[ ] –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è</li><li>[ ] –ü–æ–≤—Ç–æ—Ä–∏—Ç—å –æ—Ü–µ–Ω–∫—É</li></ul></div><div class="block"><h2>üî∑ 16. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2><ul><li><strong>–ù–∞—á–Ω–∏—Ç–µ —Å –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏</strong>: –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É—Å–ª–æ–∂–Ω—è–π—Ç–µ</li><li><strong>–í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é</strong></li><li><strong>–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è</strong></li><li><strong>–ù–µ –≥–æ–Ω–∏—Ç–µ—Å—å –∑–∞ 100% –Ω–∞ train</strong>: —ç—Ç–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</li><li><strong>–ë–∞–ª–∞–Ω—Å –≤–∞–∂–Ω–µ–µ –º–∞–∫—Å–∏–º—É–º–∞</strong>: test –≤–∞–∂–Ω–µ–µ train</li></ul><blockquote>¬´Bias-variance trade-off ‚Äî —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è ML. –ü–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ö–æ—Ä–æ—à–∏—Ö –º–æ–¥–µ–ª–µ–π¬ª.</blockquote></div><div class="block"><h2>üîó –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏</h2><ul><li><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html" target="_blank">üìö Scikit-learn: Underfitting vs Overfitting</a></li><li><a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" target="_blank">üìù Wikipedia: Bias-variance tradeoff</a></li><li><a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" target="_blank">üìñ Understanding Bias-Variance</a></li></ul></div></div></body></html>

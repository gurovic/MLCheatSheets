<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Encoder-Decoder Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen{body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Helvetica,Arial,sans-serif;color:#333;background:#fafcff;padding:10px}}
    @media print{body{background:white;padding:0}@page{size:A4 landscape;margin:10mm}}
    .container{column-count:3;column-gap:20px;max-width:100%}
    .block{break-inside:avoid;margin-bottom:1.2em;padding:12px;background:white;border-radius:6px;box-shadow:0 1px 3px rgba(0,0,0,0.05)}
    h1{font-size:1.6em;font-weight:700;color:#1a5fb4;text-align:center;margin:0 0 8px;column-span:all}
    .subtitle{text-align:center;color:#666;font-size:0.9em;margin-bottom:12px;column-span:all}
    h2{font-size:1.15em;font-weight:700;color:#1a5fb4;margin:0 0 8px;padding-bottom:4px;border-bottom:1px solid #e0e7ff}
    p,ul,ol{font-size:0.92em;margin:0.6em 0}ul,ol{padding-left:18px}li{margin-bottom:4px}
    code{font-family:'Consolas','Courier New',monospace;background-color:#f0f4ff;padding:1px 4px;border-radius:3px;font-size:0.88em}
    pre{background-color:#f0f4ff;padding:8px;border-radius:4px;overflow-x:auto;font-size:0.84em;margin:6px 0}
    pre code{padding:0;background:none;white-space:pre-wrap}
    table{width:100%;border-collapse:collapse;font-size:0.82em;margin:6px 0}
    th{background-color:#e6f0ff;text-align:left;padding:4px 6px;font-weight:600}
    td{padding:4px 6px;border-bottom:1px solid #f0f4ff}tr:nth-child(even){background-color:#f8fbff}
    blockquote{font-style:italic;margin:8px 0;padding:6px 10px;background:#f8fbff;border-left:2px solid #1a5fb4;font-size:0.88em}
    @media print{.container{column-gap:12px}.block{box-shadow:none}code,pre,table{font-size:0.78em}h1{font-size:1.4em}h2{font-size:1em}}
  </style>
</head>
<body>
<div class="container">
  <h1>üîÑ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Encoder-Decoder</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>Encoder</strong>: —Å–∂–∏–º–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ</li>
      <li><strong>Decoder</strong>: –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≤—ã—Ö–æ–¥ –∏–∑ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: –ø–µ—Ä–µ–≤–æ–¥, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, image-to-image</li>
      <li><strong>–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è</strong>: sequence-to-sequence –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ</li>
    </ul>
    <blockquote>Encoder-Decoder ‚Äî —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –≥–¥–µ –æ–¥–Ω–∞ —Å–µ—Ç—å —Å–∂–∏–º–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∞ –¥—Ä—É–≥–∞—è –µ—ë —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç.</blockquote>

  <div class="block">
    <h2>üî∑ 2. –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã</h2>
    <table>
      <tr><th>–ö–æ–º–ø–æ–Ω–µ–Ω—Ç</th><th>–§—É–Ω–∫—Ü–∏—è</th><th>–í—ã—Ö–æ–¥</th></tr>
      <tr><td><strong>Encoder</strong></td><td>–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–∞</td><td>Context vector</td></tr>
      <tr><td><strong>Context</strong></td><td>–°–∂–∞—Ç–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ</td><td>–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä</td></tr>
      <tr><td><strong>Decoder</strong></td><td>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã—Ö–æ–¥–∞</td><td>–¶–µ–ª–µ–≤–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ (Keras)</h2>
    <pre><code>from tensorflow.keras.layers import LSTM, Dense, Input
from tensorflow.keras.models import Model

# Encoder
encoder_inputs = Input(shape=(None, num_features))
encoder = LSTM(256, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)
encoder_states = [state_h, state_c]

# Decoder
decoder_inputs = Input(shape=(None, num_features))
decoder_lstm = LSTM(256, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(
    decoder_inputs, 
    initial_state=encoder_states
)
decoder_dense = Dense(num_features, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>–ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥</strong>: —Ç–µ–∫—Å—Ç ‚Üí —Ç–µ–∫—Å—Ç (–¥—Ä—É–≥–æ–π —è–∑—ã–∫)</li>
      <li><strong>Summarization</strong>: –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Üí –∫–æ—Ä–æ—Ç–∫–∏–π</li>
      <li><strong>–ß–∞—Ç-–±–æ—Ç—ã</strong>: –≤–æ–ø—Ä–æ—Å ‚Üí –æ—Ç–≤–µ—Ç</li>
      <li><strong>Image Captioning</strong>: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ‚Üí —Ç–µ–∫—Å—Ç</li>
      <li><strong>Speech Recognition</strong>: –∞—É–¥–∏–æ ‚Üí —Ç–µ–∫—Å—Ç</li>
      <li><strong>Image-to-Image</strong>: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ‚Üí –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 5. –° Attention</h2>
    <pre><code>from tensorflow.keras.layers import Attention

# Encoder —Å return_sequences=True
encoder = LSTM(256, return_sequences=True, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)

# Decoder —Å attention
decoder_lstm = LSTM(256, return_sequences=True)
decoder_outputs = decoder_lstm(
    decoder_inputs,
    initial_state=[state_h, state_c]
)

# Attention layer
attention = Attention()
context = attention([decoder_outputs, encoder_outputs])

# Concatenate
decoder_combined = Concatenate()([context, decoder_outputs])
output = Dense(num_features, activation='softmax')(decoder_combined)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. PyTorch –ø—Ä–∏–º–µ—Ä</h2>
    <pre><code>import torch
import torch.nn as nn

class EncoderDecoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.decoder = nn.LSTM(output_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, src, trg):
        # Encoder
        _, (hidden, cell) = self.encoder(src)
        
        # Decoder
        outputs, _ = self.decoder(trg, (hidden, cell))
        
        # Output layer
        predictions = self.fc(outputs)
        return predictions</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –û–±—É—á–µ–Ω–∏–µ</h2>
    <pre><code># Teacher forcing
for epoch in range(epochs):
    for src, trg in dataloader:
        # src: –≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        # trg: —Ü–µ–ª–µ–≤–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        
        # Forward pass
        output = model(src, trg[:-1])  # –±–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
        
        # Loss
        loss = criterion(output, trg[1:])  # –±–µ–∑ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# Inference (–±–µ–∑ teacher forcing)
decoder_input = start_token
for t in range(max_length):
    output = model.decode_step(decoder_input, hidden)
    decoder_input = output.argmax()  # greedy
    if decoder_input == end_token:
        break</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] Encoder –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏</li>
      <li>[ ] Context vector —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞</li>
      <li>[ ] Decoder –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å attention –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π</li>
      <li>[ ] Teacher forcing –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏</li>
      <li>[ ] Beam search –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ</li>
    </ul>
    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>¬´Encoder-Decoder ‚Äî —ç—Ç–æ –∫–∞–∫ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫: encoder –ø–æ–Ω–∏–º–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç, –∞ decoder –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞–µ—Ç –µ–≥–æ –Ω–∞ –¥—Ä—É–≥–æ–º —è–∑—ã–∫–µ, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–º—ã—Å–ª¬ª.</blockquote>
  </div>

</div>
</div>
</body>
</html>

<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>EM-–∞–ª–≥–æ—Ä–∏—Ç–º Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üîÑ EM-–∞–ª–≥–æ—Ä–∏—Ç–º</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º–∞</h2>
    <ul>
      <li><strong>–ó–∞–¥–∞—á–∞</strong>: –æ—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Å–∫—Ä—ã—Ç—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö</li>
      <li><strong>–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å</strong>: –¥–≤–∞ —à–∞–≥–∞ (E –∏ M)</li>
      <li><strong>E-—à–∞–≥</strong>: –æ—Ü–µ–Ω–∫–∞ —Å–∫—Ä—ã—Ç—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö</li>
      <li><strong>M-—à–∞–≥</strong>: –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è</li>
      <li><strong>–ì–∞—Ä–∞–Ω—Ç–∏—è</strong>: –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ likelihood</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: GMM, HMM, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è, missing data</li>
    </ul>
    <blockquote>
      EM —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫—É—Ä–∏—Ü—ã –∏ —è–π—Ü–∞: –Ω—É–∂–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ —Å–∫—Ä—ã—Ç—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç.
    </blockquote>

  <div class="block">
    <h2>üî∑ 2. –ê–ª–≥–æ—Ä–∏—Ç–º (–æ–±—â–∏–π –≤–∏–¥)</h2>
    <ol>
      <li><strong>–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è</strong>: –∑–∞–¥–∞—Ç—å –Ω–∞—á–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Œ∏‚Å∞</li>
      <li><strong>E-—à–∞–≥</strong>: –≤—ã—á–∏—Å–ª–∏—Ç—å –æ–∂–∏–¥–∞–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏ —Ç–µ–∫—É—â–∏—Ö Œ∏</li>
      <li><strong>M-—à–∞–≥</strong>: –Ω–∞–π—Ç–∏ Œ∏, –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—é—â–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ</li>
      <li><strong>–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏</strong>: –µ—Å–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –º–∞–ª–æ, —Å—Ç–æ–ø</li>
      <li><strong>–ò–Ω–∞—á–µ</strong>: –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ —à–∞–≥—É 2</li>
    </ol>
    <pre><code># –ü—Å–µ–≤–¥–æ–∫–æ–¥
Œ∏ = initialize_parameters()
while not converged:
    # E-step
    responsibilities = compute_expectations(X, Œ∏)
    
    # M-step
    Œ∏_new = maximize_likelihood(X, responsibilities)
    
    # Check convergence
    if |Œ∏_new - Œ∏| < tolerance:
        break
    Œ∏ = Œ∏_new</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. Gaussian Mixture Model (GMM)</h2>
    <p>–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è EM:</p>
    <pre><code>from sklearn.mixture import GaussianMixture
import numpy as np

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
np.random.seed(42)
X = np.concatenate([
    np.random.normal(0, 1, (300, 2)),
    np.random.normal(4, 1.5, (700, 2))
])

# GMM —Å EM –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º
gmm = GaussianMixture(
    n_components=2,        # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
    covariance_type='full', # —Ç–∏–ø –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏
    max_iter=100,          # –º–∞–∫—Å. –∏—Ç–µ—Ä–∞—Ü–∏–π
    n_init=10,             # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–π
    random_state=42
)

# –û–±—É—á–µ–Ω–∏–µ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç EM)
gmm.fit(X)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
labels = gmm.predict(X)

# –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏
probabilities = gmm.predict_proba(X)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. E-—à–∞–≥ –¥–ª—è GMM</h2>
    <p>–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–µ–π (responsibilities):</p>
    <pre><code># –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —á—Ç–æ —Ç–æ—á–∫–∞ x –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–µ k
# Œ≥(z_nk) = œÄ_k * N(x_n | Œº_k, Œ£_k) / Œ£_j œÄ_j * N(x_n | Œº_j, Œ£_j)

from scipy.stats import multivariate_normal

def e_step(X, means, covariances, weights):
    """
    X: –¥–∞–Ω–Ω—ã–µ (n_samples, n_features)
    means: —Ü–µ–Ω—Ç—Ä—ã –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (n_components, n_features)
    covariances: –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏ (n_components, n_features, n_features)
    weights: –≤–µ—Å–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (n_components,)
    """
    n_samples = X.shape[0]
    n_components = len(weights)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    responsibilities = np.zeros((n_samples, n_components))
    
    for k in range(n_components):
        responsibilities[:, k] = weights[k] * \
            multivariate_normal.pdf(X, means[k], covariances[k])
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    responsibilities /= responsibilities.sum(axis=1, keepdims=True)
    
    return responsibilities</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. M-—à–∞–≥ –¥–ª—è GMM</h2>
    <p>–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏:</p>
    <pre><code>def m_step(X, responsibilities):
    """
    –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ responsibilities
    """
    n_samples, n_features = X.shape
    n_components = responsibilities.shape[1]
    
    # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –≤ –∫–∞–∂–¥–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–µ
    N_k = responsibilities.sum(axis=0)  # (n_components,)
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
    weights = N_k / n_samples
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω—Ç—Ä–æ–≤
    means = np.dot(responsibilities.T, X) / N_k[:, np.newaxis]
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–π
    covariances = np.zeros((n_components, n_features, n_features))
    for k in range(n_components):
        diff = X - means[k]
        covariances[k] = np.dot(
            responsibilities[:, k] * diff.T, diff
        ) / N_k[k]
    
    return means, covariances, weights</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ü–æ–ª–Ω—ã–π EM –¥–ª—è GMM</h2>
    <pre><code>def fit_gmm_em(X, n_components, max_iter=100, tol=1e-4):
    """–ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è EM –¥–ª—è GMM"""
    n_samples, n_features = X.shape
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (K-means++)
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters=n_components, random_state=42)
    kmeans.fit(X)
    
    means = kmeans.cluster_centers_
    covariances = np.array([np.cov(X.T) for _ in range(n_components)])
    weights = np.ones(n_components) / n_components
    
    log_likelihood_old = -np.inf
    
    for iteration in range(max_iter):
        # E-step
        responsibilities = e_step(X, means, covariances, weights)
        
        # M-step
        means, covariances, weights = m_step(X, responsibilities)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ log-likelihood
        log_likelihood = compute_log_likelihood(
            X, means, covariances, weights
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if abs(log_likelihood - log_likelihood_old) < tol:
            print(f"Converged at iteration {iteration}")
            break
        
        log_likelihood_old = log_likelihood
    
    return means, covariances, weights, responsibilities</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Log-Likelihood</h2>
    <pre><code>def compute_log_likelihood(X, means, covariances, weights):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ log-likelihood –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    """
    n_samples = X.shape[0]
    n_components = len(weights)
    
    log_likelihood = 0
    
    for n in range(n_samples):
        sample_likelihood = 0
        for k in range(n_components):
            sample_likelihood += weights[k] * \
                multivariate_normal.pdf(
                    X[n], means[k], covariances[k]
                )
        log_likelihood += np.log(sample_likelihood + 1e-10)
    
    return log_likelihood

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤—ã–±–æ—Ä–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
bic_scores = []
aic_scores = []

for n_components in range(1, 11):
    gmm = GaussianMixture(n_components=n_components)
    gmm.fit(X)
    bic_scores.append(gmm.bic(X))
    aic_scores.append(gmm.aic(X))

# –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π BIC/AIC
best_n = np.argmin(bic_scores) + 1</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: Missing Data</h2>
    <p>EM –æ—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤:</p>
    <pre><code>from sklearn.impute import IterativeImputer

# –î–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
X_incomplete = X.copy()
X_incomplete[np.random.rand(*X.shape) < 0.3] = np.nan

# EM-based imputation
imputer = IterativeImputer(
    max_iter=10,
    random_state=42,
    verbose=0
)

X_filled = imputer.fit_transform(X_incomplete)

# –ò–ª–∏ –≤—Ä—É—á–Ω—É—é —Å GMM
def em_impute(X_incomplete, n_components=3):
    """–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ —á–µ—Ä–µ–∑ EM —Å GMM"""
    X_filled = X_incomplete.copy()
    
    # –ù–∞—á–∞–ª—å–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ (mean imputation)
    from sklearn.impute import SimpleImputer
    simple_imputer = SimpleImputer(strategy='mean')
    X_filled = simple_imputer.fit_transform(X_filled)
    
    for iteration in range(10):
        # –û–±—É—á–µ–Ω–∏–µ GMM –Ω–∞ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        gmm = GaussianMixture(n_components=n_components)
        gmm.fit(X_filled)
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ —É—Å–ª–æ–≤–Ω—ã–º–∏ –æ–∂–∏–¥–∞–Ω–∏—è–º–∏
        mask = np.isnan(X_incomplete)
        # ... (–¥–µ—Ç–∞–ª—å–Ω–∞—è –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è)
    
    return X_filled</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</h2>
    <table>
      <tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</th></tr>
      <tr><td><code>n_components</code></td><td>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç</td><td>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BIC/AIC</td></tr>
      <tr><td><code>covariance_type</code></td><td>–¢–∏–ø –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏</td><td>'full' - –ø–æ–ª–Ω–∞—è, 'diag' - –±—ã—Å—Ç—Ä–µ–µ</td></tr>
      <tr><td><code>max_iter</code></td><td>–ú–∞–∫—Å. –∏—Ç–µ—Ä–∞—Ü–∏–π</td><td>100-200 –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ</td></tr>
      <tr><td><code>n_init</code></td><td>–ß–∏—Å–ª–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–π</td><td>10+ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏</td></tr>
      <tr><td><code>tol</code></td><td>–ü–æ—Ä–æ–≥ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏</td><td>1e-3 –¥–æ 1e-6</td></tr>
    </table>
    <pre><code># –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ç–∏–ø–∞–º–∏ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏
covariance_types = ['full', 'tied', 'diag', 'spherical']

for cov_type in covariance_types:
    gmm = GaussianMixture(
        n_components=3,
        covariance_type=cov_type
    )
    gmm.fit(X)
    print(f"{cov_type}: BIC = {gmm.bic(X):.2f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ü—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è</h2>
    <div class="good-vs-bad">
      <div class="bad">
        <h3>‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã</h3>
        <ul>
          <li>–õ–æ–∫–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º—É–º—ã (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏)</li>
          <li>–ú–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</li>
          <li>–°–∏–Ω–≥—É–ª—è—Ä–Ω—ã–µ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã</li>
          <li>–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –≤—ã–±—Ä–æ—Å–∞–º</li>
          <li>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ –±–æ–ª—å—à–æ–º K</li>
        </ul>
      </div>
      <div class="good">
        <h3>‚úÖ –†–µ—à–µ–Ω–∏—è</h3>
        <ul>
          <li>–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ (n_init)</li>
          <li>–•–æ—Ä–æ—à–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (K-means++)</li>
          <li>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–π</li>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å robust–Ω—ã–µ –≤–µ—Ä—Å–∏–∏</li>
          <li>–í—ã–±–æ—Ä K —á–µ—Ä–µ–∑ BIC/AIC</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 11. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏</h2>
    <pre><code>import matplotlib.pyplot as plt

# –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ log-likelihood
log_likelihoods = []

gmm = GaussianMixture(n_components=3, n_init=1)

# –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π fit –¥–ª—è –∑–∞–ø–∏—Å–∏ –∏—Å—Ç–æ—Ä–∏–∏
X_sample = X[:1000]  # –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è

for i in range(50):
    gmm.max_iter = i + 1
    gmm.fit(X_sample)
    log_likelihoods.append(gmm.score(X_sample) * len(X_sample))

# –ì—Ä–∞—Ñ–∏–∫ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
plt.figure(figsize=(10, 6))
plt.plot(log_likelihoods)
plt.xlabel('Iteration')
plt.ylabel('Log-Likelihood')
plt.title('EM Algorithm Convergence')
plt.grid(True)
plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=gmm.predict(X), 
           cmap='viridis', alpha=0.5)
plt.scatter(gmm.means_[:, 0], gmm.means_[:, 1],
           c='red', marker='X', s=200, edgecolors='black')
plt.title('GMM Clustering Result')
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –í–∞—Ä–∏–∞–Ω—Ç—ã EM</h2>
    <ul>
      <li><strong>Hard EM</strong>: –∂–µ—Å—Ç–∫–æ–µ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π</li>
      <li><strong>Stochastic EM</strong>: —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π E-—à–∞–≥ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li><strong>Variational EM</strong>: –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥</li>
      <li><strong>Incremental EM</strong>: –æ–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ</li>
      <li><strong>ECM (Expectation-Conditional-Maximization)</strong>: —Ä–∞–∑–±–∏–µ–Ω–∏–µ M-—à–∞–≥–∞</li>
    </ul>
    <pre><code># Hard EM (K-means - —ç—Ç–æ —á–∞—Å—Ç–Ω—ã–π —Å–ª—É—á–∞–π)
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3)  # Hard EM
kmeans.fit(X)

# Soft EM (GMM)
gmm = GaussianMixture(n_components=3)  # Soft EM
gmm.fit(X)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
print("K-means (Hard):", kmeans.inertia_)
print("GMM (Soft):", gmm.score(X))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è EM</h2>
    <table>
      <tr><th>–ó–∞–¥–∞—á–∞</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ EM</th></tr>
      <tr><td>–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è</td><td>GMM –¥–ª—è soft clustering</td></tr>
      <tr><td>Missing data</td><td>–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤</td></tr>
      <tr><td>HMM</td><td>Baum-Welch –∞–ª–≥–æ—Ä–∏—Ç–º</td></tr>
      <tr><td>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</td><td>Matrix factorization —Å missing</td></tr>
      <tr><td>Semi-supervised</td><td>–û–±—É—á–µ–Ω–∏–µ —Å –Ω–µ–ø–æ–ª–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π</td></tr>
      <tr><td>Topic modeling</td><td>LDA (Latent Dirichlet Allocation)</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 14. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º–∏</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–ü–ª—é—Å—ã</th><th>–ú–∏–Ω—É—Å—ã</th></tr>
      <tr><td><strong>EM</strong></td><td>–ì–∞—Ä–∞–Ω—Ç–∏—è —É–ª—É—á—à–µ–Ω–∏—è, —Ç–µ–æ—Ä–∏—è</td><td>–õ–æ–∫–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º—É–º—ã</td></tr>
      <tr><td><strong>Gradient Descent</strong></td><td>–ü—Ä—è–º–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è</td><td>–í—ã–±–æ—Ä learning rate</td></tr>
      <tr><td><strong>Variational Bayes</strong></td><td>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, uncertainty</td><td>–°–ª–æ–∂–Ω–µ–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å</td></tr>
      <tr><td><strong>MCMC</strong></td><td>–¢–æ—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞</td><td>–û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 15. Best Practices</h2>
    <ul>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–π (n_init ‚â• 10)</li>
      <li>[ ] –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å/—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ</li>
      <li>[ ] –í—ã–±–∏—Ä–∞—Ç—å K —á–µ—Ä–µ–∑ BIC/AIC –∏–ª–∏ cross-validation</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä—è—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å (log-likelihood)</li>
      <li>[ ] –†–µ–≥—É–ª—è—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏ (–¥–æ–±–∞–≤–∏—Ç—å –¥–∏–∞–≥–æ–Ω–∞–ª—å)</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã</li>
      <li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã (K-means –¥–ª—è hard clustering)</li>
      <li>[ ] –£–¥–∞–ª—è—Ç—å –≤—ã–±—Ä–æ—Å—ã –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –î–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã/—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω—ã</li>
      <li>[ ] –í—ã–±—Ä–∞–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (BIC/AIC)</li>
      <li>[ ] –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω random_state –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–π</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–µ–Ω–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º–∞</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏</li>
      <li>[ ] –°—Ä–∞–≤–Ω–µ–Ω—ã —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–π</li>
      <li>[ ] –û–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤—ã–±—Ä–æ—Å—ã</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´EM-–∞–ª–≥–æ—Ä–∏—Ç–º —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á–∏, –≥–¥–µ –µ—Å—Ç—å —Å–∫—Ä—ã—Ç–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –º—ã –≤–∏–¥–∏–º –¥–∞–Ω–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç–æ–≤, –Ω–æ –Ω–µ –∑–Ω–∞–µ–º, –∫ –∫–∞–∫–æ–π –≥—Ä—É–ø–ø–µ –æ–Ω–∏ –æ—Ç–Ω–æ—Å—è—Ç—Å—è. EM –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–≥–∞–¥—ã–≤–∞–µ—Ç –≥—Ä—É–ø–ø—ã –∏ —É—Ç–æ—á–Ω—è–µ—Ç –∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –ø–æ–∫–∞ –Ω–µ –Ω–∞–π–¥–µ—Ç –Ω–∞–∏–ª—É—á—à–µ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

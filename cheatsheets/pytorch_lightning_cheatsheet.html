<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>PyTorch Lightning Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>‚ö° PyTorch Lightning</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã</h2>
    <ul>
      <li><strong>Lightning</strong>: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π PyTorch</li>
      <li><strong>Boilerplate</strong>: –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä—É—Ç–∏–Ω—ã</li>
      <li><strong>LightningModule</strong>: —è–¥—Ä–æ –º–æ–¥–µ–ª–∏</li>
      <li><strong>Trainer</strong>: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏–µ–º</li>
    </ul>
    <p><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:</strong></p>
    <ul>
      <li>–ú–µ–Ω—å—à–µ –∫–æ–¥–∞, –±–æ–ª—å—à–µ –Ω–∞—É–∫–∏</li>
      <li>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π multi-GPU</li>
      <li>–í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –ª–æ–≥–≥–∏–Ω–≥</li>
      <li>–í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å</li>
    </ul>

    </div>
<div class="block">
    <h2>üî∑ 2. LightningModule</h2>
    <pre><code>import pytorch_lightning as pl
import torch
import torch.nn as nn

class MyModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.layer = nn.Linear(28*28, 10)
    
    def forward(self, x):
        return self.layer(x)
    
    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = nn.functional.cross_entropy(y_hat, y)
        self.log('train_loss', loss)
        return loss
    
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. Trainer</h2>
    <pre><code>from pytorch_lightning import Trainer

# –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
trainer = Trainer(
    max_epochs=10,
    accelerator='gpu',
    devices=1,
    logger=True
)

model = MyModel()
trainer.fit(model, train_loader, val_loader)

# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ
trainer = Trainer(
    max_epochs=100,
    accelerator='gpu',
    devices=4,  # multi-GPU
    strategy='ddp',  # distributed
    precision=16,  # mixed precision
    gradient_clip_val=0.5,
    callbacks=[early_stop, checkpoint]
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ú–µ—Ç–æ–¥—ã LightningModule</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>training_step</strong></td><td>–®–∞–≥ –æ–±—É—á–µ–Ω–∏—è</td></tr>
      <tr><td><strong>validation_step</strong></td><td>–í–∞–ª–∏–¥–∞—Ü–∏—è</td></tr>
      <tr><td><strong>test_step</strong></td><td>–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ</td></tr>
      <tr><td><strong>predict_step</strong></td><td>Inference</td></tr>
      <tr><td><strong>configure_optimizers</strong></td><td>–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä</td></tr>
      <tr><td><strong>on_train_epoch_end</strong></td><td>–ö–æ–Ω–µ—Ü —ç–ø–æ—Ö–∏</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 5. Validation –∏ Test</h2>
    <pre><code>class MyModel(pl.LightningModule):
    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = nn.functional.cross_entropy(y_hat, y)
        acc = (y_hat.argmax(1) == y).float().mean()
        
        self.log('val_loss', loss, prog_bar=True)
        self.log('val_acc', acc, prog_bar=True)
        return loss
    
    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = nn.functional.cross_entropy(y_hat, y)
        acc = (y_hat.argmax(1) == y).float().mean()
        
        self.log('test_loss', loss)
        self.log('test_acc', acc)
        return loss

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
trainer.validate(model, val_loader)
trainer.test(model, test_loader)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Callbacks</h2>
    <pre><code>from pytorch_lightning.callbacks import (
    EarlyStopping, 
    ModelCheckpoint,
    LearningRateMonitor
)

# Early stopping
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    mode='min'
)

# Checkpointing
checkpoint = ModelCheckpoint(
    dirpath='checkpoints/',
    filename='{epoch}-{val_loss:.2f}',
    monitor='val_loss',
    mode='min',
    save_top_k=3
)

# LR monitor
lr_monitor = LearningRateMonitor(logging_interval='epoch')

trainer = Trainer(
    callbacks=[early_stop, checkpoint, lr_monitor]
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Logging</h2>
    <pre><code>from pytorch_lightning.loggers import (
    TensorBoardLogger,
    WandbLogger
)

# TensorBoard
tb_logger = TensorBoardLogger('logs/', name='my_model')

# Weights & Biases
wandb_logger = WandbLogger(project='my_project')

trainer = Trainer(logger=[tb_logger, wandb_logger])

# –í –º–æ–¥–µ–ª–∏
class MyModel(pl.LightningModule):
    def training_step(self, batch, batch_idx):
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π logging
        self.log('train_loss', loss)
        
        # Manual logging
        self.log_dict({
            'train_loss': loss,
            'train_acc': acc
        }, prog_bar=True)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Multi-GPU Training</h2>
    <pre><code># Data Parallel (DP) - simple but slow
trainer = Trainer(accelerator='gpu', devices=4, strategy='dp')

# Distributed Data Parallel (DDP) - faster
trainer = Trainer(accelerator='gpu', devices=4, strategy='ddp')

# DeepSpeed - –¥–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
trainer = Trainer(
    accelerator='gpu',
    devices=4,
    strategy='deepspeed',
    precision=16
)

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä
trainer = Trainer(accelerator='auto', devices='auto')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Mixed Precision Training</h2>
    <pre><code># 16-bit precision
trainer = Trainer(precision=16)

# bfloat16
trainer = Trainer(precision='bf16')

# 32-bit (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
trainer = Trainer(precision=32)

# –í –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å
class MyModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±–æ–π precision
        self.layer = nn.Linear(100, 10)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. DataModule</h2>
    <pre><code>class MyDataModule(pl.LightningDataModule):
    def __init__(self, batch_size=32):
        super().__init__()
        self.batch_size = batch_size
    
    def prepare_data(self):
        # –°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ (–≤—ã–∑—ã–≤–∞–µ—Ç—Å—è 1 —Ä–∞–∑)
        download_data()
    
    def setup(self, stage=None):
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
        if stage == 'fit' or stage is None:
            self.train_data = MyDataset('train')
            self.val_data = MyDataset('val')
        if stage == 'test' or stage is None:
            self.test_data = MyDataset('test')
    
    def train_dataloader(self):
        return DataLoader(self.train_data, batch_size=self.batch_size)
    
    def val_dataloader(self):
        return DataLoader(self.val_data, batch_size=self.batch_size)
    
    def test_dataloader(self):
        return DataLoader(self.test_data, batch_size=self.batch_size)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
dm = MyDataModule(batch_size=64)
trainer.fit(model, datamodule=dm)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –•–æ—Ä–æ—à–æ</h3>
        <ul>
          <li>Multi-GPU –æ–±—É—á–µ–Ω–∏–µ</li>
          <li>–ù—É–∂–Ω–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å</li>
          <li>–°–ª–æ–∂–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã</li>
          <li>Production-ready –∫–æ–¥</li>
          <li>–ö–æ–º–∞–Ω–¥–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ü–ª–æ—Ö–æ</h3>
        <ul>
          <li>–ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ—Ç–æ—Ç–∏–ø</li>
          <li>–ù—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å</li>
          <li>–û–±—É—á–µ–Ω–∏–µ PyTorch</li>
          <li>–ö–∞—Å—Ç–æ–º–Ω—ã–µ training loops</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å: `pip install pytorch-lightning`</li>
      <li>[ ] –ù–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å –æ—Ç LightningModule</li>
      <li>[ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å training_step</li>
      <li>[ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å configure_optimizers</li>
      <li>[ ] –î–æ–±–∞–≤–∏—Ç—å validation_step</li>
      <li>[ ] –°–æ–∑–¥–∞—Ç—å Trainer —Å –Ω—É–∂–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å callbacks (checkpoint, early stopping)</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å logging (TensorBoard/W&B)</li>
      <li>[ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ CPU –ø–µ—Ä–µ–¥ GPU</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´PyTorch Lightning ‚Äî —ç—Ç–æ –∫–∞–∫ –∞–≤—Ç–æ–ø–∏–ª–æ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: –≤—ã –≥–æ–≤–æ—Ä–∏—Ç–µ –ß–¢–û –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å (–º–æ–¥–µ–ª—å, –¥–∞–Ω–Ω—ã–µ), –∞ Lightning —Ä–µ—à–∞–µ—Ç –ö–ê–ö —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ (GPU, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, —á–µ–∫–ø–æ–∏–Ω—Ç—ã)¬ª.
    </blockquote>
  </div>



</div>
</body>
</html>

<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>EM-–∞–ª–≥–æ—Ä–∏—Ç–º Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üîÑ Expectation-Maximization (EM) Cheatsheet</h1>
  <div class="subtitle">–î–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö ‚Ä¢ –û—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–æ —Å–∫—Ä—ã—Ç—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ ‚Ä¢ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ñ–æ–∫—É—Å<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: –æ—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Å–∫—Ä—ã—Ç—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö</li>
      <li><strong>–ü–æ–¥—Ö–æ–¥</strong>: –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ E- –∏ M-—à–∞–≥–∏</li>
      <li><strong>–ì–∞—Ä–∞–Ω—Ç–∏—è</strong>: –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: GMM, HMM, missing data, latent variables</li>
      <li><strong>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å</strong>: –Ω–∞—Ö–æ–¥–∏—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –º–∞–∫—Å–∏–º—É–º</li>
    </ul>
    <blockquote>EM-–∞–ª–≥–æ—Ä–∏—Ç–º - –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –≤ –º–æ–¥–µ–ª—è—Ö —Å –Ω–µ–ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ —Å–∫—Ä—ã—Ç—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 2. –ê–ª–≥–æ—Ä–∏—Ç–º (–¥–≤–∞ —à–∞–≥–∞)</h2>
    <p><strong>E-—à–∞–≥ (Expectation):</strong></p>
    <pre><code>Q(Œ∏|Œ∏‚ÅΩ·µó‚Åæ) = E_Z|X,Œ∏‚ÅΩ·µó‚Åæ[log P(X, Z|Œ∏)]

–í—ã—á–∏—Å–ª—è–µ–º –æ–∂–∏–¥–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ 
log-–ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</code></pre>
    
    <p><strong>M-—à–∞–≥ (Maximization):</strong></p>
    <pre><code>Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ = argmax_Œ∏ Q(Œ∏|Œ∏‚ÅΩ·µó‚Åæ)

–ú–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º Q –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ Œ∏</code></pre>
    
    <p><strong>–ò—Ç–µ—Ä–∞—Ü–∏–∏:</strong></p>
    <ol>
      <li>–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Œ∏‚ÅΩ‚Å∞‚Åæ</li>
      <li>E-—à–∞–≥: –≤—ã—á–∏—Å–ª—è–µ–º Q(Œ∏|Œ∏‚ÅΩ·µó‚Åæ)</li>
      <li>M-—à–∞–≥: –Ω–∞—Ö–æ–¥–∏–º Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ</li>
      <li>–ü–æ–≤—Ç–æ—Ä—è–µ–º 2-3 –¥–æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏</li>
    </ol>
  </div>

  <div class="block">
    <h2>üî∑ 3. Gaussian Mixture Model (GMM)</h2>
    <pre><code>import numpy as np
from sklearn.mixture import GaussianMixture

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–º–µ—Å–∏
np.random.seed(42)
X = np.concatenate([
    np.random.normal(0, 1, (100, 2)),
    np.random.normal(5, 1.5, (100, 2)),
    np.random.normal(2, 0.5, (100, 2))
])

# –û–±—É—á–µ–Ω–∏–µ GMM —Å EM
gmm = GaussianMixture(n_components=3, random_state=42)
gmm.fit(X)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
print("–°—Ä–µ–¥–Ω–∏–µ:", gmm.means_)
print("–ö–æ–≤–∞—Ä–∏–∞—Ü–∏–∏:", gmm.covariances_)
print("–í–µ—Å–∞:", gmm.weights_)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
labels = gmm.predict(X)
probs = gmm.predict_proba(X)  # –ú—è–≥–∫–∏–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. EM –¥–ª—è GMM (–¥–µ—Ç–∞–ª–∏)</h2>
    <p><strong>E-—à–∞–≥:</strong> –≤—ã—á–∏—Å–ª—è–µ–º posterior –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏</p>
    <pre><code>Œ≥·µ¢‚Çñ = P(z·µ¢ = k | x·µ¢, Œ∏‚ÅΩ·µó‚Åæ)
    = œÄ‚Çñ N(x·µ¢|Œº‚Çñ, Œ£‚Çñ) / Œ£‚±º œÄ‚±º N(x·µ¢|Œº‚±º, Œ£‚±º)

–≥–¥–µ:
- Œ≥·µ¢‚Çñ - –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã k –∑–∞ —Ç–æ—á–∫—É i
- œÄ‚Çñ - –≤–µ—Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã k
- N - –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è</code></pre>
    
    <p><strong>M-—à–∞–≥:</strong> –æ–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã</p>
    <pre><code># –í–µ—Å–∞
œÄ‚Çñ = (1/N) Œ£·µ¢ Œ≥·µ¢‚Çñ

# –°—Ä–µ–¥–Ω–∏–µ
Œº‚Çñ = Œ£·µ¢ Œ≥·µ¢‚Çñ x·µ¢ / Œ£·µ¢ Œ≥·µ¢‚Çñ

# –ö–æ–≤–∞—Ä–∏–∞—Ü–∏–∏
Œ£‚Çñ = Œ£·µ¢ Œ≥·µ¢‚Çñ (x·µ¢ - Œº‚Çñ)(x·µ¢ - Œº‚Çñ)·µÄ / Œ£·µ¢ Œ≥·µ¢‚Çñ</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è GMM —Å –Ω—É–ª—è</h2>
    <pre><code>class GaussianMixtureEM:
    def __init__(self, n_components, max_iter=100, tol=1e-4):
        self.n_components = n_components
        self.max_iter = max_iter
        self.tol = tol
    
    def fit(self, X):
        n_samples, n_features = X.shape
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è k-means
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=self.n_components)
        labels = kmeans.fit_predict(X)
        
        self.means_ = kmeans.cluster_centers_
        self.covariances_ = np.array([
            np.cov(X[labels == k].T) + np.eye(n_features) * 1e-6
            for k in range(self.n_components)
        ])
        self.weights_ = np.bincount(labels) / n_samples
        
        log_likelihood_old = 0
        
        for iteration in range(self.max_iter):
            # E-step
            responsibilities = self._e_step(X)
            
            # M-step
            self._m_step(X, responsibilities)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            log_likelihood = self._log_likelihood(X)
            if abs(log_likelihood - log_likelihood_old) < self.tol:
                break
            log_likelihood_old = log_likelihood
        
        return self
    
    def _e_step(self, X):
        n_samples = X.shape[0]
        responsibilities = np.zeros((n_samples, self.n_components))
        
        for k in range(self.n_components):
            responsibilities[:, k] = self.weights_[k] * \
                self._multivariate_gaussian(X, self.means_[k], 
                                            self.covariances_[k])
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        responsibilities /= responsibilities.sum(axis=1, keepdims=True)
        return responsibilities
    
    def _m_step(self, X, responsibilities):
        n_samples = X.shape[0]
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
        self.weights_ = responsibilities.sum(axis=0) / n_samples
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–∏—Ö –∏ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–π
        for k in range(self.n_components):
            resp_k = responsibilities[:, k]
            sum_resp = resp_k.sum()
            
            self.means_[k] = (X.T @ resp_k) / sum_resp
            
            diff = X - self.means_[k]
            self.covariances_[k] = (diff.T * resp_k) @ diff / sum_resp
            self.covariances_[k] += np.eye(X.shape[1]) * 1e-6
    
    def _multivariate_gaussian(self, X, mean, cov):
        n_features = X.shape[1]
        diff = X - mean
        return np.exp(-0.5 * np.sum(diff @ np.linalg.inv(cov) * diff, axis=1)) / \
               np.sqrt((2 * np.pi) ** n_features * np.linalg.det(cov))
    
    def _log_likelihood(self, X):
        likelihoods = np.zeros((X.shape[0], self.n_components))
        for k in range(self.n_components):
            likelihoods[:, k] = self.weights_[k] * \
                self._multivariate_gaussian(X, self.means_[k], 
                                           self.covariances_[k])
        return np.log(likelihoods.sum(axis=1)).sum()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. EM –¥–ª—è Missing Data</h2>
    <pre><code>import numpy as np
from sklearn.impute import SimpleImputer

# –î–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
X = np.array([
    [1, 2, np.nan],
    [3, np.nan, 5],
    [np.nan, 4, 6],
    [7, 8, 9]
])

# –ü—Ä–æ—Å—Ç–∞—è –∏–º–ø—É—Ç–∞—Ü–∏—è —Å—Ä–µ–¥–Ω–∏–º
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# EM-based –∏–º–ø—É—Ç–∞—Ü–∏—è (–±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è)
def em_imputation(X, max_iter=100):
    X_filled = X.copy()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–µ–¥–Ω–∏–º–∏
    col_means = np.nanmean(X, axis=0)
    for j in range(X.shape[1]):
        X_filled[np.isnan(X[:, j]), j] = col_means[j]
    
    for _ in range(max_iter):
        # E-—à–∞–≥: –æ—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        mean = X_filled.mean(axis=0)
        cov = np.cov(X_filled.T)
        
        # M-—à–∞–≥: –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ —É—Å–ª–æ–≤–Ω—ã–º–∏ –æ–∂–∏–¥–∞–Ω–∏—è–º–∏
        for i in range(X.shape[0]):
            missing_idx = np.isnan(X[i])
            if not missing_idx.any():
                continue
            
            obs_idx = ~missing_idx
            
            # –£—Å–ª–æ–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            mu_miss = mean[missing_idx]
            mu_obs = mean[obs_idx]
            
            Sigma_mm = cov[np.ix_(missing_idx, missing_idx)]
            Sigma_mo = cov[np.ix_(missing_idx, obs_idx)]
            Sigma_oo = cov[np.ix_(obs_idx, obs_idx)]
            
            # –£—Å–ª–æ–≤–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ
            X_filled[i, missing_idx] = mu_miss + \
                Sigma_mo @ np.linalg.inv(Sigma_oo) @ \
                (X[i, obs_idx] - mu_obs)
    
    return X_filled

X_em_imputed = em_imputation(X)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. EM –¥–ª—è HMM</h2>
    <p><strong>–°–∫—Ä—ã—Ç—ã–µ –ú–∞—Ä–∫–æ–≤—Å–∫–∏–µ –ú–æ–¥–µ–ª–∏:</strong></p>
    <pre><code>from hmmlearn import hmm

# –°–æ–∑–¥–∞–Ω–∏–µ HMM
model = hmm.GaussianHMM(n_components=3, covariance_type="full")

# –û–±—É—á–µ–Ω–∏–µ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç EM –≤–Ω—É—Ç—Ä–∏)
# E-—à–∞–≥: Forward-Backward –∞–ª–≥–æ—Ä–∏—Ç–º
# M-—à–∞–≥: Baum-Welch –∞–ª–≥–æ—Ä–∏—Ç–º
X = np.random.randn(100, 2)
model.fit(X)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
print("–ù–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:", model.startprob_)
print("–ú–∞—Ç—Ä–∏—Ü–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤:", model.transmat_)
print("–°—Ä–µ–¥–Ω–∏–µ:", model.means_)

# –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (Viterbi)
log_prob, states = model.decode(X)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –°—Ö–æ–¥–∏–º–æ—Å—Ç—å –∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏</h2>
    <table>
      <tr>
        <th>–ö—Ä–∏—Ç–µ—Ä–∏–π</th>
        <th>–§–æ—Ä–º—É–ª–∞</th>
        <th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th>
      </tr>
      <tr>
        <td>Log-likelihood</td>
        <td><code>|L‚ÅΩ·µó‚Å∫¬π‚Åæ - L‚ÅΩ·µó‚Åæ| < Œµ</code></td>
        <td>–û—Å–Ω–æ–≤–Ω–æ–π –∫—Ä–∏—Ç–µ—Ä–∏–π</td>
      </tr>
      <tr>
        <td>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</td>
        <td><code>||Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ - Œ∏‚ÅΩ·µó‚Åæ|| < Œµ</code></td>
        <td>–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è Œ∏</td>
      </tr>
      <tr>
        <td>–ú–∞–∫—Å–∏–º—É–º –∏—Ç–µ—Ä–∞—Ü–∏–π</td>
        <td><code>t > max_iter</code></td>
        <td>–ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è</td>
      </tr>
      <tr>
        <td>–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ</td>
        <td><code>|ŒîL| / |L| < Œµ</code></td>
        <td>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—à—Ç–∞–±–∞</td>
      </tr>
    </table>
    
    <blockquote>–¢–µ–æ—Ä–µ–º–∞: EM –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ L(Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ) ‚â• L(Œ∏‚ÅΩ·µó‚Åæ) –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úì –•–æ—Ä–æ—à–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏</h3>
        <ul>
          <li>K-means –¥–ª—è GMM</li>
          <li>–ù–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—É—Å–∫–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞—á–∞–ª—å–Ω—ã–º–∏</li>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ prior knowledge</li>
          <li>–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚úó –ü–ª–æ—Ö–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏</h3>
        <ul>
          <li>–°–ª—É—á–∞–π–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–æ–∫</li>
          <li>–û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –Ω–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è</li>
          <li>–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è</strong>: GMM, soft clustering</li>
      <li><strong>–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã</strong>: HMM, state space models</li>
      <li><strong>Missing data</strong>: –∏–º–ø—É—Ç–∞—Ü–∏—è, –æ—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
      <li><strong>–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ</strong>: LDA, pLSA</li>
      <li><strong>–ë–∏–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞</strong>: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π</li>
      <li><strong>Computer Vision</strong>: —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è, tracking</li>
      <li><strong>NLP</strong>: POS tagging, alignment</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 11. –í–∞—Ä–∏–∞—Ü–∏–∏ EM</h2>
    <p><strong>Generalized EM (GEM):</strong></p>
    <ul>
      <li>M-—à–∞–≥ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç Q, –Ω–æ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç</li>
      <li>–ü–æ–ª–µ–∑–Ω–æ –∫–æ–≥–¥–∞ M-—à–∞–≥ —Å–ª–æ–∂–µ–Ω</li>
    </ul>
    
    <p><strong>Online EM:</strong></p>
    <ul>
      <li>–û–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∞ –º–∏–Ω–∏-–±–∞—Ç—á–∞—Ö</li>
      <li>–î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤</li>
    </ul>
    
    <p><strong>Stochastic EM:</strong></p>
    <ul>
      <li>E-—à–∞–≥ —Å —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º</li>
      <li>–î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π</li>
    </ul>
    
    <p><strong>Variational EM:</strong></p>
    <ul>
      <li>–í–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –≤–º–µ—Å—Ç–æ —Ç–æ—á–Ω–æ–≥–æ E-—à–∞–≥–∞</li>
      <li>–î–ª—è –±–æ–ª—å—à–∏—Ö –±–∞–π–µ—Å–æ–≤—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úì –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</h3>
        <ul>
          <li>–ì–∞—Ä–∞–Ω—Ç–∏—è –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏</li>
          <li>–ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è</li>
          <li>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å</li>
          <li>–†–∞–±–æ—Ç–∞ —Å missing data</li>
          <li>–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚úó –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h3>
        <ul>
          <li>–õ–æ–∫–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º—É–º—ã</li>
          <li>–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏</li>
          <li>–ú–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</li>
          <li>–¢—Ä–µ–±—É–µ—Ç –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏</li>
          <li>–ú–æ–∂–µ—Ç –Ω–µ —Å—Ö–æ–¥–∏—Ç—å—Å—è –∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É –º–∞–∫—Å–∏–º—É–º—É</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 13. –£—Å–∫–æ—Ä–µ–Ω–∏–µ EM</h2>
    <ul>
      <li><strong>Incremental EM</strong>: –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∞ –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞—Ö</li>
      <li><strong>Sparse EM</strong>: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏</li>
      <li><strong>Parallel EM</strong>: –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è E-—à–∞–≥–∞</li>
      <li><strong>Conjugate gradients</strong>: –¥–ª—è M-—à–∞–≥–∞</li>
      <li><strong>Newton-Raphson</strong>: –≤—Ç–æ—Ä–æ–π –ø–æ—Ä—è–¥–æ–∫ –≤ M-—à–∞–≥–µ</li>
    </ul>
    
    <pre><code># –ü—Ä–∏–º–µ—Ä: Mini-batch EM
def minibatch_em(X, n_components, batch_size=100, n_epochs=10):
    n_samples = X.shape[0]
    gmm = GaussianMixture(n_components=n_components)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    gmm.fit(X[:1000])
    
    for epoch in range(n_epochs):
        indices = np.random.permutation(n_samples)
        
        for i in range(0, n_samples, batch_size):
            batch = X[indices[i:i+batch_size]]
            
            # E-—à–∞–≥ –Ω–∞ –±–∞—Ç—á–µ
            responsibilities = gmm.predict_proba(batch)
            
            # M-—à–∞–≥ —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ–º
            # (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
            gmm.fit(batch)
    
    return gmm</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏</h2>
    <ul>
      <li><strong>–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–ø—É—Å–∫–∏</strong>: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li><strong>–í—ã–±–æ—Ä n_components</strong>: BIC/AIC –¥–ª—è GMM</li>
      <li><strong>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</strong>: –¥–æ–±–∞–≤–ª—è–π—Ç–µ –∫ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏—è–º ŒµI</li>
      <li><strong>–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥</strong>: –æ—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ log-likelihood</li>
      <li><strong>–†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞</strong>: –∏–∑–±–µ–≥–∞–π—Ç–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è</li>
      <li><strong>–í–∞–ª–∏–¥–∞—Ü–∏—è</strong>: –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
    </ul>
  </div>

</div>

</body>
</html>

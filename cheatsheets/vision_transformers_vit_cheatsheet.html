<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Vision Transformers (ViT) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ Vision Transformers (ViT)</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ß—Ç–æ —Ç–∞–∫–æ–µ ViT</h2>
    <p><strong>Vision Transformer</strong> ‚Äî –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Transformer –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–≤—ë—Ä—Ç–æ–∫</p>
    <ul>
      <li><strong>–†–µ–≤–æ–ª—é—Ü–∏—è</strong>: –¥–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ CNN –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã</li>
      <li><strong>–ü–∞—Ç—á–∏</strong>: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–µ–ª–∏—Ç—Å—è –Ω–∞ –ø–∞—Ç—á–∏</li>
      <li><strong>Attention</strong>: self-attention –Ω–∞ –ø–∞—Ç—á–∞—Ö</li>
      <li><strong>–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å</strong>: –ª—É—á—à–µ —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏</li>
    </ul>
    <blockquote>
      "An Image is Worth 16x16 Words" - Google Research, 2020
    </blockquote>

  <div class="block">
    <h2>üî∑ 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ ViT</h2>
    <ol>
      <li><strong>Patch Embedding</strong>: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ‚Üí –ø–∞—Ç—á–∏</li>
      <li><strong>Linear Projection</strong>: –ø–∞—Ç—á–∏ ‚Üí –≤–µ–∫—Ç–æ—Ä—ã</li>
      <li><strong>Position Embedding</strong>: –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π</li>
      <li><strong>Class Token</strong>: [CLS] –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</li>
      <li><strong>Transformer Encoder</strong>: N —Å–ª–æ–µ–≤ attention</li>
      <li><strong>MLP Head</strong>: —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</li>
    </ol>
  </div>

  <div class="block">
    <h2>üî∑ 3. Patch Embedding</h2>
    <p>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ H√óW√óC –¥–µ–ª–∏—Ç—Å—è –Ω–∞ –ø–∞—Ç—á–∏ P√óP:</p>
    <pre><code># –ü—Ä–∏–º–µ—Ä: 224√ó224√ó3 ‚Üí –ø–∞—Ç—á–∏ 16√ó16
–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ç—á–µ–π = (224/16)¬≤ = 196

# –ö–∞–∂–¥—ã–π –ø–∞—Ç—á
–†–∞–∑–º–µ—Ä –ø–∞—Ç—á–∞ = 16√ó16√ó3 = 768 –∑–Ω–∞—á–µ–Ω–∏–π

# Linear projection
patch ‚Üí embedding (768 ‚Üí D)
–≥–¥–µ D = 768 (–¥–ª—è ViT-Base)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ ViT</h2>
    <pre><code>import torch
from transformers import ViTForImageClassification
from transformers import ViTFeatureExtractor
from PIL import Image

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = ViTForImageClassification.from_pretrained(
    'google/vit-base-patch16-224'
)
feature_extractor = ViTFeatureExtractor.from_pretrained(
    'google/vit-base-patch16-224'
)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
image = Image.open('image.jpg')
inputs = feature_extractor(images=image, return_tensors="pt")

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
outputs = model(**inputs)
logits = outputs.logits
predicted_class = logits.argmax(-1).item()

print(f"Predicted class: {predicted_class}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –í–∞—Ä–∏–∞–Ω—Ç—ã ViT</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–°–ª–æ–∏</th><th>Hidden</th><th>Heads</th><th>Params</th></tr>
      <tr><td><strong>ViT-Tiny</strong></td><td>12</td><td>192</td><td>3</td><td>5.5M</td></tr>
      <tr><td><strong>ViT-Small</strong></td><td>12</td><td>384</td><td>6</td><td>22M</td></tr>
      <tr><td><strong>ViT-Base</strong></td><td>12</td><td>768</td><td>12</td><td>86M</td></tr>
      <tr><td><strong>ViT-Large</strong></td><td>24</td><td>1024</td><td>16</td><td>307M</td></tr>
      <tr><td><strong>ViT-Huge</strong></td><td>32</td><td>1280</td><td>16</td><td>632M</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 6. ViT vs CNN</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>ViT</th><th>CNN</th></tr>
      <tr><td><strong>Inductive bias</strong></td><td>–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π</td><td>–õ–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å, –∏–µ—Ä–∞—Ä—Ö–∏—è</td></tr>
      <tr><td><strong>–î–∞–Ω–Ω—ã–µ</strong></td><td>–ù—É–∂–Ω–æ –º–Ω–æ–≥–æ</td><td>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω —Å –º–∞–ª—ã–º–∏</td></tr>
      <tr><td><strong>–í—ã—á–∏—Å–ª–µ–Ω–∏—è</strong></td><td>–ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã–µ –ø–æ –ø–∞—Ç—á–∞–º</td><td>–õ–∏–Ω–µ–π–Ω—ã–µ</td></tr>
      <tr><td><strong>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å</strong></td><td>Attention maps</td><td>Feature maps</td></tr>
      <tr><td><strong>Transfer learning</strong></td><td>–û—Ç–ª–∏—á–Ω–æ</td><td>–•–æ—Ä–æ—à–æ</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ ViT</h2>
    <p>ViT —Ç—Ä–µ–±—É–µ—Ç <strong>–±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞</strong> –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è</p>
    <ul>
      <li><strong>ImageNet-1K</strong>: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Ö—É–∂–µ CNN</li>
      <li><strong>ImageNet-21K</strong>: 14M –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ö–æ—Ä–æ—à–æ</li>
      <li><strong>JFT-300M</strong>: 300M –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, SOTA</li>
    </ul>
    <blockquote>
      –° –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ ViT –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç ResNet –∏ EfficientNet
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 8. Fine-tuning ViT</h2>
    <pre><code>from transformers import ViTForImageClassification
from transformers import Trainer, TrainingArguments

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
model = ViTForImageClassification.from_pretrained(
    'google/vit-base-patch16-224-in21k',
    num_labels=10,  # –¥–ª—è –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏
    ignore_mismatched_sizes=True
)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir='./vit-finetuned',
    num_train_epochs=10,
    per_device_train_batch_size=32,
    learning_rate=3e-4,
    warmup_steps=500,
    logging_steps=50,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

trainer.train()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Data Augmentation –¥–ª—è ViT</h2>
    <p>ViT –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç –æ—Ç —Å–∏–ª—å–Ω–æ–π –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏:</p>
    <ul>
      <li><strong>RandAugment</strong>: —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è</li>
      <li><strong>MixUp</strong>: —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</li>
      <li><strong>CutMix</strong>: –≤—ã—Ä–µ–∑–∞–Ω–∏–µ –∏ –≤—Å—Ç–∞–≤–∫–∞</li>
      <li><strong>AutoAugment</strong>: learned augmentation</li>
      <li><strong>Random Erasing</strong>: —Å–ª—É—á–∞–π–Ω–æ–µ —Å—Ç–∏—Ä–∞–Ω–∏–µ</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 10. –£–ª—É—á—à–µ–Ω–∏—è ViT</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–£–ª—É—á—à–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>DeiT</strong></td><td>Distillation, –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö</td></tr>
      <tr><td><strong>Swin Transformer</strong></td><td>Shifted windows, –∏–µ—Ä–∞—Ä—Ö–∏—è</td></tr>
      <tr><td><strong>BEiT</strong></td><td>BERT-style pretraining</td></tr>
      <tr><td><strong>MAE</strong></td><td>Masked Autoencoder</td></tr>
      <tr><td><strong>CvT</strong></td><td>Convolutional token embedding</td></tr>
      <tr><td><strong>CoAtNet</strong></td><td>CNN + ViT –≥–∏–±—Ä–∏–¥</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 11. DeiT: Data-efficient ViT</h2>
    <p><strong>DeiT</strong> –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å ViT –Ω–∞ ImageNet-1K</p>
    <ul>
      <li><strong>Knowledge Distillation</strong>: —É—á–∏—Ç–µ–ª—å (CNN) ‚Üí —É—á–µ–Ω–∏–∫ (ViT)</li>
      <li><strong>Distillation Token</strong>: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω [DIST]</li>
      <li><strong>Strong Augmentation</strong>: RandAugment + –¥—Ä.</li>
    </ul>
    <pre><code># –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ DeiT
from transformers import DeiTForImageClassification

model = DeiTForImageClassification.from_pretrained(
    'facebook/deit-base-distilled-patch16-224'
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Swin Transformer</h2>
    <p><strong>Swin</strong> ‚Äî –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π ViT —Å shifted windows</p>
    <ul>
      <li><strong>–õ–æ–∫–∞–ª—å–Ω—ã–µ –æ–∫–Ω–∞</strong>: attention –≤ –æ–∫–Ω–∞—Ö</li>
      <li><strong>Shifted windows</strong>: —Å–≤—è–∑—å –º–µ–∂–¥—É –æ–∫–Ω–∞–º–∏</li>
      <li><strong>–ò–µ—Ä–∞—Ä—Ö–∏—è</strong>: –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ feature maps</li>
      <li><strong>–õ–∏–Ω–µ–π–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å</strong>: O(n) –≤–º–µ—Å—Ç–æ O(n¬≤)</li>
    </ul>
    <blockquote>
      Swin –ø—Ä–µ–≤–∑–æ—à—ë–ª ViT –Ω–∞ –º–Ω–æ–≥–∏—Ö –∑–∞–¥–∞—á–∞—Ö CV (detection, segmentation)
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 13. Attention Visualization</h2>
    <p>–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è attention weights –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞ —á—Ç–æ —Å–º–æ—Ç—Ä–∏—Ç –º–æ–¥–µ–ª—å</p>
    <pre><code>import matplotlib.pyplot as plt

# –ü–æ–ª—É—á–µ–Ω–∏–µ attention weights
outputs = model(**inputs, output_attentions=True)
attentions = outputs.attentions  # tuple of layers

# Attention –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
last_attention = attentions[-1]  # [batch, heads, tokens, tokens]

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
attention_map = last_attention[0].mean(0)[0, 1:]  # [CLS] –∫ –ø–∞—Ç—á–∞–º
attention_map = attention_map.reshape(14, 14)  # –¥–ª—è 224/16

plt.imshow(attention_map.detach().numpy(), cmap='viridis')
plt.colorbar()
plt.title('ViT Attention Map')
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è ViT</h2>
    <ul>
      <li><strong>Image Classification</strong>: –æ—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞</li>
      <li><strong>Object Detection</strong>: DETR —Å ViT</li>
      <li><strong>Semantic Segmentation</strong>: SegFormer</li>
      <li><strong>Instance Segmentation</strong>: Mask2Former</li>
      <li><strong>Video Understanding</strong>: TimeSformer</li>
      <li><strong>Medical Imaging</strong>: –∞–Ω–∞–ª–∏–∑ —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≤, –ú–†–¢</li>
      <li><strong>Multimodal</strong>: CLIP (vision + language)</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 15. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ViT</h2>
    <ul>
      <li><strong>Mixed Precision</strong>: fp16 –æ–±—É—á–µ–Ω–∏–µ</li>
      <li><strong>Gradient Checkpointing</strong>: —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏</li>
      <li><strong>DeepSpeed</strong>: distributed training</li>
      <li><strong>Flash Attention</strong>: –±—ã—Å—Ç—Ä—ã–π attention</li>
      <li><strong>Quantization</strong>: int8 inference</li>
      <li><strong>Pruning</strong>: —É–¥–∞–ª–µ–Ω–∏–µ heads/—Å–ª–æ–µ–≤</li>
    </ul>
    <pre><code># Mixed precision training
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for data, target in dataloader:
    optimizer.zero_grad()
    
    with autocast():
        output = model(data)
        loss = criterion(output, target)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. Best Practices</h2>
    <ol>
      <li><strong>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ</strong>: –Ω–∞ ImageNet-21K</li>
      <li><strong>Strong augmentation</strong>: RandAugment, MixUp</li>
      <li><strong>–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π LR</strong>: 1e-3 –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è, 3e-4 –¥–ª—è fine-tuning</li>
      <li><strong>Warmup</strong>: 5-10 —ç–ø–æ—Ö –∏–ª–∏ 500-1000 —à–∞–≥–æ–≤</li>
      <li><strong>Label smoothing</strong>: 0.1</li>
      <li><strong>Stochastic depth</strong>: dropout –¥–ª—è —Å–ª–æ—ë–≤</li>
      <li><strong>Batch size</strong>: –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ (512-4096)</li>
    </ol>
  </div>

  <div class="block">
    <h2>üî∑ 17. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</h3>
        <ul>
          <li>SOTA –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö</li>
          <li>–û—Ç–ª–∏—á–Ω—ã–π transfer learning</li>
          <li>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å (attention)</li>
          <li>–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å</li>
        </ul>
      </div>
      <div class="bad">
        <h3>–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h3>
        <ul>
          <li>–¢—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö</li>
          <li>–í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –¥–æ—Ä–æ–≥–æ</li>
          <li>–•—É–∂–µ CNN –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
          <li>–ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å attention</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ß–µ–∫-–ª–∏—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è</h2>
    <ol>
      <li>‚úì –í—ã–±—Ä–∞—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (Tiny/Small/Base/Large)</li>
      <li>‚úì –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ (ImageNet-21K)</li>
      <li>‚úì –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π</li>
      <li>‚úì –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (LR, warmup)</li>
      <li>‚úì Fine-tune —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º learning rate</li>
      <li>‚úì –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å overfitting</li>
      <li>‚úì –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å attention maps</li>
      <li>‚úì –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è production</li>
    </ol>
  </div>

</div>

</div>
</body>
</html>

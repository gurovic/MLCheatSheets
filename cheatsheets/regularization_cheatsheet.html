<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤ Machine Learning Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤ Machine Learning</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ß—Ç–æ —Ç–∞–∫–æ–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è?</h2>
    <p><strong>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</strong> ‚Äî —Ç–µ—Ö–Ω–∏–∫–∞ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –ø—É—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —à—Ç—Ä–∞—Ñ–∞ –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.</p>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: —É–ª—É—á—à–∏—Ç—å –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å</li>
      <li><strong>–ò–¥–µ—è</strong>: –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏</li>
      <li><strong>–≠—Ñ—Ñ–µ–∫—Ç</strong>: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É bias –∏ variance</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: –ø–æ—á—Ç–∏ –≤—Å–µ ML –∞–ª–≥–æ—Ä–∏—Ç–º—ã</li>
    </ul>
    <blockquote>üí° "–õ—É—á—à–µ –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç, —á–µ–º —Å–ª–æ–∂–Ω–∞—è, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è"</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 2. –ü—Ä–æ–±–ª–µ–º–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è</h2>
    <p><strong>–ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è</strong>:</p>
    <ul>
      <li>–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train, –Ω–∏–∑–∫–∞—è –Ω–∞ test</li>
      <li>–ë–æ–ª—å—à–æ–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É train –∏ validation loss</li>
      <li>–ú–æ–¥–µ–ª—å –≤—ã—É—á–∏–≤–∞–µ—Ç —à—É–º –≤ –¥–∞–Ω–Ω—ã—Ö</li>
      <li>–ü–ª–æ—Ö–∞—è —Ä–∞–±–æ—Ç–∞ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
    </ul>
    
    <p><strong>–ü—Ä–∏—á–∏–Ω—ã</strong>:</p>
    <ul>
      <li>–°–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞—è –º–æ–¥–µ–ª—å</li>
      <li>–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è</li>
      <li>–ú–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (high dimensionality)</li>
      <li>–î–æ–ª–≥–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª—è</li>
    </ul>

    <p><strong>–†–µ—à–µ–Ω–∏—è</strong>:</p>
    <ul>
      <li>‚úÖ –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (–Ω–∞—à–∞ —Ç–µ–º–∞)</li>
      <li>‚úÖ –ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö</li>
      <li>‚úÖ –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
      <li>‚úÖ –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ (early stopping)</li>
      <li>‚úÖ –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Lasso)</h2>
    <p><strong>–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞</strong>: –¥–æ–±–∞–≤–ª—è–µ—Ç —Å—É–º–º—É –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤–µ—Å–æ–≤</p>
    <p>Loss = MSE + Œ± √ó Œ£|w_i|</p>
    
    <p><strong>–°–≤–æ–π—Å—Ç–≤–∞</strong>:</p>
    <ul>
      <li>–û–±–Ω—É–ª—è–µ—Ç –Ω–µ–≤–∞–∂–Ω—ã–µ –≤–µ—Å–∞ ‚Üí <strong>—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å</strong></li>
      <li>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π feature selection</li>
      <li>–†–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è + –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
      <li>–ù–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞ –≤ –Ω—É–ª–µ</li>
    </ul>

    <pre><code>from sklearn.linear_model import Lasso

# Lasso —Ä–µ–≥—Ä–µ—Å—Å–∏—è
model = Lasso(alpha=0.1)  # Œ± –ø–∞—Ä–∞–º–µ—Ç—Ä
model.fit(X_train, y_train)

# –í–µ—Å–∞ (–º–Ω–æ–≥–∏–µ –±—É–¥—É—Ç = 0)
print(model.coef_)

# –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
selected_features = X.columns[model.coef_ != 0]
print(f"–û—Ç–æ–±—Ä–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(selected_features)}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Ridge)</h2>
    <p><strong>–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞</strong>: –¥–æ–±–∞–≤–ª—è–µ—Ç —Å—É–º–º—É –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –≤–µ—Å–æ–≤</p>
    <p>Loss = MSE + Œ± √ó Œ£w_i¬≤</p>
    
    <p><strong>–°–≤–æ–π—Å—Ç–≤–∞</strong>:</p>
    <ul>
      <li>–£–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Å–∞, –Ω–æ –Ω–µ –æ–±–Ω—É–ª—è–µ—Ç</li>
      <li>–í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—Å—Ç–∞—é—Ç—Å—è –≤ –º–æ–¥–µ–ª–∏</li>
      <li>–ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–∞ –ø—Ä–∏ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏</li>
      <li>–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞ –≤–µ–∑–¥–µ</li>
      <li>–ò–º–µ–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ</li>
    </ul>

    <pre><code>from sklearn.linear_model import Ridge

# Ridge —Ä–µ–≥—Ä–µ—Å—Å–∏—è
model = Ridge(alpha=1.0)
model.fit(X_train, y_train)

# –í—Å–µ –≤–µ—Å–∞ –º–∞–ª—ã, –Ω–æ –Ω–µ–Ω—É–ª–µ–≤—ã–µ
print(model.coef_)

# –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
from sklearn.linear_model import RidgeClassifier
clf = RidgeClassifier(alpha=1.0)
clf.fit(X_train, y_train)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. ElasticNet (L1 + L2)</h2>
    <p><strong>–ö–æ–º–±–∏–Ω–∞—Ü–∏—è</strong> L1 –∏ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏:</p>
    <p>Loss = MSE + Œ± √ó (r √ó Œ£|w_i| + (1-r) √ó Œ£w_i¬≤)</p>
    
    <ul>
      <li>r = 1: —Ç–æ–ª—å–∫–æ L1 (Lasso)</li>
      <li>r = 0: —Ç–æ–ª—å–∫–æ L2 (Ridge)</li>
      <li>0 < r < 1: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è</li>
    </ul>

    <pre><code>from sklearn.linear_model import ElasticNet

# ElasticNet
model = ElasticNet(
    alpha=0.1,      # —Å–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    l1_ratio=0.5    # –±–∞–ª–∞–Ω—Å L1/L2
)
model.fit(X_train, y_train)

# –õ—É—á—à–µ–µ –∏–∑ –¥–≤—É—Ö –º–∏—Ä–æ–≤:
# - –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Å–∞ –æ–±–Ω—É–ª—è—é—Ç—Å—è (L1)
# - –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö (L2)</code></pre>

    <blockquote>üí° ElasticNet —Ö–æ—Ä–æ—à –∫–æ–≥–¥–∞ –º–Ω–æ–≥–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 6. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ L1, L2, ElasticNet</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>L1 (Lasso)</th><th>L2 (Ridge)</th><th>ElasticNet</th></tr>
      <tr>
        <td><strong>–®—Ç—Ä–∞—Ñ</strong></td>
        <td>Œ£|w_i|</td>
        <td>Œ£w_i¬≤</td>
        <td>–ö–æ–º–±–∏–Ω–∞—Ü–∏—è</td>
      </tr>
      <tr>
        <td><strong>–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å</strong></td>
        <td>–î–∞ (–æ–±–Ω—É–ª—è–µ—Ç)</td>
        <td>–ù–µ—Ç</td>
        <td>–ß–∞—Å—Ç–∏—á–Ω–æ</td>
      </tr>
      <tr>
        <td><strong>Feature selection</strong></td>
        <td>‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏</td>
        <td>‚ùå –ù–µ—Ç</td>
        <td>‚úÖ –ß–∞—Å—Ç–∏—á–Ω–æ</td>
      </tr>
      <tr>
        <td><strong>–ö–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏</strong></td>
        <td>–í—ã–±–∏—Ä–∞–µ—Ç –æ–¥–∏–Ω</td>
        <td>–†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–µ—Å–∞</td>
        <td>–ö–æ–º–ø—Ä–æ–º–∏—Å—Å</td>
      </tr>
      <tr>
        <td><strong>–í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å</strong></td>
        <td>–°—Ä–µ–¥–Ω—è—è</td>
        <td>–ù–∏–∑–∫–∞—è</td>
        <td>–í—ã—Å–æ–∫–∞—è</td>
      </tr>
      <tr>
        <td><strong>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</strong></td>
        <td>–ú–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</td>
        <td>–ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å</td>
        <td>–û–±–∞ —Å–ª—É—á–∞—è</td>
      </tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. –í—ã–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ Œ± (—Å–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏)</h2>
    <p><strong>Œ± (alpha)</strong> –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç —Å–∏–ª—É —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏:</p>
    <ul>
      <li>Œ± = 0: –Ω–µ—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (–ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)</li>
      <li>Œ± ‚Üí ‚àû: –≤—Å–µ –≤–µ—Å–∞ ‚Üí 0 (–Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ)</li>
      <li>–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ Œ±: –±–∞–ª–∞–Ω—Å bias-variance</li>
    </ul>

    <p><strong>–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ Œ±</strong>:</p>
    <pre><code>from sklearn.linear_model import RidgeCV, LassoCV

# Grid Search —Å CV
alphas = [0.001, 0.01, 0.1, 1, 10, 100]

# Ridge —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π CV
ridge_cv = RidgeCV(alphas=alphas, cv=5)
ridge_cv.fit(X_train, y_train)
print(f"–õ—É—á—à–∏–π Œ±: {ridge_cv.alpha_}")

# Lasso —Å CV
lasso_cv = LassoCV(alphas=alphas, cv=5)
lasso_cv.fit(X_train, y_train)
print(f"–õ—É—á—à–∏–π Œ±: {lasso_cv.alpha_}")

# ElasticNet —Å CV
from sklearn.linear_model import ElasticNetCV
elastic_cv = ElasticNetCV(
    alphas=alphas,
    l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99],
    cv=5
)
elastic_cv.fit(X_train, y_train)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏</h2>
    <pre><code>from sklearn.linear_model import LogisticRegression

# L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
model = LogisticRegression(
    penalty='l2',
    C=1.0,  # –æ–±—Ä–∞—Ç–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (–º–µ–Ω—å—à–µ C = —Å–∏–ª—å–Ω–µ–µ)
    solver='lbfgs'
)

# L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
model = LogisticRegression(
    penalty='l1',
    C=0.1,
    solver='saga'  # liblinear –∏–ª–∏ saga –¥–ª—è L1
)

# ElasticNet
model = LogisticRegression(
    penalty='elasticnet',
    C=1.0,
    l1_ratio=0.5,
    solver='saga'
)

# –ë–µ–∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
model = LogisticRegression(penalty='none')</code></pre>
    
    <blockquote>‚ö†Ô∏è –í sklearn –ø–∞—Ä–∞–º–µ—Ç—Ä C ‚Äî —ç—Ç–æ –æ–±—Ä–∞—Ç–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (C = 1/Œ±)</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 9. Dropout (–¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π)</h2>
    <p><strong>–ò–¥–µ—è</strong>: —Å–ª—É—á–∞–π–Ω–æ "–≤—ã–∫–ª—é—á–∞—Ç—å" –Ω–µ–π—Ä–æ–Ω—ã –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è</p>
    
    <ul>
      <li>–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∫–æ-–∞–¥–∞–ø—Ç–∞—Ü–∏—é –Ω–µ–π—Ä–æ–Ω–æ–≤</li>
      <li>–≠—Ñ—Ñ–µ–∫—Ç –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π</li>
      <li>–ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏</li>
      <li>–¢–∏–ø–∏—á–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: 0.2-0.5</li>
    </ul>

    <pre><code>import tensorflow as tf
from tensorflow.keras import layers

model = tf.keras.Sequential([
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),  # 50% –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤—ã–∫–ª—é—á–∞—é—Ç—Å—è
    
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),  # 30% –≤—ã–∫–ª—é—á–∞—é—Ç—Å—è
    
    layers.Dense(10, activation='softmax')
])</code></pre>

    <p><strong>PyTorch</strong>:</p>
    <pre><code>import torch.nn as nn

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)  # —Ç–æ–ª—å–∫–æ –≤ train mode
        x = self.fc2(x)
        return x</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Batch Normalization</h2>
    <p>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π –º–µ–∂–¥—É —Å–ª–æ—è–º–∏:</p>
    <ul>
      <li>–£—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ</li>
      <li>–ü–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–µ learning rates</li>
      <li>–ò–º–µ–µ—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑—É—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç</li>
      <li>–£–º–µ–Ω—å—à–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏</li>
    </ul>

    <pre><code># Keras/TensorFlow
from tensorflow.keras import layers

model = tf.keras.Sequential([
    layers.Dense(128),
    layers.BatchNormalization(),  # –ø–æ—Å–ª–µ Dense
    layers.Activation('relu'),
    
    layers.Dense(64),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    
    layers.Dense(10, activation='softmax')
])

# PyTorch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.bn1 = nn.BatchNorm1d(128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        return x</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Early Stopping</h2>
    <p>–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ —É—Ö—É–¥—à–µ–Ω–∏–∏ –Ω–∞ validation:</p>
    <pre><code># Keras/TensorFlow
from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor='val_loss',     # –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
    patience=10,            # —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
    restore_best_weights=True,  # –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ª—É—á—à–∏–µ –≤–µ—Å–∞
    min_delta=0.001        # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
)

model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=1000,
    callbacks=[early_stop]
)

# Sklearn (–¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)
from sklearn.neural_network import MLPRegressor

model = MLPRegressor(
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=10
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Data Augmentation</h2>
    <p>–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:</p>
    <ul>
      <li>–î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –ø–æ–≤–æ—Ä–æ—Ç, –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ, –æ–±—Ä–µ–∑–∫–∞</li>
      <li>–î–ª—è —Ç–µ–∫—Å—Ç–∞: —Å–∏–Ω–æ–Ω–∏–º—ã, back-translation</li>
      <li>–î–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤: jittering, scaling</li>
    </ul>

    <pre><code># Keras –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.2
)

# –û–±—É—á–µ–Ω–∏–µ —Å augmentation
model.fit(
    datagen.flow(X_train, y_train, batch_size=32),
    epochs=50
)

# PyTorch
from torchvision import transforms

transform = transforms.Compose([
    transforms.RandomRotation(20),
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ColorJitter(brightness=0.2)
])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. Weight Decay</h2>
    <p>–î–æ–±–∞–≤–ª–µ–Ω–∏–µ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—É:</p>
    <pre><code># PyTorch
import torch.optim as optim

# Weight decay = L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
optimizer = optim.Adam(
    model.parameters(),
    lr=0.001,
    weight_decay=0.01  # L2 penalty
)

# TensorFlow/Keras
from tensorflow.keras.optimizers import Adam

optimizer = Adam(
    learning_rate=0.001,
    weight_decay=0.01
)

# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä—ã
from tensorflow.keras import regularizers

model.add(layers.Dense(
    64,
    kernel_regularizer=regularizers.l2(0.01),
    bias_regularizer=regularizers.l2(0.01)
))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤ –¥–µ—Ä–µ–≤—å—è—Ö</h2>
    <p>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–µ—Ä–µ–≤—å–µ–≤:</p>
    <pre><code>from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(
    max_depth=5,           # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞
    min_samples_split=20,  # –º–∏–Ω–∏–º—É–º –¥–ª—è split
    min_samples_leaf=10,   # –º–∏–Ω–∏–º—É–º –≤ –ª–∏—Å—Ç–µ
    max_features='sqrt',   # –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ split
    ccp_alpha=0.01        # cost complexity pruning
)

# –°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    max_depth=10,
    min_samples_split=20,
    max_features='sqrt',   # —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å
    n_estimators=100
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º –±—É—Å—Ç–∏–Ω–≥–µ</h2>
    <pre><code>import xgboost as xgb

# XGBoost
model = xgb.XGBClassifier(
    learning_rate=0.1,     # shrinkage
    max_depth=5,           # –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤
    min_child_weight=1,    # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—É–º–º–∞ –≤–µ—Å–æ–≤
    gamma=0,               # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ loss –¥–ª—è split
    subsample=0.8,         # –¥–æ–ª—è —Å—ç–º–ø–ª–æ–≤ –¥–ª—è –¥–µ—Ä–µ–≤–∞
    colsample_bytree=0.8,  # –¥–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ—Ä–µ–≤–∞
    reg_alpha=0.1,         # L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
    reg_lambda=1.0         # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
)

# LightGBM
import lightgbm as lgb

model = lgb.LGBMClassifier(
    learning_rate=0.1,
    max_depth=5,
    min_child_samples=20,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ú–µ—Ç–æ–¥—ã –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è</h2>
    <p>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π:</p>
    <ul>
      <li><strong>Bagging</strong>: —É–º–µ–Ω—å—à–∞–µ—Ç variance</li>
      <li><strong>Boosting</strong>: —É–º–µ–Ω—å—à–∞–µ—Ç bias</li>
      <li><strong>Stacking</strong>: –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</li>
    </ul>

    <pre><code>from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# Bagging –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
bagging = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=100,
    max_samples=0.8,      # –¥–æ–ª—è —Å—ç–º–ø–ª–æ–≤
    max_features=0.8,     # –¥–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    bootstrap=True
)

# Voting
from sklearn.ensemble import VotingClassifier

voting = VotingClassifier(
    estimators=[
        ('lr', LogisticRegression()),
        ('rf', RandomForestClassifier()),
        ('svm', SVC(probability=True))
    ],
    voting='soft'
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. Best Practices</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –î–µ–ª–∞—Ç—å</h3>
        <ul>
          <li>–ù–∞—á–∏–Ω–∞—Ç—å —Å —Å–∏–ª—å–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</li>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CV –¥–ª—è –≤—ã–±–æ—Ä–∞ Œ±</li>
          <li>–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–¥ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π</li>
          <li>–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã</li>
          <li>–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å train/val –º–µ—Ç—Ä–∏–∫–∏</li>
          <li>Early stopping –≤—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞—Ç—å</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ò–∑–±–µ–≥–∞—Ç—å</h3>
        <ul>
        <li>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –±–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏</li>
          <li>–í—ã–±–∏—Ä–∞—Ç—å Œ± –Ω–∞ –≥–ª–∞–∑</li>
          <li>–ü—Ä–∏–º–µ–Ω—è—Ç—å –∫–æ –≤—Å–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º –æ–¥–∏–Ω–∞–∫–æ–≤–æ</li>
          <li>–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å validation loss</li>
          <li>–°–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 18. –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –º–µ—Ç–æ–¥</th></tr>
      <tr><td>–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è</td><td>Ridge (L2) –∏–ª–∏ ElasticNet</td></tr>
      <tr><td>–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è</td><td>L2 –∏–ª–∏ ElasticNet</td></tr>
      <tr><td>–ú–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</td><td>Lasso (L1) –¥–ª—è –æ—Ç–±–æ—Ä–∞</td></tr>
      <tr><td>–ö–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏</td><td>Ridge (L2) –∏–ª–∏ ElasticNet</td></tr>
      <tr><td>–ù–µ–π—Ä–æ—Å–µ—Ç–∏ (MLP)</td><td>Dropout + L2 + Early Stopping</td></tr>
      <tr><td>CNN</td><td>Dropout + Batch Norm + Data Aug</td></tr>
      <tr><td>–î–µ—Ä–µ–≤—å—è</td><td>max_depth, min_samples</td></tr>
      <tr><td>–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥</td><td>learning_rate + subsample</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 19. –ü—Ä–∏–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ pipeline</h2>
    <pre><code>from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import ElasticNetCV

# Pipeline —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!
    ('model', ElasticNetCV(
        l1_ratio=[0.1, 0.5, 0.7, 0.9],
        alphas=np.logspace(-4, 1, 50),
        cv=5
    ))
])

# –û–±—É—á–µ–Ω–∏–µ
pipeline.fit(X_train, y_train)

# –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
print(f"Alpha: {pipeline.named_steps['model'].alpha_}")
print(f"L1 ratio: {pipeline.named_steps['model'].l1_ratio_}")

# –û—Ü–µ–Ω–∫–∞
train_score = pipeline.score(X_train, y_train)
test_score = pipeline.score(X_test, y_test)

print(f"Train R¬≤: {train_score:.3f}")
print(f"Test R¬≤: {test_score:.3f}")
print(f"–†–∞–∑–Ω–∏—Ü–∞: {train_score - test_score:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 20. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞</h2>
    <pre><code>import matplotlib.pyplot as plt

# –ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö Œ±
alphas = np.logspace(-4, 1, 50)
train_scores = []
val_scores = []

for alpha in alphas:
    model = Ridge(alpha=alpha)
    model.fit(X_train, y_train)
    train_scores.append(model.score(X_train, y_train))
    val_scores.append(model.score(X_val, y_val))

plt.figure(figsize=(10, 6))
plt.plot(alphas, train_scores, label='Train')
plt.plot(alphas, val_scores, label='Validation')
plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('R¬≤ Score')
plt.legend()
plt.title('–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è: –≤—ã–±–æ—Ä alpha')
plt.grid(True)
plt.show()</code></pre>
  </div>

</div>

</body>
</html>

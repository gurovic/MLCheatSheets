<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Fairness –≤ ML Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>‚öñÔ∏è Fairness –≤ Machine Learning</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ß—Ç–æ —Ç–∞–∫–æ–µ fairness –≤ ML?</h2>
    <p><strong>Fairness</strong> ‚Äî –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ —Ä–∞–∑–Ω—ã–º –≥—Ä—É–ø–ø–∞–º –ª—é–¥–µ–π.</p>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏–∏</li>
      <li><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: bias –≤ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª—è—Ö</li>
      <li><strong>–í–∞–∂–Ω–æ—Å—Ç—å</strong>: —ç—Ç–∏—á–µ—Å–∫–∏–µ –∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ, –Ω–∞–π–º, –º–µ–¥–∏—Ü–∏–Ω–∞, —Å—É–¥</li>
    </ul>
    <blockquote>üí° "–ú–æ–¥–µ–ª—å –Ω–µ –¥–æ–ª–∂–Ω–∞ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–æ –ø–æ–ª—É, —Ä–∞—Å–µ, –≤–æ–∑—Ä–∞—Å—Ç—É –∏–ª–∏ –¥—Ä—É–≥–∏–º –∑–∞—â–∏—â–µ–Ω–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º"</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 2. –ò—Å—Ç–æ—á–Ω–∏–∫–∏ bias</h2>
    <p><strong>–ì–¥–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å</strong>:</p>
    <ul>
      <li><strong>–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ</strong>: –ø—Ä–æ—à–ª—ã–µ —Ä–µ—à–µ–Ω–∏—è –ª—é–¥–µ–π</li>
      <li><strong>Sampling bias</strong>: –Ω–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞</li>
      <li><strong>Label bias</strong>: –ø—Ä–µ–¥–≤–∑—è—Ç–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞</li>
      <li><strong>Feature selection</strong>: –≤—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
      <li><strong>Algorithm bias</strong>: –∞–ª–≥–æ—Ä–∏—Ç–º —É—Å–∏–ª–∏–≤–∞–µ—Ç bias</li>
      <li><strong>Feedback loops</strong>: –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π bias</li>
    </ul>

    <pre><code># –ü—Ä–∏–º–µ—Ä bias –≤ –¥–∞–Ω–Ω—ã—Ö
import pandas as pd

# –î–∞–Ω–Ω—ã–µ –æ –∑–∞—Ä–ø–ª–∞—Ç–µ
df = pd.DataFrame({
    'gender': ['M', 'M', 'F', 'F', 'M'],
    'experience': [5, 3, 5, 3, 4],
    'salary': [100, 80, 70, 60, 90]
})

# –°—Ä–µ–¥–Ω—è—è –∑–∞—Ä–ø–ª–∞—Ç–∞ –ø–æ –ø–æ–ª—É
print(df.groupby('gender')['salary'].mean())
# M    90.0  <- –≤—ã—à–µ
# F    65.0  <- –Ω–∏–∂–µ

# –ú–æ–¥–µ–ª—å –≤—ã—É—á–∏—Ç —ç—Ç–æ—Ç bias!</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –¢–∏–ø—ã fairness</h2>
    <table>
      <tr><th>–¢–∏–ø</th><th>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</th></tr>
      <tr>
        <td><strong>Individual Fairness</strong></td>
        <td>–ü–æ—Ö–æ–∂–∏–µ –ª—é–¥–∏ ‚Üí –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è</td>
      </tr>
      <tr>
        <td><strong>Group Fairness</strong></td>
        <td>–†–∞–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø</td>
      </tr>
      <tr>
        <td><strong>Demographic Parity</strong></td>
        <td>–†–∞–≤–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏—Å—Ö–æ–¥–∞</td>
      </tr>
      <tr>
        <td><strong>Equal Opportunity</strong></td>
        <td>–†–∞–≤–Ω—ã–π TPR –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø</td>
      </tr>
      <tr>
        <td><strong>Equalized Odds</strong></td>
        <td>–†–∞–≤–Ω—ã–µ TPR –∏ FPR</td>
      </tr>
      <tr>
        <td><strong>Predictive Parity</strong></td>
        <td>–†–∞–≤–Ω—ã–π PPV –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø</td>
      </tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 4. Demographic Parity</h2>
    <p><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: P(≈∂=1|A=0) = P(≈∂=1|A=1)</p>
    <p>–ì–¥–µ A ‚Äî –∑–∞—â–∏—â–µ–Ω–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ (–ø–æ–ª, —Ä–∞—Å–∞ –∏ —Ç.–¥.)</p>
    
    <pre><code>def demographic_parity(y_pred, sensitive_attr):
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ demographic parity
    
    Args:
        y_pred: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
        sensitive_attr: –∑–∞—â–∏—â–µ–Ω–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
    
    Returns:
        difference: —Ä–∞–∑–Ω–∏—Ü–∞ –≤ acceptance rates
    """
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –≥—Ä—É–ø–ø—ã
    group_0 = y_pred[sensitive_attr == 0]
    group_1 = y_pred[sensitive_attr == 1]
    
    # Acceptance rate –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
    rate_0 = (group_0 == 1).mean()
    rate_1 = (group_1 == 1).mean()
    
    # –†–∞–∑–Ω–∏—Ü–∞ (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –±–ª–∏–∑–∫–∞ –∫ 0)
    difference = abs(rate_0 - rate_1)
    
    return {
        'group_0_rate': rate_0,
        'group_1_rate': rate_1,
        'difference': difference,
        'is_fair': difference < 0.1  # –ø–æ—Ä–æ–≥
    }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
result = demographic_parity(predictions, gender)
print(f"Difference: {result['difference']:.3f}")
print(f"Fair: {result['is_fair']}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Equal Opportunity</h2>
    <p><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: P(≈∂=1|Y=1, A=0) = P(≈∂=1|Y=1, A=1)</p>
    <p>–†–∞–≤–Ω—ã–π True Positive Rate –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø.</p>
    
    <pre><code>from sklearn.metrics import confusion_matrix

def equal_opportunity(y_true, y_pred, sensitive_attr):
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ equal opportunity (—Ä–∞–≤–Ω—ã–π TPR)
    """
    groups = sensitive_attr.unique()
    tprs = {}
    
    for group in groups:
        # –ú–∞—Å–∫–∞ –¥–ª—è –≥—Ä—É–ø–ø—ã
        mask = (sensitive_attr == group)
        
        # Confusion matrix
        tn, fp, fn, tp = confusion_matrix(
            y_true[mask], 
            y_pred[mask]
        ).ravel()
        
        # True Positive Rate
        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
        tprs[group] = tpr
    
    # –†–∞–∑–Ω–∏—Ü–∞ –≤ TPR
    tpr_values = list(tprs.values())
    difference = max(tpr_values) - min(tpr_values)
    
    return {
        'tprs': tprs,
        'difference': difference,
        'is_fair': difference < 0.1
    }

# –ü—Ä–∏–º–µ—Ä
result = equal_opportunity(y_test, y_pred, gender)
print(f"TPRs: {result['tprs']}")
print(f"Difference: {result['difference']:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Equalized Odds</h2>
    <p>–†–∞–≤–Ω—ã–µ TPR –∏ FPR –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø:</p>
    <ul>
      <li>P(≈∂=1|Y=1, A=0) = P(≈∂=1|Y=1, A=1)</li>
      <li>P(≈∂=1|Y=0, A=0) = P(≈∂=1|Y=0, A=1)</li>
    </ul>

    <pre><code>def equalized_odds(y_true, y_pred, sensitive_attr):
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ equalized odds (—Ä–∞–≤–Ω—ã–µ TPR –∏ FPR)
    """
    groups = sensitive_attr.unique()
    metrics = {}
    
    for group in groups:
        mask = (sensitive_attr == group)
        tn, fp, fn, tp = confusion_matrix(
            y_true[mask], 
            y_pred[mask]
        ).ravel()
        
        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        
        metrics[group] = {'tpr': tpr, 'fpr': fpr}
    
    # –†–∞–∑–Ω–∏—Ü–∞ –≤ TPR –∏ FPR
    tpr_vals = [m['tpr'] for m in metrics.values()]
    fpr_vals = [m['fpr'] for m in metrics.values()]
    
    tpr_diff = max(tpr_vals) - min(tpr_vals)
    fpr_diff = max(fpr_vals) - min(fpr_vals)
    
    return {
        'metrics': metrics,
        'tpr_difference': tpr_diff,
        'fpr_difference': fpr_diff,
        'is_fair': (tpr_diff < 0.1 and fpr_diff < 0.1)
    }</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Measuring fairness with AIF360</h2>
    <p><strong>AI Fairness 360</strong> ‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –æ—Ç IBM –¥–ª—è fairness.</p>
    
    <pre><code>from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
dataset = BinaryLabelDataset(
    df=df,
    label_names=['outcome'],
    protected_attribute_names=['gender', 'race']
)

# –ú–µ—Ç—Ä–∏–∫–∏ fairness
metric = BinaryLabelDatasetMetric(
    dataset,
    unprivileged_groups=[{'gender': 0}],
    privileged_groups=[{'gender': 1}]
)

# –†–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
print(f"Statistical Parity: {metric.statistical_parity_difference()}")
print(f"Disparate Impact: {metric.disparate_impact()}")
print(f"Consistency: {metric.consistency()}")

# –î–ª—è –º–æ–¥–µ–ª–∏
from aif360.metrics import ClassificationMetric

# –ü–æ—Å–ª–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
dataset_pred = dataset.copy()
dataset_pred.labels = predictions

metric_pred = ClassificationMetric(
    dataset, dataset_pred,
    unprivileged_groups=[{'gender': 0}],
    privileged_groups=[{'gender': 1}]
)

print(f"Equal Opportunity: {metric_pred.equal_opportunity_difference()}")
print(f"Average Odds: {metric_pred.average_odds_difference()}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Pre-processing: —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ bias –≤ –¥–∞–Ω–Ω—ã—Ö</h2>
    <p><strong>–ú–µ—Ç–æ–¥—ã</strong>:</p>
    <ul>
      <li><strong>Reweighting</strong>: –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –ø—Ä–∏–º–µ—Ä–æ–≤</li>
      <li><strong>Sampling</strong>: –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –≥—Ä—É–ø–ø</li>
      <li><strong>Learning fair representations</strong></li>
    </ul>

    <pre><code>from aif360.algorithms.preprocessing import Reweighing

# Reweighing –¥–ª—è fair –¥–∞–Ω–Ω—ã—Ö
reweigher = Reweighing(
    unprivileged_groups=[{'gender': 0}],
    privileged_groups=[{'gender': 1}]
)

# –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ –¥–∞–Ω–Ω—ã–º
dataset_transf = reweigher.fit_transform(dataset)

# –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å–∞ –≤ –æ–±—É—á–µ–Ω–∏–∏
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(
    X_train, y_train,
    sample_weight=dataset_transf.instance_weights
)</code></pre>

    <p><strong>Sampling</strong>:</p>
    <pre><code># –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –≥—Ä—É–ø–ø
from imblearn.over_sampling import SMOTE

# Over-sample underrepresented group
smote = SMOTE()
X_balanced, y_balanced = smote.fit_resample(
    X_train[sensitive_col == 0], 
    y_train[sensitive_col == 0]
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. In-processing: fair –∞–ª–≥–æ—Ä–∏—Ç–º—ã</h2>
    <p>–û–±—É—á–µ–Ω–∏–µ —Å fairness constraints:</p>
    
    <pre><code>from aif360.algorithms.inprocessing import PrejudiceRemover

# Fair classifier
model = PrejudiceRemover(
    sensitive_attr='gender',
    eta=25.0  # fairness penalty
)

model.fit(dataset)
predictions = model.predict(dataset_test)

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: Adversarial Debiasing
from aif360.algorithms.inprocessing import AdversarialDebiasing

# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç adversarial network
model = AdversarialDebiasing(
    unprivileged_groups=[{'gender': 0}],
    privileged_groups=[{'gender': 1}],
    scope_name='debiased_classifier',
    debias=True,
    num_epochs=50
)

model.fit(dataset)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Post-processing: –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π</h2>
    <p>–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏:</p>
    
    <pre><code>from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing

# –û–±—É—á–∞–µ–º –æ–±—ã—á–Ω—É—é –º–æ–¥–µ–ª—å
model = LogisticRegression()
model.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred_proba = model.predict_proba(X_val)[:, 1]

# Post-processing –¥–ª—è fairness
cpp = CalibratedEqOddsPostprocessing(
    unprivileged_groups=[{'gender': 0}],
    privileged_groups=[{'gender': 1}],
    cost_constraint='weighted'
)

# –û–±—É—á–∞–µ–º –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä –Ω–∞ validation
dataset_val_pred = dataset_val.copy()
dataset_val_pred.scores = y_pred_proba

cpp.fit(dataset_val, dataset_val_pred)

# –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ test
dataset_test_pred = cpp.predict(dataset_test_pred)

# –¢–µ–ø–µ—Ä—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–æ–ª–µ–µ fair!</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Trade-offs: accuracy vs fairness</h2>
    <p>–ß–∞—Å—Ç–æ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –∂–µ—Ä—Ç–≤–æ–≤–∞—Ç—å accuracy —Ä–∞–¥–∏ fairness:</p>
    
    <pre><code>import matplotlib.pyplot as plt

# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ fairness constraints
fairness_params = [0, 0.1, 0.5, 1.0, 5.0]
results = {'accuracy': [], 'fairness': []}

for eta in fairness_params:
    model = train_fair_model(eta=eta)
    
    # –û—Ü–µ–Ω–∏–≤–∞–µ–º
    acc = model.score(X_test, y_test)
    fairness = measure_fairness(model, X_test, sensitive)
    
    results['accuracy'].append(acc)
    results['fairness'].append(fairness)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è trade-off
plt.plot(results['fairness'], results['accuracy'], 'o-')
plt.xlabel('Fairness (lower is better)')
plt.ylabel('Accuracy')
plt.title('Accuracy-Fairness Trade-off')
plt.grid(True)
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Fairness –≤ –ø—Ä–∞–∫—Ç–∏–∫–µ</h2>
    <p><strong>Best practices</strong>:</p>
    <ul>
      <li>‚úÖ –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∑–∞—â–∏—â–µ–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã</li>
      <li>‚úÖ –í—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–µ—Ç—Ä–∏–∫—É fairness</li>
      <li>‚úÖ –ò–∑–º–µ—Ä–∏—Ç—å bias –≤ –¥–∞–Ω–Ω—ã—Ö</li>
      <li>‚úÖ –ü—Ä–∏–º–µ–Ω–∏—Ç—å mitigation methods</li>
      <li>‚úÖ –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø–∞—Ö</li>
      <li>‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å fairness –≤ production</li>
      <li>‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è</li>
      <li>‚úÖ –ü—Ä–∏–≤–ª–µ–∫–∞—Ç—å domain experts</li>
    </ul>

    <pre><code># –ü—Ä–∏–º–µ—Ä fairness pipeline
class FairMLPipeline:
    def __init__(self, sensitive_attrs):
        self.sensitive_attrs = sensitive_attrs
        self.metrics_history = []
    
    def fit(self, X, y):
        # 1. –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        bias_report = self.analyze_data_bias(X, y)
        
        # 2. Pre-processing
        X_fair, y_fair, weights = self.debias_data(X, y)
        
        # 3. –û–±—É—á–µ–Ω–∏–µ —Å fairness constraints
        self.model = self.train_fair_model(X_fair, y_fair, weights)
        
        # 4. Post-processing
        self.calibrator = self.fit_calibrator(X_fair, y_fair)
        
        return self
    
    def predict(self, X):
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = self.model.predict_proba(X)
        
        # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –¥–ª—è fairness
        y_pred_fair = self.calibrator.transform(y_pred, X[self.sensitive_attrs])
        
        return y_pred_fair
    
    def evaluate_fairness(self, X, y):
        y_pred = self.predict(X)
        
        metrics = {
            'demographic_parity': self.calc_demographic_parity(y_pred, X),
            'equal_opportunity': self.calc_equal_opportunity(y, y_pred, X),
            'equalized_odds': self.calc_equalized_odds(y, y_pred, X)
        }
        
        self.metrics_history.append(metrics)
        return metrics</code></pre>
  </div>

</div>

</body>
</html>

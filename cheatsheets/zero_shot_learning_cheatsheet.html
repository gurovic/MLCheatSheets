<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Zero-shot Learning Cheatsheet</title>
  <style>
    @media screen { body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; color: #333; background: #fafcff; padding: 10px; } }
    @media print { body { background: white; padding: 0; } @page { size: A4 landscape; margin: 10mm; } }
    .container { column-count: 3; column-gap: 20px; max-width: 100%; }
    .block { break-inside: avoid; margin-bottom: 1.2em; padding: 12px; background: white; border-radius: 6px; box-shadow: 0 1px 3px rgba(0,0,0,0.05); }
    h1 { font-size: 1.6em; font-weight: 700; color: #1a5fb4; text-align: center; margin: 0 0 8px; column-span: all; }
    .subtitle { text-align: center; color: #666; font-size: 0.9em; margin-bottom: 12px; column-span: all; }
    h2 { font-size: 1.15em; font-weight: 700; color: #1a5fb4; margin: 0 0 8px; padding-bottom: 4px; border-bottom: 1px solid #e0e7ff; }
    p, ul, ol { font-size: 0.92em; margin: 0.6em 0; }
    ul, ol { padding-left: 18px; }
    li { margin-bottom: 4px; }
    code { font-family: 'Consolas', 'Courier New', monospace; background-color: #f0f4ff; padding: 1px 4px; border-radius: 3px; font-size: 0.88em; }
    pre { background-color: #f0f4ff; padding: 8px; border-radius: 4px; overflow-x: auto; font-size: 0.84em; margin: 6px 0; }
    pre code { padding: 0; background: none; white-space: pre-wrap; }
    table { width: 100%; border-collapse: collapse; font-size: 0.82em; margin: 6px 0; }
    th { background-color: #e6f0ff; text-align: left; padding: 4px 6px; font-weight: 600; }
    td { padding: 4px 6px; border-bottom: 1px solid #f0f4ff; }
    tr:nth-child(even) { background-color: #f8fbff; }
    blockquote { font-style: italic; margin: 8px 0; padding: 6px 10px; background: #f8fbff; border-left: 2px solid #1a5fb4; font-size: 0.88em; }
  </style>
</head>
<body>
<div class="container">
  <h1>üéØ Zero-shot Learning</h1>
  <div class="subtitle">–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ñ–æ–∫—É—Å ‚Ä¢ –Ø–Ω–≤–∞—Ä—å 2026</div>
  
  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏</h2>
    <ul>
      <li><strong>–ó–∞–¥–∞—á–∞</strong>: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –≤–∏–¥–µ–ª–∞</li>
      <li><strong>–ü—Ä–∏–º–µ—Ä</strong>: –æ–±—É—á–∏–ª–∏ –Ω–∞ –∫–æ—à–∫–∞—Ö/—Å–æ–±–∞–∫–∞—Ö, —Ä–∞—Å–ø–æ–∑–Ω–∞–µ–º –∑–µ–±—Ä</li>
      <li><strong>–ü–æ–¥—Ö–æ–¥</strong>: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: –Ω–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 2. CLIP –º–æ–¥–µ–ª—å</h2>
    <pre><code>import torch
import clip
from PIL import Image

# Load model
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Load image
image = preprocess(Image.open("photo.jpg")).unsqueeze(0).to(device)

# Define possible classes (zero-shot!)
text = clip.tokenize([
    "a photo of a cat",
    "a photo of a dog",
    "a photo of a zebra",  # Never seen before!
    "a photo of an airplane"
]).to(device)

# Compute features
with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)
    
    # Cosine similarity
    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
    values, indices = similarity[0].topk(3)

print("Predictions:")
for value, index in zip(values, indices):
    print(f"  {text[index]}: {100 * value.item():.2f}%")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. Attribute-based Zero-shot</h2>
    <pre><code># –ò–¥–µ—è: –æ–ø–∏—Å–∞—Ç—å –∫–ª–∞—Å—Å—ã —á–µ—Ä–µ–∑ –∞—Ç—Ä–∏–±—É—Ç—ã
# Cat: [has_fur, has_tail, small, carnivore]
# Zebra: [has_fur, has_tail, large, herbivore, striped]

import numpy as np

# Class-attribute matrix
attributes = {
    'cat': [1, 1, 1, 1, 0, 0],      # fur, tail, small, carnivore, large, striped
    'dog': [1, 1, 1, 1, 0, 0],
    'zebra': [1, 1, 0, 0, 1, 1],    # unseen class
    'elephant': [0, 1, 0, 0, 1, 0]
}

# Model predicts attributes from image
predicted_attributes = attribute_predictor.predict(image)
# [0.9, 0.8, 0.1, 0.2, 0.9, 0.95]

# Find closest class
similarities = {}
for class_name, class_attrs in attributes.items():
    sim = cosine_similarity([predicted_attributes], [class_attrs])[0][0]
    similarities[class_name] = sim

best_class = max(similarities, key=similarities.get)
print(f"Predicted class: {best_class}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Semantic Embeddings</h2>
    <pre><code>from sentence_transformers import SentenceTransformer

# Encode class names
model = SentenceTransformer('all-MiniLM-L6-v2')

class_names = ['cat', 'dog', 'zebra', 'elephant']
class_embeddings = model.encode(class_names)

# Encode image features (from ResNet, etc.)
image_features = resnet_model.predict(image)

# Project to common space
from sklearn.metrics.pairwise import cosine_similarity

similarities = cosine_similarity(
    image_features.reshape(1, -1),
    class_embeddings
)

predicted_class = class_names[similarities.argmax()]
print(f"Zero-shot prediction: {predicted_class}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Word2Vec –¥–ª—è ZSL</h2>
    <pre><code>import gensim.downloader as api

# Load pretrained word vectors
word_vectors = api.load('word2vec-google-news-300')

# Get class vectors
seen_classes = ['cat', 'dog']
unseen_classes = ['zebra', 'tiger']

class_vectors = {
    cls: word_vectors[cls] 
    for cls in seen_classes + unseen_classes
}

# Visual model maps image to semantic space
visual_features = visual_model.predict(image)

# Find nearest class in semantic space
min_dist = float('inf')
predicted_class = None

for class_name, class_vec in class_vectors.items():
    dist = np.linalg.norm(visual_features - class_vec)
    if dist < min_dist:
        min_dist = dist
        predicted_class = class_name

print(f"Predicted: {predicted_class}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Generative ZSL</h2>
    <pre><code># –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥: —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã unseen –∫–ª–∞—Å—Å–æ–≤

from sklearn.svm import SVC

# 1. Train generator: semantic ‚Üí visual features
# Generator learns from seen classes
generator = train_generator(seen_class_embeddings, seen_visual_features)

# 2. Generate synthetic samples for unseen classes
unseen_embeddings = get_word_embeddings(unseen_classes)
synthetic_features = generator.generate(unseen_embeddings)

# 3. Train classifier on seen + synthetic unseen
X_train = np.vstack([seen_features, synthetic_features])
y_train = seen_labels + unseen_labels

classifier = SVC()
classifier.fit(X_train, y_train)

# 4. Classify new image
prediction = classifier.predict(new_image_features)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Few-shot vs Zero-shot</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–ü—Ä–∏–º–µ—Ä—ã</th><th>–°–ª–æ–∂–Ω–æ—Å—Ç—å</th></tr>
      <tr><td>Zero-shot</td><td>0 –ø—Ä–∏–º–µ—Ä–æ–≤</td><td>–í—ã—Å–æ–∫–∞—è</td></tr>
      <tr><td>One-shot</td><td>1 –ø—Ä–∏–º–µ—Ä</td><td>–°—Ä–µ–¥–Ω—è—è</td></tr>
      <tr><td>Few-shot</td><td>2-10 –ø—Ä–∏–º–µ—Ä–æ–≤</td><td>–ù–∏–∑–∫–∞—è</td></tr>
      <tr><td>Full</td><td>1000+ –ø—Ä–∏–º–µ—Ä–æ–≤</td><td>–ë–∞–∑–æ–≤–∞—è</td></tr>
    </table>
    <pre><code># Few-shot –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å zero-shot
# –î–∞—Ç—å –º–æ–¥–µ–ª–∏ 1-3 –ø—Ä–∏–º–µ—Ä–∞ –Ω–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞

# Zero-shot baseline
clip_model.classify(image, ["cat", "dog", "zebra"])

# Few-shot improvement  
examples = load_few_examples("zebra", n=3)
clip_model.add_examples(examples)
clip_model.classify(image, ["cat", "dog", "zebra"])
# Accuracy improves!</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Evaluation Metrics</h2>
    <pre><code># –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è ZSL

# 1. Conventional ZSL: —Ç–æ–ª—å–∫–æ unseen –∫–ª–∞—Å—Å—ã
unseen_accuracy = accuracy_score(y_true_unseen, y_pred_unseen)

# 2. Generalized ZSL: seen + unseen –∫–ª–∞—Å—Å—ã
seen_accuracy = accuracy_score(y_true_seen, y_pred_seen)
unseen_accuracy = accuracy_score(y_true_unseen, y_pred_unseen)

# Harmonic mean (H-mean)
H = 2 * (seen_accuracy * unseen_accuracy) / (seen_accuracy + unseen_accuracy)

print(f"Seen accuracy: {seen_accuracy:.3f}")
print(f"Unseen accuracy: {unseen_accuracy:.3f}")
print(f"H-mean: {H:.3f}")

# Area Under Seen-Unseen Curve (AUSUC)
# –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É seen –∏ unseen performance</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>E-commerce</strong>: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤</li>
      <li><strong>–ú–µ–¥–∏—Ü–∏–Ω–∞</strong>: –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–µ–¥–∫–∏—Ö –±–æ–ª–µ–∑–Ω–µ–π</li>
      <li><strong>Wildlife</strong>: —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö –≤–∏–¥–æ–≤</li>
      <li><strong>Document classification</strong>: –Ω–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤</li>
      <li><strong>Fashion</strong>: –Ω–æ–≤—ã–µ —Å—Ç–∏–ª–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è</li>
    </ul>
    <pre><code># –ü—Ä–∏–º–µ—Ä: –Ω–æ–≤—ã–π —Ç–æ–≤–∞—Ä –≤ –º–∞–≥–∞–∑–∏–Ω–µ
new_product_image = load_image("new_product.jpg")
possible_categories = [
    "sports equipment",
    "kitchen appliance",
    "electronic device",
    "furniture",
    "toy"
]

# Zero-shot classification
category = clip_classify(new_product_image, possible_categories)
print(f"Category: {category}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å (CLIP, semantic embeddings)</li>
      <li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å seen/unseen split</li>
      <li>[ ] –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å semantic descriptions</li>
      <li>[ ] –û–±—É—á–∏—Ç—å –Ω–∞ seen –∫–ª–∞—Å—Å–∞—Ö</li>
      <li>[ ] –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ unseen –∫–ª–∞—Å—Å–∞—Ö</li>
      <li>[ ] –ò–∑–º–µ—Ä–∏—Ç—å seen/unseen accuracy</li>
      <li>[ ] –í—ã—á–∏—Å–ª–∏—Ç—å H-mean –¥–ª—è GZSL</li>
      <li>[ ] –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å prompts</li>
      <li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å few-shot –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è</li>
      <li>[ ] –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
    </ul>
    <blockquote>
      ¬´Zero-shot learning ‚Äî —ç—Ç–æ –∫–∞–∫ –æ–±—ä—è—Å–Ω–∏—Ç—å —Ä–µ–±–µ–Ω–∫—É, —á—Ç–æ —Ç–∞–∫–æ–µ "–∑–µ–±—Ä–∞", –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞—è —Ñ–æ—Ç–æ: "–≠—Ç–æ –∫–∞–∫ –ª–æ—à–∞–¥—å, –Ω–æ —Å —á–µ—Ä–Ω–æ-–±–µ–ª—ã–º–∏ –ø–æ–ª–æ—Å–∞–º–∏". –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–Ω–∞–Ω–∏—è –æ –ø–æ—Ö–æ–∂–∏—Ö –æ–±—ä–µ–∫—Ç–∞—Ö –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ¬ª.
    </blockquote>
  </div>


</div>
</body>
</html>

<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üé® –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π Cheatsheet</h1>
  <div class="subtitle">U-Net ‚Ä¢ DeepLab ‚Ä¢ Mask R-CNN ‚Ä¢ Semantic & Instance Segmentation<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –¢–∏–ø—ã —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏</h2>
    <table>
      <tr><th>–¢–∏–ø</th><th>–ó–∞–¥–∞—á–∞</th><th>–í—ã—Ö–æ–¥</th></tr>
      <tr><td><strong>Semantic</strong></td><td>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è</td><td>–ú–∞—Å–∫–∞ –∫–ª–∞—Å—Å–æ–≤</td></tr>
      <tr><td><strong>Instance</strong></td><td>–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤</td><td>–ú–∞—Å–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞</td></tr>
      <tr><td><strong>Panoptic</strong></td><td>Semantic + Instance</td><td>–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–∞—Å–∫–∞</td></tr>
    </table>
    
    <p><strong>Semantic Segmentation:</strong></p>
    <ul>
      <li>–í—Å–µ –ø–∏–∫—Å–µ–ª–∏ "—á–µ–ª–æ–≤–µ–∫" = –æ–¥–∏–Ω –∫–ª–∞—Å—Å</li>
      <li>–ù–µ —Ä–∞–∑–ª–∏—á–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ª—é–¥–µ–π</li>
      <li>–ü—Ä–∏–º–µ—Ä: –¥–æ—Ä–æ–≥–∞, –Ω–µ–±–æ, –∑–¥–∞–Ω–∏—è</li>
    </ul>
    
    <p><strong>Instance Segmentation:</strong></p>
    <ul>
      <li>–ö–∞–∂–¥—ã–π —á–µ–ª–æ–≤–µ–∫ = –æ—Ç–¥–µ–ª—å–Ω–∞—è –º–∞—Å–∫–∞</li>
      <li>–û—Ç–ª–∏—á–∞–µ—Ç –æ–±—ä–µ–∫—Ç—ã –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä: person_1, person_2, ...</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 2. U-Net –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</h2>
    <p><strong>–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong> –¥–ª—è semantic segmentation:</p>
    
    <p><strong>–°—Ç—Ä—É–∫—Ç—É—Ä–∞:</strong></p>
    <ol>
      <li><strong>Encoder (downsampling)</strong>:
        <ul>
          <li>–°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ + MaxPooling</li>
          <li>–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
          <li>–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞</li>
        </ul>
      </li>
      <li><strong>Decoder (upsampling)</strong>:
        <ul>
          <li>–¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–≤–µ—Ä—Ç–∫–∏</li>
          <li>–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞</li>
          <li>Skip connections —Å encoder</li>
        </ul>
      </li>
    </ol>
    
    <p><strong>Skip Connections:</strong></p>
    <ul>
      <li>–°–æ–µ–¥–∏–Ω—è—é—Ç encoder –∏ decoder</li>
      <li>–°–æ—Ö—Ä–∞–Ω—è—é—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é</li>
      <li>–ö–ª—é—á–µ–≤–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å U-Net</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. U-Net –∫–æ–¥</h2>
    <pre><code>import torch
import torch.nn as nn

class UNet(nn.Module):
    def __init__(self, in_channels=3, num_classes=21):
        super().__init__()
        
        # Encoder
        self.enc1 = self.conv_block(in_channels, 64)
        self.enc2 = self.conv_block(64, 128)
        self.enc3 = self.conv_block(128, 256)
        self.enc4 = self.conv_block(256, 512)
        
        # Bottleneck
        self.bottleneck = self.conv_block(512, 1024)
        
        # Decoder
        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = self.conv_block(1024, 512)
        
        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = self.conv_block(512, 256)
        
        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = self.conv_block(256, 128)
        
        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = self.conv_block(128, 64)
        
        # Output
        self.out = nn.Conv2d(64, num_classes, 1)
        
        self.pool = nn.MaxPool2d(2)
    
    def conv_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        # Encoder
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        enc3 = self.enc3(self.pool(enc2))
        enc4 = self.enc4(self.pool(enc3))
        
        # Bottleneck
        bottleneck = self.bottleneck(self.pool(enc4))
        
        # Decoder with skip connections
        dec4 = self.up4(bottleneck)
        dec4 = torch.cat([dec4, enc4], dim=1)
        dec4 = self.dec4(dec4)
        
        dec3 = self.up3(dec4)
        dec3 = torch.cat([dec3, enc3], dim=1)
        dec3 = self.dec3(dec3)
        
        dec2 = self.up2(dec3)
        dec2 = torch.cat([dec2, enc2], dim=1)
        dec2 = self.dec2(dec2)
        
        dec1 = self.up1(dec2)
        dec1 = torch.cat([dec1, enc1], dim=1)
        dec1 = self.dec1(dec1)
        
        return self.out(dec1)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –û–±—É—á–µ–Ω–∏–µ U-Net</h2>
    <pre><code># –ú–æ–¥–µ–ª—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
model = UNet(in_channels=3, num_classes=21)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# Loss functions
criterion = nn.CrossEntropyLoss()

# –ò–ª–∏ Dice Loss –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
class DiceLoss(nn.Module):
    def forward(self, pred, target):
        smooth = 1.
        pred = torch.softmax(pred, dim=1)
        
        intersection = (pred * target).sum(dim=(2, 3))
        union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))
        
        dice = (2. * intersection + smooth) / (union + smooth)
        return 1 - dice.mean()

# Training loop
model.train()
for epoch in range(num_epochs):
    for images, masks in dataloader:
        # masks shape: (B, H, W) —Å –∫–ª–∞—Å—Å–∞–º–∏ 0..num_classes-1
        
        optimizer.zero_grad()
        outputs = model(images)  # (B, num_classes, H, W)
        
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()
    
    print(f"Epoch {epoch}, Loss: {loss.item():.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å</h2>
    <p><strong>Cross Entropy Loss:</strong></p>
    <pre><code># –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –¥–ª—è semantic segmentation
criterion = nn.CrossEntropyLoss()</code></pre>
    
    <p><strong>Weighted Cross Entropy:</strong></p>
    <pre><code># –î–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
class_weights = torch.tensor([0.1, 1.0, 2.0, ...])
criterion = nn.CrossEntropyLoss(weight=class_weights)</code></pre>
    
    <p><strong>Dice Loss:</strong></p>
    <pre><code># –î–ª—è –º–∞–ª—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
dice_loss = DiceLoss()</code></pre>
    
    <p><strong>Focal Loss:</strong></p>
    <pre><code># –î–ª—è —Å–∏–ª—å–Ω–æ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, pred, target):
        ce_loss = F.cross_entropy(pred, target, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()</code></pre>
    
    <p><strong>–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è:</strong></p>
    <pre><code># CE + Dice
loss = 0.5 * ce_loss + 0.5 * dice_loss</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. DeepLab v3+</h2>
    <p><strong>–°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong> –¥–ª—è semantic segmentation:</p>
    
    <p><strong>–ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:</strong></p>
    <ul>
      <li><strong>Atrous Convolution</strong> (Dilated): —É–≤–µ–ª–∏—á–µ–Ω–∏–µ receptive field</li>
      <li><strong>ASPP</strong> (Atrous Spatial Pyramid Pooling): multi-scale –∫–æ–Ω—Ç–µ–∫—Å—Ç</li>
      <li><strong>Encoder-Decoder</strong>: ResNet/Xception backbone</li>
    </ul>
    
    <p><strong>Atrous Convolution:</strong></p>
    <pre><code># –û–±—ã—á–Ω–∞—è —Å–≤–µ—Ä—Ç–∫–∞
conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)

# Atrous —Å–≤–µ—Ä—Ç–∫–∞ —Å dilation=2
atrous_conv = nn.Conv2d(
    in_ch, out_ch,
    kernel_size=3,
    padding=2,
    dilation=2  # —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç receptive field
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. DeepLab –∫–æ–¥</h2>
    <pre><code>import torch
import torchvision

# –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è DeepLab v3
model = torchvision.models.segmentation.deeplabv3_resnet50(
    pretrained=True,
    num_classes=21  # PASCAL VOC
)

# –ó–∞–º–µ–Ω–∞ –¥–ª—è —Å–≤–æ–∏—Ö –∫–ª–∞—Å—Å–æ–≤
model.classifier[4] = nn.Conv2d(256, num_classes, 1)

# Inference
model.eval()
with torch.no_grad():
    output = model(image)['out']  # (B, num_classes, H, W)
    pred = output.argmax(1)  # (B, H, W)

# –û–±—É—á–µ–Ω–∏–µ
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.01,
    momentum=0.9,
    weight_decay=1e-4
)

for images, masks in dataloader:
    outputs = model(images)['out']
    loss = criterion(outputs, masks)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Mask R-CNN</h2>
    <p><strong>Instance Segmentation</strong> ‚Äî —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ Faster R-CNN:</p>
    
    <p><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:</strong></p>
    <ol>
      <li><strong>Backbone</strong>: ResNet + FPN</li>
      <li><strong>RPN</strong>: Region Proposal Network</li>
      <li><strong>RoI Align</strong>: —Ç–æ—á–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ (–Ω–µ pooling!)</li>
      <li><strong>Heads</strong>:
        <ul>
          <li>Classification head</li>
          <li>Box regression head</li>
          <li><strong>Mask head</strong>: FCN –¥–ª—è –º–∞—Å–∫–∏</li>
        </ul>
      </li>
    </ol>
    
    <p><strong>RoI Align vs RoI Pooling:</strong></p>
    <ul>
      <li><strong>RoI Pooling</strong>: –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç ‚Üí –ø–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏</li>
      <li><strong>RoI Align</strong>: –±–∏–ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è ‚Üí —Ç–æ—á–Ω—ã–µ –º–∞—Å–∫–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 9. Mask R-CNN –∫–æ–¥</h2>
    <pre><code>import torchvision
from torchvision.models.detection import maskrcnn_resnet50_fpn

# –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
model = maskrcnn_resnet50_fpn(pretrained=True)
model.eval()

# Inference
with torch.no_grad():
    predictions = model([image_tensor])

# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
boxes = predictions[0]['boxes']      # (N, 4)
labels = predictions[0]['labels']    # (N,)
scores = predictions[0]['scores']    # (N,)
masks = predictions[0]['masks']      # (N, 1, H, W)

# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ threshold
threshold = 0.5
keep = scores > threshold
masks = masks[keep, 0] > 0.5  # –±–∏–Ω–∞—Ä–Ω—ã–µ –º–∞—Å–∫–∏

# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö
model = maskrcnn_resnet50_fpn(
    pretrained=True,
    num_classes=num_classes
)

# –ó–∞–º–µ–Ω–∏—Ç—å heads
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(
    in_features,
    num_classes
)

in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
model.roi_heads.mask_predictor = MaskRCNNPredictor(
    in_features_mask,
    256,
    num_classes
)

# Training
params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(
    params,
    lr=0.005,
    momentum=0.9,
    weight_decay=0.0005
)

for images, targets in dataloader:
    # targets = [{'boxes': ..., 'labels': ..., 'masks': ...}]
    loss_dict = model(images, targets)
    losses = sum(loss for loss in loss_dict.values())
    
    optimizer.zero_grad()
    losses.backward()
    optimizer.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ú–µ—Ç—Ä–∏–∫–∏</h2>
    <p><strong>IoU (Intersection over Union):</strong></p>
    <pre><code>def calculate_iou(pred_mask, gt_mask):
    intersection = (pred_mask & gt_mask).sum()
    union = (pred_mask | gt_mask).sum()
    return intersection / union if union > 0 else 0</code></pre>
    
    <p><strong>Dice Coefficient:</strong></p>
    <pre><code>def dice_coefficient(pred_mask, gt_mask):
    intersection = (pred_mask & gt_mask).sum()
    return (2. * intersection) / (pred_mask.sum() + gt_mask.sum())</code></pre>
    
    <p><strong>Pixel Accuracy:</strong></p>
    <pre><code>def pixel_accuracy(pred, target):
    correct = (pred == target).sum()
    total = target.numel()
    return correct / total</code></pre>
    
    <p><strong>Mean IoU:</strong></p>
    <pre><code>def mean_iou(pred, target, num_classes):
    ious = []
    for cls in range(num_classes):
        pred_cls = (pred == cls)
        target_cls = (target == cls)
        iou = calculate_iou(pred_cls, target_cls)
        ious.append(iou)
    return np.mean(ious)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Post-processing</h2>
    <p><strong>CRF (Conditional Random Fields)</strong> ‚Äî —É—Ç–æ—á–Ω–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü:</p>
    <pre><code># pip install pydensecrf
import pydensecrf.densecrf as dcrf

def apply_crf(image, probabilities):
    # probabilities: (num_classes, H, W)
    d = dcrf.DenseCRF2D(W, H, num_classes)
    
    # Unary potential
    U = -np.log(probabilities)
    U = U.reshape((num_classes, -1))
    d.setUnaryEnergy(U)
    
    # Pairwise potentials
    d.addPairwiseGaussian(sxy=3, compat=3)
    d.addPairwiseBilateral(
        sxy=80, srgb=13,
        rgbim=image,
        compat=10
    )
    
    # Inference
    Q = d.inference(5)
    Q = np.array(Q).reshape((num_classes, H, W))
    
    return Q.argmax(0)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Data Augmentation</h2>
    <pre><code>import albumentations as A

transform = A.Compose([
    # Geometric (–æ–¥–∏–Ω–∞–∫–æ–≤–æ –¥–ª—è image –∏ mask!)
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    A.ShiftScaleRotate(
        shift_limit=0.1,
        scale_limit=0.1,
        rotate_limit=45,
        p=0.5
    ),
    
    # Elastic transform
    A.ElasticTransform(
        alpha=120,
        sigma=120 * 0.05,
        alpha_affine=120 * 0.03,
        p=0.3
    ),
    
    # Crop
    A.RandomCrop(height=512, width=512),
    
    # Color (—Ç–æ–ª—å–∫–æ –¥–ª—è image!)
    A.RandomBrightnessContrast(p=0.3),
    A.HueSaturationValue(p=0.3),
    A.GaussianBlur(p=0.2),
    
    # Normalize
    A.Normalize()
])

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
transformed = transform(image=image, mask=mask)
aug_image = transformed['image']
aug_mask = transformed['mask']</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. Semantic Segmentation –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏</h2>
    <p><strong>–õ–µ–≥–∫–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:</strong></p>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</th><th>mIoU</th><th>FPS</th></tr>
      <tr><td>ENet</td><td>0.4M</td><td>58.3</td><td>76</td></tr>
      <tr><td>ICNet</td><td>6.7M</td><td>69.5</td><td>30</td></tr>
      <tr><td>BiSeNet</td><td>13.3M</td><td>74.8</td><td>72</td></tr>
      <tr><td>Fast-SCNN</td><td>1.1M</td><td>68.0</td><td>123</td></tr>
    </table>
    
    <pre><code># –ü—Ä–∏–º–µ—Ä: —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤–∏–¥–µ–æ
import cv2

model = load_fast_model()  # –ª–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    input_tensor = preprocess(frame)
    
    # Inference
    with torch.no_grad():
        output = model(input_tensor)
        mask = output.argmax(1)[0].cpu().numpy()
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    colored_mask = colorize_mask(mask)
    result = cv2.addWeighted(frame, 0.7, colored_mask, 0.3, 0)
    
    cv2.imshow('Segmentation', result)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. Transfer Learning</h2>
    <p><strong>–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:</strong></p>
    <ul>
      <li><strong>ImageNet</strong>: —Ç–æ–ª—å–∫–æ backbone</li>
      <li><strong>COCO</strong>: –ø–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è instance seg</li>
      <li><strong>Cityscapes</strong>: urban scenes</li>
      <li><strong>ADE20K</strong>: 150 –∫–ª–∞—Å—Å–æ–≤ indoor/outdoor</li>
    </ul>
    
    <pre><code># Fine-tuning —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
model = deeplabv3_resnet50(pretrained=True)

# –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å backbone
for param in model.backbone.parameters():
    param.requires_grad = False

# –†–∞–∑–º–æ—Ä–æ–∑–∏—Ç—å batch norm –≤ backbone
for module in model.backbone.modules():
    if isinstance(module, nn.BatchNorm2d):
        module.requires_grad_(True)

# –û–±—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ decoder
optimizer = torch.optim.Adam([
    {'params': model.classifier.parameters(), 'lr': 1e-3},
    {'params': model.backbone.parameters(), 'lr': 1e-5}
])

# –ü–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç–ø–æ—Ö - —Ä–∞–∑–º–æ—Ä–æ–∑–∏—Ç—å –≤—Å–µ
for param in model.parameters():
    param.requires_grad = True

optimizer = torch.optim.Adam(
    model.parameters(),
    lr=1e-4
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤</h2>
    <pre><code>import matplotlib.pyplot as plt
import numpy as np

def visualize_segmentation(image, mask, pred_mask):
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Original image
    axes[0].imshow(image)
    axes[0].set_title('Original')
    axes[0].axis('off')
    
    # Ground truth
    axes[1].imshow(mask, cmap='tab20')
    axes[1].set_title('Ground Truth')
    axes[1].axis('off')
    
    # Prediction
    axes[2].imshow(pred_mask, cmap='tab20')
    axes[2].set_title('Prediction')
    axes[2].axis('off')
    
    plt.tight_layout()
    plt.show()

def overlay_mask(image, mask, alpha=0.5):
    """–ù–∞–ª–æ–∂–∏—Ç—å –º–∞—Å–∫—É –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
    colored_mask = colorize_mask(mask)
    return cv2.addWeighted(image, 1-alpha, colored_mask, alpha, 0)

def colorize_mask(mask):
    """–†–∞—Å–∫—Ä–∞—Å–∏—Ç—å –º–∞—Å–∫—É –∫–ª–∞—Å—Å–æ–≤"""
    h, w = mask.shape
    colored = np.zeros((h, w, 3), dtype=np.uint8)
    
    # –ü–∞–ª–∏—Ç—Ä–∞ —Ü–≤–µ—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤
    palette = [
        [128, 64, 128],   # road
        [244, 35, 232],   # sidewalk
        [70, 70, 70],     # building
        # ... –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ —Ü–≤–µ—Ç–æ–≤
    ]
    
    for cls_id, color in enumerate(palette):
        colored[mask == cls_id] = color
    
    return colored</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ Best Practices</h3>
        <ul>
          <li><strong>U-Net</strong> ‚Äî –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</li>
          <li><strong>DeepLab</strong> ‚Äî –¥–ª—è outdoor scenes</li>
          <li><strong>Mask R-CNN</strong> ‚Äî –¥–ª—è instance segmentation</li>
          <li><strong>Transfer learning</strong> –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω</li>
          <li><strong>Data augmentation</strong> –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–Ω–∞</li>
          <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ <strong>mixed precision</strong> –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ö†Ô∏è –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏</h3>
        <ul>
          <li>–ù–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–ª–∏ –º–∞—Å–∫–∏ (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å 0..num_classes-1)</li>
          <li>–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –º–∞—Å–∫–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è</li>
          <li>–ó–∞–±—ã–ª–∏ –ø—Ä–æ class imbalance</li>
          <li>–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è image, –Ω–µ –¥–ª—è mask</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 17. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è inference</h2>
    <pre><code># 1. TorchScript
model.eval()
scripted = torch.jit.script(model)
scripted.save('model_scripted.pt')

# 2. ONNX export
torch.onnx.export(
    model,
    dummy_input,
    'segmentation.onnx',
    input_names=['image'],
    output_names=['mask'],
    dynamic_axes={'image': {0: 'batch', 2: 'height', 3: 'width'}}
)

# 3. TensorRT (fastest)
import tensorrt as trt
# ... TensorRT conversion ...

# 4. Quantization (INT8)
from torch.quantization import quantize_dynamic
quantized_model = quantize_dynamic(
    model,
    {nn.Conv2d, nn.Linear},
    dtype=torch.qint8
)

# 5. Pruning
from torch.nn.utils import prune
for module in model.modules():
    if isinstance(module, nn.Conv2d):
        prune.l1_unstructured(module, name='weight', amount=0.3)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –í—ã–±—Ä–∞–ª–∏ —Ç–∏–ø —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (semantic/instance)</li>
      <li>[ ] –ü–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ –º–∞—Å–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ</li>
      <li>[ ] –í—ã–±—Ä–∞–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É (U-Net/DeepLab/Mask R-CNN)</li>
      <li>[ ] –ü—Ä–∏–º–µ–Ω–∏–ª–∏ transfer learning</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏–ª–∏ data augmentation –¥–ª—è image+mask</li>
      <li>[ ] –í—ã–±—Ä–∞–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â—É—é loss function</li>
      <li>[ ] –£—á–ª–∏ class imbalance (weighted loss)</li>
      <li>[ ] –í—ã—á–∏—Å–ª–∏–ª–∏ mIoU –∏ Dice –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏</li>
      <li>[ ] –ü—Ä–∏–º–µ–Ω–∏–ª–∏ post-processing (CRF)</li>
      <li>[ ] –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–ª–∏ –¥–ª—è inference</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è ‚Äî —ç—Ç–æ –∫–æ–≥–¥–∞ –º—ã —Ä–∞—Å–∫—Ä–∞—à–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ø–∏–∫—Å–µ–ª—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ü–≤–µ—Ç –µ–≥–æ –∫–ª–∞—Å—Å–∞. Semantic segmentation –≥–æ–≤–æ—Ä–∏—Ç "—ç—Ç–æ –¥–æ—Ä–æ–≥–∞, —ç—Ç–æ –º–∞—à–∏–Ω–∞", –∞ instance segmentation —É—Ç–æ—á–Ω—è–µ—Ç "—ç—Ç–æ –ø–µ—Ä–≤–∞—è –º–∞—à–∏–Ω–∞, —ç—Ç–æ –≤—Ç–æ—Ä–∞—è –º–∞—à–∏–Ω–∞". –ö–∞–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞—Å–∫—Ä–∞—Å–∫–∞ –ø–æ –Ω–æ–º–µ—Ä–∞–º, –Ω–æ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ç–æ¬ª.
    </blockquote>
  </div>

</div>

</body>
</html>

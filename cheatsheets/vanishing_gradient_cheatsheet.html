<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–ü—Ä–æ–±–ª–µ–º–∞ –∏—Å—á–µ–∑–∞—é—â–µ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen { body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; color: #333; background: #fafcff; padding: 10px; } }
    @media print { body { background: white; padding: 0; } @page { size: A4 landscape; margin: 10mm; } }
    .container { column-count: 3; column-gap: 20px; max-width: 100%; }
    .block { break-inside: avoid; margin-bottom: 1.2em; padding: 12px; background: white; border-radius: 6px; box-shadow: 0 1px 3px rgba(0,0,0,0.05); }
    h1 { font-size: 1.6em; font-weight: 700; color: #1a5fb4; text-align: center; margin: 0 0 8px; column-span: all; }
    .subtitle { text-align: center; color: #666; font-size: 0.9em; margin-bottom: 12px; column-span: all; }
    h2 { font-size: 1.15em; font-weight: 700; color: #1a5fb4; margin: 0 0 8px; padding-bottom: 4px; border-bottom: 1px solid #e0e7ff; }
    p, ul, ol { font-size: 0.92em; margin: 0.6em 0; }
    ul, ol { padding-left: 18px; }
    li { margin-bottom: 4px; }
    code { font-family: 'Consolas', 'Courier New', monospace; background-color: #f0f4ff; padding: 1px 4px; border-radius: 3px; font-size: 0.88em; }
    pre { background-color: #f0f4ff; padding: 8px; border-radius: 4px; overflow-x: auto; font-size: 0.84em; margin: 6px 0; }
    pre code { padding: 0; background: none; white-space: pre-wrap; }
    table { width: 100%; border-collapse: collapse; font-size: 0.82em; margin: 6px 0; }
    th { background-color: #e6f0ff; text-align: left; padding: 4px 6px; font-weight: 600; }
    td { padding: 4px 6px; border-bottom: 1px solid #f0f4ff; }
    tr:nth-child(even) { background-color: #f8fbff; }
    blockquote { font-style: italic; margin: 8px 0; padding: 6px 10px; background: #f8fbff; border-left: 2px solid #1a5fb4; font-size: 0.88em; }
  </style>
</head>
<body>
<div class="container">
  <h1>‚ö° –ü—Ä–æ–±–ª–µ–º–∞ –∏—Å—á–µ–∑–∞—é—â–µ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ Cheatsheet</h1>
  <div class="subtitle">–ü—Ä–∏—á–∏–Ω—ã, –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>
  <div class="block">
    <h2>üî∑ 1. –ß—Ç–æ —ç—Ç–æ —Ç–∞–∫–æ–µ?</h2>
<p><strong>Vanishing Gradient Problem</strong> ‚Äî –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –º–∞–ª—ã–º–∏ –ø—Ä–∏ backpropagation —á–µ—Ä–µ–∑ –≥–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏.</p>
<ul><li>–ü—Ä–æ—è–≤–ª–µ–Ω–∏–µ: —Ä–∞–Ω–Ω–∏–µ —Å–ª–æ–∏ –Ω–µ –æ–±—É—á–∞—é—Ç—Å—è</li><li>–ü—Ä–∏—á–∏–Ω–∞: –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –º–Ω–æ–≥–∏—Ö –º–∞–ª—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π</li><li>–û—Å–æ–±–µ–Ω–Ω–æ –∫—Ä–∏—Ç–∏—á–Ω–æ: –¥–ª—è RNN –∏ –æ—á–µ–Ω—å –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π</li><li>–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ—Å—Ç—å: exploding gradient</li></ul>
<blockquote>üí° –ì—Ä–∞–¥–∏–µ–Ω—Ç —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –≤ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–∏ –ø—Ä–∏ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–∏ –Ω–∞–∑–∞–¥ —á–µ—Ä–µ–∑ —Å–ª–æ–∏</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 2. –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã</h2>
<p>–ü—Ä–∏ backpropagation –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è —á–µ—Ä–µ–∑ —Ü–µ–ø–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ:</p>
<p>‚àÇL/‚àÇw‚ÇÅ = ‚àÇL/‚àÇa‚Çô √ó ‚àÇa‚Çô/‚àÇa‚Çô‚Çã‚ÇÅ √ó ... √ó ‚àÇa‚ÇÇ/‚àÇa‚ÇÅ √ó ‚àÇa‚ÇÅ/‚àÇw‚ÇÅ</p>
<p>–ï—Å–ª–∏ –∫–∞–∂–¥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è |‚àÇa·µ¢/‚àÇa·µ¢‚Çã‚ÇÅ| < 1, —Ç–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —Å—Ç—Ä–µ–º–∏—Ç—Å—è –∫ 0</p>
<ul><li>Sigmoid: œÉ'(x) ‚â§ 0.25 (–º–∞–∫—Å–∏–º—É–º)</li><li>Tanh: tanh'(x) ‚â§ 1 (–æ–±—ã—á–Ω–æ < 1)</li><li>–î–ª—è n —Å–ª–æ–µ–≤: –≥—Ä–∞–¥–∏–µ–Ω—Ç ~ 0.25‚Åø</li></ul>
<pre><code>import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

gradient = 1.0
for layer in range(10):
    gradient *= 0.25  # derivative

print(f"–ü–æ—Å–ª–µ 10 —Å–ª–æ–µ–≤: {gradient:.10f}")  # –æ—á–µ–Ω—å –º–∞–ª–æ–µ —á–∏—Å–ª–æ!</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ü—Ä–∏—á–∏–Ω—ã</h2>
<table>
<tr><th>–ü—Ä–∏—á–∏–Ω–∞</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th></tr>
<tr><td>Saturating activations</td><td>Sigmoid/tanh —Å –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–º–∏ ‚âà 0 –≤ –Ω–∞—Å—ã—â–µ–Ω–∏–∏</td></tr>
<tr><td>–ì–ª—É–±–∏–Ω–∞ —Å–µ—Ç–∏</td><td>–ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –º–Ω–æ–≥–∏—Ö —á–∏—Å–µ–ª < 1</td></tr>
<tr><td>–ü–ª–æ—Ö–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è</td><td>–í–µ—Å–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª—ã/–≤–µ–ª–∏–∫–∏</td></tr>
<tr><td>–î–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (RNN)</td><td>–ú–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤</td></tr>
</table>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è</h2>
<ul>
<li>‚ùå –†–∞–Ω–Ω–∏–µ —Å–ª–æ–∏ –Ω–µ –æ–±—É—á–∞—é—Ç—Å—è: –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã ‚âà 0</li>
<li>‚ùå –ú–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å: –æ–±—É—á–µ–Ω–∏–µ –∑–∞—Å—Ç—Ä–µ–≤–∞–µ—Ç</li>
<li>‚ùå –ü–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: –º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç –∏–∑—É—á–∏—Ç—å —Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã</li>
<li>‚ùå –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π</li>
<li>‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ –≤ RNN</li>
</ul>
<pre><code># –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ vanishing gradient
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm().item()
        print(f"{name}: {grad_norm:.6f}")
        if grad_norm < 1e-7:
            print(f"‚ö†Ô∏è Vanishing gradient!")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –†–µ—à–µ–Ω–∏–µ 1: ReLU –∞–∫—Ç–∏–≤–∞—Ü–∏—è</h2>
<p><strong>ReLU(x) = max(0, x)</strong> –Ω–µ –Ω–∞—Å—ã—â–∞–µ—Ç—Å—è –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π</p>
<ul><li>‚úÖ –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è = 1 –¥–ª—è x > 0</li><li>‚úÖ –ù–µ –∏—Å—á–µ–∑–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç</li><li>‚úÖ –ë—ã—Å—Ç—Ä—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è</li><li>‚ùå Dying ReLU (–Ω–µ–π—Ä–æ–Ω—ã –º–æ–≥—É—Ç "—É–º–µ—Ä–µ—Ç—å")</li></ul>
<pre><code>import torch.nn as nn

model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),  # –≤–º–µ—Å—Ç–æ Sigmoid
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 10)
)

# –í–∞—Ä–∏–∞—Ü–∏–∏
nn.LeakyReLU(0.01)  # –Ω–µ–±–æ–ª—å—à–æ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è x < 0
nn.PReLU()  # –æ–±—É—á–∞–µ–º—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
nn.ELU()  # smooth –¥–ª—è x < 0
nn.GELU()  # –¥–ª—è Transformers</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –†–µ—à–µ–Ω–∏–µ 2: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è</h2>
<table>
<tr><th>–ú–µ—Ç–æ–¥</th><th>–î–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏</th><th>–§–æ—Ä–º—É–ª–∞</th></tr>
<tr><td>Xavier/Glorot</td><td>Sigmoid, Tanh</td><td>Uniform[-‚àö(6/(n_in+n_out)), ...]</td></tr>
<tr><td>He</td><td>ReLU, LeakyReLU</td><td>Normal(0, ‚àö(2/n_in))</td></tr>
<tr><td>LeCun</td><td>SELU</td><td>Normal(0, ‚àö(1/n_in))</td></tr>
</table>
<pre><code>import torch.nn.init as init

for m in model.modules():
    if isinstance(m, nn.Linear):
        # He initialization –¥–ª—è ReLU
        init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
        if m.bias is not None:
            init.constant_(m.bias, 0)

# Xavier –¥–ª—è Sigmoid/Tanh
init.xavier_normal_(layer.weight)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –†–µ—à–µ–Ω–∏–µ 3: Batch Normalization</h2>
<ul>
<li>‚úÖ –°—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ</li>
<li>‚úÖ –ü–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–µ learning rates</li>
<li>‚úÖ –£–º–µ–Ω—å—à–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏</li>
<li>‚úÖ –†–µ–≥—É–ª—è—Ä–∏–∑—É—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç</li>
</ul>
<pre><code># PyTorch
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.bn1 = nn.BatchNorm1d(256)
        self.fc2 = nn.Linear(256, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return x</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –†–µ—à–µ–Ω–∏–µ 4: Residual Connections</h2>
<p><strong>Skip connections</strong> –ø–æ–∑–≤–æ–ª—è—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é: y = F(x) + x</p>
<ul><li>‚úÖ –ì—Ä–∞–¥–∏–µ–Ω—Ç –º–æ–∂–µ—Ç —Ç–µ—á—å –±–µ–∑ –∑–∞—Ç—É—Ö–∞–Ω–∏—è</li><li>‚úÖ –ü–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å 100+ —Å–ª–æ–µ–≤</li><li>‚úÖ –õ–µ–≥–∫–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å</li></ul>
<pre><code>class ResidualBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.fc1 = nn.Linear(dim, dim)
        self.bn1 = nn.BatchNorm1d(dim)
        self.fc2 = nn.Linear(dim, dim)
        self.bn2 = nn.BatchNorm1d(dim)
    
    def forward(self, x):
        identity = x
        out = F.relu(self.bn1(self.fc1(x)))
        out = self.bn2(self.fc2(out))
        out += identity  # skip connection
        out = F.relu(out)
        return out</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –†–µ—à–µ–Ω–∏–µ 5: LSTM/GRU –¥–ª—è RNN</h2>
<p><strong>LSTM</strong> –∏—Å–ø–æ–ª—å–∑—É–µ—Ç gating –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞</p>
<ul><li>Forget gate: —á—Ç–æ –∑–∞–±—ã—Ç—å</li><li>Input gate: —á—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å</li><li>Output gate: —á—Ç–æ –≤—ã–¥–∞—Ç—å</li><li>Cell state: –º–∞–≥–∏—Å—Ç—Ä–∞–ª—å –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</li></ul>
<pre><code># PyTorch LSTM
lstm = nn.LSTM(
    input_size=100,
    hidden_size=256,
    num_layers=2,
    dropout=0.2,
    batch_first=True
)

# GRU
gru = nn.GRU(
    input_size=100,
    hidden_size=256,
    num_layers=2
)

output, (hidden, cell) = lstm(input_seq)</code></pre>
<blockquote>üí° LSTM —Ä–µ—à–∞–µ—Ç vanishing gradient —á–µ—Ä–µ–∑ cell state –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 10. Gradient Clipping</h2>
<p>–î–ª—è exploding gradients (–æ–±—Ä–∞—Ç–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞)</p>
<pre><code># PyTorch gradient clipping
torch.nn.utils.clip_grad_norm_(
    model.parameters(), 
    max_norm=1.0
)

# –í training loop
optimizer.zero_grad()
loss.backward()
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
optimizer.step()

# TensorFlow/Keras
optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —á–µ–∫–ª–∏—Å—Ç</h2>
<p><strong>–õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏:</strong></p>
<ul>
<li>‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ReLU/LeakyReLU/GELU –≤–º–µ—Å—Ç–æ Sigmoid/Tanh</li>
<li>‚úÖ He initialization –¥–ª—è ReLU</li>
<li>‚úÖ Batch Normalization –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è</li>
<li>‚úÖ Residual connections –¥–ª—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π</li>
<li>‚úÖ LSTM/GRU –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π</li>
<li>‚úÖ Gradient clipping (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è RNN)</li>
<li>‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å gradient norms</li>
<li>‚úÖ Adaptive optimizers (Adam, AdamW)</li>
</ul>
<table>
<tr><th>–ê–∫—Ç–∏–≤–∞—Ü–∏—è</th><th>Vanishing?</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr>
<tr><td>Sigmoid</td><td>‚ùå –î–∞</td><td>–ë–∏–Ω–∞—Ä–Ω—ã–π output layer</td></tr>
<tr><td>Tanh</td><td>‚ùå –î–∞</td><td>–õ—É—á—à–µ sigmoid –¥–ª—è hidden</td></tr>
<tr><td>ReLU</td><td>‚úÖ –ù–µ—Ç</td><td>–°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–ª—É—á–∞–µ–≤</td></tr>
<tr><td>LeakyReLU</td><td>‚úÖ –ù–µ—Ç</td><td>–ö–æ–≥–¥–∞ dying ReLU –ø—Ä–æ–±–ª–µ–º–∞</td></tr>
<tr><td>GELU</td><td>‚úÖ –ù–µ—Ç</td><td>Transformers, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏</td></tr>
</table>
  </div>


</div>
</body>
</html>
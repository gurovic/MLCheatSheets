<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Transfer Learning (Deep Learning) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üîÑ Transfer Learning (Deep Learning) Cheatsheet</h1>
  <div class="subtitle">–ü–µ—Ä–µ–Ω–æ—Å –æ–±—É—á–µ–Ω–∏—è ‚Ä¢ Pre-trained –º–æ–¥–µ–ª–∏ ‚Ä¢ Fine-tuning<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –∏–∑ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏ –¥–ª—è –¥—Ä—É–≥–æ–π</li>
      <li><strong>–ó–∞—á–µ–º</strong>: –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö, –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ, –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ</li>
      <li><strong>–ö–ª—é—á</strong>: pre-trained –º–æ–¥–µ–ª–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö</li>
      <li><strong>–ú–µ—Ç–æ–¥—ã</strong>: feature extraction, fine-tuning, domain adaptation</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 2. –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ Transfer Learning</h2>
    <table>
      <tr><th>–°—Ç—Ä–∞—Ç–µ–≥–∏—è</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr>
      <tr><td><strong>Feature Extraction</strong></td><td>–ú–∞–ª—ã–π –¥–∞—Ç–∞—Å–µ—Ç, –ø–æ—Ö–æ–∂–∞—è –∑–∞–¥–∞—á–∞</td></tr>
      <tr><td><strong>Fine-tuning</strong></td><td>–°—Ä–µ–¥–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç, —Å—Ö–æ–∂–∏–π –¥–æ–º–µ–Ω</td></tr>
      <tr><td><strong>Full Training</strong></td><td>–ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç, –ª—é–±–∞—è –∑–∞–¥–∞—á–∞</td></tr>
      <tr><td><strong>Few-shot Learning</strong></td><td>–û—á–µ–Ω—å –º–∞–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (5-50)</td></tr>
      <tr><td><strong>Domain Adaptation</strong></td><td>–†–∞–∑–Ω—ã–µ, –Ω–æ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. Feature Extraction</h2>
    <p><strong>–ü—Ä–∏–Ω—Ü–∏–ø</strong>: –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º backbone, –æ–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é –≥–æ–ª–æ–≤—É</p>
    <pre><code>import torch
import torchvision.models as models
import torch.nn as nn

# –ó–∞–≥—Ä—É–∑–∫–∞ pre-trained –º–æ–¥–µ–ª–∏
model = models.resnet50(pretrained=True)

# –ó–∞–º–æ—Ä–æ–∑–∫–∞ –≤—Å–µ—Ö —Å–ª–æ–µ–≤
for param in model.parameters():
    param.requires_grad = False

# –ó–∞–º–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
num_classes = 10
model.fc = nn.Linear(model.fc.in_features, num_classes)

# –¢–æ–ª—å–∫–æ fc –æ–±—É—á–∞–µ—Ç—Å—è
optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Fine-tuning</h2>
    <p><strong>–ü–æ–ª–Ω—ã–π fine-tuning</strong></p>
    <pre><code># –ó–∞–≥—Ä—É–∑–∫–∞ pre-trained –º–æ–¥–µ–ª–∏
model = models.resnet50(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)

# –û–±—É—á–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# –í–∞–∂–Ω–æ: –º–∞–ª–µ–Ω—å–∫–∏–π learning rate!</code></pre>
    <p><strong>–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ (Gradual Unfreezing)</strong></p>
    <pre><code># –≠—Ç–∞–ø 1: —Ç–æ–ª—å–∫–æ –≥–æ–ª–æ–≤–∞ (10 —ç–ø–æ—Ö)
for param in model.parameters():
    param.requires_grad = False
model.fc.requires_grad = True
# train...

# –≠—Ç–∞–ø 2: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –±–ª–æ–∫–∞ (10 —ç–ø–æ—Ö)
for param in model.layer4.parameters():
    param.requires_grad = True
# train...

# –≠—Ç–∞–ø 3: –≤—Å—è –º–æ–¥–µ–ª—å (10 —ç–ø–æ—Ö)
for param in model.parameters():
    param.requires_grad = True
# train...</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Discriminative Learning Rates</h2>
    <p><strong>–†–∞–∑–Ω—ã–µ LR –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–µ–≤</strong></p>
    <pre><code># –ë–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏–µ —Å–ª–æ–∏ - –º–µ–Ω—å—à–∏–π LR
optimizer = torch.optim.Adam([
    {'params': model.layer1.parameters(), 'lr': 1e-5},
    {'params': model.layer2.parameters(), 'lr': 5e-5},
    {'params': model.layer3.parameters(), 'lr': 1e-4},
    {'params': model.layer4.parameters(), 'lr': 5e-4},
    {'params': model.fc.parameters(), 'lr': 1e-3}
])

# –ò–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
def get_layer_lr(base_lr, layer_idx, num_layers, factor=0.95):
    """LR —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –¥–ª—è —Ä–∞–Ω–Ω–∏—Ö —Å–ª–æ–µ–≤"""
    return base_lr * (factor ** (num_layers - layer_idx))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ pre-trained –º–æ–¥–µ–ª–∏</h2>
    <p><strong>Computer Vision</strong></p>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–î–∞—Ç–∞—Å–µ—Ç</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>ResNet</strong></td><td>ImageNet</td><td>–û–±—â–∏–µ –∑–∞–¥–∞—á–∏ CV</td></tr>
      <tr><td><strong>EfficientNet</strong></td><td>ImageNet</td><td>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å</td></tr>
      <tr><td><strong>ViT</strong></td><td>ImageNet-21k</td><td>Transformer-based</td></tr>
      <tr><td><strong>CLIP</strong></td><td>400M –ø–∞—Ä</td><td>Multi-modal</td></tr>
      <tr><td><strong>DINOv2</strong></td><td>Self-supervised</td><td>Universal features</td></tr>
    </table>
    <p><strong>NLP</strong></p>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>BERT</strong></td><td>–ü–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞</td></tr>
      <tr><td><strong>GPT</strong></td><td>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞</td></tr>
      <tr><td><strong>T5</strong></td><td>Seq2seq –∑–∞–¥–∞—á–∏</td></tr>
      <tr><td><strong>RoBERTa</strong></td><td>–£–ª—É—á—à–µ–Ω–Ω—ã–π BERT</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Hugging Face</h2>
    <pre><code>from transformers import AutoModel, AutoTokenizer

# Computer Vision
model = AutoModel.from_pretrained("google/vit-base-patch16-224")

# NLP
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

# Fine-tuning –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained(
    "google/vit-base-patch16-224",
    num_labels=10,
    ignore_mismatched_sizes=True
)

# –û–±—É—á–µ–Ω–∏–µ
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    learning_rate=2e-5
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset
)
trainer.train()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Data Augmentation –ø—Ä–∏ Transfer Learning</h2>
    <pre><code>from torchvision import transforms

# –î–ª—è fine-tuning –≤–∞–∂–Ω–∞ augmentation
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.4, 0.4, 0.4),
    transforms.RandomRotation(15),
    transforms.ToTensor(),
    # –í–∞–∂–Ω–æ: —Ç–µ –∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, —á—Ç–æ –ø—Ä–∏ pre-training
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# –î–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Learning Rate Scheduling</h2>
    <pre><code>from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR

# Cosine Annealing
scheduler = CosineAnnealingLR(
    optimizer, 
    T_max=num_epochs,
    eta_min=1e-6
)

# One Cycle Policy (—á–∞—Å—Ç–æ –ª—É—á—à–µ –¥–ª—è fine-tuning)
scheduler = OneCycleLR(
    optimizer,
    max_lr=1e-3,
    epochs=num_epochs,
    steps_per_epoch=len(train_loader)
)

# Warmup + Cosine Decay
from transformers import get_cosine_schedule_with_warmup

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=len(train_loader),  # 1 —ç–ø–æ—Ö–∞ warmup
    num_training_steps=len(train_loader) * num_epochs
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Regularization techniques</h2>
    <p><strong>Dropout</strong></p>
    <ul>
      <li>–î–ª—è –Ω–æ–≤—ã—Ö —Å–ª–æ–µ–≤: dropout=0.3-0.5</li>
      <li>–î–ª—è fine-tuning: dropout=0.1-0.2</li>
    </ul>
    <p><strong>Weight Decay</strong></p>
    <ul>
      <li>L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è</li>
      <li>–¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: 1e-4 –¥–æ 1e-2</li>
    </ul>
    <p><strong>Label Smoothing</strong></p>
    <pre><code>import torch.nn.functional as F

def label_smoothing_loss(outputs, targets, smoothing=0.1):
    n_classes = outputs.size(-1)
    confidence = 1.0 - smoothing
    smooth_label = smoothing / (n_classes - 1)
    
    with torch.no_grad():
        true_dist = torch.zeros_like(outputs)
        true_dist.fill_(smooth_label)
        true_dist.scatter_(1, targets.unsqueeze(1), confidence)
    
    return F.kl_div(F.log_softmax(outputs, -1), true_dist, reduction='batchmean')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è</h3>
        <ul>
          <li>–ù–∞—á–∏–Ω–∞—Ç—å —Å feature extraction</li>
          <li>–ú–∞–ª–µ–Ω—å–∫–∏–π LR –¥–ª—è pre-trained —Å–ª–æ–µ–≤</li>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ –∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏</li>
          <li>Data augmentation</li>
          <li>Gradual unfreezing</li>
          <li>Early stopping</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ò–∑–±–µ–≥–∞—Ç—å</h3>
        <ul>
          <li>–ë–æ–ª—å—à–æ–π LR –¥–ª—è –≤—Å–µ–π –º–æ–¥–µ–ª–∏ —Å—Ä–∞–∑—É</li>
          <li>–û–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è –ø—Ä–∏ –º–∞–ª–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ</li>
          <li>–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏</li>
          <li>Fine-tuning –±–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏</li>
          <li>–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —ç–ø–æ—Ö (–ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ö–æ–≥–¥–∞ —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2>
    <table>
      <tr><th>–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö</th><th>–ü–æ—Ö–æ–∂–µ—Å—Ç—å –∑–∞–¥–∞—á</th><th>–°—Ç—Ä–∞—Ç–µ–≥–∏—è</th></tr>
      <tr><td>–ú–∞–ª—ã–π (&lt;1K)</td><td>–û—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏</td><td>Feature extraction</td></tr>
      <tr><td>–ú–∞–ª—ã–π (&lt;1K)</td><td>–†–∞–∑–Ω—ã–µ</td><td>Few-shot / Meta-learning</td></tr>
      <tr><td>–°—Ä–µ–¥–Ω–∏–π (1K-100K)</td><td>–ü–æ—Ö–æ–∂–∏</td><td>Fine-tuning –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ–µ–≤</td></tr>
      <tr><td>–°—Ä–µ–¥–Ω–∏–π (1K-100K)</td><td>–†–∞–∑–Ω—ã–µ</td><td>Full fine-tuning</td></tr>
      <tr><td>–ë–æ–ª—å—à–æ–π (&gt;100K)</td><td>–õ—é–±—ã–µ</td><td>Full training –≤–æ–∑–º–æ–∂–µ–Ω</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è</h2>
    <pre><code>from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter()

for epoch in range(num_epochs):
    train_loss = train_one_epoch(model, train_loader)
    val_loss, val_acc = validate(model, val_loader)
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    writer.add_scalar('Loss/train', train_loss, epoch)
    writer.add_scalar('Loss/val', val_loss, epoch)
    writer.add_scalar('Accuracy/val', val_acc, epoch)
    
    # Learning rate
    writer.add_scalar('LR', optimizer.param_groups[0]['lr'], epoch)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
    for name, param in model.named_parameters():
        if 'weight' in name:
            writer.add_histogram(name, param, epoch)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>–ú–µ–¥–∏—Ü–∏–Ω–∞</strong>: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö)</li>
      <li><strong>E-commerce</strong>: –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–æ–≤–∞—Ä–æ–≤</li>
      <li><strong>–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å</strong>: –¥–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π</li>
      <li><strong>–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ</strong>: –∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞</li>
      <li><strong>–°–µ–ª—å—Å–∫–æ–µ —Ö–æ–∑—è–π—Å—Ç–≤–æ</strong>: –∞–Ω–∞–ª–∏–∑ –ø–æ—Å–µ–≤–æ–≤</li>
      <li><strong>Robotics</strong>: —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç—å —É–∂–µ —É—á–∏–ª–∞—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –º–∏–ª–ª–∏–æ–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú—ã –±–µ—Ä–µ–º —ç—Ç–∏ –∑–Ω–∞–Ω–∏—è 
      –∏ "–¥–æ–æ–±—É—á–∞–µ–º" –µ—ë –Ω–∞ –≤–∞—à–∏—Ö —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –∫–∞–∫ –Ω–∞–Ω—è—Ç—å –æ–ø—ã—Ç–Ω–æ–≥–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –∏ –Ω–∞—É—á–∏—Ç—å 
      –µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è–º –≤–∞—à–µ–≥–æ –±–∏–∑–Ω–µ—Å–∞ ‚Äî –±—ã—Å—Ç—Ä–µ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ, —á–µ–º —Ä–∞—Å—Ç–∏—Ç—å –Ω–æ–≤–∏—á–∫–∞ —Å –Ω—É–ª—è¬ª.
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â—É—é pre-trained –º–æ–¥–µ–ª—å</li>
      <li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é (extraction/fine-tuning)</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é</li>
      <li>[ ] –ü–æ–¥–æ–±—Ä–∞—Ç—å learning rate (–º–∞–ª–µ–Ω—å–∫–∏–π!)</li>
      <li>[ ] –ü—Ä–∏–º–µ–Ω–∏—Ç—å data augmentation</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å regularization (dropout, weight decay)</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å early stopping</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å train/val loss</li>
      <li>[ ] –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å gradual unfreezing</li>
      <li>[ ] –û—Ü–µ–Ω–∏—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
    </ul>
  </div>

</div>

</body>
</html>

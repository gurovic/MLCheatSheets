<!DOCTYPE html>
<html lang="ru"><head><meta charset="UTF-8"><title>–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
<style>@media screen{{body{{font-family:-apple-system,sans-serif;color:#333;background:#fafcff;padding:10px}}}}.container{{column-count:3;column-gap:20px}}.block{{break-inside:avoid;margin-bottom:1.2em;padding:12px;background:white;border-radius:6px}}h1{{font-size:1.6em;color:#1a5fb4;text-align:center;column-span:all}}.subtitle{{text-align:center;color:#666;font-size:0.9em;margin-bottom:12px;column-span:all}}h2{{font-size:1.15em;color:#1a5fb4;margin:8px 0;border-bottom:1px solid #e0e7ff}}code{{background:#f0f4ff;padding:2px 4px}}pre{{background:#f0f4ff;padding:8px;border-radius:4px;font-size:0.84em}}table{{width:100%;border-collapse:collapse;font-size:0.82em}}th{{background:#e6f0ff;padding:4px}}td{{padding:4px;border-bottom:1px solid #f0f4ff}}</style></head><body>
<div class="container"><h1>üîç –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</h1>
<div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>
<div class="block"><h2>1. Grid Search</h2><pre><code>from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

grid = GridSearchCV(
    RandomForestClassifier(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid.fit(X_train, y_train)
print(f"Best params: {grid.best_params_}")
print(f"Best score: {grid.best_score_:.3f}")</code></pre></div>
<div class="block"><h2>2. Random Search</h2><pre><code>from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

param_dist = {
    'n_estimators': randint(50, 500),
    'max_depth': randint(5, 50),
    'min_samples_split': randint(2, 20),
    'max_features': uniform(0.1, 0.9)
}

random = RandomizedSearchCV(
    RandomForestClassifier(),
    param_dist,
    n_iter=100,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
    cv=5,
    random_state=42
)

random.fit(X_train, y_train)</code></pre></div>
<div class="block"><h2>3. Bayesian Optimization</h2><pre><code>from skopt import BayesSearchCV
from skopt.space import Real, Integer

search_spaces = {
    'n_estimators': Integer(50, 500),
    'max_depth': Integer(5, 50),
    'min_samples_split': Integer(2, 20),
    'max_features': Real(0.1, 1.0)
}

bayes = BayesSearchCV(
    RandomForestClassifier(),
    search_spaces,
    n_iter=50,
    cv=5,
    n_jobs=-1
)

bayes.fit(X_train, y_train)</code></pre></div>
<div class="block"><h2>4. Optuna</h2><pre><code>import optuna
from sklearn.ensemble import RandomForestClassifier

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'max_depth': trial.suggest_int('max_depth', 5, 50),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20)
    }
    
    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)
    score = model.score(X_val, y_val)
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

print(f"Best params: {study.best_params}")
print(f"Best score: {study.best_value:.3f}")</code></pre></div>
<div class="block"><h2>5. Hyperopt</h2><pre><code>from hyperopt import fmin, tpe, hp, Trials

space = {
    'n_estimators': hp.choice('n_estimators', range(50, 500)),
    'max_depth': hp.choice('max_depth', range(5, 50)),
    'min_samples_split': hp.choice('min_samples_split', range(2, 20))
}

def objective(params):
    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)
    return -model.score(X_val, y_val)  # –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º

trials = Trials()
best = fmin(objective, space, algo=tpe.suggest, 
            max_evals=100, trials=trials)</code></pre></div>
<div class="block"><h2>6. Halving Grid Search</h2><pre><code>from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV

# –ë—ã—Å—Ç—Ä–µ–µ —á–µ–º GridSearch
halving_grid = HalvingGridSearchCV(
    RandomForestClassifier(),
    param_grid,
    factor=3,  # –°–∫–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –æ—Ç—Å–µ–∏–≤–∞—Ç—å
    cv=5
)

halving_grid.fit(X_train, y_train)</code></pre></div>
<div class="block"><h2>7. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤</h2><table><tr><th>–ú–µ—Ç–æ–¥</th><th>–°–∫–æ—Ä–æ—Å—Ç—å</th><th>–ö–∞—á–µ—Å—Ç–≤–æ</th><th>–ö–æ–≥–¥–∞</th></tr><tr><td>Grid</td><td>–ú–µ–¥–ª–µ–Ω–Ω–æ</td><td>–û—Ç–ª–∏—á–Ω–æ</td><td>–ú–∞–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</td></tr><tr><td>Random</td><td>–ë—ã—Å—Ç—Ä–æ</td><td>–•–æ—Ä–æ—à–æ</td><td>–ú–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</td></tr><tr><td>Bayesian</td><td>–°—Ä–µ–¥–Ω–µ</td><td>–û—Ç–ª–∏—á–Ω–æ</td><td>–î–æ—Ä–æ–≥–∏–µ –º–æ–¥–µ–ª–∏</td></tr><tr><td>Optuna</td><td>–ë—ã—Å—Ç—Ä–æ</td><td>–û—Ç–ª–∏—á–Ω–æ</td><td>–°–ª–æ–∂–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞</td></tr></table></div>
<div class="block"><h2>8. Early Stopping</h2><pre><code># –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —É–ª—É—á—à–µ–Ω–∏—è
from sklearn.model_selection import GridSearchCV

grid = GridSearchCV(
    model,
    param_grid,
    cv=5,
    error_score='raise'  # –ü—Ä–µ—Ä–≤–∞—Ç—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
)

# –î–ª—è Optuna
study.optimize(objective, n_trials=100, 
               timeout=600)  # –ú–∞–∫—Å 10 –º–∏–Ω—É—Ç</code></pre></div>
<div class="block"><h2>9. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏</h2><ul><li>–ù–∞—á–∞—Ç—å —Å Random Search</li><li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ —à–∫–∞–ª—ã</li><li>Cross-validation –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ</li><li>–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –ø–æ–∏—Å–∫–∞</li><li>Bayesian –¥–ª—è –¥–æ—Ä–æ–≥–∏—Ö –º–æ–¥–µ–ª–µ–π</li></ul></div>
<div class="block"><h2>10. –ß–µ–∫-–ª–∏—Å—Ç</h2><ul><li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li><li>[ ] –í—ã–±—Ä–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ–∏—Å–∫–∞</li><li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å cross-validation</li><li>[ ] –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã</li><li>[ ] –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ test</li><li>[ ] –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å best params</li></ul></div>
<div class="block">
<h2>11. Transfer Learning –¥–ª—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</h2>
<pre><code># –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á
# Meta-learning –ø–æ–¥—Ö–æ–¥

best_params_history = {
    'task1': {'C': 10, 'gamma': 0.001},
    'task2': {'C': 8, 'gamma': 0.002},
    'task3': {'C': 12, 'gamma': 0.0015}
}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ —Å —Å—Ä–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
import numpy as np

initial_C = np.mean([p['C'] for p in best_params_history.values()])
initial_gamma = np.mean([p['gamma'] for p in best_params_history.values()])

# Warm start –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
param_grid = {
    'C': [initial_C * 0.5, initial_C, initial_C * 2],
    'gamma': [initial_gamma * 0.5, initial_gamma, initial_gamma * 2]
}

# Grid search —Å warm start
grid_search = GridSearchCV(SVC(), param_grid, cv=5)
grid_search.fit(X_new, y_new)</code></pre>
</div>

<div class="block">
<h2>12. Multi-fidelity –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è</h2>
<pre><code># –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ–Ω—å—à–∏—Ö subset –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏
from sklearn.model_selection import learning_curve

def multi_fidelity_search(X, y, param_space, budget=100):
    best_params = None
    best_score = -np.inf
    
    # –£—Ä–æ–≤–µ–Ω—å 1: –±—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ 10% –¥–∞–Ω–Ω—ã—Ö
    n_samples_level1 = int(0.1 * len(X))
    X_level1 = X[:n_samples_level1]
    y_level1 = y[:n_samples_level1]
    
    candidates = []
    for _ in range(budget // 3):
        params = sample_params(param_space)
        model = create_model(params)
        score = cross_val_score(model, X_level1, y_level1, cv=3).mean()
        candidates.append((params, score))
    
    # –£—Ä–æ–≤–µ–Ω—å 2: —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ 50% –¥–∞–Ω–Ω—ã—Ö
    top_candidates = sorted(candidates, key=lambda x: x[1], reverse=True)[:10]
    n_samples_level2 = int(0.5 * len(X))
    
    for params, _ in top_candidates:
        model = create_model(params)
        score = cross_val_score(model, X[:n_samples_level2], 
                               y[:n_samples_level2], cv=5).mean()
        if score > best_score:
            best_score = score
            best_params = params
    
    # –£—Ä–æ–≤–µ–Ω—å 3: —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
    model = create_model(best_params)
    final_score = cross_val_score(model, X, y, cv=10).mean()
    
    return best_params, final_score</code></pre>
</div>

<div class="block">
<h2>13. Hyperband algorithm</h2>
<pre><code># Hyperband: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –±–æ–ª—å—à–∏—Ö search spaces
# –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤

class Hyperband:
    def __init__(self, max_iter, eta=3):
        self.max_iter = max_iter
        self.eta = eta
    
    def run(self, X, y, param_space, n_configs=81):
        best_config = None
        best_score = -np.inf
        
        # Successive halving
        configs = [sample_params(param_space) for _ in range(n_configs)]
        
        for r in range(int(np.log(n_configs) / np.log(self.eta))):
            # Train –∫–∞–∂–¥—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –Ω–∞ r –∏—Ç–µ—Ä–∞—Ü–∏–π
            n_iterations = int(self.max_iter * self.eta ** (-r))
            
            scores = []
            for config in configs:
                model = create_model(config)
                score = evaluate_model(model, X, y, n_iterations)
                scores.append(score)
            
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ top 1/eta –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
            top_k = int(len(configs) / self.eta)
            top_indices = np.argsort(scores)[-top_k:]
            configs = [configs[i] for i in top_indices]
            
            if scores[top_indices[-1]] > best_score:
                best_score = scores[top_indices[-1]]
                best_config = configs[-1]
        
        return best_config, best_score

# Usage
hb = Hyperband(max_iter=100, eta=3)
best_params, score = hb.run(X_train, y_train, param_space)</code></pre>
</div>

<div class="block">
<h2>14. Monitoring –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞</h2>
<pre><code>import matplotlib.pyplot as plt
from sklearn.model_selection import RandomizedSearchCV

# –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
class OptimizationMonitor:
    def __init__(self):
        self.iteration = []
        self.scores = []
        self.params = []
    
    def callback(self, search_result):
        self.iteration.append(len(self.scores))
        self.scores.append(search_result.best_score_)
        self.params.append(search_result.best_params_)
    
    def plot_convergence(self):
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        plt.plot(self.iteration, self.scores, marker='o')
        plt.xlabel('Iteration')
        plt.ylabel('Best Score')
        plt.title('Optimization Convergence')
        plt.grid(alpha=0.3)
        
        plt.subplot(1, 2, 2)
        running_best = np.maximum.accumulate(self.scores)
        plt.plot(self.iteration, running_best, marker='s', color='green')
        plt.xlabel('Iteration')
        plt.ylabel('Running Best Score')
        plt.title('Best Score Over Time')
        plt.grid(alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# Usage with custom callback
monitor = OptimizationMonitor()

search = RandomizedSearchCV(
    estimator, param_distributions,
    n_iter=100, cv=5,
    verbose=2, random_state=42
)

# –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
for i in range(10):
    search.fit(X_train, y_train)
    monitor.callback(search)

monitor.plot_convergence()</code></pre>
</div>

<div class="block">
<h2>15. Cheatsheet summary</h2>
<ul>
<li>‚úÖ –ù–∞—á–Ω–∏—Ç–µ —Å Random Search –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è</li>
<li>‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Bayesian Optimization –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è</li>
<li>‚úÖ –ü—Ä–∏–º–µ–Ω—è–π—Ç–µ early stopping –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏</li>
<li>‚úÖ –ö–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ –º–µ—Ç–æ–¥—ã (Multi-stage search)</li>
<li>‚úÖ –õ–æ–≥–∏—Ä—É–π—Ç–µ –≤—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã</li>
<li>‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ cross-validation</li>
<li>‚úÖ –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–π—Ç–µ computational budget</li>
<li>‚úÖ –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
<li>‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –≤—ã–±—Ä–∞–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é</li>
<li>‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–π—Ç–µ pipeline –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏</li>
</ul>
</div>

</div>
</body></html>
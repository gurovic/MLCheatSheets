<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>LSTM (Long Short-Term Memory) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ LSTM (Long Short-Term Memory)</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>LSTM</strong> ‚Äî —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è RNN</li>
      <li><strong>–†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É</strong> –∏—Å—á–µ–∑–∞—é—â–µ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞</li>
      <li><strong>–ó–∞–ø–æ–º–∏–Ω–∞–µ—Ç</strong> –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</li>
      <li><strong>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç</strong> —Å–∏—Å—Ç–µ–º—É –≤–µ–Ω—Ç–∏–ª–µ–π (gates)</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è</strong> –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ LSTM</h2>
    <p>–¢—Ä–∏ –≤–µ–Ω—Ç–∏–ª—è —É–ø—Ä–∞–≤–ª—è—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π:</p>
    <table>
      <tr><th>–í–µ–Ω—Ç–∏–ª—å</th><th>–§—É–Ω–∫—Ü–∏—è</th></tr>
      <tr><td><strong>Forget gate</strong></td><td>–ß—Ç–æ –∑–∞–±—ã—Ç—å –∏–∑ –ø–∞–º—è—Ç–∏</td></tr>
      <tr><td><strong>Input gate</strong></td><td>–ß—Ç–æ –∑–∞–ø–æ–º–Ω–∏—Ç—å</td></tr>
      <tr><td><strong>Output gate</strong></td><td>–ß—Ç–æ –≤—ã–≤–µ—Å—Ç–∏</td></tr>
    </table>
    <p><strong>Cell state</strong> (C) ‚Äî –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å</p>
    <p><strong>Hidden state</strong> (h) ‚Äî –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å</p>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ë–∞–∑–æ–≤—ã–π LSTM (PyTorch)</h2>
    <pre><code>import torch
import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM —Å–ª–æ–π
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True
        )
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è hidden state –∏ cell state
        h0 = torch.zeros(self.num_layers, x.size(0), 
                        self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), 
                        self.hidden_size).to(x.device)
        
        # Forward pass
        out, (hn, cn) = self.lstm(x, (h0, c0))
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥
        out = self.fc(out[:, -1, :])
        return out

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = LSTMModel(
    input_size=10,    # —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    hidden_size=64,   # —Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è
    num_layers=2,     # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ LSTM —Å–ª–æ—ë–≤
    output_size=1     # —Ä–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–∞
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ë–∞–∑–æ–≤—ã–π LSTM (TensorFlow/Keras)</h2>
    <pre><code>import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

model = Sequential([
    # LSTM —Å–ª–æ–π
    LSTM(
        units=64,              # —á–∏—Å–ª–æ –Ω–µ–π—Ä–æ–Ω–æ–≤
        return_sequences=True, # –≤–µ—Ä–Ω—É—Ç—å –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        input_shape=(timesteps, features)
    ),
    Dropout(0.2),
    
    # –í—Ç–æ—Ä–æ–π LSTM —Å–ª–æ–π
    LSTM(units=32, return_sequences=False),
    Dropout(0.2),
    
    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    Dense(1)
])

model.compile(
    optimizer='adam',
    loss='mse',
    metrics=['mae']
)

model.summary()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã</h2>
    <table>
      <tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</th></tr>
      <tr><td><code>units</code></td><td>–†–∞–∑–º–µ—Ä hidden state</td><td>32-256</td></tr>
      <tr><td><code>num_layers</code></td><td>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤</td><td>1-3</td></tr>
      <tr><td><code>return_sequences</code></td><td>–í–µ—Ä–Ω—É—Ç—å –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å</td><td>True –¥–ª—è stack</td></tr>
      <tr><td><code>dropout</code></td><td>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</td><td>0.2-0.5</td></tr>
      <tr><td><code>batch_first</code></td><td>–ü–æ—Ä—è–¥–æ–∫ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π (PyTorch)</td><td>True</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM</h2>
    <pre><code>import numpy as np

def create_sequences(data, seq_length):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è LSTM"""
    X, y = [], []
    
    for i in range(len(data) - seq_length):
        # –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        X.append(data[i:i+seq_length])
        # –¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        y.append(data[i+seq_length])
    
    return np.array(X), np.array(y)

# –ü—Ä–∏–º–µ—Ä
data = np.sin(np.linspace(0, 100, 1000))
seq_length = 10

X, y = create_sequences(data, seq_length)

print(f"X shape: {X.shape}")  # (990, 10)
print(f"y shape: {y.shape}")  # (990,)

# –î–ª—è –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# X shape –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å: (samples, timesteps, features)
X = X.reshape((X.shape[0], X.shape[1], 1))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –û–±—É—á–µ–Ω–∏–µ LSTM (PyTorch)</h2>
    <pre><code>import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
X_train = torch.FloatTensor(X_train)
y_train = torch.FloatTensor(y_train)

train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# –ú–æ–¥–µ–ª—å, loss, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
model = LSTMModel(input_size=1, hidden_size=64, num_layers=2, output_size=1)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# –û–±—É—á–µ–Ω–∏–µ
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    
    for batch_X, batch_y in train_loader:
        # Forward pass
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y.view(-1, 1))
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –û–±—É—á–µ–Ω–∏–µ LSTM (Keras)</h2>
    <pre><code>from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Callbacks
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

checkpoint = ModelCheckpoint(
    'best_model.h5',
    monitor='val_loss',
    save_best_only=True
)

# –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stop, checkpoint],
    verbose=1
)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title('Loss')

plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Val MAE')
plt.legend()
plt.title('MAE')
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Bidirectional LSTM</h2>
    <p>–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –æ–±–µ —Å—Ç–æ—Ä–æ–Ω—ã</p>
    <pre><code># Keras
from tensorflow.keras.layers import Bidirectional

model = Sequential([
    Bidirectional(LSTM(64, return_sequences=True), 
                  input_shape=(timesteps, features)),
    Dropout(0.2),
    Bidirectional(LSTM(32)),
    Dropout(0.2),
    Dense(1)
])

# PyTorch
lstm = nn.LSTM(input_size, hidden_size, num_layers, 
               batch_first=True, bidirectional=True)

# –ü—Ä–∏ bidirectional=True hidden_size —É–¥–≤–∞–∏–≤–∞–µ—Ç—Å—è!</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π LSTM (Stacked)</h2>
    <pre><code># Keras
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(timesteps, features)),
    Dropout(0.3),
    
    LSTM(64, return_sequences=True),  # return_sequences=True!
    Dropout(0.3),
    
    LSTM(32, return_sequences=False),  # –ø–æ—Å–ª–µ–¥–Ω–∏–π - False
    Dropout(0.2),
    
    Dense(1)
])

# –í–∞–∂–Ω–æ: return_sequences=True –¥–ª—è –≤—Å–µ—Ö, –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ LSTM</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Many-to-One vs Many-to-Many</h2>
    <table>
      <tr><th>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</th><th>return_sequences</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>Many-to-One</strong></td><td>False</td><td>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</td></tr>
      <tr><td><strong>Many-to-Many</strong></td><td>True</td><td>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</td></tr>
      <tr><td><strong>One-to-Many</strong></td><td>True</td><td>Image captioning</td></tr>
    </table>
    <pre><code># Many-to-One (–Ω–∞–ø—Ä–∏–º–µ—Ä, sentiment analysis)
model = Sequential([
    LSTM(64, return_sequences=False),
    Dense(1, activation='sigmoid')
])

# Many-to-Many (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä–µ–≤–æ–¥)
model = Sequential([
    LSTM(64, return_sequences=True),
    Dense(vocab_size, activation='softmax')
])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –•–æ—Ä–æ—à–æ</h3>
        <ul>
          <li>–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã</li>
          <li>–¢–µ–∫—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è</li>
          <li>–ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥</li>
          <li>–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏</li>
          <li>–ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ</li>
          <li>–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ü–ª–æ—Ö–æ / –ï—Å—Ç—å –ª—É—á—à–µ</h3>
        <ul>
          <li>–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Transformer)</li>
          <li>–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Transformer)</li>
          <li>–ü—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ GRU –∏–ª–∏ 1D CNN)</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 13. LSTM vs GRU</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>LSTM</th><th>GRU</th></tr>
      <tr><td><strong>–í–µ–Ω—Ç–∏–ª–∏</strong></td><td>3 (forget, input, output)</td><td>2 (update, reset)</td></tr>
      <tr><td><strong>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</strong></td><td>–ë–æ–ª—å—à–µ</td><td>–ú–µ–Ω—å—à–µ (–±—ã—Å—Ç—Ä–µ–µ)</td></tr>
      <tr><td><strong>–ü–∞–º—è—Ç—å</strong></td><td>Cell state + hidden</td><td>–¢–æ–ª—å–∫–æ hidden</td></tr>
      <tr><td><strong>–¢–æ—á–Ω–æ—Å—Ç—å</strong></td><td>–ù–µ–º–Ω–æ–≥–æ –≤—ã—à–µ</td><td>–ù–µ–º–Ω–æ–≥–æ –Ω–∏–∂–µ</td></tr>
      <tr><td><strong>–°–∫–æ—Ä–æ—Å—Ç—å</strong></td><td>–ú–µ–¥–ª–µ–Ω–Ω–µ–µ</td><td>–ë—ã—Å—Ç—Ä–µ–µ</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ</h2>
    <pre><code># Keras
predictions = model.predict(X_test)

# PyTorch
model.eval()
with torch.no_grad():
    X_test_tensor = torch.FloatTensor(X_test)
    predictions = model(X_test_tensor).numpy()

# –î–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
def predict_future(model, last_sequence, n_steps):
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ n —à–∞–≥–æ–≤ –≤–ø–µ—Ä—ë–¥"""
    predictions = []
    current_seq = last_sequence.copy()
    
    for _ in range(n_steps):
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        pred = model.predict(current_seq.reshape(1, seq_length, 1))
        predictions.append(pred[0, 0])
        
        # –û–±–Ω–æ–≤–∏—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        current_seq = np.append(current_seq[1:], pred)
    
    return np.array(predictions)

future = predict_future(model, X_test[-1], n_steps=50)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è LSTM</h2>
    <pre><code># 1. Dropout
LSTM(64, dropout=0.2, recurrent_dropout=0.2)

# 2. L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
from tensorflow.keras.regularizers import l2
LSTM(64, kernel_regularizer=l2(0.01))

# 3. Batch Normalization
from tensorflow.keras.layers import BatchNormalization
model = Sequential([
    LSTM(64, return_sequences=True),
    BatchNormalization(),
    LSTM(32),
    Dense(1)
])

# 4. Gradient clipping (PyTorch)
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 5. Early stopping
EarlyStopping(monitor='val_loss', patience=10)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</h2>
    <ul>
      <li><strong>CuDNN LSTM</strong>: –±—ã—Å—Ç—Ä–µ–µ –Ω–∞ GPU (Keras –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)</li>
      <li><strong>Batch size</strong>: –±–æ–ª—å—à–µ = –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏</li>
      <li><strong>Truncated BPTT</strong>: –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–ª–∏–Ω—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</li>
      <li><strong>GRU –≤–º–µ—Å—Ç–æ LSTM</strong>: –µ—Å–ª–∏ —Å–∫–æ—Ä–æ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞</li>
      <li><strong>Mixed precision</strong>: float16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è</li>
    </ul>
    <pre><code># Mixed precision (TensorFlow)
from tensorflow.keras import mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# PyTorch
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. –¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏</h2>
    <ul>
      <li><strong>–ù–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ</strong> ‚Äî LSTM —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –º–∞—Å—à—Ç–∞–±—É</li>
      <li><strong>–ó–∞–±—ã—Ç—å reshape</strong> ‚Äî –Ω—É–∂–Ω–∞ —Ñ–æ—Ä–º–∞ (batch, timesteps, features)</li>
      <li><strong>–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</strong> ‚Äî —É–º–µ–Ω—å—à–∏—Ç—å –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å attention</li>
      <li><strong>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</strong> ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å dropout –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é</li>
      <li><strong>–ù–µ shuffle –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã</strong> ‚Äî –ø–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω!</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å/—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ</li>
      <li>[ ] –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≤—Ö–æ–¥–∞: (batch, timesteps, features)</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</li>
      <li>[ ] Early stopping –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è</li>
      <li>[ ] –ü–æ–¥–æ–±—Ä–∞—Ç—å batch_size (32-128)</li>
      <li>[ ] –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å 1-3 —Å–ª–æ—è LSTM</li>
      <li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å GRU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è</li>
      <li>[ ] –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: Transformer</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´LSTM ‚Äî —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å –ø–∞–º—è—Ç—å—é, –∫–æ—Ç–æ—Ä–∞—è —É–º–µ–µ—Ç –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ—ë –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π. –ö–∞–∫ —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–Ω–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–≥–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—É—â–µ–π —Ñ—Ä–∞–∑—ã¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

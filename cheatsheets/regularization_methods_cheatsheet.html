<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–ú–µ—Ç–æ–¥—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ –ú–µ—Ç–æ–¥—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</h2>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</li>
      <li><strong>–ö–∞–∫</strong>: –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—Ç—Ä–∞—Ñ–∞ –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏</li>
      <li><strong>–†–µ–∑—É–ª—å—Ç–∞—Ç</strong>: –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –∏ –æ–±–æ–±—â–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏</li>
      <li><strong>Trade-off</strong>: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –ø—Ä–æ—Å—Ç–æ—Ç–æ–π</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. –ó–∞—á–µ–º –Ω—É–∂–Ω–∞</h2>
    <ul>
      <li><strong>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</strong>: –º–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞—è</li>
      <li><strong>–í—ã—Å–æ–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è</strong>: –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è</li>
      <li><strong>–ú–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</strong>: –≤—ã—Å–æ–∫–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å</li>
      <li><strong>–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö</strong>: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤</li>
      <li><strong>–ö–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å</strong>: –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. –¢–∏–ø—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</h2>
    <table>
      <tr><th>–¢–∏–ø</th><th>–ù–∞–∑–≤–∞–Ω–∏–µ</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>L1</strong></td><td>Lasso</td><td>Feature selection</td></tr>
      <tr><td><strong>L2</strong></td><td>Ridge</td><td>–û–±—â–µ–µ —Å–∂–∞—Ç–∏–µ –≤–µ—Å–æ–≤</td></tr>
      <tr><td><strong>L1+L2</strong></td><td>Elastic Net</td><td>–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π</td></tr>
      <tr><td><strong>Dropout</strong></td><td>-</td><td>–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏</td></tr>
      <tr><td><strong>Early Stopping</strong></td><td>-</td><td>–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è</td></tr>
      <tr><td><strong>Data Aug</strong></td><td>-</td><td>–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 4. L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Ridge)</h2>
    <p><strong>–®—Ç—Ä–∞—Ñ</strong>: —Å—É–º–º–∞ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –≤–µ—Å–æ–≤</p>
    <p>Loss = MSE + Œ± * Œ£(w¬≤)</p>
    <ul>
      <li>–£–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Å–∞, –Ω–æ –Ω–µ –æ–±–Ω—É–ª—è–µ—Ç</li>
      <li>–í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è</li>
      <li>–•–æ—Ä–æ—à–æ –ø—Ä–∏ –∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏</li>
      <li>Œ± (alpha) - —Å–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</li>
    </ul>
    <pre><code>from sklearn.linear_model import Ridge

ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
y_pred = ridge.predict(X_test)

# –í–µ—Å–∞ —É–º–µ–Ω—å—à–µ–Ω—ã, –Ω–æ –Ω–µ 0
print(ridge.coef_)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Lasso)</h2>
    <p><strong>–®—Ç—Ä–∞—Ñ</strong>: —Å—É–º–º–∞ –º–æ–¥—É–ª–µ–π –≤–µ—Å–æ–≤</p>
    <p>Loss = MSE + Œ± * Œ£|w|</p>
    <ul>
      <li>–û–±–Ω—É–ª—è–µ—Ç –Ω–µ–≤–∞–∂–Ω—ã–µ –≤–µ—Å–∞</li>
      <li>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π feature selection</li>
      <li>–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏</li>
      <li>–•—É–∂–µ –ø—Ä–∏ –∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏</li>
    </ul>
    <pre><code>from sklearn.linear_model import Lasso

lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# –ú–Ω–æ–≥–∏–µ –≤–µ—Å–∞ = 0
print(f"Non-zero: {np.sum(lasso.coef_ != 0)}")
print(f"Zero: {np.sum(lasso.coef_ == 0)}")

# –í–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
important = np.where(lasso.coef_ != 0)[0]
print(f"Important features: {important}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Elastic Net (L1 + L2)</h2>
    <p><strong>–®—Ç—Ä–∞—Ñ</strong>: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è L1 –∏ L2</p>
    <p>Loss = MSE + Œ±‚ÇÅ*Œ£|w| + Œ±‚ÇÇ*Œ£(w¬≤)</p>
    <ul>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –æ–±–æ–∏—Ö –º–µ—Ç–æ–¥–æ–≤</li>
      <li>l1_ratio: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É L1 –∏ L2</li>
      <li>l1_ratio=1: —á–∏—Å—Ç—ã–π Lasso</li>
      <li>l1_ratio=0: —á–∏—Å—Ç—ã–π Ridge</li>
    </ul>
    <pre><code>from sklearn.linear_model import ElasticNet

elastic = ElasticNet(
    alpha=0.1,
    l1_ratio=0.5,  # 50% L1, 50% L2
    random_state=42
)

elastic.fit(X_train, y_train)
y_pred = elastic.predict(X_test)

# –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É feature selection –∏ stability
print(f"Non-zero coef: {np.sum(elastic.coef_ != 0)}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ L1, L2, Elastic Net</h2>
    <table>
      <tr><th>–ö—Ä–∏—Ç–µ—Ä–∏–π</th><th>L1 (Lasso)</th><th>L2 (Ridge)</th><th>Elastic Net</th></tr>
      <tr><td>Feature selection</td><td>–î–∞</td><td>–ù–µ—Ç</td><td>–î–∞</td></tr>
      <tr><td>–ö–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å</td><td>–ü–ª–æ—Ö–æ</td><td>–•–æ—Ä–æ—à–æ</td><td>–•–æ—Ä–æ—à–æ</td></tr>
      <tr><td>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</td><td>–õ–µ–≥–∫–æ</td><td>–°—Ä–µ–¥–Ω–µ</td><td>–°—Ä–µ–¥–Ω–µ</td></tr>
      <tr><td>–°–∫–æ—Ä–æ—Å—Ç—å</td><td>–°—Ä–µ–¥–Ω—è—è</td><td>–ë—ã—Å—Ç—Ä–æ</td><td>–°—Ä–µ–¥–Ω—è—è</td></tr>
      <tr><td>–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å</td><td>–î–∞</td><td>–ù–µ—Ç</td><td>–î–∞</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ü–æ–¥–±–æ—Ä alpha (Ridge/Lasso)</h2>
    <pre><code>from sklearn.linear_model import RidgeCV, LassoCV

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä alpha
alphas = [0.001, 0.01, 0.1, 1, 10, 100]

# Ridge —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
ridge_cv = RidgeCV(alphas=alphas, cv=5)
ridge_cv.fit(X_train, y_train)
print(f"Best alpha: {ridge_cv.alpha_}")

# Lasso —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
lasso_cv = LassoCV(alphas=alphas, cv=5)
lasso_cv.fit(X_train, y_train)
print(f"Best alpha: {lasso_cv.alpha_}")

# Elastic Net
from sklearn.linear_model import ElasticNetCV
elastic_cv = ElasticNetCV(
    alphas=alphas,
    l1_ratio=[0.1, 0.5, 0.7, 0.9],
    cv=5
)
elastic_cv.fit(X_train, y_train)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</h2>
    <pre><code>import matplotlib.pyplot as plt

alphas = np.logspace(-3, 3, 100)
coefs = []

for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train, y_train)
    coefs.append(ridge.coef_)

plt.figure(figsize=(10, 6))
plt.plot(alphas, coefs)
plt.xscale('log')
plt.xlabel('Alpha (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)')
plt.ylabel('–í–µ—Å–∞ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤')
plt.title('Ridge: –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤–µ—Å–æ–≤ –æ—Ç alpha')
plt.grid(True)
plt.show()

# –ß–µ–º –±–æ–ª—å—à–µ alpha, —Ç–µ–º –º–µ–Ω—å—à–µ –≤–µ—Å–∞</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Dropout (–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏)</h2>
    <p>–°–ª—É—á–∞–π–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏</p>
    <ul>
      <li>rate: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç–∫–ª—é—á–µ–Ω–∏—è (0.2-0.5)</li>
      <li>–¢–æ–ª—å–∫–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏</li>
      <li>–ü—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –≤—Å–µ –Ω–µ–π—Ä–æ–Ω—ã –∞–∫—Ç–∏–≤–Ω—ã</li>
      <li>–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∫–æ-–∞–¥–∞–ø—Ç–∞—Ü–∏—é</li>
    </ul>
    <pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential([
    Dense(128, activation='relu', input_shape=(n_features,)),
    Dropout(0.3),  # –æ—Ç–∫–ª—é—á–∞–µ–º 30% –Ω–µ–π—Ä–æ–Ω–æ–≤
    
    Dense(64, activation='relu'),
    Dropout(0.2),  # –æ—Ç–∫–ª—é—á–∞–µ–º 20%
    
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy')
model.fit(X_train, y_train, epochs=50, validation_split=0.2)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Early Stopping</h2>
    <p>–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–∏</p>
    <pre><code>from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,  # –∂–¥–µ–º 10 —ç–ø–æ—Ö
    restore_best_weights=True,
    verbose=1
)

history = model.fit(
    X_train, y_train,
    epochs=100,
    validation_split=0.2,
    callbacks=[early_stop]
)

# –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∫–æ–≥–¥–∞ val_loss –ø–µ—Ä–µ—Å—Ç–∞–Ω–µ—Ç —É–ª—É—á—à–∞—Ç—å—Å—è
print(f"Stopped at epoch: {early_stop.stopped_epoch}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Batch Normalization</h2>
    <p>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–Ω—É—Ç—Ä–∏ —Å–µ—Ç–∏</p>
    <ul>
      <li>–°—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ</li>
      <li>–ü–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–π learning rate</li>
      <li>–£–º–µ–Ω—å—à–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</li>
      <li>–ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è</li>
    </ul>
    <pre><code>from tensorflow.keras.layers import BatchNormalization

model = Sequential([
    Dense(128, activation='relu'),
    BatchNormalization(),
    
    Dense(64, activation='relu'),
    BatchNormalization(),
    
    Dense(1, activation='sigmoid')
])

# –ú–æ–∂–Ω–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Å Dropout
model = Sequential([
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.2),
    
    Dense(1)
])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. Weight Decay (PyTorch)</h2>
    <pre><code>import torch.optim as optim

# Weight decay = L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
optimizer = optim.Adam(
    model.parameters(),
    lr=0.001,
    weight_decay=0.01  # L2 penalty
)

# –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—é L2 –∫ loss
# loss = criterion(y_pred, y_true) + 0.01 * sum(w¬≤)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. Data Augmentation</h2>
    <p>–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö</p>
    <pre><code>from tensorflow.keras.preprocessing.image import ImageDataGenerator

# –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.2
)

# –û–±—É—á–µ–Ω–∏–µ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π
model.fit(
    datagen.flow(X_train, y_train, batch_size=32),
    epochs=50,
    validation_data=(X_val, y_val)
)

# –ë–æ–ª—å—à–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö = –º–µ–Ω—å—à–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. Max Norm Constraint</h2>
    <p>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –Ω–æ—Ä–º—ã –≤–µ—Å–æ–≤</p>
    <pre><code>from tensorflow.keras.constraints import max_norm

model = Sequential([
    Dense(
        128,
        activation='relu',
        kernel_constraint=max_norm(3.0)  # ||w|| <= 3
    ),
    Dropout(0.3),
    
    Dense(
        64,
        activation='relu',
        kernel_constraint=max_norm(3.0)
    ),
    
    Dense(1, activation='sigmoid')
])

# –í–µ—Å–∞ –æ–±—Ä–µ–∑–∞—é—Ç—Å—è –µ—Å–ª–∏ –Ω–æ—Ä–º–∞ > 3.0</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. Gradient Clipping</h2>
    <p>–û–±—Ä–µ–∑–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</p>
    <pre><code># TensorFlow/Keras
from tensorflow.keras.optimizers import Adam

optimizer = Adam(clipnorm=1.0)  # clip by norm
model.compile(optimizer=optimizer, loss='mse')

# –∏–ª–∏
optimizer = Adam(clipvalue=0.5)  # clip by value

# PyTorch
import torch.nn.utils as utils

# –í —Ü–∏–∫–ª–µ –æ–±—É—á–µ–Ω–∏—è
optimizer.zero_grad()
loss.backward()
utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()

# –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –≤–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. Ensemble Methods</h2>
    <p>–ö–æ–º–±–∏–Ω–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –∫–∞–∫ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</p>
    <pre><code>from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

# Voting
ensemble = VotingClassifier([
    ('lr', LogisticRegression()),
    ('dt', DecisionTreeClassifier()),
    ('svm', SVC())
], voting='soft')

ensemble.fit(X_train, y_train)

# –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —É–º–µ–Ω—å—à–∞–µ—Ç –¥–∏—Å–ø–µ—Ä—Å–∏—é
# Random Forest, Gradient Boosting —Ç–æ–∂–µ</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 18. –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</h2>
    <pre><code>if —Ç–∏–ø_–º–æ–¥–µ–ª–∏ == "–ª–∏–Ω–µ–π–Ω–∞—è":
    if –Ω—É–∂–µ–Ω_feature_selection:
        –∏—Å–ø–æ–ª—å–∑—É–π Lasso
    elif –ø—Ä–∏–∑–Ω–∞–∫–∏_–∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç:
        –∏—Å–ø–æ–ª—å–∑—É–π Ridge –∏–ª–∏ Elastic Net
    else:
        –Ω–∞—á–Ω–∏ —Å Ridge
        
elif —Ç–∏–ø_–º–æ–¥–µ–ª–∏ == "–Ω–µ–π—Ä–æ—Å–µ—Ç—å":
    –∏—Å–ø–æ–ª—å–∑—É–π Dropout + BatchNorm + EarlyStopping
    
elif —Ç–∏–ø_–º–æ–¥–µ–ª–∏ == "–¥–µ—Ä–µ–≤–æ":
    –∏—Å–ø–æ–ª—å–∑—É–π max_depth, min_samples_split
    
else:
    –ø–æ–ø—Ä–æ–±—É–π –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 19. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–µ—Ä–µ–≤—å–µ–≤</h2>
    <pre><code>from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(
    max_depth=5,           # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã
    min_samples_split=20,  # –º–∏–Ω. –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è split
    min_samples_leaf=10,   # –º–∏–Ω. –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –ª–∏—Å—Ç–µ
    max_features='sqrt',   # —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    ccp_alpha=0.01        # cost-complexity pruning
)

tree.fit(X_train, y_train)

# Random Forest
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=20,
    max_features='sqrt'
)

rf.fit(X_train, y_train)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 20. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è XGBoost</h2>
    <pre><code>import xgboost as xgb

params = {
    'max_depth': 5,
    'min_child_weight': 3,
    'gamma': 0.1,           # min loss reduction
    'lambda': 1.0,          # L2 regularization
    'alpha': 0.1,           # L1 regularization
    'subsample': 0.8,       # row sampling
    'colsample_bytree': 0.8 # column sampling
}

model = xgb.XGBClassifier(**params)
model.fit(X_train, y_train)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 21. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏</h2>
    <ul>
      <li><strong>–ù–∞—á–Ω–∏—Ç–µ —Å –ø—Ä–æ—Å—Ç–æ–≥–æ</strong>: –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –±–µ–∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</li>
      <li><strong>–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è</strong>: –ø–æ–¥–±–∏—Ä–∞–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ</li>
      <li><strong>–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ</strong>: –≤–∞–∂–Ω–æ –¥–ª—è L1/L2</li>
      <li><strong>–ö–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ</strong>: Dropout + BatchNorm + EarlyStopping</li>
      <li><strong>–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥</strong>: —Å–ª–µ–¥–∏—Ç–µ –∑–∞ train/val –º–µ—Ç—Ä–∏–∫–∞–º–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 22. –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏</h2>
    <ul>
      <li>‚ùå –°–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (–Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ)</li>
      <li>‚ùå –ù–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ L1/L2</li>
      <li>‚ùå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Dropout –ø—Ä–∏ inference</li>
      <li>‚ùå –ù–µ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏</li>
      <li>‚ùå –ü–æ–¥–±–∏—Ä–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ test set</li>
      <li>‚ùå –ó–∞–±—ã—Ç—å –ø—Ä–æ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –≤ –¥–µ—Ä–µ–≤—å—è—Ö</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 23. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è</h2>
    <pre><code># –°—Ä–∞–≤–Ω–µ–Ω–∏–µ train –∏ val
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    model, X, y,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5
)

plt.plot(train_sizes, train_scores.mean(axis=1), label='Train')
plt.plot(train_sizes, val_scores.mean(axis=1), label='Val')
plt.legend()
plt.xlabel('Training size')
plt.ylabel('Score')
plt.title('Learning Curve')
plt.show()

# –ë–æ–ª—å—à–æ–π gap = –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ ‚Üí –Ω—É–∂–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 24. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥</h2>
    <pre><code># –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤
model = Sequential([
    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    
    Dense(128, activation='relu', kernel_regularizer='l2'),
    BatchNormalization(),
    Dropout(0.2),
    
    Dense(64, activation='relu', kernel_regularizer='l2'),
    Dropout(0.1),
    
    Dense(1, activation='sigmoid')
])

early_stop = EarlyStopping(patience=10)

model.fit(
    X_train, y_train,
    epochs=100,
    validation_split=0.2,
    callbacks=[early_stop]
)

# + Data augmentation –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ</code></pre>
  </div>

</div>

</div>
</body>
</html>

<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–¶–∏—Ñ—Ä–æ–≤–∞—è –≥—É–º–∞–Ω–∏—Ç–∞—Ä–∏—Å—Ç–∏–∫–∞ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üìö –¶–∏—Ñ—Ä–æ–≤–∞—è –≥—É–º–∞–Ω–∏—Ç–∞—Ä–∏—Å—Ç–∏–∫–∞</h1>
  <div class="subtitle">üìÖ 5 —è–Ω–≤–∞—Ä—è 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏</h2>

    <ul>
      <li><strong>–¶–∏—Ñ—Ä–æ–≤–∞—è –≥—É–º–∞–Ω–∏—Ç–∞—Ä–∏—Å—Ç–∏–∫–∞</strong>: –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ computational –º–µ—Ç–æ–¥–æ–≤ –∫ –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã–º –Ω–∞—É–∫–∞–º</li>
      <li><strong>–¢–µ–∫—Å—Ç–æ–≤–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞</strong>: –∞–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã</li>
      <li><strong>–ö—É–ª—å—Ç—É—Ä–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞</strong>: –∏–∑—É—á–µ–Ω–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —á–µ—Ä–µ–∑ –¥–∞–Ω–Ω—ã–µ</li>
      <li><strong>–¶–∏—Ñ—Ä–æ–≤—ã–µ –∞—Ä—Ö–∏–≤—ã</strong>: –æ—Ü–∏—Ñ—Ä–æ–≤–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤</li>
    </ul>
    <blockquote>
      ML –ø–æ–º–æ–≥–∞–µ—Ç –≥—É–º–∞–Ω–∏—Ç–∞—Ä–∏—è–º –Ω–∞—Ö–æ–¥–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
    </blockquote>

  <div class="block">
    <h2>üî∑ 2. –¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑</h2>

    <table>
      <tr><th>–ó–∞–¥–∞—á–∞</th><th>–ú–µ—Ç–æ–¥—ã</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td>Topic Modeling</td><td>LDA, NMF</td><td>–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ—Ä–ø—É—Å–æ–≤</td></tr>
      <tr><td>Stylometry</td><td>PCA, SVM</td><td>–ê—Ç—Ä–∏–±—É—Ü–∏—è –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞</td></tr>
      <tr><td>Sentiment Analysis</td><td>LSTM, BERT</td><td>–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π –≤ —Ç–µ–∫—Å—Ç–∞—Ö</td></tr>
      <tr><td>NER</td><td>CRF, Transformers</td><td>–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π</td></tr>
      <tr><td>Text Classification</td><td>fastText, CNN</td><td>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. Topic Modeling –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤</h2>

    <pre><code>from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

# –ö–æ—Ä–ø—É—Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
documents = load_historical_corpus()

# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
vectorizer = CountVectorizer(max_df=0.95, min_df=2)
doc_term_matrix = vectorizer.fit_transform(documents)

# LDA –¥–ª—è —Ç–æ–ø–∏–∫–æ–≤
lda = LatentDirichletAllocation(n_components=10, random_state=42)
lda.fit(doc_term_matrix)

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ø–∏–∫–æ–≤
def print_top_words(model, feature_names, n_top_words=10):
    for topic_idx, topic in enumerate(model.components_):
        message = f"Topic {topic_idx}: "
        message += " ".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])
        print(message)

print_top_words(lda, vectorizer.get_feature_names_out())</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Stylometry - –∞–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª—è</h2>

    <ul>
      <li><strong>–ê—Ç—Ä–∏–±—É—Ü–∏—è –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞</strong>: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞ –∞–Ω–æ–Ω–∏–º–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤</li>
      <li><strong>–ü—Ä–∏–∑–Ω–∞–∫–∏</strong>:
        <ul>
          <li>–î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ —Å–ª–æ–≤</li>
          <li>–ß–∞—Å—Ç–æ—Ç–∞ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤</li>
          <li>–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏</li>
          <li>–õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ</li>
        </ul>
      </li>
    </ul>
    <pre><code># Stylometric features
def extract_style_features(text):
    words = word_tokenize(text)
    sentences = sent_tokenize(text)
    
    features = {
        'avg_word_length': np.mean([len(w) for w in words]),
        'avg_sentence_length': np.mean([len(sent.split()) for sent in sentences]),
        'lexical_diversity': len(set(words)) / len(words),
        'function_word_freq': sum([1 for w in words if w in function_words]) / len(words)
    }
    return features</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–µ—Ç–µ–π</h2>

    <pre><code>import networkx as nx
from community import community_louvain

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π —Å–µ—Ç–∏ –∏–∑ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
G = nx.Graph()

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É–∑–ª–æ–≤ (–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∏)
for person in historical_persons:
    G.add_node(person)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π (—É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ –æ–¥–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ)
for doc in documents:
    persons_in_doc = extract_persons(doc)
    for p1, p2 in combinations(persons_in_doc, 2):
        if G.has_edge(p1, p2):
            G[p1][p2]['weight'] += 1
        else:
            G.add_edge(p1, p2, weight=1)

# Community detection
communities = community_louvain.best_partition(G)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import matplotlib.pyplot as plt
pos = nx.spring_layout(G)
nx.draw(G, pos, node_color=list(communities.values()), with_labels=True)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∏—Å–∫—É—Å—Å—Ç–≤–æ–≤–µ–¥–µ–Ω–∏–∏</h2>

    <ul>
      <li><strong>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—Ç–∏–ª–µ–π</strong>: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —à–∫–æ–ª –∂–∏–≤–æ–ø–∏—Å–∏</li>
      <li><strong>–î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤</strong>: –∞–Ω–∞–ª–∏–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏</li>
      <li><strong>–ö–æ–ª–æ—Ä–∏–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑</strong>: –∏–∑—É—á–µ–Ω–∏–µ –ø–∞–ª–∏—Ç—Ä—ã</li>
      <li><strong>Similarity search</strong>: –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π</li>
    </ul>
    <pre><code>from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing import image

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
model = VGG16(weights='imagenet', include_top=False)

def extract_features(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    features = model.predict(x)
    return features.flatten()

# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∂–∏–≤–æ–ø–∏—Å–∏
paintings = load_paintings()
features = [extract_features(p) for p in paintings]
kmeans = KMeans(n_clusters=5)
clusters = kmeans.fit_predict(features)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤</h2>

    <pre><code># –ê–Ω–∞–ª–∏–∑ —ç–≤–æ–ª—é—Ü–∏–∏ —è–∑—ã–∫–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–∏
def temporal_word_frequency(corpus_by_year):
    """
    –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤ –ø–æ –≥–æ–¥–∞–º
    """
    word_freq_by_year = {}
    
    for year, documents in corpus_by_year.items():
        all_words = []
        for doc in documents:
            all_words.extend(word_tokenize(doc.lower()))
        
        word_freq = Counter(all_words)
        word_freq_by_year[year] = word_freq
    
    return word_freq_by_year

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–¥–æ–≤
def plot_word_trend(word, word_freq_by_year):
    years = sorted(word_freq_by_year.keys())
    freqs = [word_freq_by_year[y].get(word, 0) for y in years]
    plt.plot(years, freqs)
    plt.xlabel('–ì–æ–¥')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.title(f'–¢—Ä–µ–Ω–¥ —Å–ª–æ–≤–∞ "{word}"')
    plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Named Entity Recognition –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏</h2>

    <pre><code>import spacy

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
nlp = spacy.load('ru_core_news_lg')

def extract_historical_entities(text):
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
    """
    doc = nlp(text)
    
    entities = {
        'persons': [],
        'locations': [],
        'organizations': [],
        'dates': []
    }
    
    for ent in doc.ents:
        if ent.label_ == 'PER':
            entities['persons'].append(ent.text)
        elif ent.label_ == 'LOC' or ent.label_ == 'GPE':
            entities['locations'].append(ent.text)
        elif ent.label_ == 'ORG':
            entities['organizations'].append(ent.text)
        elif ent.label_ == 'DATE':
            entities['dates'].append(ent.text)
    
    return entities

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫ –∫–æ—Ä–ø—É—Å—É
historical_text = "–í 1812 –≥–æ–¥—É –ù–∞–ø–æ–ª–µ–æ–Ω –≤—Ç–æ—Ä–≥—Å—è –≤ –†–æ—Å—Å–∏—é."
entities = extract_historical_entities(historical_text)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã</h2>

    <table>
      <tr><th>–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞</th><th>–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><code>spaCy</code></td><td>NLP</td><td>–¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑</td></tr>
      <tr><td><code>NLTK</code></td><td>NLP</td><td>–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, —Å—Ç–µ–º–º–∏–Ω–≥</td></tr>
      <tr><td><code>Gensim</code></td><td>Topic modeling</td><td>LDA, Word2Vec</td></tr>
      <tr><td><code>NetworkX</code></td><td>–ì—Ä–∞—Ñ—ã</td><td>–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏</td></tr>
      <tr><td><code>PyTesseract</code></td><td>OCR</td><td>–û—Ü–∏—Ñ—Ä–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤</td></tr>
      <tr><td><code>Pillow</code></td><td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è</td><td>–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ä—Ç–∏–Ω</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 10. OCR –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤</h2>

    <pre><code>import pytesseract
from PIL import Image

# OCR —Å—Ç–∞—Ä—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
def ocr_historical_document(image_path, lang='rus'):
    """
    –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å–∫–∞–Ω–æ–≤
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    img = Image.open(image_path)
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    img = img.convert('L')  # Grayscale
    img = img.point(lambda x: 0 if x < 140 else 255, '1')  # Binarization
    
    # OCR
    text = pytesseract.image_to_string(img, lang=lang)
    
    return text

# –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ä—Ö–∏–≤–∞
archive_texts = []
for img_file in os.listdir('historical_archive/'):
    text = ocr_historical_document(img_file)
    archive_texts.append(text)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ì–µ–æ–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑</h2>

    <ul>
      <li><strong>–ì–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ</strong>: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã</li>
      <li><strong>–ö–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞–Ω–∏–µ</strong>: –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π</li>
      <li><strong>–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è</strong>: —Ä–µ–≥–∏–æ–Ω—ã –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏</li>
    </ul>
    <pre><code>import folium
from geopy.geocoders import Nominatim

# –ì–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –º–µ—Å—Ç
geolocator = Nominatim(user_agent="digital_humanities")

def geocode_location(place_name):
    try:
        location = geolocator.geocode(place_name)
        return location.latitude, location.longitude
    except:
        return None, None

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –∫–∞—Ä—Ç–µ
m = folium.Map(location=[55.7558, 37.6173], zoom_start=10)

for event in historical_events:
    lat, lon = geocode_location(event['place'])
    if lat and lon:
        folium.Marker([lat, lon], popup=event['description']).add_to(m)

m.save('historical_map.html')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>

    <div class="good-vs-bad">
      <div class="good">
        <h3>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h3>
        <ul>
          <li>–ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤</li>
          <li>–ê—Ç—Ä–∏–±—É—Ü–∏—è –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞</li>
          <li>–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —ç–≤–æ–ª—é—Ü–∏–∏ —è–∑—ã–∫–∞</li>
          <li>–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –≤ –∏—Å—Ç–æ—Ä–∏–∏</li>
          <li>–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π –∏—Å–∫—É—Å—Å—Ç–≤–∞</li>
          <li>–û—Ü–∏—Ñ—Ä–æ–≤–∫–∞ –∞—Ä—Ö–∏–≤–æ–≤</li>
        </ul>
      </div>
      <div class="bad">
        <h3>–í—ã–∑–æ–≤—ã</h3>
        <ul>
          <li>–ö–∞—á–µ—Å—Ç–≤–æ OCR —Å—Ç–∞—Ä—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤</li>
          <li>–ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
          <li>–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —è–∑—ã–∫–∞</li>
          <li>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤</li>
        </ul>
      </div>
    </div>
  </div>



</div>

</div>
</body>
</html>

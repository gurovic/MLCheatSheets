<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Loss Functions Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 600;
    }

    .good p, .bad p {
      margin: 0;
      font-size: 0.88em;
    }

    em {
      color: #d32f2f;
      font-style: normal;
      font-weight: 600;
    }

    strong {
      color: #1a5fb4;
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üìâ Loss Functions</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ß—Ç–æ —Ç–∞–∫–æ–µ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å?</h2>
    <ul>
      <li><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: –º–µ—Ç—Ä–∏–∫–∞ –æ—à–∏–±–∫–∏ –º–æ–¥–µ–ª–∏</li>
      <li><strong>–¶–µ–ª—å</strong>: –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è</li>
      <li><strong>–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫</strong>: –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å</li>
      <li><strong>–í—ã–±–æ—Ä</strong>: –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∑–∞–¥–∞—á–∏ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)</li>
      <li><strong>–í–∞–∂–Ω–æ—Å—Ç—å</strong>: –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è —É—Å–ø–µ—Ö–∞!</li>
    </ul>

    </div>
<div class="block">
    <h2>üî∑ 2. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏</h2>
    <table>
      <tr><th>–§—É–Ω–∫—Ü–∏—è</th><th>–§–æ—Ä–º—É–ª–∞</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr>
      <tr><td><strong>MSE</strong></td><td>mean((y-≈∑)¬≤)</td><td>–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é</td></tr>
      <tr><td><strong>MAE</strong></td><td>mean(|y-≈∑|)</td><td>–£—Å—Ç–æ–π—á–∏–≤–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º</td></tr>
      <tr><td><strong>Huber</strong></td><td>MSE + MAE</td><td>–ö–æ–º–±–∏–Ω–∞—Ü–∏—è –æ–±–µ–∏—Ö</td></tr>
      <tr><td><strong>RMSE</strong></td><td>‚àöMSE</td><td>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –µ–¥–∏–Ω–∏—Ü–∞—Ö</td></tr>
      <tr><td><strong>MSLE</strong></td><td>mean((log(y+1)-log(≈∑+1))¬≤)</td><td>–î–ª—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. MSE (Mean Squared Error)</h2>
    <pre><code>import numpy as np
import torch
import torch.nn as nn

# NumPy
def mse_numpy(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# PyTorch
criterion = nn.MSELoss()
loss = criterion(y_pred, y_true)

# TensorFlow/Keras
from tensorflow.keras.losses import MeanSquaredError
criterion = MeanSquaredError()
loss = criterion(y_true, y_pred)

# –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
# - –°–∏–ª—å–Ω–æ —à—Ç—Ä–∞—Ñ—É–µ—Ç –±–æ–ª—å—à–∏–µ –æ—à–∏–±–∫–∏
# - –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞ –≤–µ–∑–¥–µ
# - –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º
# - –ì—Ä–∞–¥–∏–µ–Ω—Ç: 2(≈∑ - y)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. MAE (Mean Absolute Error)</h2>
    <pre><code># NumPy
def mae_numpy(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

# PyTorch
criterion = nn.L1Loss()
loss = criterion(y_pred, y_true)

# TensorFlow/Keras
from tensorflow.keras.losses import MeanAbsoluteError
criterion = MeanAbsoluteError()

# –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
# - –£—Å—Ç–æ–π—á–∏–≤–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º
# - –í—Å–µ –æ—à–∏–±–∫–∏ —à—Ç—Ä–∞—Ñ—É—é—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ
# - –ù–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞ –≤ 0
# - –ì—Ä–∞–¥–∏–µ–Ω—Ç: sign(≈∑ - y)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Huber Loss</h2>
    <pre><code>import torch.nn as nn

# PyTorch
criterion = nn.HuberLoss(delta=1.0)
loss = criterion(y_pred, y_true)

# TensorFlow/Keras
from tensorflow.keras.losses import Huber
criterion = Huber(delta=1.0)

# NumPy (—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    abs_error = np.abs(error)
    
    quadratic = np.minimum(abs_error, delta)
    linear = abs_error - quadratic
    
    return np.mean(0.5 * quadratic**2 + delta * linear)

# –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
# - MSE –¥–ª—è –º–∞–ª—ã—Ö –æ—à–∏–±–æ–∫ (|e| < Œ¥)
# - MAE –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ—à–∏–±–æ–∫ (|e| ‚â• Œ¥)
# - –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É MSE –∏ MAE</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</h2>
    <table>
      <tr><th>–§—É–Ω–∫—Ü–∏—è</th><th>–í—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr>
      <tr><td><strong>BCE</strong></td><td>Sigmoid (0-1)</td><td>–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é</td></tr>
      <tr><td><strong>BCE with logits</strong></td><td>Logits (–ª—é–±—ã–µ)</td><td>–ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ</td></tr>
      <tr><td><strong>Hinge Loss</strong></td><td>-1 –∏–ª–∏ +1</td><td>SVM-–ø–æ–¥–æ–±–Ω—ã–µ –º–æ–¥–µ–ª–∏</td></tr>
      <tr><td><strong>Focal Loss</strong></td><td>Sigmoid (0-1)</td><td>–ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. Binary Cross-Entropy (BCE)</h2>
    <pre><code>import torch
import torch.nn as nn

# PyTorch (—Å sigmoid –≤–Ω—É—Ç—Ä–∏ - –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ!)
criterion = nn.BCEWithLogitsLoss()
loss = criterion(logits, y_true)

# PyTorch (–µ—Å–ª–∏ —É–∂–µ sigmoid –ø—Ä–∏–º–µ–Ω–µ–Ω)
criterion = nn.BCELoss()
loss = criterion(y_pred, y_true)

# TensorFlow/Keras
from tensorflow.keras.losses import BinaryCrossentropy

# –° logits (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
criterion = BinaryCrossentropy(from_logits=True)

# –ü–æ—Å–ª–µ sigmoid
criterion = BinaryCrossentropy(from_logits=False)

# NumPy
def bce_numpy(y_true, y_pred, epsilon=1e-7):
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_pred) + 
                    (1 - y_true) * np.log(1 - y_pred))

# –§–æ—Ä–º—É–ª–∞: -[y¬∑log(≈∑) + (1-y)¬∑log(1-≈∑)]</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</h2>
    <table>
      <tr><th>–§—É–Ω–∫—Ü–∏—è</th><th>–§–æ—Ä–º–∞—Ç y</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr>
      <tr><td><strong>Categorical CE</strong></td><td>One-hot</td><td>One-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ</td></tr>
      <tr><td><strong>Sparse CE</strong></td><td>–¶–µ–ª—ã–µ —á–∏—Å–ª–∞</td><td>–ò–Ω–¥–µ–∫—Å—ã –∫–ª–∞—Å—Å–æ–≤</td></tr>
      <tr><td><strong>KL Divergence</strong></td><td>–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è</td><td>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 9. Categorical Cross-Entropy</h2>
    <pre><code># PyTorch
import torch.nn as nn

# –û–∂–∏–¥–∞–µ—Ç logits (–±–µ–∑ softmax!)
criterion = nn.CrossEntropyLoss()
loss = criterion(logits, y_true)

# TensorFlow/Keras
from tensorflow.keras.losses import CategoricalCrossentropy

# –° one-hot labels
criterion = CategoricalCrossentropy(from_logits=True)
loss = criterion(y_true_onehot, logits)

# NumPy (–ø–æ—Å–ª–µ softmax)
def categorical_crossentropy(y_true, y_pred, epsilon=1e-7):
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))

# –§–æ—Ä–º—É–ª–∞: -Œ£ y·µ¢¬∑log(≈∑·µ¢)
# –≥–¥–µ i - –∏–Ω–¥–µ–∫—Å –∫–ª–∞—Å—Å–∞</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Sparse Categorical Cross-Entropy</h2>
    <pre><code># TensorFlow/Keras (—É–¥–æ–±–Ω–µ–µ –¥–ª—è —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫)
from tensorflow.keras.losses import SparseCategoricalCrossentropy

criterion = SparseCategoricalCrossentropy(from_logits=True)

# y_true = [0, 1, 2] (–∏–Ω–¥–µ–∫—Å—ã –∫–ª–∞—Å—Å–æ–≤)
# y_pred = logits —Ä–∞–∑–º–µ—Ä–æ–º (batch, num_classes)

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# PyTorch (CrossEntropyLoss —É–∂–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∏–Ω–¥–µ–∫—Å—ã)
criterion = nn.CrossEntropyLoss()
# y_true –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å LongTensor –∏–Ω–¥–µ–∫—Å–æ–≤
loss = criterion(logits, y_true_indices)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Focal Loss</h2>
    <pre><code># –î–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
import torch
import torch.nn as nn
import torch.nn.functional as F

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        BCE_loss = F.binary_cross_entropy_with_logits(
            inputs, targets, reduction='none'
        )
        pt = torch.exp(-BCE_loss)
        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss
        return torch.mean(F_loss)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
criterion = FocalLoss(alpha=0.25, gamma=2)
loss = criterion(logits, targets)

# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
# - –§–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
# - –£–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Å –ª–µ–≥–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
# - –û—Ç–ª–∏—á–Ω–æ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Hinge Loss</h2>
    <pre><code># –î–ª—è SVM –∏ margin-based –º–æ–¥–µ–ª–µ–π
import torch.nn as nn

# PyTorch (–±–∏–Ω–∞—Ä–Ω–∞—è)
criterion = nn.HingeEmbeddingLoss()

# –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è hinge loss
criterion = nn.MultiMarginLoss()

# NumPy —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
def hinge_loss(y_true, y_pred):
    # y_true –≤ {-1, +1}
    return np.mean(np.maximum(0, 1 - y_true * y_pred))

# –§–æ—Ä–º—É–ª–∞: max(0, 1 - y¬∑≈∑)
# –≥–¥–µ y ‚àà {-1, +1}

# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
# - –°–æ–∑–¥–∞–µ—Ç margin –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏
# - –ù–µ —Ç—Ä–µ–±—É–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö –≤—ã—Ö–æ–¥–æ–≤
# - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ SVM</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. KL Divergence</h2>
    <pre><code>import torch.nn as nn

# PyTorch
criterion = nn.KLDivLoss(reduction='batchmean')
# input –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å log-probabilities
loss = criterion(log_probs, target_probs)

# TensorFlow
from tensorflow.keras.losses import KLDivergence
criterion = KLDivergence()

# NumPy
def kl_divergence(p, q, epsilon=1e-7):
    p = np.clip(p, epsilon, 1)
    q = np.clip(q, epsilon, 1)
    return np.sum(p * np.log(p / q))

# –§–æ—Ä–º—É–ª–∞: KL(P||Q) = Œ£ P(x)¬∑log(P(x)/Q(x))

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è:
# - Knowledge distillation
# - –í–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã (VAE)
# - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. Contrastive Loss</h2>
    <pre><code># –î–ª—è –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
import torch
import torch.nn as nn

class ContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super().__init__()
        self.margin = margin
    
    def forward(self, output1, output2, label):
        # label: 1 –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö, 0 –¥–ª—è —Ä–∞–∑–Ω—ã—Ö
        euclidean_distance = F.pairwise_distance(
            output1, output2
        )
        
        loss_contrastive = torch.mean(
            label * torch.pow(euclidean_distance, 2) +
            (1 - label) * torch.pow(
                torch.clamp(self.margin - euclidean_distance, 
                           min=0.0), 2
            )
        )
        return loss_contrastive

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è:
# - Siamese networks
# - Face verification
# - Similarity learning</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. Triplet Loss</h2>
    <pre><code># –î–ª—è –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
import torch
import torch.nn as nn

class TripletLoss(nn.Module):
    def __init__(self, margin=1.0):
        super().__init__()
        self.margin = margin
    
    def forward(self, anchor, positive, negative):
        distance_positive = (anchor - positive).pow(2).sum(1)
        distance_negative = (anchor - negative).pow(2).sum(1)
        
        losses = F.relu(
            distance_positive - distance_negative + self.margin
        )
        return losses.mean()

# –§–æ—Ä–º—É–ª–∞: max(0, ||a-p||¬≤ - ||a-n||¬≤ + margin)
# a: anchor, p: positive, n: negative

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è:
# - Face recognition
# - Image retrieval
# - Ranking tasks</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. Dice Loss</h2>
    <pre><code># –î–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
import torch
import torch.nn as nn

class DiceLoss(nn.Module):
    def __init__(self, smooth=1.0):
        super().__init__()
        self.smooth = smooth
    
    def forward(self, pred, target):
        pred = torch.sigmoid(pred)
        
        # Flatten
        pred = pred.view(-1)
        target = target.view(-1)
        
        intersection = (pred * target).sum()
        dice = (2. * intersection + self.smooth) / (
            pred.sum() + target.sum() + self.smooth
        )
        
        return 1 - dice

# –§–æ—Ä–º—É–ª–∞: 1 - (2¬∑|X‚à©Y|)/(|X|+|Y|)

# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
# - –û—Ç–ª–∏—á–Ω–æ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–π
# - –£—á–∏—Ç—ã–≤–∞–µ—Ç overlap –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –∏ GT
# - –ü–æ–ø—É–ª—è—Ä–Ω–∞ –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. Cosine Embedding Loss</h2>
    <pre><code>import torch.nn as nn

# PyTorch
criterion = nn.CosineEmbeddingLoss(margin=0.5)

# input1, input2: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
# y: +1 –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö, -1 –¥–ª—è —Ä–∞–∑–Ω—ã—Ö
loss = criterion(input1, input2, y)

# –§–æ—Ä–º—É–ª–∞:
# loss = 1 - cos(x‚ÇÅ, x‚ÇÇ)        –µ—Å–ª–∏ y = 1
# loss = max(0, cos(x‚ÇÅ, x‚ÇÇ) - margin)  –µ—Å–ª–∏ y = -1

# –≥–¥–µ cos(x‚ÇÅ, x‚ÇÇ) = (x‚ÇÅ¬∑x‚ÇÇ)/(||x‚ÇÅ||¬∑||x‚ÇÇ||)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è:
# - Similarity learning
# - Text embeddings
# - Recommendation systems</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 18. –í—ã–±–æ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å</h2>
    <table>
      <tr><th>–ó–∞–¥–∞—á–∞</th><th>–§—É–Ω–∫—Ü–∏—è</th><th>–ü—Ä–∏—á–∏–Ω–∞</th></tr>
      <tr><td><strong>–†–µ–≥—Ä–µ—Å—Å–∏—è</strong></td><td>MSE</td><td>–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –≤—ã–±–æ—Ä</td></tr>
      <tr><td><strong>–†–µ–≥—Ä–µ—Å—Å–∏—è —Å –≤—ã–±—Ä–æ—Å–∞–º–∏</strong></td><td>MAE, Huber</td><td>–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å</td></tr>
      <tr><td><strong>–ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</strong></td><td>BCE with logits</td><td>–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</td></tr>
      <tr><td><strong>–ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è</strong></td><td>Cross-Entropy</td><td>–°—Ç–∞–Ω–¥–∞—Ä—Ç</td></tr>
      <tr><td><strong>–ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã</strong></td><td>Focal Loss</td><td>–§–æ–∫—É—Å –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö</td></tr>
      <tr><td><strong>–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è</strong></td><td>Dice Loss</td><td>Overlap –º–µ—Ç—Ä–∏–∫–∞</td></tr>
      <tr><td><strong>–ú–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ</strong></td><td>Triplet Loss</td><td>–†–∞—Å—Å—Ç–æ—è–Ω–∏—è</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>ÔøΩÔøΩ 19. –í–∑–≤–µ—à–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å</h2>
    <pre><code># –î–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤

# PyTorch - –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
class_weights = torch.FloatTensor([0.3, 0.7])
criterion = nn.CrossEntropyLoss(weight=class_weights)

# TensorFlow/Keras
from tensorflow.keras.losses import SparseCategoricalCrossentropy

# –í–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤
class_weight = {0: 1.0, 1: 2.0, 2: 1.5}

model.fit(
    X_train, y_train,
    class_weight=class_weight,
    epochs=10
)

# –ò–ª–∏ —á–µ—Ä–µ–∑ sample_weight
sample_weights = np.array([weight_map[y] for y in y_train])
model.fit(
    X_train, y_train,
    sample_weight=sample_weights,
    epochs=10
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 20. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å</h2>
    <pre><code># –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö loss —Ñ—É–Ω–∫—Ü–∏–π
import torch.nn as nn

class CombinedLoss(nn.Module):
    def __init__(self, alpha=0.5, beta=0.5):
        super().__init__()
        self.alpha = alpha
        self.beta = beta
        self.bce = nn.BCEWithLogitsLoss()
        self.dice = DiceLoss()
    
    def forward(self, pred, target):
        loss1 = self.bce(pred, target)
        loss2 = self.dice(pred, target)
        return self.alpha * loss1 + self.beta * loss2

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
criterion = CombinedLoss(alpha=0.5, beta=0.5)
loss = criterion(predictions, targets)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è:
# - –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è: BCE + Dice
# - Multi-task learning
# - Balancing —Ä–∞–∑–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –∑–∞–¥–∞—á–∏</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 21. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –î–µ–ª–∞—Ç—å</h3>
        <p>‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å "with logits" –≤–µ—Ä—Å–∏–∏ (—Å—Ç–∞–±–∏–ª—å–Ω–µ–µ)<br>
        ‚Ä¢ –ü—Ä–æ–≤–µ—Ä—è—Ç—å range –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏<br>
        ‚Ä¢ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è loss<br>
        ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Å–∞ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤<br>
        ‚Ä¢ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ loss —Ñ—É–Ω–∫—Ü–∏–∏</p>
      </div>
      <div class="bad">
        <h3>‚ùå –ù–µ –¥–µ–ª–∞—Ç—å</h3>
        <p>‚Ä¢ –ü—Ä–∏–º–µ–Ω—è—Ç—å sigmoid/softmax –ø–µ—Ä–µ–¥ "with logits"<br>
        ‚Ä¢ –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å NaN/Inf –≤ loss<br>
        ‚Ä¢ –ó–∞–±—ã–≤–∞—Ç—å –ø—Ä–æ reduction='mean'<br>
        ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MSE –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏<br>
        ‚Ä¢ –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤</p>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 22. –¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏</h2>
    <table>
      <tr><th>–û—à–∏–±–∫–∞</th><th>–†–µ—à–µ–Ω–∏–µ</th></tr>
      <tr><td>NaN –≤ loss</td><td>–ü—Ä–æ–≤–µ—Ä–∏—Ç—å learning rate, clipping</td></tr>
      <tr><td>Loss –Ω–µ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è</td><td>–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É</td></tr>
      <tr><td>–î–≤–æ–π–Ω–æ–π sigmoid</td><td>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å "with logits"</td></tr>
      <tr><td>–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç y</td><td>–ü—Ä–æ–≤–µ—Ä–∏—Ç—å shape –∏ dtype</td></tr>
      <tr><td>–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞</td><td>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Å–∞ –∏–ª–∏ focal loss</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 23. –†–µ—Å—É—Ä—Å—ã</h2>
    <ul>
      <li><strong>–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è PyTorch</strong>: torch.nn loss functions</li>
      <li><strong>–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è TensorFlow</strong>: tf.keras.losses</li>
      <li><strong>–°—Ç–∞—Ç—å—è</strong>: "Loss Functions Explained" –Ω–∞ Towards DS</li>
      <li><strong>–ö–Ω–∏–≥–∞</strong>: "Deep Learning" - Goodfellow</li>
      <li><strong>Paper</strong>: "Focal Loss for Dense Object Detection"</li>
    </ul>
  </div>



</div>
</body>
</html>

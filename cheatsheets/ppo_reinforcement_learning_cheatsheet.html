<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>PPO (Proximal Policy Optimization) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ PPO (Proximal Policy Optimization)</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã Policy Gradient</h2>
    <p><strong>Policy Gradient</strong> ‚Äî –∫–ª–∞—Å—Å –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ RL, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—â–∏—Ö –ø–æ–ª–∏—Ç–∏–∫—É –Ω–∞–ø—Ä—è–º—É—é</p>
    <ul>
      <li><strong>–ü–æ–ª–∏—Ç–∏–∫–∞</strong>: œÄ(a|s, Œ∏) ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏—è</li>
      <li><strong>–¶–µ–ª—å</strong>: –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–∂–∏–¥–∞–µ–º—É—é –Ω–∞–≥—Ä–∞–¥—É</li>
      <li><strong>–ì—Ä–∞–¥–∏–µ–Ω—Ç</strong>: ‚àáJ(Œ∏) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Œ∏</li>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ</strong>: —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. Policy Gradient Theorem</h2>
    <pre><code>‚àáJ(Œ∏) = E_œÄ[‚àá log œÄ(a|s,Œ∏) √ó Q^œÄ(s,a)]

–≥–¥–µ:
- J(Œ∏) = E[‚àë Œ≥^t r_t]  (–æ–∂–∏–¥–∞–µ–º–∞—è –Ω–∞–≥—Ä–∞–¥–∞)
- Q^œÄ(s,a) = E[‚àë Œ≥^k r_{t+k} | s_t=s, a_t=a]
- Œ≥ = discount factor

–ò–Ω—Ç—É–∏—Ü–∏—è: —É–≤–µ–ª–∏—á–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ö–æ—Ä–æ—à–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. PPO –∞–ª–≥–æ—Ä–∏—Ç–º</h2>
    <p>–ë–∞–∑–æ–≤—ã–π Monte Carlo Policy Gradient</p>
    <pre><code>1. –°–æ–±—Ä–∞—Ç—å —ç–ø–∏–∑–æ–¥: (s_0, a_0, r_0), ..., (s_T, a_T, r_T)
2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ t:
   G_t = ‚àë_{k=0}^{T-t} Œ≥^k r_{t+k}  (return)
3. –û–±–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
   Œ∏ ‚Üê Œ∏ + Œ± √ó ‚àá log œÄ(a_t|s_t,Œ∏) √ó G_t</code></pre>
    <ul>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ</strong>: –ø—Ä–æ—Å—Ç–æ—Ç–∞</li>
      <li><strong>–ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫</strong>: –≤—ã—Å–æ–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 4. Baseline –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏</h2>
    <p>–í—ã—á–∏—Ç–∞–Ω–∏–µ baseline —É–º–µ–Ω—å—à–∞–µ—Ç variance –±–µ–∑ bias</p>
    <pre><code>‚àáJ(Œ∏) = E[‚àá log œÄ(a|s,Œ∏) √ó (Q(s,a) - b(s))]

–ß–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç: b(s) = V(s)

Advantage: A(s,a) = Q(s,a) - V(s)</code></pre>
    <ul>
      <li><strong>–≠—Ñ—Ñ–µ–∫—Ç</strong>: —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ</li>
      <li><strong>V(s)</strong>: –æ—Ü–µ–Ω–∫–∞ value function</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 5. Actor-Critic –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</h2>
    <p>–ö–æ–º–±–∏–Ω–∞—Ü–∏—è policy gradient + value function</p>
    <ul>
      <li><strong>Actor</strong>: policy œÄ(a|s, Œ∏)</li>
      <li><strong>Critic</strong>: value function V(s, w) –∏–ª–∏ Q(s,a, w)</li>
      <li><strong>Actor –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ</strong>: ‚àáJ(Œ∏) —Å advantage –æ—Ç critic</li>
      <li><strong>Critic –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ</strong>: TD-error –∏–ª–∏ MSE loss</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 6. Advantage Actor-Critic (A2C)</h2>
    <pre><code>import torch
import torch.nn as nn
import torch.optim as optim

class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU()
        )
        self.actor = nn.Linear(128, action_dim)
        self.critic = nn.Linear(128, 1)
    
    def forward(self, state):
        x = self.shared(state)
        policy = torch.softmax(self.actor(x), dim=-1)
        value = self.critic(x)
        return policy, value

# –û–±—É—á–µ–Ω–∏–µ
policy, value = model(state)
action = torch.multinomial(policy, 1)
next_state, reward, done, _ = env.step(action.item())

# TD-error
td_target = reward + gamma * model(next_state)[1] * (1 - done)
td_error = td_target - value

# Losses
actor_loss = -torch.log(policy[action]) * td_error.detach()
critic_loss = td_error.pow(2)
loss = actor_loss + 0.5 * critic_loss</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. A3C (Asynchronous A2C)</h2>
    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –≤ —Ä–∞–∑–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö</li>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ</strong>: –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–∞–Ω–Ω—ã—Ö</li>
      <li><strong>Sync vs Async</strong>: A2C —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π, A3C –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: Atari –∏–≥—Ä—ã, –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 8. PPO (Proximal Policy Optimization)</h2>
    <p><strong>PPO</strong> ‚Äî state-of-the-art policy gradient –º–µ—Ç–æ–¥</p>
    <ul>
      <li><strong>Clipped objective</strong>: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è</li>
      <li><strong>–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</strong>: –º–µ–Ω—å—à–µ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏—Ö –æ–±–≤–∞–ª–æ–≤</li>
      <li><strong>–ü—Ä–æ—Å—Ç–æ—Ç–∞</strong>: –ª–µ–≥—á–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å, —á–µ–º TRPO</li>
      <li><strong>–ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å</strong>: ChatGPT, AlphaGo, —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 9. PPO objective</h2>
    <pre><code>r_t(Œ∏) = œÄ_Œ∏(a_t|s_t) / œÄ_Œ∏_old(a_t|s_t)

L^CLIP(Œ∏) = E[min(
    r_t(Œ∏) √ó A_t,
    clip(r_t(Œ∏), 1-Œµ, 1+Œµ) √ó A_t
)]

–≥–¥–µ:
- r_t = probability ratio
- A_t = advantage
- Œµ = clip parameter (–æ–±—ã—á–Ω–æ 0.2)

–ò–¥–µ—è: –Ω–µ –¥–∞—Ç—å –ø–æ–ª–∏—Ç–∏–∫–µ —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å—Å—è</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. PPO –ø—Å–µ–≤–¥–æ–∫–æ–¥</h2>
    <pre><code>for iteration in range(N):
    # –°–±–æ—Ä —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π
    for actor in actors:
        œÑ = collect_trajectory(œÄ_Œ∏_old)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ advantage
    A = compute_advantages(œÑ)
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ (K —ç–ø–æ—Ö)
    for epoch in range(K):
        for minibatch in shuffle(œÑ):
            L = compute_ppo_loss(minibatch, A)
            optimize(Œ∏, L)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Continuous action spaces</h2>
    <p>–î–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</p>
    <pre><code># –ü–æ–ª–∏—Ç–∏–∫–∞ –≤—ã–¥–∞–µ—Ç Œº –∏ œÉ
Œº, log_œÉ = actor(state)
œÉ = torch.exp(log_œÉ)

# Sampling
action = Œº + œÉ * torch.randn_like(Œº)

# Log probability
log_prob = -0.5 * ((action - Œº) / œÉ).pow(2) - log_œÉ - 0.5 * log(2œÄ)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Entropy regularization</h2>
    <p>–î–æ–±–∞–≤–ª–µ–Ω–∏–µ entropy –¥–ª—è exploration</p>
    <pre><code>H(œÄ) = -‚àë œÄ(a|s) log œÄ(a|s)

Loss = actor_loss - Œ≤ √ó H(œÄ)

–≥–¥–µ Œ≤ ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç entropy (0.01-0.1)</code></pre>
    <ul>
      <li><strong>–≠—Ñ—Ñ–µ–∫—Ç</strong>: –±–æ–ª—å—à–µ exploration</li>
      <li><strong>–£–º–µ–Ω—å—à–µ–Ω–∏–µ Œ≤</strong>: –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 13. Generalized Advantage Estimation (GAE)</h2>
    <p>–ú–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ advantage —Å –±–∞–ª–∞–Ω—Å–æ–º bias-variance</p>
    <pre><code>Œ¥_t = r_t + Œ≥V(s_{t+1}) - V(s_t)  (TD-error)

A_t^{GAE} = ‚àë_{l=0}^‚àû (Œ≥Œª)^l Œ¥_{t+l}

–≥–¥–µ Œª ‚àà [0,1] ‚Äî GAE parameter:
- Œª=0: —Ç–æ–ª—å–∫–æ TD-error (low variance, high bias)
- Œª=1: Monte Carlo return (high variance, low bias)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</th><th>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å</th><th>–°–ª–æ–∂–Ω–æ—Å—Ç—å</th></tr>
      <tr><td><strong>PPO</strong></td><td>–ù–∏–∑–∫–∞—è</td><td>–ù–∏–∑–∫–∞—è</td><td>–ü—Ä–æ—Å—Ç–æ–π</td></tr>
      <tr><td><strong>A2C</strong></td><td>–°—Ä–µ–¥–Ω—è—è</td><td>–°—Ä–µ–¥–Ω—è—è</td><td>–°—Ä–µ–¥–Ω—è—è</td></tr>
      <tr><td><strong>A3C</strong></td><td>–°—Ä–µ–¥–Ω—è—è</td><td>–í—ã—Å–æ–∫–∞—è</td><td>–°—Ä–µ–¥–Ω—è—è</td></tr>
      <tr><td><strong>PPO</strong></td><td>–í—ã—Å–æ–∫–∞—è</td><td>–í—ã—Å–æ–∫–∞—è</td><td>–°—Ä–µ–¥–Ω—è—è</td></tr>
      <tr><td><strong>TRPO</strong></td><td>–í—ã—Å–æ–∫–∞—è</td><td>–°—Ä–µ–¥–Ω—è—è</td><td>–°–ª–æ–∂–Ω—ã–π</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <ol>
      <li><strong>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è</strong>: states –∏ rewards</li>
      <li><strong>Learning rate</strong>: 3e-4 –¥–ª—è actor, 1e-3 –¥–ª—è critic</li>
      <li><strong>Batch size</strong>: 64-4096 —à–∞–≥–æ–≤</li>
      <li><strong>GAE Œª</strong>: 0.95-0.99</li>
      <li><strong>PPO Œµ</strong>: 0.1-0.2</li>
      <li><strong>Value loss coef</strong>: 0.5</li>
      <li><strong>Entropy coef</strong>: 0.01</li>
      <li><strong>Discount Œ≥</strong>: 0.99</li>
    </ol>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>–ò–≥—Ä—ã</strong>: Atari, Dota 2, StarCraft</li>
      <li><strong>–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞</strong>: –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏, —Ö–æ–¥—å–±–∞</li>
      <li><strong>NLP</strong>: RLHF –¥–ª—è LLM (ChatGPT)</li>
      <li><strong>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</strong>: online —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã</li>
      <li><strong>–§–∏–Ω–∞–Ω—Å—ã</strong>: –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π —Ç—Ä–µ–π–¥–∏–Ω–≥</li>
      <li><strong>–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≤—Ç–æ</strong>: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 17. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</h3>
        <ul>
          <li>–†–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏</li>
          <li>–°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏</li>
          <li>–•–æ—Ä–æ—à–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å (PPO)</li>
          <li>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ exploration</li>
        </ul>
      </div>
      <div class="bad">
        <h3>–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h3>
        <ul>
          <li>Sample inefficient</li>
          <li>–¢—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ –æ–ø—ã—Ç–∞</li>
          <li>–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º</li>
          <li>–õ–æ–∫–∞–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º—É–º—ã</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ß–µ–∫-–ª–∏—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è</h2>
    <ol>
      <li>‚úì –í—ã–±—Ä–∞—Ç—å –º–µ—Ç–æ–¥ (A2C, PPO, TRPO)</li>
      <li>‚úì –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É actor-critic</li>
      <li>‚úì –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã</li>
      <li>‚úì –î–æ–±–∞–≤–∏—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –≤—Ö–æ–¥–æ–≤</li>
      <li>‚úì –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å advantage estimation (GAE)</li>
      <li>‚úì –î–æ–±–∞–≤–∏—Ç—å entropy regularization</li>
      <li>‚úì –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å —Å—Ä–µ–¥–Ω—é—é –Ω–∞–≥—Ä–∞–¥—É –∏ –¥–ª–∏–Ω—É —ç–ø–∏–∑–æ–¥–∞</li>
      <li>‚úì –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ</li>
    </ol>
  </div>

</div>

</div>
</body>
</html>

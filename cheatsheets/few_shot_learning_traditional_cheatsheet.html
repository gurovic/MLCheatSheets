<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Few-shot learning (—Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ Few-shot learning (—Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã)</h1>
  <div class="subtitle">üìÖ 5 —è–Ω–≤–∞—Ä—è 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏</h2>

    <ul>
      <li><strong>Few-shot learning</strong>: –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞–ª–æ–º —á–∏—Å–ª–µ –ø—Ä–∏–º–µ—Ä–æ–≤</li>
      <li><strong>N-way K-shot</strong>: N –∫–ª–∞—Å—Å–æ–≤, K –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å</li>
      <li><strong>Support set</strong>: –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è</li>
      <li><strong>Query set</strong>: –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è</li>
    </ul>
    <blockquote>
      –ó–∞–¥–∞—á–∞: –Ω–∞—É—á–∏—Ç—å—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã, –∏–º–µ—è –≤—Å–µ–≥–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤.
    </blockquote>

    </div>
<div class="block">
    <h2>üî∑ 2. –¢–∏–ø—ã –ø–æ–¥—Ö–æ–¥–æ–≤</h2>

    <table>
      <tr><th>–ü–æ–¥—Ö–æ–¥</th><th>–ò–¥–µ—è</th><th>–ü—Ä–∏–º–µ—Ä—ã</th></tr>
      <tr><td>Metric-based</td><td>–û–±—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –±–ª–∏–∑–æ—Å—Ç–∏</td><td>Siamese, Prototypical</td></tr>
      <tr><td>Model-based</td><td>–ú–æ–¥–µ–ª–∏ —Å –ø–∞–º—è—Ç—å—é</td><td>Memory-augmented NN</td></tr>
      <tr><td>Optimization-based</td><td>–ë—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è</td><td>MAML</td></tr>
      <tr><td>Generation-based</td><td>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö</td><td>Data augmentation</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. Siamese Networks</h2>

    <pre><code>import torch.nn as nn

class SiameseNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 64, 3),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(128*5*5, 256)
        )
    
    def forward(self, x1, x2):
        emb1 = self.encoder(x1)
        emb2 = self.encoder(x2)
        return emb1, emb2

# Contrastive loss
def contrastive_loss(emb1, emb2, label, margin=1.0):
    dist = F.pairwise_distance(emb1, emb2)
    loss = (1-label) * dist**2 + label * torch.clamp(margin-dist, min=0)**2
    return loss.mean()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Prototypical Networks</h2>

    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Å –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–º (—Ü–µ–Ω—Ç—Ä–æ–º)</li>
      <li><strong>–ü—Ä–æ—Ç–æ—Ç–∏–ø</strong>: —Å—Ä–µ–¥–Ω–µ–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ support set</li>
      <li><strong>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</strong>: –±–ª–∏–∂–∞–π—à–∏–π –ø—Ä–æ—Ç–æ—Ç–∏–ø</li>
    </ul>
    <pre><code># Prototypical Networks
def compute_prototypes(support_embeddings, support_labels, n_way):
    prototypes = []
    for class_id in range(n_way):
        mask = support_labels == class_id
        class_embeddings = support_embeddings[mask]
        prototype = class_embeddings.mean(dim=0)
        prototypes.append(prototype)
    return torch.stack(prototypes)

def classify_query(query_embedding, prototypes):
    distances = torch.cdist(query_embedding, prototypes)
    predictions = distances.argmin(dim=1)
    return predictions</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Matching Networks</h2>

    <ul>
      <li><strong>Attention mechanism</strong>: –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ</li>
      <li><strong>Bidirectional LSTM</strong>: –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</li>
      <li><strong>Full Context Embeddings</strong>: —É—á–µ—Ç –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤</li>
    </ul>
    <pre><code># Matching Networks
def attention_kernel(query, support):
    # Cosine similarity
    similarities = F.cosine_similarity(
        query.unsqueeze(1), 
        support.unsqueeze(0), 
        dim=2
    )
    attention = F.softmax(similarities, dim=1)
    return attention

def predict(query_emb, support_emb, support_labels):
    attention = attention_kernel(query_emb, support_emb)
    # Weighted voting
    predictions = torch.matmul(attention, support_labels)
    return predictions</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Relation Networks</h2>

    <pre><code># Relation Networks: –æ–±—É—á–∞–µ–º–∞—è –º–µ—Ç—Ä–∏–∫–∞
class RelationModule(nn.Module):
    def __init__(self):
        super().__init__()
        self.relation = nn.Sequential(
            nn.Conv2d(128, 64, 3),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 64, 3),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(64*2*2, 8),
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()
        )
    
    def forward(self, query_features, support_features):
        # Concatenate features
        combined = torch.cat([query_features, support_features], dim=1)
        relation_score = self.relation(combined)
        return relation_score</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Meta-features</h2>

    <ul>
      <li><strong>Task-level features</strong>: —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∑–∞–¥–∞—á–∏</li>
      <li><strong>Instance-level features</strong>: –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø—Ä–∏–º–µ—Ä–æ–≤</li>
      <li><strong>Meta-knowledge</strong>: –∑–Ω–∞–Ω–∏—è –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∑–∞–¥–∞—á</li>
    </ul>
    <table>
      <tr><th>–¢–∏–ø</th><th>–ü—Ä–∏–º–µ—Ä—ã</th></tr>
      <tr><td>–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ</td><td>–°—Ä–µ–¥–Ω–µ–µ, –¥–∏—Å–ø–µ—Ä—Å–∏—è, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏</td></tr>
      <tr><td>–ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ</td><td>–†–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏</td></tr>
      <tr><td>–¢–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ</td><td>–ü–ª–æ—Ç–Ω–æ—Å—Ç—å, —Å–≤—è–∑–Ω–æ—Å—Ç—å</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 8. Data Augmentation –¥–ª—è FSL</h2>

    <pre><code># Augmentation –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è support set
from torchvision import transforms

augmentation = transforms.Compose([
    transforms.RandomRotation(20),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.2, 0.2, 0.2),
    transforms.RandomResizedCrop(84, scale=(0.8, 1.0))
])

def augment_support_set(support_images, k_aug=5):
    augmented = []
    for img in support_images:
        for _ in range(k_aug):
            aug_img = augmentation(img)
            augmented.append(aug_img)
    return augmented</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –¢—Ä–∞–Ω—Å–¥—É–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</h2>

    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å query set –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π</li>
      <li><strong>Label propagation</strong>: —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–∫</li>
      <li><strong>Transductive Fine-Tuning</strong>: –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞ query</li>
    </ul>
    <pre><code># Transductive inference
def transductive_inference(support_emb, query_emb, support_labels, iterations=5):
    # –ù–∞—á–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø—ã
    prototypes = compute_prototypes(support_emb, support_labels)
    
    for _ in range(iterations):
        # –ü—Å–µ–≤–¥–æ-–º–µ—Ç–∫–∏ –¥–ª—è query
        query_predictions = classify(query_emb, prototypes)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤ —Å —É—á–µ—Ç–æ–º query
        all_embeddings = torch.cat([support_emb, query_emb])
        all_labels = torch.cat([support_labels, query_predictions])
        prototypes = compute_prototypes(all_embeddings, all_labels)
    
    return query_predictions</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Benchmarks –∏ –¥–∞—Ç–∞—Å–µ—Ç—ã</h2>

    <table>
      <tr><th>–î–∞—Ç–∞—Å–µ—Ç</th><th>–ó–∞–¥–∞—á–∞</th><th>–°–ª–æ–∂–Ω–æ—Å—Ç—å</th></tr>
      <tr><td>Omniglot</td><td>–†—É–∫–æ–ø–∏—Å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã</td><td>–õ–µ–≥–∫–∞—è</td></tr>
      <tr><td>miniImageNet</td><td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤</td><td>–°—Ä–µ–¥–Ω—è—è</td></tr>
      <tr><td>tieredImageNet</td><td>–ò–µ—Ä–∞—Ä—Ö–∏—è –∫–ª–∞—Å—Å–æ–≤</td><td>–°—Ä–µ–¥–Ω—è—è</td></tr>
      <tr><td>CIFAR-FS</td><td>Few-shot CIFAR</td><td>–°—Ä–µ–¥–Ω—è—è</td></tr>
      <tr><td>Meta-Dataset</td><td>–ú–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ</td><td>–í—ã—Å–æ–∫–∞—è</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 11. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</h2>

    <pre><code># Evaluation protocol
def evaluate_few_shot(model, test_tasks, n_way=5, k_shot=1):
    accuracies = []
    
    for task in test_tasks:
        # Sample N-way K-shot task
        support, query = task.sample(n_way, k_shot)
        
        # Get predictions
        predictions = model.predict(support, query)
        
        # Compute accuracy
        acc = (predictions == query_labels).float().mean()
        accuracies.append(acc)
    
    mean_acc = torch.tensor(accuracies).mean()
    conf_interval = 1.96 * torch.tensor(accuracies).std() / sqrt(len(accuracies))
    
    return mean_acc, conf_interval

# –¢–∏–ø–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: 95.2% ¬± 0.3% –Ω–∞ Omniglot 5-way 1-shot</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>

    <div class="good-vs-bad">
      <div class="good">
        <h3>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</h3>
        <ul>
          <li>–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ</li>
          <li>–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ä–∞–∑–º–µ—Ä–æ–º 64-256</li>
          <li>Data augmentation –≤–∞–∂–µ–Ω</li>
          <li>Episodic training —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω</li>
        </ul>
      </div>
      <div class="bad">
        <h3>–ü—Ä–æ–±–ª–µ–º—ã</h3>
        <ul>
          <li>Overfitting –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
          <li>Domain shift –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏</li>
          <li>–í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å</li>
        </ul>
      </div>
    </div>
  </div>




</div>
</body>
</html>

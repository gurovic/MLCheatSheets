<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üîÑ –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏</li>
      <li><strong>–ú–µ—Ç–æ–¥</strong>: —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –æ—Ç –≤—ã—Ö–æ–¥–∞ –∫ –≤—Ö–æ–¥—É</li>
      <li><strong>–û—Å–Ω–æ–≤–∞</strong>: –ø—Ä–∞–≤–∏–ª–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π (chain rule)</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: –æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π</li>
    </ul>
    <blockquote>
      Backpropagation ‚Äî —ç—Ç–æ —Å–ø–æ—Å–æ–± —É–∑–Ω–∞—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω –≤–∏–Ω–æ–≤–∞—Ç –≤ –æ—à–∏–±–∫–µ, —á—Ç–æ–±—ã –µ–≥–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å.
    </blockquote>

    </div>
<div class="block">
    <h2>üî∑ 2. –î–≤–∞ —ç—Ç–∞–ø–∞ –æ–±—É—á–µ–Ω–∏—è</h2>
    <ol>
      <li><strong>Forward pass (–ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥)</strong>:
        <ul>
          <li>–î–∞–Ω–Ω—ã–µ –∏–¥—É—Ç –æ—Ç –≤—Ö–æ–¥–∞ –∫ –≤—ã—Ö–æ–¥—É</li>
          <li>–í—ã—á–∏—Å–ª—è—é—Ç—Å—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤—Å–µ—Ö —Å–ª–æ—ë–≤</li>
          <li>–ü–æ–ª—É—á–∞–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ</li>
          <li>–í—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å</li>
        </ul>
      </li>
      <li><strong>Backward pass (–æ–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥)</strong>:
        <ul>
          <li>–û—à–∏–±–∫–∞ –∏–¥—ë—Ç –æ—Ç –≤—ã—Ö–æ–¥–∞ –∫ –≤—Ö–æ–¥—É</li>
          <li>–í—ã—á–∏—Å–ª—è—é—Ç—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–µ—Å–∞</li>
          <li>–í–µ—Å–∞ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º</li>
        </ul>
      </li>
    </ol>
  </div>

  <div class="block">
    <h2>üî∑ 3. Chain Rule (–ø—Ä–∞–≤–∏–ª–æ —Ü–µ–ø–æ—á–∫–∏)</h2>
    <p><strong>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞ backpropagation:</strong></p>
    <pre><code>–ï—Å–ª–∏ z = f(y) –∏ y = g(x), —Ç–æ:

‚àÇz/‚àÇx = (‚àÇz/‚àÇy) * (‚àÇy/‚àÇx)

–î–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:

‚àÇLoss/‚àÇw‚ÇÅ = (‚àÇLoss/‚àÇoutput) * (‚àÇoutput/‚àÇh) * (‚àÇh/‚àÇw‚ÇÅ)

–≥–¥–µ h ‚Äî —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä: 1 —Å–ª–æ–π</h2>
    <pre><code>import numpy as np

# Forward pass
x = np.array([1.0, 2.0, 3.0])  # –≤—Ö–æ–¥
w = np.array([0.5, -0.2, 0.1])  # –≤–µ—Å–∞
b = 0.1  # bias

# –õ–∏–Ω–µ–π–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
z = np.dot(x, w) + b  # z = 1.2

# –ê–∫—Ç–∏–≤–∞—Ü–∏—è (sigmoid)
a = 1 / (1 + np.exp(-z))  # a ‚âà 0.77

# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (MSE)
y_true = 1.0
loss = (a - y_true)**2  # loss ‚âà 0.053

# Backward pass
# –ì—Ä–∞–¥–∏–µ–Ω—Ç loss –ø–æ a
dL_da = 2 * (a - y_true)  # ‚âà -0.46

# –ì—Ä–∞–¥–∏–µ–Ω—Ç a –ø–æ z (–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è sigmoid)
da_dz = a * (1 - a)  # ‚âà 0.177

# –ì—Ä–∞–¥–∏–µ–Ω—Ç loss –ø–æ z (chain rule)
dL_dz = dL_da * da_dz  # ‚âà -0.081

# –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –≤–µ—Å–∞–º
dL_dw = dL_dz * x  # [-0.081, -0.163, -0.244]
dL_db = dL_dz  # -0.081

# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
learning_rate = 0.1
w = w - learning_rate * dL_dw
b = b - learning_rate * dL_db</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω–∞—è —Å–µ—Ç—å</h2>
    <pre><code>class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def sigmoid_derivative(self, a):
        return a * (1 - a)
    
    def forward(self, X):
        # –°–ª–æ–π 1
        self.z1 = X.dot(self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        
        # –°–ª–æ–π 2
        self.z2 = self.a1.dot(self.W2) + self.b2
        self.a2 = self.sigmoid(self.z2)
        
        return self.a2
    
    def backward(self, X, y, output):
        m = X.shape[0]
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å–ª–æ—è 2
        dL_da2 = output - y  # –¥–ª—è MSE loss
        da2_dz2 = self.sigmoid_derivative(self.a2)
        dL_dz2 = dL_da2 * da2_dz2
        
        dL_dW2 = (1/m) * self.a1.T.dot(dL_dz2)
        dL_db2 = (1/m) * np.sum(dL_dz2, axis=0, keepdims=True)
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å–ª–æ—è 1
        dL_da1 = dL_dz2.dot(self.W2.T)
        da1_dz1 = self.sigmoid_derivative(self.a1)
        dL_dz1 = dL_da1 * da1_dz1
        
        dL_dW1 = (1/m) * X.T.dot(dL_dz1)
        dL_db1 = (1/m) * np.sum(dL_dz1, axis=0, keepdims=True)
        
        return dL_dW1, dL_db1, dL_dW2, dL_db2
    
    def train(self, X, y, epochs=1000, lr=0.1):
        for epoch in range(epochs):
            # Forward
            output = self.forward(X)
            
            # Backward
            dW1, db1, dW2, db2 = self.backward(X, y, output)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
            self.W1 -= lr * dW1
            self.b1 -= lr * db1
            self.W2 -= lr * dW2
            self.b2 -= lr * db2
            
            if epoch % 100 == 0:
                loss = np.mean((output - y)**2)
                print(f"Epoch {epoch}, Loss: {loss:.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π</h2>
    <table>
      <tr><th>–ê–∫—Ç–∏–≤–∞—Ü–∏—è</th><th>–§—É–Ω–∫—Ü–∏—è</th><th>–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è</th></tr>
      <tr><td><strong>Sigmoid</strong></td><td>œÉ(x) = 1/(1+e‚ÅªÀ£)</td><td>œÉ(x)(1-œÉ(x))</td></tr>
      <tr><td><strong>Tanh</strong></td><td>tanh(x)</td><td>1 - tanh¬≤(x)</td></tr>
      <tr><td><strong>ReLU</strong></td><td>max(0, x)</td><td>1 if x>0, else 0</td></tr>
      <tr><td><strong>Leaky ReLU</strong></td><td>max(Œ±x, x)</td><td>1 if x>0, else Œ±</td></tr>
      <tr><td><strong>Softmax</strong></td><td>eÀ£‚Å±/Œ£eÀ£ ≤</td><td>œÉ·µ¢(1-œÉ‚±º)</td></tr>
    </table>
    <pre><code>def relu_derivative(z):
    return (z > 0).astype(float)

def tanh_derivative(a):
    return 1 - np.tanh(a)**2

def leaky_relu_derivative(z, alpha=0.01):
    dz = np.ones_like(z)
    dz[z < 0] = alpha
    return dz</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å</h2>
    <table>
      <tr><th>Loss</th><th>–§–æ—Ä–º—É–ª–∞</th><th>–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è ‚àÇL/‚àÇ≈∑</th></tr>
      <tr><td><strong>MSE</strong></td><td>(y - ≈∑)¬≤</td><td>2(≈∑ - y)</td></tr>
      <tr><td><strong>MAE</strong></td><td>|y - ≈∑|</td><td>sign(≈∑ - y)</td></tr>
      <tr><td><strong>BCE</strong></td><td>-y log(≈∑) - (1-y)log(1-≈∑)</td><td>(≈∑ - y)/(≈∑(1-≈∑))</td></tr>
      <tr><td><strong>CCE</strong></td><td>-Œ£y·µ¢log(≈∑·µ¢)</td><td>-y/≈∑</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 8. Computational Graph</h2>
    <p><strong>–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π:</strong></p>
    <pre><code>–ü—Ä–∏–º–µ—Ä: z = (x + y) * w

Forward:
x=2, y=3 ‚Üí a=5 (—Å–ª–æ–∂–µ–Ω–∏–µ)
a=5, w=4 ‚Üí z=20 (—É–º–Ω–æ–∂–µ–Ω–∏–µ)

Backward:
‚àÇz/‚àÇz = 1
‚àÇz/‚àÇa = w = 4
‚àÇz/‚àÇw = a = 5
‚àÇz/‚àÇx = ‚àÇz/‚àÇa * ‚àÇa/‚àÇx = 4 * 1 = 4
‚àÇz/‚àÇy = ‚àÇz/‚àÇa * ‚àÇa/‚àÇy = 4 * 1 = 4</code></pre>
    <p>–ö–∞–∂–¥–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏—Ç:</p>
    <ul>
      <li>–í—Ö–æ–¥—ã (–¥–ª—è backward pass)</li>
      <li>–í—ã—Ö–æ–¥ (–¥–ª—è forward pass)</li>
      <li>–õ–æ–∫–∞–ª—å–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ</h2>
    <p><strong>–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –¥–µ–ª–∞—é—Ç —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:</strong></p>
    <pre><code># PyTorch
import torch

x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
w = torch.tensor([0.5, -0.2, 0.1], requires_grad=True)
b = torch.tensor(0.1, requires_grad=True)

# Forward
z = torch.dot(x, w) + b
a = torch.sigmoid(z)
y_true = torch.tensor(1.0)
loss = (a - y_true)**2

# Backward (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏!)
loss.backward()

print(f"–ì—Ä–∞–¥–∏–µ–Ω—Ç w: {w.grad}")
print(f"–ì—Ä–∞–¥–∏–µ–Ω—Ç b: {b.grad}")

# TensorFlow
import tensorflow as tf

x = tf.constant([1.0, 2.0, 3.0])
w = tf.Variable([0.5, -0.2, 0.1])
b = tf.Variable(0.1)

with tf.GradientTape() as tape:
    z = tf.reduce_sum(x * w) + b
    a = tf.sigmoid(z)
    loss = (a - 1.0)**2

# –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
gradients = tape.gradient(loss, [w, b])
print(f"–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã: {gradients}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ü—Ä–æ–±–ª–µ–º—ã backpropagation</h2>
    <table>
      <tr><th>–ü—Ä–æ–±–ª–µ–º–∞</th><th>–ü—Ä–∏—á–∏–Ω–∞</th><th>–†–µ—à–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>Vanishing gradient</strong></td><td>–ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –±–ª–∏–∑–∫–∏ –∫ 0</td><td>ReLU, Batch Norm, ResNet</td></tr>
      <tr><td><strong>Exploding gradient</strong></td><td>–ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ</td><td>Gradient clipping, Normalization</td></tr>
      <tr><td><strong>Dead ReLU</strong></td><td>–ù–µ–π—Ä–æ–Ω—ã –≤—Å–µ–≥–¥–∞ –≤—ã–¥–∞—é—Ç 0</td><td>Leaky ReLU, –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è</td></tr>
      <tr><td><strong>–ú–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</strong></td><td>–ü–ª–æ—Ö–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è</td><td>Xavier, He initialization</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 11. Vanishing Gradient</h2>
    <p><strong>–ü—Ä–æ–±–ª–µ–º–∞ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π:</strong></p>
    <ul>
      <li>–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —É–º–Ω–æ–∂–∞—é—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ</li>
      <li>–ï—Å–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è < 1, –≥—Ä–∞–¥–∏–µ–Ω—Ç —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è</li>
      <li>–í –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã ‚Üí 0</li>
      <li>–†–∞–Ω–Ω–∏–µ —Å–ª–æ–∏ –ø–æ—á—Ç–∏ –Ω–µ –æ–±—É—á–∞—é—Ç—Å—è</li>
    </ul>
    <pre><code># –ü—Ä–∏–º–µ—Ä: sigmoid –∏–º–µ–µ—Ç max –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é 0.25
# –í 10-—Å–ª–æ–π–Ω–æ–π —Å–µ—Ç–∏: (0.25)^10 ‚âà 0.000001

# –†–µ—à–µ–Ω–∏—è:
# 1. ReLU –≤–º–µ—Å—Ç–æ sigmoid
def relu(x):
    return np.maximum(0, x)

# 2. Batch Normalization
from tensorflow.keras.layers import BatchNormalization

# 3. Residual connections (ResNet)
output = input + F(input)  # skip connection

# 4. LSTM/GRU –¥–ª—è RNN</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Exploding Gradient</h2>
    <p><strong>–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞:</strong></p>
    <ul>
      <li>–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º–∏</li>
      <li>–í–µ—Å–∞ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ</li>
      <li>–ú–æ–¥–µ–ª—å —Ä–∞—Å—Ö–æ–¥–∏—Ç—Å—è</li>
    </ul>
    <pre><code># –†–µ—à–µ–Ω–∏—è:

# 1. Gradient Clipping
import torch.nn as nn

max_grad_norm = 1.0
torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)

# 2. –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
# Xavier/Glorot –¥–ª—è sigmoid/tanh
w = np.random.randn(n_in, n_out) * np.sqrt(1.0/n_in)

# He initialization –¥–ª—è ReLU
w = np.random.randn(n_in, n_out) * np.sqrt(2.0/n_in)

# 3. –ú–µ–Ω—å—à–∏–π learning rate
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

# 4. Batch Normalization
# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</h2>
    <pre><code>def gradient_check(f, x, analytic_grad, epsilon=1e-5):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    —á–∏—Å–ª–µ–Ω–Ω—ã–º –º–µ—Ç–æ–¥–æ–º
    """
    numeric_grad = np.zeros_like(x)
    
    for i in range(x.size):
        x_plus = x.copy()
        x_minus = x.copy()
        
        x_plus.flat[i] += epsilon
        x_minus.flat[i] -= epsilon
        
        # –ß–∏—Å–ª–µ–Ω–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
        numeric_grad.flat[i] = (f(x_plus) - f(x_minus)) / (2*epsilon)
    
    # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞
    numerator = np.linalg.norm(numeric_grad - analytic_grad)
    denominator = np.linalg.norm(numeric_grad) + np.linalg.norm(analytic_grad)
    relative_error = numerator / denominator
    
    if relative_error < 1e-7:
        print("‚úÖ –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞: {relative_error}")
        print(f"–ß–∏—Å–ª–µ–Ω–Ω—ã–π: {numeric_grad}")
        print(f"–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π: {analytic_grad}")
    
    return relative_error</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ (Xavier/He)</li>
      <li>[ ] –í—ã–±–æ—Ä –ø–æ–¥—Ö–æ–¥—è—â–µ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (ReLU –¥–ª—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π)</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (tensorboard, wandb)</li>
      <li>[ ] Gradient clipping –¥–ª—è RNN</li>
      <li>[ ] Batch Normalization –¥–ª—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø—Ä–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è</li>
      <li>[ ] –†–µ–≥—É–ª—è—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ vanishing/exploding</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´Backpropagation ‚Äî —ç—Ç–æ —Å–ø–æ—Å–æ–±, –∫–æ—Ç–æ—Ä—ã–º –Ω–µ–π—Ä–æ—Å–µ—Ç—å –ø–æ–Ω–∏–º–∞–µ—Ç, –∫–∞–∫–∏–µ –µ—ë —á–∞—Å—Ç–∏ —Ä–∞–±–æ—Ç–∞—é—Ç —Ö–æ—Ä–æ—à–æ, –∞ –∫–∞–∫–∏–µ –ø–ª–æ—Ö–æ, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏¬ª.
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 15. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</h2>
    <ul>
      <li><strong>Vectorization</strong>: –∏–∑–±–µ–≥–∞–π—Ç–µ —Ü–∏–∫–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏</li>
      <li><strong>GPU acceleration</strong>: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ PyTorch/TensorFlow —Å CUDA</li>
      <li><strong>Mixed precision</strong>: FP16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è</li>
      <li><strong>Gradient accumulation</strong>: –¥–ª—è –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–µ–π</li>
      <li><strong>Checkpointing</strong>: —Å–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏</li>
    </ul>
    <pre><code># PyTorch automatic mixed precision
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for data, target in dataloader:
    optimizer.zero_grad()
    
    with autocast():
        output = model(data)
        loss = criterion(output, target)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()</code></pre>
  </div>



</div>
</body>
</html>

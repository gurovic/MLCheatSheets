<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Dropout and Regularization Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ Dropout & Regularization</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</h2>
    <ul>
      <li><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ</li>
      <li><strong>–¶–µ–ª—å</strong>: —É–ª—É—á—à–∏—Ç—å –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å</li>
      <li><strong>–ú–µ—Ç–æ–¥</strong>: –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏–ª–∏ —à—Ç—Ä–∞—Ñ–æ–≤</li>
      <li><strong>–†–µ–∑—É–ª—å—Ç–∞—Ç</strong>: –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏/—Ç–µ—Å—Ç–µ</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. Dropout ‚Äî –æ—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è</h2>
    <ul>
      <li><strong>–ú–µ—Ö–∞–Ω–∏–∑–º</strong>: —Å–ª—É—á–∞–π–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏</li>
      <li><strong>–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å p</strong>: –¥–æ–ª—è –Ω–µ–π—Ä–æ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–∫–ª—é—á–∞—é—Ç—Å—è</li>
      <li><strong>–ü—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ</strong>: –≤—Å–µ –Ω–µ–π—Ä–æ–Ω—ã –∞–∫—Ç–∏–≤–Ω—ã</li>
      <li><strong>–≠—Ñ—Ñ–µ–∫—Ç</strong>: –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –Ω–µ –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω—ã</li>
      <li><strong>–ê–Ω–∞–ª–æ–≥–∏—è</strong>: –∞–Ω—Å–∞–º–±–ª—å –ø–æ–¥—Å–µ—Ç–µ–π</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. Dropout –≤ PyTorch</h2>
    <pre><code>import torch.nn as nn

# –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
model = nn.Sequential(
    nn.Linear(784, 512),
    nn.ReLU(),
    nn.Dropout(p=0.5),  # 50% –Ω–µ–π—Ä–æ–Ω–æ–≤ –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è
    
    nn.Linear(512, 256),
    nn.ReLU(),
    nn.Dropout(p=0.3),  # 30% –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è
    
    nn.Linear(256, 10)
)

# –í–ê–ñ–ù–û: –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å —Ä–µ–∂–∏–º—ã!
model.train()  # Dropout –∞–∫—Ç–∏–≤–µ–Ω
model.eval()   # Dropout –≤—ã–∫–ª—é—á–µ–Ω

# –Ø–≤–Ω–æ:
output = model(x)  # –í train —Ä–µ–∂–∏–º–µ ‚Äî —Å dropout
with torch.no_grad():
    model.eval()
    output = model(x)  # –ë–µ–∑ dropout</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Dropout –≤ Keras/TensorFlow</h2>
    <pre><code>from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Dense(512, activation='relu', input_shape=(784,)),
    layers.Dropout(0.5),
    
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.3),
    
    layers.Dense(10, activation='softmax')
])

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤:
model.fit(X_train, y_train, ...)  # training=True
model.predict(X_test)              # training=False

# –Ø–≤–Ω–æ:
model(x, training=True)   # –° dropout
model(x, training=False)  # –ë–µ–∑ dropout</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è p</h2>
    <table>
      <tr><th>–°–ª–æ–π</th><th>Dropout rate (p)</th><th>–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ</th></tr>
      <tr><td><strong>–í—Ö–æ–¥–Ω–æ–π</strong></td><td>0.1 - 0.2</td><td>–†–µ–¥–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è</td></tr>
      <tr><td><strong>–°–∫—Ä—ã—Ç—ã–µ</strong></td><td>0.3 - 0.5</td><td>–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ</td></tr>
      <tr><td><strong>–ë–æ–ª—å—à–∏–µ —Å–ª–æ–∏</strong></td><td>0.5</td><td>–ß–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è</td></tr>
      <tr><td><strong>–ú–∞–ª—ã–µ —Å–ª–æ–∏</strong></td><td>0.2 - 0.3</td><td>–ú–µ–Ω—å—à–µ dropout</td></tr>
      <tr><td><strong>CNN</strong></td><td>0.25 - 0.5</td><td>–ü–æ—Å–ª–µ pooling</td></tr>
      <tr><td><strong>RNN</strong></td><td>0.2 - 0.5</td><td>–†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–π dropout</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 6. L1 –∏ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</h2>
    <p><strong>L2 (Weight Decay)</strong>: —à—Ç—Ä–∞—Ñ –∑–∞ –±–æ–ª—å—à–∏–µ –≤–µ—Å–∞</p>
    <p>Loss = Loss_original + Œª ¬∑ Œ£ w·µ¢¬≤</p>
    
    <p><strong>L1 (Lasso)</strong>: —Å–æ–∑–¥–∞—ë—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –≤–µ—Å–∞</p>
    <p>Loss = Loss_original + Œª ¬∑ Œ£ |w·µ¢|</p>
    
    <table>
      <tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>L1</th><th>L2</th></tr>
      <tr><td><strong>–≠—Ñ—Ñ–µ–∫—Ç</strong></td><td>–û–±–Ω—É–ª—è–µ—Ç –≤–µ—Å–∞</td><td>–£–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Å–∞</td></tr>
      <tr><td><strong>–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å</strong></td><td>–î–∞</td><td>–ù–µ—Ç</td></tr>
      <tr><td><strong>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ</strong></td><td>Feature selection</td><td>–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤ PyTorch</h2>
    <pre><code>import torch.optim as optim

# Weight decay = L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
optimizer = optim.Adam(
    model.parameters(),
    lr=0.001,
    weight_decay=1e-4  # Œª = 0.0001
)

# –¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è weight_decay:
# - 1e-4 –¥–æ 1e-2 –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á
# - –ë–æ–ª—å—à–µ = —Å–∏–ª—å–Ω–µ–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è

# SGD —Å momentum
optimizer = optim.SGD(
    model.parameters(),
    lr=0.01,
    momentum=0.9,
    weight_decay=1e-4
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. L1/L2 –≤ Keras</h2>
    <pre><code>from tensorflow.keras import layers, regularizers

# L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
model = models.Sequential([
    layers.Dense(
        512, 
        activation='relu',
        kernel_regularizer=regularizers.l2(0.01)
    ),
    layers.Dense(
        256,
        activation='relu',
        kernel_regularizer=regularizers.l2(0.01)
    ),
    layers.Dense(10, activation='softmax')
])

# L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
layers.Dense(512, kernel_regularizer=regularizers.l1(0.01))

# L1 + L2 (Elastic Net)
layers.Dense(512, kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Early Stopping</h2>
    <p>–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ —É—Ö—É–¥—à–µ–Ω–∏–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏</p>
    <pre><code># PyTorch (–≤—Ä—É—á–Ω—É—é)
best_val_loss = float('inf')
patience = 10
counter = 0

for epoch in range(num_epochs):
    train_loss = train_epoch(model, train_loader)
    val_loss = validate(model, val_loader)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_model.pt')
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping!")
            break

# Keras (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π)
from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

model.fit(X_train, y_train,
         validation_data=(X_val, y_val),
         callbacks=[early_stop])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –î—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</h2>
    <ul>
      <li><strong>Data Augmentation</strong>: –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö</li>
      <li><strong>Batch Normalization</strong>: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è + —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</li>
      <li><strong>Label Smoothing</strong>: —Å–º—è–≥—á–µ–Ω–∏–µ –º–µ—Ç–æ–∫ (–Ω–µ 0/1, –∞ 0.1/0.9)</li>
      <li><strong>Mixup</strong>: —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤</li>
      <li><strong>Gradient Clipping</strong>: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</li>
      <li><strong>Max Norm Constraint</strong>: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–æ—Ä–º—ã –≤–µ—Å–æ–≤</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 11. Gradient Clipping</h2>
    <pre><code># PyTorch
import torch.nn.utils as utils

# –í training loop
for batch in train_loader:
    optimizer.zero_grad()
    output = model(batch)
    loss = criterion(output, target)
    loss.backward()
    
    # Clip gradients
    utils.clip_grad_norm_(
        model.parameters(),
        max_norm=1.0
    )
    
    optimizer.step()

# –î–ª—è RNN –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ!
# –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç exploding gradients</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Label Smoothing</h2>
    <pre><code># PyTorch
class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, epsilon=0.1):
        super().__init__()
        self.epsilon = epsilon
    
    def forward(self, pred, target):
        n_classes = pred.size(-1)
        log_pred = F.log_softmax(pred, dim=-1)
        
        # Smooth labels
        loss = -log_pred.sum(dim=-1).mean()
        loss = loss * self.epsilon / n_classes
        
        nll = F.nll_loss(log_pred, target)
        return (1 - self.epsilon) * nll + loss

# Keras/TensorFlow
model.compile(
    loss=tf.keras.losses.CategoricalCrossentropy(
        label_smoothing=0.1
    ),
    optimizer='adam'
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫</h2>
    <pre><code># –ü—Ä–∏–º–µ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
import torch.nn as nn

class RegularizedModel(nn.Module):
    def __init__(self):
        super().__init__()
        
        self.fc1 = nn.Linear(784, 512)
        self.bn1 = nn.BatchNorm1d(512)
        self.drop1 = nn.Dropout(0.5)
        
        self.fc2 = nn.Linear(512, 256)
        self.bn2 = nn.BatchNorm1d(256)
        self.drop2 = nn.Dropout(0.3)
        
        self.fc3 = nn.Linear(256, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.drop1(x)
        
        x = self.fc2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.drop2(x)
        
        x = self.fc3(x)
        return x

# + L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ weight_decay
# + Early stopping
# + Data augmentation</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á—Ç–æ?</h2>
    <table>
      <tr><th>–°–∏—Ç—É–∞—Ü–∏—è</th><th>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è</th></tr>
      <tr><td><strong>–ú–∞–ª—ã–π –¥–∞—Ç–∞—Å–µ—Ç</strong></td><td>Dropout + L2 + Augmentation</td></tr>
      <tr><td><strong>–ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç</strong></td><td>–ú–µ–Ω—å—à–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</td></tr>
      <tr><td><strong>–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å</strong></td><td>–¢–æ–ª—å–∫–æ L2</td></tr>
      <tr><td><strong>–ì–ª—É–±–æ–∫–∞—è —Å–µ—Ç—å</strong></td><td>Dropout + BatchNorm</td></tr>
      <tr><td><strong>CNN</strong></td><td>Dropout + BatchNorm + Augmentation</td></tr>
      <tr><td><strong>RNN</strong></td><td>Recurrent Dropout + Gradient Clipping</td></tr>
      <tr><td><strong>Transformer</strong></td><td>Dropout + Label Smoothing</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è</h2>
    <ul>
      <li>Train accuracy >> Val accuracy</li>
      <li>Train loss –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –ø–∞–¥–∞—Ç—å, val loss —Ä–∞—Å—Ç—ë—Ç</li>
      <li>–ú–æ–¥–µ–ª—å –æ—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li>–ü–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li>–í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 16. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏</h2>
    <ul>
      <li><strong>–ù–∞—á–Ω–∏—Ç–µ —Å –ø—Ä–æ—Å—Ç–æ–≥–æ</strong>: —Å–Ω–∞—á–∞–ª–∞ L2, –ø–æ—Ç–æ–º –¥–æ–±–∞–≤–ª—è–π—Ç–µ</li>
      <li><strong>–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥</strong>: —Å–ª–µ–¥–∏—Ç–µ –∑–∞ train/val –º–µ—Ç—Ä–∏–∫–∞–º–∏</li>
      <li><strong>Early stopping</strong>: –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ!</li>
      <li><strong>–ù–µ –ø–µ—Ä–µ—É—Å–µ—Ä–¥—Å—Ç–≤—É–π—Ç–µ</strong>: —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ = –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ</li>
      <li><strong>–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–µ–∂–∏–º</strong>: train/eval –¥–ª—è dropout</li>
      <li><strong>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ</strong>: –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞–≤–∏—Å—è—Ç –æ—Ç –∑–∞–¥–∞—á–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 17. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –î–æ–±–∞–≤–∏—Ç—å L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é (weight_decay)</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Dropout (0.3-0.5)</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å Early Stopping</li>
      <li>[ ] –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å train/eval —Ä–µ–∂–∏–º—ã</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å train/val –º–µ—Ç—Ä–∏–∫–∏</li>
      <li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å Batch Normalization</li>
      <li>[ ] –î–ª—è CNN ‚Äî –¥–æ–±–∞–≤–∏—Ç—å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é</li>
      <li>[ ] –î–ª—è RNN ‚Äî Gradient Clipping</li>
      <li>[ ] –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è ‚Äî —ç—Ç–æ –∫–∞–∫ –æ–±—É—á–µ–Ω–∏–µ —Å "–ø–æ–º–µ—Ö–∞–º–∏": –º—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ–º –∑–∞–¥–∞—á—É –º–æ–¥–µ–ª–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ–±—ã –æ–Ω–∞ —É—á–∏–ª–∞—Å—å –≤—ã—è–≤–ª—è—Ç—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤–∞–∂–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏, –∞ –Ω–µ –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã. Dropout –º–æ–∂–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å —Å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–æ–π –∫–æ–º–∞–Ω–¥—ã, –≥–¥–µ –Ω–∞ –∫–∞–∂–¥–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ —Å–ª—É—á–∞–π–Ω—ã–µ –∏–≥—Ä–æ–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç ‚Äî –∫–æ–º–∞–Ω–¥–∞ —É—á–∏—Ç—Å—è –∏–≥—Ä–∞—Ç—å –ø—Ä–∏ –ª—é–±–æ–º —Å–æ—Å—Ç–∞–≤–µ¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

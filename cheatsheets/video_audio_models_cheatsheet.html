<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>–í–∏–¥–µ–æ-–∞—É–¥–∏–æ –º–æ–¥–µ–ª–∏ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üé•üîä –í–∏–¥–µ–æ-–∞—É–¥–∏–æ –º–æ–¥–µ–ª–∏</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: —Å–æ–≤–º–µ—Å—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ</li>
      <li><strong>–°–∏–Ω–µ—Ä–≥–∏—è</strong>: –∞—É–¥–∏–æ + –≤–∏–∑—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è</li>
      <li><strong>–ó–∞–¥–∞—á–∏</strong>: —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π, –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –∑–≤—É–∫–æ–≤, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è</li>
      <li><strong>Fusion</strong>: early, late, –∏–ª–∏ intermediate fusion</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. –û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏</h2>
    <table>
      <tr><th>–ó–∞–¥–∞—á–∞</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th></tr>
      <tr><td><strong>Action Recognition</strong></td><td>–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—É–¥–∏–æ+–≤–∏–¥–µ–æ</td></tr>
      <tr><td><strong>Audio-Visual Localization</strong></td><td>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∑–≤—É–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ</td></tr>
      <tr><td><strong>Audio-Visual Separation</strong></td><td>–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–≤—É–∫–æ–≤ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º</td></tr>
      <tr><td><strong>Lip Sync</strong></td><td>–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≥—É–± –∏ —Ä–µ—á–∏</td></tr>
      <tr><td><strong>Cross-Modal Retrieval</strong></td><td>–ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ –ø–æ –∞—É–¥–∏–æ –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç</td></tr>
      <tr><td><strong>Video Captioning</strong></td><td>–û–ø–∏—Å–∞–Ω–∏–µ –≤–∏–¥–µ–æ —Å —É—á–µ—Ç–æ–º –∑–≤—É–∫–∞</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã</h2>
    <p><strong>Early Fusion</strong></p>
    <ul>
      <li>–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –≤—Ö–æ–¥–µ</li>
      <li>–ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è</li>
      <li>–ú–æ–∂–µ—Ç —Ç–µ—Ä—è—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π</li>
    </ul>
    <p><strong>Late Fusion</strong></p>
    <ul>
      <li>–û—Ç–¥–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞, –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ –≤—ã—Ö–æ–¥–µ</li>
      <li>–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π</li>
      <li>–ì–∏–±–∫–æ—Å—Ç—å –≤ –æ–±—É—á–µ–Ω–∏–∏</li>
    </ul>
    <p><strong>Intermediate Fusion</strong></p>
    <ul>
      <li>–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—è—Ö</li>
      <li>Cross-attention –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏</li>
      <li>–õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ —Å–ª–æ–∂–Ω–µ–µ</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ë–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</h2>
    <pre><code>import torch
import torch.nn as nn

class AudioVisualModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        # Video encoder (3D CNN)
        self.video_encoder = nn.Sequential(
            nn.Conv3d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool3d(kernel_size=(1,2,2))
        )
        # Audio encoder (2D CNN –Ω–∞ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞—Ö)
        self.audio_encoder = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )
        # Fusion
        self.fusion = nn.Linear(128, 256)
        self.classifier = nn.Linear(256, num_classes)
    
    def forward(self, video, audio):
        v_feat = self.video_encoder(video).mean(dim=(2,3,4))
        a_feat = self.audio_encoder(audio).mean(dim=(2,3))
        # Concatenate
        fused = torch.cat([v_feat, a_feat], dim=1)
        fused = self.fusion(fused)
        return self.classifier(fused)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ö–ª—é—á–µ–≤—ã–µ –º–æ–¥–µ–ª–∏</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏</th></tr>
      <tr><td><strong>AVE-Net</strong></td><td>Audio-Visual Event localization</td></tr>
      <tr><td><strong>Sound of Pixels</strong></td><td>–õ–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –∑–≤—É–∫–æ–≤ –Ω–∞ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º —É—Ä–æ–≤–Ω–µ</td></tr>
      <tr><td><strong>Wav2Lip</strong></td><td>Lip-sync –≥–µ–Ω–µ—Ä–∞—Ü–∏—è</td></tr>
      <tr><td><strong>AVSpeech</strong></td><td>–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∏–¥–µ–æ</td></tr>
      <tr><td><strong>AVTS</strong></td><td>Audio-Visual Temporal Synchronization</td></tr>
      <tr><td><strong>ImageBind</strong></td><td>–ï–¥–∏–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è 6 –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π (Meta)</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 6. Audio-Visual Localization</h2>
    <pre><code>from torchvision.models import resnet18
import torchaudio

# –õ–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∑–≤—É–∫–∞
class SoundLocalizer(nn.Module):
    def __init__(self):
        super().__init__()
        self.visual_net = resnet18(pretrained=True)
        self.audio_net = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1,1))
        )
        # Cross-modal attention
        self.attention = nn.MultiheadAttention(512, 8)
        
    def forward(self, frames, spectrogram):
        # Visual features [B, T, H, W, C]
        v_feats = [self.visual_net(frame) for frame in frames]
        v_feats = torch.stack(v_feats, dim=1)  # [B, T, 512]
        
        # Audio features
        a_feat = self.audio_net(spectrogram)  # [B, 64]
        
        # Attention: –≥–¥–µ –∑–≤—É–∫ –Ω–∞ –≤–∏–¥–µ–æ?
        attn_out, attn_weights = self.attention(
            a_feat.unsqueeze(1), v_feats, v_feats
        )
        return attn_weights  # –ö–∞—Ä—Ç–∞ –≤–Ω–∏–º–∞–Ω–∏—è</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Audio-Visual Separation</h2>
    <p><strong>–ó–∞–¥–∞—á–∞</strong>: —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∑–≤—É–∫–∏ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ –≤–∏–¥–µ–æ</p>
    <pre><code># –ü—Ä–∏–º–µ—Ä: –¥–≤–∞ —á–µ–ª–æ–≤–µ–∫–∞ –≥–æ–≤–æ—Ä—è—Ç, —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∏—Ö –≥–æ–ª–æ—Å–∞
class AudioVisualSeparation(nn.Module):
    def __init__(self):
        super().__init__()
        self.face_encoder = FaceEncoder()  # –≠–Ω–∫–æ–¥–µ—Ä –ª–∏—Ü
        self.audio_encoder = AudioUNet()    # U-Net –¥–ª—è –∞—É–¥–∏–æ
        
    def forward(self, video_frames, mixed_audio):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ª–∏—Ü–∞ –∏–∑ –∫–∞–¥—Ä–æ–≤
        face_embeds = self.face_encoder(video_frames)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –∞—É–¥–∏–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ
        masks = self.audio_encoder(mixed_audio, face_embeds)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫–∏
        separated = [mixed_audio * mask for mask in masks]
        return separated</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Contrastive Learning</h2>
    <p><strong>–û–±—É—á–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ</strong></p>
    <pre><code>def audio_visual_contrastive_loss(video_emb, audio_emb, temp=0.07):
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    video_emb = F.normalize(video_emb, dim=-1)
    audio_emb = F.normalize(audio_emb, dim=-1)
    
    # Similarity matrix
    logits = (video_emb @ audio_emb.T) / temp
    
    # Positive pairs –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–∏
    labels = torch.arange(len(video_emb))
    
    # Symmetric contrastive loss
    loss_v2a = F.cross_entropy(logits, labels)
    loss_a2v = F.cross_entropy(logits.T, labels)
    
    return (loss_v2a + loss_a2v) / 2

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
video_features = video_encoder(video_clips)
audio_features = audio_encoder(audio_clips)
loss = audio_visual_contrastive_loss(video_features, audio_features)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Temporal Alignment</h2>
    <p><strong>–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ</strong></p>
    <ul>
      <li><strong>Lip-sync detection</strong>: –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏ –≥—É–± –∏ —Ä–µ—á–∏</li>
      <li><strong>Audio-visual sync</strong>: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–¥–≤–∏–≥–∞ –º–µ–∂–¥—É –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ</li>
      <li><strong>Event localization</strong>: –∫–æ–≥–¥–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–æ–±—ã—Ç–∏–µ</li>
    </ul>
    <pre><code># –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–¥–≤–∏–≥–∞ –º–µ–∂–¥—É –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ
class SyncNet(nn.Module):
    def forward(self, video_seq, audio_seq):
        v_emb = self.video_encoder(video_seq)  # [B, T, D]
        a_emb = self.audio_encoder(audio_seq)  # [B, T, D]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–¥–≤–∏–≥–æ–≤
        max_shift = 10
        correlations = []
        for shift in range(-max_shift, max_shift+1):
            corr = F.cosine_similarity(
                v_emb, 
                torch.roll(a_emb, shift, dims=1),
                dim=-1
            ).mean()
            correlations.append(corr)
        
        # –õ—É—á—à–∏–π —Å–¥–≤–∏–≥
        best_shift = torch.tensor(correlations).argmax() - max_shift
        return best_shift</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö</h2>
    <p><strong>–í–∏–¥–µ–æ</strong></p>
    <ul>
      <li>–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤: 1-30 fps –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏</li>
      <li>Resize: 224x224 –∏–ª–∏ 112x112</li>
      <li>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ ImageNet —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ</li>
    </ul>
    <p><strong>–ê—É–¥–∏–æ</strong></p>
    <ul>
      <li>Mel-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã: 128 mel bins, hop=10ms</li>
      <li>MFCC –¥–ª—è —Ä–µ—á–∏</li>
      <li>Raw waveform –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–¥–∞—á</li>
    </ul>
    <pre><code>import torchaudio
import torchvision.transforms as T

# –í–∏–¥–µ–æ preprocessing
video_transform = T.Compose([
    T.Resize(224),
    T.CenterCrop(224),
    T.Normalize(mean=[0.485,0.456,0.406], 
                std=[0.229,0.224,0.225])
])

# –ê—É–¥–∏–æ preprocessing
audio_transform = torchaudio.transforms.MelSpectrogram(
    sample_rate=16000,
    n_mels=128,
    hop_length=160
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è</h3>
        <ul>
          <li>Pre-trained —ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ</li>
          <li>Data augmentation: tempo shift, time mask</li>
          <li>–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∫–∞–ª</li>
          <li>Cross-modal attention layers</li>
          <li>Contrastive learning –¥–ª—è alignment</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ò–∑–±–µ–≥–∞—Ç—å</h3>
        <ul>
          <li>–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è</li>
          <li>–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –∫–∞–¥—Ä–æ–≤ (—Å–ª–∏—à–∫–æ–º –∑–∞—Ç—Ä–∞—Ç–Ω–æ)</li>
          <li>–û–¥–∏–Ω batch —Ä–∞–∑–º–µ—Ä –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π</li>
          <li>–û–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è –±–µ–∑ pre-training</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 12. –î–∞—Ç–∞—Å–µ—Ç—ã</h2>
    <table>
      <tr><th>–î–∞—Ç–∞—Å–µ—Ç</th><th>–ó–∞–¥–∞—á–∞</th><th>–†–∞–∑–º–µ—Ä</th></tr>
      <tr><td><strong>AVSpeech</strong></td><td>Speech separation</td><td>290K –≤–∏–¥–µ–æ</td></tr>
      <tr><td><strong>VGGSound</strong></td><td>Audio-visual learning</td><td>200K –≤–∏–¥–µ–æ</td></tr>
      <tr><td><strong>AudioSet</strong></td><td>Event classification</td><td>2M –≤–∏–¥–µ–æ</td></tr>
      <tr><td><strong>Kinetics</strong></td><td>Action recognition</td><td>650K –≤–∏–¥–µ–æ</td></tr>
      <tr><td><strong>LRS2/LRS3</strong></td><td>Lip reading</td><td>100K+ –≤–∏–¥–µ–æ</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>–í–∏–¥–µ–æ–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏</strong>: —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ, —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ—á–∏</li>
      <li><strong>–ö–∏–Ω–æ–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ</strong>: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–∑–≤—É—á–∫–∞, —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è</li>
      <li><strong>–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å</strong>: —Å—É–±—Ç–∏—Ç—Ä—ã, –æ–ø–∏—Å–∞–Ω–∏–µ –∑–≤—É–∫–æ–≤</li>
      <li><strong>–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å</strong>: –¥–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π</li>
      <li><strong>–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞</strong>: –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –∑–≤—É–∫–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ</li>
      <li><strong>AR/VR</strong>: –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∑–≤—É–∫</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ –∏ –∑–≤—É–∫ –≤–º–µ—Å—Ç–µ, –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫. –ú–æ–∂–µ—Ç –Ω–∞–π—Ç–∏, –∫—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –Ω–∞ —ç–∫—Ä–∞–Ω–µ, 
      —Ä–∞–∑–¥–µ–ª–∏—Ç—å –≥–æ–ª–æ—Å–∞ —Ä–∞–∑–Ω—ã—Ö –ª—é–¥–µ–π, –∏–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —á—Ç–æ –∑–≤—É–∫ –≥–∏—Ç–∞—Ä—ã –∏–¥–µ—Ç –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ —Å–ª–µ–≤–∞. 
      –≠—Ç–æ –∫–∞–∫ –Ω–∞—à –º–æ–∑–≥, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–µ–¥–∏–Ω—è–µ—Ç —Ç–æ, —á—Ç–æ –º—ã –≤–∏–¥–∏–º –∏ —Å–ª—ã—à–∏–º¬ª.
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ú–µ—Ç—Ä–∏–∫–∏</h2>
    <table>
      <tr><th>–ú–µ—Ç—Ä–∏–∫–∞</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>mAP</strong></td><td>Action recognition, event detection</td></tr>
      <tr><td><strong>SDR</strong></td><td>Source-to-Distortion Ratio (separation)</td></tr>
      <tr><td><strong>LSE-D</strong></td><td>Lip Sync Error Distance</td></tr>
      <tr><td><strong>AUC</strong></td><td>Audio-visual localization</td></tr>
      <tr><td><strong>IoU</strong></td><td>Spatial localization</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∑–∞–¥–∞—á—É (localization/separation/retrieval)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é fusion (early/late/intermediate)</li>
      <li>[ ] –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–∏–¥–µ–æ-–∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å —ç–Ω–∫–æ–¥–µ—Ä—ã (3D CNN –¥–ª—è –≤–∏–¥–µ–æ, 2D –¥–ª—è –∞—É–¥–∏–æ)</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å temporal alignment</li>
      <li>[ ] –ü—Ä–∏–º–µ–Ω–∏—Ç—å data augmentation –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π</li>
      <li>[ ] –û–±—É—á–∏—Ç—å —Å contrastive loss –¥–ª—è alignment</li>
      <li>[ ] –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏</li>
      <li>[ ] –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å inference (model pruning)</li>
    </ul>
  </div>

</div>

</div>
</body>
</html>

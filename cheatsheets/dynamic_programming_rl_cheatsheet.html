<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Динамическое программирование в RL Cheatsheet — 3 колонки</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    strong {
      color: #1a5fb4;
    }
  </style>
</head>
<body>
  <h1>Динамическое программирование в Reinforcement Learning</h1>
  <div class="subtitle"></div>

  <div class="container">

    <div class="block">
      <h2>1. Основы DP в RL</h2>
      <p><strong>Dynamic Programming</strong> — класс алгоритмов для решения MDP, когда модель среды (P, R) известна полностью</p>
      <p><strong>Ключевые принципы:</strong></p>
      <ul>
        <li><strong>Оптимальная подструктура</strong>: оптимальное решение состоит из оптимальных подрешений</li>
        <li><strong>Перекрывающиеся подзадачи</strong>: одни и те же подзадачи решаются многократно</li>
      </ul>
      <p><strong>Преимущества:</strong></p>
      <ul>
        <li>Точное решение для конечных MDP</li>
        <li>Гарантированная сходимость</li>
        <li>Теоретическая основа для RL</li>
      </ul>
      <p><strong>Ограничения:</strong> требует полной модели, вычислительно затратен для больших состояний</p>

    <div class="block">
      <h2>2. Policy Evaluation</h2>
      <p><strong>Задача:</strong> вычислить V^π(s) для данной стратегии π</p>
      <p><strong>Итеративный алгоритм:</strong></p>
      <pre><code>Инициализация: V(s) = 0 для всех s
Повторять пока не сойдётся:
  Для каждого s ∈ S:
    V_{k+1}(s) ← Σ_a π(a|s) [
      R(s,a) + γ Σ_{s'} P(s'|s,a) V_k(s')
    ]</code></pre>
      <p><strong>Критерий останова:</strong></p>
      <pre><code>max_s |V_{k+1}(s) - V_k(s)| < θ</code></pre>
      <p><strong>Матричная форма:</strong></p>
      <pre><code>V^π = R^π + γ P^π V^π
V^π = (I - γ P^π)^{-1} R^π  # прямое решение</code></pre>
      <p><strong>Сложность:</strong> O(|S|² |A|) за итерацию</p>
    </div>

    <div class="block">
      <h2>3. Policy Improvement</h2>
      <p><strong>Идея:</strong> улучшить стратегию π, действуя жадно относительно V^π</p>
      <p><strong>Жадное улучшение:</strong></p>
      <pre><code>π'(s) = argmax_a Q^π(s,a)
      = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a) V^π(s')]</code></pre>
      <p><strong>Policy Improvement Theorem:</strong></p>
      <ul>
        <li>Если π' жадна относительно V^π, то V^{π'}(s) ≥ V^π(s) для всех s</li>
        <li>Строгое неравенство, если π не оптимальна</li>
      </ul>
      <p><strong>Доказательство:</strong></p>
      <pre><code>Q^π(s, π'(s)) = max_a Q^π(s,a) ≥ Q^π(s, π(s)) = V^π(s)</code></pre>
    </div>

    <div class="block">
      <h2>4. Policy Iteration</h2>
      <p><strong>Алгоритм:</strong> чередование evaluation и improvement до сходимости</p>
      <pre><code>1. Инициализировать π произвольно
2. Повторять:
   a) Policy Evaluation:
      вычислить V^π
   
   b) Policy Improvement:
      π' ← жадная стратегия для V^π
   
   c) Если π' = π, STOP
      Иначе π ← π'</code></pre>
      <p><strong>Сходимость:</strong> сходится к π* за конечное число итераций (обычно очень быстро)</p>
      <p><strong>Каждая итерация:</strong></p>
      <ul>
        <li>Evaluation: O(|S|³) или O(k|S|²|A|) итераций</li>
        <li>Improvement: O(|S||A|)</li>
      </ul>
    </div>

    <div class="block">
      <h2>5. Value Iteration</h2>
      <p><strong>Идея:</strong> комбинировать evaluation и improvement в один шаг</p>
      <p><strong>Алгоритм:</strong></p>
      <pre><code>Инициализация: V(s) = 0 для всех s
Повторять пока не сойдётся:
  Для каждого s ∈ S:
    V_{k+1}(s) ← max_a [
      R(s,a) + γ Σ_{s'} P(s'|s,a) V_k(s')
    ]</code></pre>
      <p><strong>Критерий останова:</strong></p>
      <pre><code>max_s |V_{k+1}(s) - V_k(s)| < θ</code></pre>
      <p><strong>Извлечение стратегии:</strong></p>
      <pre><code>π(s) = argmax_a [R(s,a) + γ Σ_{s'} P(s'|s,a) V(s')]</code></pre>
      <p><strong>Преимущества:</strong> не требует Policy Evaluation до сходимости, быстрее Policy Iteration на практике</p>
    </div>

    <div class="block">
      <h2>6. Асинхронное DP</h2>
      <p><strong>Проблема синхронного DP:</strong> обновление всех состояний каждую итерацию затратно</p>
      <p><strong>Асинхронные варианты:</strong></p>
      <ul>
        <li><strong>In-place</strong>: использовать обновлённые значения сразу</li>
        <li><strong>Prioritized sweeping</strong>: обновлять состояния по приоритету</li>
        <li><strong>Real-time DP</strong>: обновлять только посещаемые состояния</li>
      </ul>
      <p><strong>Prioritized Value Iteration:</strong></p>
      <pre><code>Приоритет: p(s) = |max_a Σ_{s'} P(s'|s,a)[r + γV(s')] - V(s)|
Обновлять состояния с наибольшим p(s)</code></pre>
      <p><strong>Преимущества:</strong> фокусируется на важных состояниях, экономит вычисления</p>
    </div>

    <div class="block">
      <h2>7. Generalized Policy Iteration (GPI)</h2>
      <p><strong>Концепция:</strong> общий принцип взаимодействия evaluation и improvement</p>
      <p><strong>Процессы:</strong></p>
      <ul>
        <li><strong>Evaluation</strong>: сделать V более согласованным с π</li>
        <li><strong>Improvement</strong>: сделать π жадным относительно V</li>
      </ul>
      <p><strong>Визуализация:</strong> evaluation и improvement "тянут" друг друга к оптимальности</p>
      <p><strong>Варианты GPI:</strong></p>
      <ul>
        <li>Policy Iteration (полный evaluation)</li>
        <li>Value Iteration (один шаг evaluation)</li>
        <li>Асинхронные методы (выборочный evaluation)</li>
      </ul>
      <p><strong>В model-free RL:</strong> TD-learning, Q-learning, Actor-Critic следуют паттерну GPI</p>
    </div>

    <div class="block">
      <h2>8. Modified Policy Iteration</h2>
      <p><strong>Идея:</strong> не вычислять V^π точно, делать k шагов evaluation</p>
      <pre><code>1. Инициализировать π, V
2. Повторять:
   a) Частичная evaluation (k итераций):
      Для i = 1 до k:
        V ← Bellman backup для π
   
   b) Policy Improvement:
      π ← жадная стратегия для V
   
   c) Проверка сходимости</code></pre>
      <p><strong>Выбор k:</strong></p>
      <ul>
        <li>k = 1: эквивалентно Value Iteration</li>
        <li>k = ∞: эквивалентно Policy Iteration</li>
        <li>k = 3-10: хороший компромисс на практике</li>
      </ul>
    </div>

    <div class="block">
      <h2>9. Реализация на Python</h2>
      <pre><code>import numpy as np

class DynamicProgramming:
    def __init__(self, mdp, θ=1e-6):
        self.mdp = mdp
        self.θ = θ  # порог сходимости
    
    def policy_evaluation(self, π, max_iter=1000):
        """Вычислить V^π"""
        V = np.zeros(self.mdp.n_states)
        
        for _ in range(max_iter):
            Δ = 0
            for s in range(self.mdp.n_states):
                v = V[s]
                # Bellman expectation backup
                a = π[s]
                V[s] = self.mdp.bellman_backup(V, s, a)
                Δ = max(Δ, abs(v - V[s]))
            
            if Δ < self.θ:
                break
        
        return V
    
    def policy_improvement(self, V):
        """Жадное улучшение стратегии"""
        π = np.zeros(self.mdp.n_states, dtype=int)
        
        for s in range(self.mdp.n_states):
            # Выбрать лучшее действие
            q_values = [
                self.mdp.bellman_backup(V, s, a)
                for a in range(self.mdp.n_actions)
            ]
            π[s] = np.argmax(q_values)
        
        return π
    
    def policy_iteration(self):
        """Policy Iteration алгоритм"""
        π = np.random.randint(
            0, self.mdp.n_actions, 
            self.mdp.n_states
        )
        
        while True:
            # Evaluation
            V = self.policy_evaluation(π)
            
            # Improvement
            π_new = self.policy_improvement(V)
            
            # Проверка сходимости
            if np.array_equal(π, π_new):
                break
            
            π = π_new
        
        return π, V
    
    def value_iteration(self, max_iter=1000):
        """Value Iteration алгоритм"""
        V = np.zeros(self.mdp.n_states)
        
        for _ in range(max_iter):
            Δ = 0
            for s in range(self.mdp.n_states):
                v = V[s]
                # Bellman optimality backup
                V[s] = self.mdp.optimal_value(V, s)
                Δ = max(Δ, abs(v - V[s]))
            
            if Δ < self.θ:
                break
        
        # Извлечь оптимальную стратегию
        π = self.policy_improvement(V)
        
        return π, V</code></pre>
    </div>

    <div class="block">
      <h2>10. Пример: GridWorld</h2>
      <pre><code># 4x4 GridWorld
# Цель: (0,0) и (3,3)
# Награда: -1 за каждый шаг

def create_gridworld():
    n_states = 16
    n_actions = 4  # up, down, left, right
    
    mdp = MDP(n_states, n_actions, γ=1.0)
    
    # Задать переходы и награды
    for s in range(n_states):
        if s in [0, 15]:  # терминальные
            for a in range(n_actions):
                mdp.set_transition(s, a, s, 1.0)
                mdp.set_reward(s, a, 0)
        else:
            # Движения на сетке
            for a in range(n_actions):
                s_next = get_next_state(s, a)
                mdp.set_transition(s, a, s_next, 1.0)
                mdp.set_reward(s, a, -1)
    
    return mdp

# Решение
mdp = create_gridworld()
dp = DynamicProgramming(mdp)
π_opt, V_opt = dp.value_iteration()

print("Оптимальная стратегия:", π_opt)
print("Оптимальные значения:", V_opt)</code></pre>
    </div>

    <div class="block">
      <h2>11. Сравнение методов</h2>
      <p><strong>Policy Iteration:</strong></p>
      <ul>
        <li><strong>+</strong> Сходится за мало итераций (часто 3-10)</li>
        <li><strong>-</strong> Каждая итерация дорогая (evaluation)</li>
        <li><strong>Когда</strong>: малые MDP, нужна точность</li>
      </ul>
      <p><strong>Value Iteration:</strong></p>
      <ul>
        <li><strong>+</strong> Проще реализовать, быстрые итерации</li>
        <li><strong>-</strong> Больше итераций до сходимости</li>
        <li><strong>Когда</strong>: средние MDP, стандартный выбор</li>
      </ul>
      <p><strong>Modified Policy Iteration:</strong></p>
      <ul>
        <li><strong>+</strong> Лучший компромисс на практике</li>
        <li><strong>-</strong> Требует настройки k</li>
        <li><strong>Когда</strong>: оптимизация производительности</li>
      </ul>
    </div>

    <div class="block">
      <h2>12. Применение и ограничения</h2>
      <p><strong>Когда использовать DP:</strong></p>
      <ul>
        <li>Модель среды известна полностью</li>
        <li>Пространство состояний конечно и не слишком велико</li>
        <li>Нужно точное решение</li>
        <li>Планирование (offline)</li>
      </ul>
      <p><strong>Ограничения:</strong></p>
      <ul>
        <li><strong>Curse of dimensionality</strong>: |S| растёт экспоненциально</li>
        <li><strong>Требует модели</strong>: P и R должны быть известны</li>
        <li><strong>Full sweep</strong>: обновление всех состояний</li>
      </ul>
      <p><strong>Решения:</strong></p>
      <ul>
        <li>Асинхронное DP для больших пространств</li>
        <li>Аппроксимация: approximate DP с функциональной аппроксимацией</li>
        <li>Model-free методы: Q-learning, SARSA когда модель неизвестна</li>
      </ul>
    </div>

  </div>
</div>
</body>
</html>

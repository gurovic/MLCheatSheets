<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Optimization-based Meta-learning Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ Optimization-based Meta-learning Cheatsheet</h1>
  <div class="subtitle">MAML ‚Ä¢ Reptile ‚Ä¢ –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ ‚Ä¢ Few-shot learning<br>üìÖ 5 —è–Ω–≤–∞—Ä—è 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏</h2>
    <ul>
      <li><strong>Meta-learning</strong>: –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–º—É, –∫–∞–∫ –æ–±—É—á–∞—Ç—å—Å—è</li>
      <li><strong>Optimization-based</strong>: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏</li>
      <li><strong>Task distribution</strong>: –Ω–∞–±–æ—Ä —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –¥–ª—è –æ–±—É—á–µ–Ω–∏—è</li>
      <li><strong>Inner/Outer loop</strong>: –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –∑–∞–¥–∞—á–µ / –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
    </ul>
    <blockquote>
      –¶–µ–ª—å: –Ω–∞–π—Ç–∏ –Ω–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞, –∫–æ—Ç–æ—Ä—ã–µ –±—ã—Å—Ç—Ä–æ –∞–¥–∞–ø—Ç–∏—Ä—É—é—Ç—Å—è –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞.
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 2. MAML (Model-Agnostic Meta-Learning)</h2>
    <p><strong>–ê–ª–≥–æ—Ä–∏—Ç–º</strong>:</p>
    <ol>
      <li>–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–∞-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã Œ∏</li>
      <li>–î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ œÑ·µ¢:
        <ul>
          <li>–°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å Œ∏ ‚Üí Œ∏'·µ¢</li>
          <li>–°–¥–µ–ª–∞—Ç—å K —à–∞–≥–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –Ω–∞ support set</li>
          <li>–û—Ü–µ–Ω–∏—Ç—å –Ω–∞ query set</li>
        </ul>
      </li>
      <li>–û–±–Ω–æ–≤–∏—Ç—å Œ∏ –∏—Å–ø–æ–ª—å–∑—É—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º</li>
    </ol>
    <pre><code>import torch
import torch.nn as nn

def maml_train(model, tasks, alpha=0.01, beta=0.001):
    meta_params = list(model.parameters())
    
    for epoch in range(num_epochs):
        meta_loss = 0
        for task in tasks:
            # Inner loop: –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –∑–∞–¥–∞—á–µ
            adapted_params = adapt_to_task(
                model, task.support, alpha, K=5
            )
            # Outer loop: –æ—Ü–µ–Ω–∫–∞ –Ω–∞ query set
            loss = evaluate(model, adapted_params, task.query)
            meta_loss += loss
        
        # –ú–µ—Ç–∞-–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        meta_loss.backward()
        optimizer.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –∏ –≤–Ω–µ—à–Ω–∏–π —Ü–∏–∫–ª—ã</h2>
    <table>
      <tr><th>–¶–∏–∫–ª</th><th>–¶–µ–ª—å</th><th>–î–∞–Ω–Ω—ã–µ</th><th>Learning rate</th></tr>
      <tr><td>Inner</td><td>–ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –∑–∞–¥–∞—á–µ</td><td>Support set</td><td>Œ± (–≤—ã—Å–æ–∫–∏–π)</td></tr>
      <tr><td>Outer</td><td>–ú–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ</td><td>Query set</td><td>Œ≤ (–Ω–∏–∑–∫–∏–π)</td></tr>
    </table>
    <pre><code># –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Ü–∏–∫–ª (–∞–¥–∞–ø—Ç–∞—Ü–∏—è)
def adapt_to_task(model, support_data, lr, K):
    adapted_model = copy.deepcopy(model)
    for k in range(K):
        loss = compute_loss(adapted_model, support_data)
        grads = torch.autograd.grad(loss, adapted_model.parameters())
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        adapted_params = [p - lr * g for p, g in zip(adapted_model.parameters(), grads)]
    return adapted_params

# –í–Ω–µ—à–Ω–∏–π —Ü–∏–∫–ª (–º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ)
def meta_update(model, tasks, meta_lr):
    meta_grads = []
    for task in tasks:
        adapted_params = adapt_to_task(model, task.support)
        query_loss = evaluate(adapted_params, task.query)
        grads = torch.autograd.grad(query_loss, model.parameters())
        meta_grads.append(grads)
    
    # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–¥–∞—á–∞–º
    avg_grads = average_gradients(meta_grads)
    update_parameters(model, avg_grads, meta_lr)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –í–∞—Ä–∏–∞–Ω—Ç—ã MAML</h2>
    <ul>
      <li><strong>First-order MAML (FOMAML)</strong>:
        <ul>
          <li>–ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –≤—Ç–æ—Ä—ã–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ</li>
          <li>–ë—ã—Å—Ç—Ä–µ–µ –∏ –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏</li>
          <li>–ù–µ–º–Ω–æ–≥–æ —Ö—É–∂–µ –∫–∞—á–µ—Å—Ç–≤–æ</li>
        </ul>
      </li>
      <li><strong>MAML++</strong>:
        <ul>
          <li>Multi-step loss optimization</li>
          <li>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è learning rate per-layer</li>
          <li>–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</li>
        </ul>
      </li>
      <li><strong>Meta-SGD</strong>:
        <ul>
          <li>–û–±—É—á–µ–Ω–∏–µ learning rate</li>
          <li>–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —à–∞–≥–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞</li>
        </ul>
      </li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 5. Reptile</h2>
    <p><strong>–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è MAML</strong>:</p>
    <pre><code>def reptile_train(model, tasks, epsilon=0.1, K=10):
    """
    Reptile: –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ MAML
    """
    for epoch in range(num_epochs):
        # –í—ã–±–æ—Ä —Å–ª—É—á–∞–π–Ω–æ–π –∑–∞–¥–∞—á–∏
        task = sample_task(tasks)
        
        # –ö–æ–ø–∏—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        initial_params = copy_parameters(model)
        
        # K —à–∞–≥–æ–≤ SGD –Ω–∞ –∑–∞–¥–∞—á–µ
        for k in range(K):
            batch = task.sample_batch()
            loss = compute_loss(model, batch)
            loss.backward()
            optimizer.step()
        
        # –ú–µ—Ç–∞-–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: –¥–≤–∏–≥–∞–µ–º—Å—è –∫ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        adapted_params = model.parameters()
        for p_init, p_adapt in zip(initial_params, adapted_params):
            p_init.data += epsilon * (p_adapt.data - p_init.data)</code></pre>
    <blockquote>
      Reptile –ø—Ä–æ—â–µ MAML: –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≤—Ç–æ—Ä—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫ –∂–µ.
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 6. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–°–ª–æ–∂–Ω–æ—Å—Ç—å</th><th>–ü–∞–º—è—Ç—å</th><th>–ö–∞—á–µ—Å—Ç–≤–æ</th></tr>
      <tr><td>MAML</td><td>–í—ã—Å–æ–∫–∞—è (2-–π –ø–æ—Ä—è–¥–æ–∫)</td><td>–ë–æ–ª—å—à–∞—è</td><td>–û—Ç–ª–∏—á–Ω–æ–µ</td></tr>
      <tr><td>FOMAML</td><td>–°—Ä–µ–¥–Ω—è—è (1-–π –ø–æ—Ä—è–¥–æ–∫)</td><td>–°—Ä–µ–¥–Ω—è—è</td><td>–•–æ—Ä–æ—à–µ–µ</td></tr>
      <tr><td>Reptile</td><td>–ù–∏–∑–∫–∞—è</td><td>–ú–∞–ª–∞—è</td><td>–•–æ—Ä–æ—à–µ–µ</td></tr>
      <tr><td>MAML++</td><td>–í—ã—Å–æ–∫–∞—è</td><td>–ë–æ–ª—å—à–∞—è</td><td>–õ—É—á—à–µ–µ</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. Task Construction</h2>
    <pre><code>class FewShotTask:
    def __init__(self, n_way, k_shot, q_query):
        """
        n_way: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        k_shot: –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å –≤ support
        q_query: –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å –≤ query
        """
        self.n_way = n_way
        self.k_shot = k_shot
        self.q_query = q_query
    
    def sample(self, dataset):
        # –í—ã–±–∏—Ä–∞–µ–º n_way –∫–ª–∞—Å—Å–æ–≤
        classes = random.sample(dataset.classes, self.n_way)
        
        # Support set: k_shot –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        support = []
        query = []
        for cls in classes:
            samples = dataset.get_samples(cls)
            support.extend(samples[:self.k_shot])
            query.extend(samples[self.k_shot:self.k_shot+self.q_query])
        
        return support, query

# –ü—Ä–∏–º–µ—Ä: 5-way 1-shot –∑–∞–¥–∞—á–∞
task = FewShotTask(n_way=5, k_shot=1, q_query=15)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫ few-shot learning</h2>
    <ul>
      <li><strong>5-way 1-shot</strong>: 5 –∫–ª–∞—Å—Å–æ–≤, 1 –ø—Ä–∏–º–µ—Ä –Ω–∞ –∫–ª–∞—Å—Å</li>
      <li><strong>5-way 5-shot</strong>: 5 –∫–ª–∞—Å—Å–æ–≤, 5 –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å</li>
      <li><strong>Episode training</strong>: –∫–∞–∂–¥—ã–π —ç–ø–∏–∑–æ–¥ = –Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞</li>
    </ul>
    <pre><code># Few-shot classification —Å MAML
def few_shot_classify(model, support, query):
    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞ support set
    adapted_model = maml_adapt(model, support, steps=5)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ query set
    predictions = adapted_model(query)
    return predictions

# –û—Ü–µ–Ω–∫–∞
accuracy = evaluate_few_shot(
    model, 
    test_tasks, 
    n_way=5, 
    k_shot=1
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Implicit MAML (iMAML)</h2>
    <ul>
      <li><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: MAML —Ç—Ä–µ–±—É–µ—Ç —Ä–∞—Å–∫—Ä—É—Ç–∫–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤—Å–µ K —à–∞–≥–æ–≤</li>
      <li><strong>–†–µ—à–µ–Ω–∏–µ</strong>: –Ω–µ—è–≤–Ω–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ</li>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</strong>:
        <ul>
          <li>–ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–∞—è –ø–∞–º—è—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ K</li>
          <li>–ë–æ–ª—å—à–µ —à–∞–≥–æ–≤ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–µ–∑ —Ä–æ—Å—Ç–∞ –ø–∞–º—è—Ç–∏</li>
        </ul>
      </li>
    </ul>
    <pre><code># iMAML –∏—Å–ø–æ–ª—å–∑—É–µ—Ç implicit function theorem
# –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –±–µ–∑ —Ä–∞—Å–∫—Ä—É—Ç–∫–∏
def imaml_meta_gradient(meta_params, task):
    # –†–µ—à–∞–µ–º inner optimization –¥–æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    adapted_params = solve_inner_opt(meta_params, task.support)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç –Ω–µ—è–≤–Ω–æ
    meta_grad = implicit_gradient(
        adapted_params,
        meta_params,
        task.query
    )
    return meta_grad</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π learning rate</h2>
    <pre><code># Meta-SGD: –æ–±—É—á–∞–µ–º—ã–π learning rate
class MetaSGD(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
        # Learning rate –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä
        self.alphas = nn.ParameterDict({
            name: nn.Parameter(torch.ones_like(p) * 0.01)
            for name, p in model.named_parameters()
        })
    
    def adapt(self, support_data):
        for step in range(K):
            loss = compute_loss(self.model, support_data)
            grads = torch.autograd.grad(loss, self.model.parameters())
            
            # –ê–¥–∞–ø—Ç–∞—Ü–∏—è —Å –æ–±—É—á–∞–µ–º—ã–º–∏ learning rates
            for (name, param), grad, alpha in zip(
                self.model.named_parameters(), 
                grads, 
                self.alphas.values()
            ):
                param.data -= alpha * grad</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</h3>
        <ul>
          <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ FOMAML –¥–ª—è –Ω–∞—á–∞–ª–∞ (–ø—Ä–æ—â–µ)</li>
          <li>Inner LR –æ–±—ã—á–Ω–æ 0.01-0.1</li>
          <li>Meta LR –æ–±—ã—á–Ω–æ 0.001</li>
          <li>5-10 inner steps –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ</li>
          <li>Batch normalization —Ç—Ä–µ–±—É–µ—Ç –æ—Å–æ–±–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è</li>
        </ul>
      </div>
      <div class="bad">
        <h3>–ü—Ä–æ–±–ª–µ–º—ã</h3>
        <ul>
          <li>–í—ã—Å–æ–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏</li>
          <li>–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è</li>
          <li>–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º</li>
          <li>–î–æ–ª–≥–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>Few-shot classification</strong>: Omniglot, miniImageNet</li>
      <li><strong>Few-shot regression</strong>: sin/cos —Ñ—É–Ω–∫—Ü–∏–∏</li>
      <li><strong>Reinforcement Learning</strong>: –±—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞</li>
      <li><strong>Neural Architecture Search</strong>: –ø–æ–∏—Å–∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä</li>
      <li><strong>Personalization</strong>: –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é</li>
    </ul>
    <pre><code># –ü—Ä–∏–º–µ—Ä: MAML –¥–ª—è RL
def maml_rl(agent, envs, alpha=0.01):
    for epoch in range(num_epochs):
        for env in envs:
            # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ –∫ –æ–∫—Ä—É–∂–µ–Ω–∏—é
            adapted_agent = agent.clone()
            for step in range(K):
                trajectory = collect_trajectory(adapted_agent, env)
                policy_loss = compute_policy_loss(trajectory)
                adapted_agent.update(policy_loss, lr=alpha)
            
            # –ú–µ—Ç–∞-–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
            meta_loss = evaluate_agent(adapted_agent, env)
            agent.meta_update(meta_loss)</code></pre>
  </div>

</div>

</body>
</html>

<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Multi-head Attention Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ Multi-head Attention Cheatsheet</h1>
  <div class="subtitle">–ö–ª—é—á–µ–≤–æ–π –º–µ—Ö–∞–Ω–∏–∑–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ ‚Ä¢ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ß—Ç–æ —Ç–∞–∫–æ–µ Multi-head Attention</h2>
    <p><strong>Multi-head Attention</strong> ‚Äî –∫–ª—é—á–µ–≤–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤</p>
    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö attention –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤</li>
      <li><strong>–†–∞–∑–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è</strong>: –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ —É—á–∏—Ç —Å–≤–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω</li>
      <li><strong>–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ</strong>: –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è + –ª–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: BERT, GPT, ViT, –≤—Å–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 2. Scaled Dot-Product Attention</h2>
    <p>–ë–∞–∑–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º attention:</p>
    <pre><code>Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) V

–≥–¥–µ:
- Q = queries (—á—Ç–æ –∏—â–µ–º)
- K = keys (–≥–¥–µ –∏—â–µ–º)
- V = values (—á—Ç–æ –∏–∑–≤–ª–µ–∫–∞–µ–º)
- d_k = —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–ª—é—á–∞</code></pre>
    <blockquote>
      –î–µ–ª–µ–Ω–∏–µ –Ω–∞ ‚àöd_k —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 3. Multi-head —Ñ–æ—Ä–º—É–ª–∞</h2>
    <pre><code>MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O

–≥–¥–µ –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞:
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)

–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
- W_i^Q, W_i^K, W_i^V: –ø—Ä–æ–µ–∫—Ü–∏–∏ –¥–ª—è –≥–æ–ª–æ–≤—ã i
- W^O: –≤—ã—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è
- h: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ (–æ–±—ã—á–Ω–æ 8 –∏–ª–∏ 12)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ PyTorch</h2>
    <pre><code>import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=512, num_heads=8):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≥–æ–ª–æ–≤—ã
        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k)
        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k)
        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k)
        
        # Transpose –¥–ª—è batch processing
        Q = Q.transpose(1, 2)  # [batch, heads, seq, d_k]
        K = K.transpose(1, 2)
        V = V.transpose(1, 2)
        
        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention = torch.softmax(scores, dim=-1)
        context = torch.matmul(attention, V)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≥–æ–ª–æ–≤
        context = context.transpose(1, 2).contiguous()
        context = context.view(batch_size, -1, self.d_model)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è
        output = self.W_o(context)
        return output, attention</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ó–∞—á–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤?</h2>
    <ul>
      <li><strong>–†–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã</strong>: –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å–≤–æ—ë–º</li>
      <li><strong>–ë–æ–≥–∞—Ç—Å—Ç–≤–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è</strong>: –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤</li>
      <li><strong>–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è</strong>: —Å–∏–Ω—Ç–∞–∫—Å–∏—Å, —Å–µ–º–∞–Ω—Ç–∏–∫–∞, –ø–æ–∑–∏—Ü–∏–∏</li>
      <li><strong>–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å</strong>: redundancy –ø–æ–º–æ–≥–∞–µ—Ç –æ–±—É—á–µ–Ω–∏—é</li>
    </ul>
    <blockquote>
      –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏: –±–æ–ª—å—à–µ –≥–æ–ª–æ–≤ = –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ (–¥–æ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–µ–ª–∞)
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 6. –¢–∏–ø–∏—á–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>d_model</th><th>Heads</th><th>d_k</th></tr>
      <tr><td><strong>BERT-Base</strong></td><td>768</td><td>12</td><td>64</td></tr>
      <tr><td><strong>BERT-Large</strong></td><td>1024</td><td>16</td><td>64</td></tr>
      <tr><td><strong>GPT-2</strong></td><td>768</td><td>12</td><td>64</td></tr>
      <tr><td><strong>GPT-3</strong></td><td>12288</td><td>96</td><td>128</td></tr>
      <tr><td><strong>ViT-Base</strong></td><td>768</td><td>12</td><td>64</td></tr>
      <tr><td><strong>T5-Base</strong></td><td>768</td><td>12</td><td>64</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. Self-Attention vs Cross-Attention</h2>
    <table>
      <tr><th>–¢–∏–ø</th><th>Q</th><th>K, V</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>Self-Attention</strong></td><td>X</td><td>X</td><td>Encoder, Decoder</td></tr>
      <tr><td><strong>Cross-Attention</strong></td><td>Decoder</td><td>Encoder</td><td>Encoder-Decoder</td></tr>
      <tr><td><strong>Causal Self-Attention</strong></td><td>X</td><td>X (masked)</td><td>GPT, Decoder</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 8. Masking –≤ Attention</h2>
    <p>–î–≤–∞ —Ç–∏–ø–∞ –º–∞—Å–æ–∫:</p>
    <ul>
      <li><strong>Padding mask</strong>: –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å padding —Ç–æ–∫–µ–Ω—ã</li>
      <li><strong>Causal mask</strong>: –∑–∞–ø—Ä–µ—Ç–∏—Ç—å —Å–º–æ—Ç—Ä–µ—Ç—å –≤ –±—É–¥—É—â–µ–µ (GPT)</li>
    </ul>
    <pre><code># Padding mask
mask = (input_ids != pad_token_id).unsqueeze(1).unsqueeze(2)

# Causal mask (–¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–∞—Å—Å–∏–∏)
seq_len = x.size(1)
causal_mask = torch.triu(
    torch.ones(seq_len, seq_len), diagonal=1
).bool()

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: scores.masked_fill(mask, -1e9)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å</h2>
    <p>–î–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω—ã n:</p>
    <pre><code>Complexity = O(n¬≤ √ó d_model)

–ì–¥–µ:
- n¬≤: –≤—Å–µ –ø–∞—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤
- d_model: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞

–ü–∞–º—è—Ç—å: O(n¬≤ √ó num_heads)</code></pre>
    <p><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π</p>
  </div>

  <div class="block">
    <h2>üî∑ 10. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–°–ª–æ–∂–Ω–æ—Å—Ç—å</th><th>–ò–¥–µ—è</th></tr>
      <tr><td><strong>Sparse Attention</strong></td><td>O(n‚àön)</td><td>–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã</td></tr>
      <tr><td><strong>Linformer</strong></td><td>O(n)</td><td>Low-rank –ø—Ä–æ–µ–∫—Ü–∏—è</td></tr>
      <tr><td><strong>Performer</strong></td><td>O(n)</td><td>Kernel approximation</td></tr>
      <tr><td><strong>Flash Attention</strong></td><td>O(n¬≤)</td><td>IO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è</td></tr>
      <tr><td><strong>Local Attention</strong></td><td>O(n√ów)</td><td>–û–∫–Ω–æ —Ä–∞–∑–º–µ—Ä–∞ w</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 11. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Attention</h2>
    <pre><code>import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(attention_weights, tokens):
    """
    attention_weights: [seq_len, seq_len]
    tokens: list of token strings
    """
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        attention_weights,
        xticklabels=tokens,
        yticklabels=tokens,
        cmap='viridis',
        cbar=True
    )
    plt.xlabel('Key')
    plt.ylabel('Query')
    plt.title('Attention Weights')
    plt.tight_layout()
    plt.show()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
model.eval()
with torch.no_grad():
    outputs = model(input_ids, output_attentions=True)
    attention = outputs.attentions[0][0].mean(0)  # —É—Å—Ä–µ–¥–Ω–∏—Ç—å –ø–æ –≥–æ–ª–æ–≤–∞–º
    visualize_attention(attention.cpu(), tokens)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Attention Patterns</h2>
    <p>–¢–∏–ø–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ —É—á–∞—Ç –≥–æ–ª–æ–≤—ã:</p>
    <ul>
      <li><strong>Positional</strong>: –≤–Ω–∏–º–∞–Ω–∏–µ –∫ —Å–æ—Å–µ–¥–Ω–∏–º —Ç–æ–∫–µ–Ω–∞–º</li>
      <li><strong>Syntactic</strong>: –≤–Ω–∏–º–∞–Ω–∏–µ –∫ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–º</li>
      <li><strong>Semantic</strong>: –≤–Ω–∏–º–∞–Ω–∏–µ –∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏–º</li>
      <li><strong>Delimiter</strong>: —Ñ–æ–∫—É—Å –Ω–∞ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è—Ö ([CLS], [SEP])</li>
      <li><strong>Broadcast</strong>: –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –≤—Å–µ</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 13. Grouped Query Attention (GQA)</h2>
    <p>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–∑ LLaMA-2:</p>
    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –≥—Ä—É–ø–ø—ã –≥–æ–ª–æ–≤ –¥–µ–ª—è—Ç K –∏ V</li>
      <li><strong>–≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏</strong>: –º–µ–Ω—å—à–µ KV cache</li>
      <li><strong>–°–∫–æ—Ä–æ—Å—Ç—å</strong>: –±—ã—Å—Ç—Ä–µ–µ inference</li>
      <li><strong>–ö–∞—á–µ—Å—Ç–≤–æ</strong>: –ø–æ—á—Ç–∏ –∫–∞–∫ MHA</li>
    </ul>
    <pre><code># MHA: –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –∏–º–µ–µ—Ç —Å–≤–æ–∏ K, V
# GQA: –≥—Ä—É–ø–ø–∞ –≥–æ–ª–æ–≤ –¥–µ–ª–∏—Ç K, V
num_heads = 32
num_kv_heads = 8  # 4 –≥–æ–ª–æ–≤—ã –Ω–∞ 1 KV</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. Dropout –≤ Attention</h2>
    <p>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏:</p>
    <pre><code>attention = torch.softmax(scores, dim=-1)
attention = F.dropout(attention, p=0.1, training=self.training)
context = torch.matmul(attention, V)</code></pre>
    <ul>
      <li><strong>–ì–¥–µ</strong>: –ø–æ—Å–ª–µ softmax</li>
      <li><strong>–ó–Ω–∞—á–µ–Ω–∏–µ</strong>: –æ–±—ã—á–Ω–æ 0.1</li>
      <li><strong>–≠—Ñ—Ñ–µ–∫—Ç</strong>: –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overfitting</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <ol>
      <li><strong>d_k –≤—ã–±–æ—Ä</strong>: –æ–±—ã—á–Ω–æ d_model / num_heads = 64</li>
      <li><strong>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤</strong>: 8-16 –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á</li>
      <li><strong>–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è</strong>: Xavier –∏–ª–∏ He –¥–ª—è –≤–µ—Å–æ–≤</li>
      <li><strong>Gradient clipping</strong>: max_norm=1.0</li>
      <li><strong>Flash Attention</strong>: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è</li>
      <li><strong>KV cache</strong>: –∫–µ—à–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏</li>
    </ol>
  </div>

  <div class="block">
    <h2>üî∑ 16. Debugging Multi-head Attention</h2>
    <pre><code># –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
print(f"Q shape: {Q.shape}")  # [batch, heads, seq, d_k]
print(f"K shape: {K.shape}")
print(f"V shape: {V.shape}")
print(f"Scores shape: {scores.shape}")  # [batch, heads, seq, seq]
print(f"Attention shape: {attention.shape}")
print(f"Output shape: {output.shape}")  # [batch, seq, d_model]

# –ü—Ä–æ–≤–µ—Ä–∫–∞ attention weights
assert torch.allclose(attention.sum(dim=-1), torch.ones_like(attention.sum(dim=-1)))

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è attention
for head_idx in range(num_heads):
    plt.subplot(3, 4, head_idx + 1)
    plt.imshow(attention[0, head_idx].detach())
    plt.title(f'Head {head_idx}')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</h3>
        <ul>
          <li>–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑—É–µ–º–æ (–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç RNN)</li>
          <li>–ó–∞—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç long-range –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</li>
          <li>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ (attention maps)</li>
          <li>–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ (NLP, CV, audio)</li>
        </ul>
      </div>
      <div class="bad">
        <h3>–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h3>
        <ul>
          <li>–ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å O(n¬≤)</li>
          <li>–¢—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏</li>
          <li>–ú–µ–¥–ª–µ–Ω–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π</li>
          <li>–ù—É–∂–µ–Ω positional encoding</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ß–µ–∫-–ª–∏—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è</h2>
    <ol>
      <li>‚úì –í—ã–±—Ä–∞—Ç—å d_model –∏ num_heads</li>
      <li>‚úì –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ d_model –¥–µ–ª–∏—Ç—Å—è –Ω–∞ num_heads</li>
      <li>‚úì –î–æ–±–∞–≤–∏—Ç—å positional encoding –∫ –≤—Ö–æ–¥–∞–º</li>
      <li>‚úì –ù–∞—Å—Ç—Ä–æ–∏—Ç—å masking (padding, causal)</li>
      <li>‚úì –î–æ–±–∞–≤–∏—Ç—å dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</li>
      <li>‚úì –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ</li>
      <li>‚úì –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å attention patterns</li>
      <li>‚úì –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å Flash Attention</li>
    </ol>
  </div>

</div>

</body>
</html>

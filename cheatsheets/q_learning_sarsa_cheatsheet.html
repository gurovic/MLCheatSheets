<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Q-learning –∏ SARSA Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.88em;
      margin: 6px 0;
    }

    th, td {
      padding: 6px 8px;
      text-align: left;
      border: 1px solid #e0e7ff;
    }

    th {
      background-color: #1a5fb4;
      color: white;
      font-weight: 700;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ Q-learning –∏ SARSA</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º</h2>
    <p><strong>Reinforcement Learning (RL)</strong>: –∞–≥–µ–Ω—Ç —É—á–∏—Ç—Å—è –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—è —Å–æ —Å—Ä–µ–¥–æ–π</p>
    <ul>
      <li><strong>Agent</strong>: —Ç–æ—Ç, –∫—Ç–æ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ—à–µ–Ω–∏—è</li>
      <li><strong>Environment</strong>: –º–∏—Ä, –≤ –∫–æ—Ç–æ—Ä–æ–º –¥–µ–π—Å—Ç–≤—É–µ—Ç –∞–≥–µ–Ω—Ç</li>
      <li><strong>State (s)</strong>: —Ç–µ–∫—É—â–∞—è —Å–∏—Ç—É–∞—Ü–∏—è</li>
      <li><strong>Action (a)</strong>: –≤—ã–±–æ—Ä –∞–≥–µ–Ω—Ç–∞</li>
      <li><strong>Reward (r)</strong>: –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –æ—Ç —Å—Ä–µ–¥—ã</li>
      <li><strong>Policy (œÄ)</strong>: —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π</li>
      <li><strong>Value function (V/Q)</strong>: –æ–∂–∏–¥–∞–µ–º–∞—è –Ω–∞–≥—Ä–∞–¥–∞</li>
    </ul>
    <blockquote>
      üí° –¶–µ–ª—å: –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—É–º–º–∞—Ä–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É –≤–æ –≤—Ä–µ–º–µ–Ω–∏
    </blockquote>

  <div class="block">
    <h2>üî∑ 2. Q-learning (Off-Policy)</h2>
    <p><strong>–ò–¥–µ—è</strong>: —É—á–∏—Ç—Å—è –Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–æ–π</p>
    <p><strong>–§–æ—Ä–º—É–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è</strong>:</p>
    <pre><code>Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max_a' Q(s',a') - Q(s,a)]

–≥–¥–µ:
Œ± - learning rate (—Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è)
Œ≥ - discount factor (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
r - reward (–Ω–∞–≥—Ä–∞–¥–∞)
s' - next state (—Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ)
max_a' Q(s',a') - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è Q-value –≤ —Å–ª–µ–¥—É—é—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏</code></pre>
    <ul>
      <li><strong>Off-policy</strong>: –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç max Q(s',a'), –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è</li>
      <li><strong>–û–ø—Ç–∏–º–∏—Å—Ç–∏—á–Ω—ã–π</strong>: –≤—Å–µ–≥–¥–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è</li>
      <li><strong>Exploration</strong>: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Œµ-greedy –∏–ª–∏ –¥—Ä—É–≥–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Q-learning</h2>
    <pre><code>import numpy as np

class QLearningAgent:
    def __init__(self, n_states, n_actions, 
                 alpha=0.1, gamma=0.99, epsilon=0.1):
        self.n_states = n_states
        self.n_actions = n_actions
        self.alpha = alpha  # learning rate
        self.gamma = gamma  # discount factor
        self.epsilon = epsilon  # exploration rate
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Q-table
        self.Q = np.zeros((n_states, n_actions))
    
    def choose_action(self, state):
        # Œµ-greedy policy
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.n_actions)  # explore
        else:
            return np.argmax(self.Q[state])  # exploit
    
    def update(self, state, action, reward, next_state):
        # Q-learning update
        best_next_action = np.argmax(self.Q[next_state])
        td_target = reward + self.gamma * self.Q[next_state, best_next_action]
        td_error = td_target - self.Q[state, action]
        self.Q[state, action] += self.alpha * td_error
    
    def train(self, env, episodes=1000):
        rewards_history = []
        
        for episode in range(episodes):
            state = env.reset()
            total_reward = 0
            done = False
            
            while not done:
                action = self.choose_action(state)
                next_state, reward, done, _ = env.step(action)
                self.update(state, action, reward, next_state)
                
                state = next_state
                total_reward += reward
            
            rewards_history.append(total_reward)
            
            # Decay epsilon
            self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return rewards_history</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. SARSA (On-Policy)</h2>
    <p><strong>–ò–¥–µ—è</strong>: —É—á–∏—Ç—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞</p>
    <p><strong>–§–æ—Ä–º—É–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è</strong>:</p>
    <pre><code>Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ Q(s',a') - Q(s,a)]

–≥–¥–µ:
a' - —Ä–µ–∞–ª—å–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–µ —Å–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ
    (–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ!)</code></pre>
    <ul>
      <li><strong>On-policy</strong>: –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ a'</li>
      <li><strong>–ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π</strong>: –±–æ–ª–µ–µ –æ—Å—Ç–æ—Ä–æ–∂–µ–Ω –≤ —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö</li>
      <li><strong>SARSA</strong> = State-Action-Reward-State-Action</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 5. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è SARSA</h2>
    <pre><code>class SARSAAgent:
    def __init__(self, n_states, n_actions,
                alpha=0.1, gamma=0.99, epsilon=0.1):
        self.n_states = n_states
        self.n_actions = n_actions
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.Q = np.zeros((n_states, n_actions))
    
    def choose_action(self, state):
        # Œµ-greedy policy
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.n_actions)
        else:
            return np.argmax(self.Q[state])
    
    def update(self, state, action, reward, 
              next_state, next_action):
        # SARSA update (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç next_action, –Ω–µ max!)
        td_target = reward + self.gamma * self.Q[next_state, next_action]
        td_error = td_target - self.Q[state, action]
        self.Q[state, action] += self.alpha * td_error
    
    def train(self, env, episodes=1000):
        rewards_history = []
        
        for episode in range(episodes):
            state = env.reset()
            action = self.choose_action(state)  # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            total_reward = 0
            done = False
            
            while not done:
                next_state, reward, done, _ = env.step(action)
                next_action = self.choose_action(next_state)  # –°–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ
                
                self.update(state, action, reward, 
                          next_state, next_action)
                
                state = next_state
                action = next_action  # –í–∞–∂–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
                total_reward += reward
            
            rewards_history.append(total_reward)
            self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return rewards_history</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ö–ª—é—á–µ–≤—ã–µ —Ä–∞–∑–ª–∏—á–∏—è Q-learning vs SARSA</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>Q-learning</th><th>SARSA</th></tr>
      <tr><td>–¢–∏–ø</td><td>Off-policy</td><td>On-policy</td></tr>
      <tr><td>–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ</td><td>max Q(s',a')</td><td>Q(s',a') —Ä–µ–∞–ª—å–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è</td></tr>
      <tr><td>–ü–æ–≤–µ–¥–µ–Ω–∏–µ</td><td>–û–ø—Ç–∏–º–∏—Å—Ç–∏—á–Ω–æ–µ</td><td>–ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ</td></tr>
      <tr><td>–†–∏—Å–∫</td><td>–ë–æ–ª–µ–µ —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã–π</td><td>–ë–æ–ª–µ–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–π</td></tr>
      <tr><td>–°–∫–æ—Ä–æ—Å—Ç—å</td><td>–ú–æ–∂–µ—Ç –±—ã—Å—Ç—Ä–µ–µ</td><td>–ú–æ–∂–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–µ–µ</td></tr>
      <tr><td>–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</td><td>–ú–µ–Ω–µ–µ —Å—Ç–∞–±–∏–ª–µ–Ω</td><td>–ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª–µ–Ω</td></tr>
      <tr><td>Cliff Walking</td><td>–ò–¥—ë—Ç –±–ª–∏–∑–∫–æ –∫ –æ–±—Ä—ã–≤—É</td><td>–ò–¥—ë—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –ø—É—Ç–µ–º</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ü—Ä–∏–º–µ—Ä: Cliff Walking</h2>
    <pre><code>import gym

# –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã
env = gym.make('CliffWalking-v0')

# Q-learning –∞–≥–µ–Ω—Ç
q_agent = QLearningAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    alpha=0.5,
    gamma=0.99,
    epsilon=0.1
)

# SARSA –∞–≥–µ–Ω—Ç
sarsa_agent = SARSAAgent(
    n_states=env.observation_space.n,
    n_actions=env.action_space.n,
    alpha=0.5,
    gamma=0.99,
    epsilon=0.1
)

# –û–±—É—á–µ–Ω–∏–µ
q_rewards = q_agent.train(env, episodes=500)
sarsa_rewards = sarsa_agent.train(env, episodes=500)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(q_rewards, label='Q-learning', alpha=0.7)
plt.plot(sarsa_rewards, label='SARSA', alpha=0.7)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Training Performance')
plt.legend()
plt.grid(alpha=0.3)

# –°–≥–ª–∞–∂–µ–Ω–Ω—ã–µ –∫—Ä–∏–≤—ã–µ
window = 20
q_smooth = np.convolve(q_rewards, np.ones(window)/window, mode='valid')
sarsa_smooth = np.convolve(sarsa_rewards, np.ones(window)/window, mode='valid')

plt.subplot(1, 2, 2)
plt.plot(q_smooth, label='Q-learning (smoothed)', linewidth=2)
plt.plot(sarsa_smooth, label='SARSA (smoothed)', linewidth=2)
plt.xlabel('Episode')
plt.ylabel('Average Reward')
plt.title(f'Smoothed Performance (window={window})')
plt.legend()
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –∏—Ö –≤–ª–∏—è–Ω–∏–µ</h2>
    <ul>
      <li><strong>Learning rate (Œ±)</strong>:
        <ul>
          <li>–í—ã—Å–æ–∫–∏–π (0.5-1.0): –±—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–º</li>
          <li>–ù–∏–∑–∫–∏–π (0.01-0.1): –º–µ–¥–ª–µ–Ω–Ω–æ–µ, –Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</li>
          <li>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: 0.1-0.5, decay —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º</li>
        </ul>
      </li>
      <li><strong>Discount factor (Œ≥)</strong>:
        <ul>
          <li>–ë–ª–∏–∑–∫–æ –∫ 1 (0.95-0.99): —É—á–∏—Ç—ã–≤–∞–µ—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã</li>
          <li>–ë–ª–∏–∑–∫–æ –∫ 0: —Ñ–æ–∫—É—Å –Ω–∞ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥–∞—Ö</li>
          <li>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: 0.95-0.99</li>
        </ul>
      </li>
      <li><strong>Exploration rate (Œµ)</strong>:
        <ul>
          <li>–ù–∞—á–∞–ª–æ: 0.5-1.0 (–º–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è)</li>
          <li>–ö–æ–Ω–µ—Ü: 0.01-0.05 (—ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è)</li>
          <li>–°—Ç—Ä–∞—Ç–µ–≥–∏—è: —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π decay</li>
        </ul>
      </li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 9. –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ exploration</h2>
    <pre><code># 1. Œµ-greedy (–æ—Å–Ω–æ–≤–Ω–∞—è)
def epsilon_greedy(Q, state, epsilon):
    if np.random.rand() < epsilon:
        return np.random.randint(len(Q[state]))
    return np.argmax(Q[state])

# 2. Softmax (Boltzmann)
def softmax_action(Q, state, temperature=1.0):
    q_values = Q[state]
    exp_q = np.exp(q_values / temperature)
    probs = exp_q / np.sum(exp_q)
    return np.random.choice(len(q_values), p=probs)

# 3. UCB (Upper Confidence Bound)
def ucb_action(Q, state, counts, total_count, c=2.0):
    q_values = Q[state]
    counts_state = counts[state]
    
    ucb_values = q_values + c * np.sqrt(
        np.log(total_count + 1) / (counts_state + 1)
    )
    return np.argmax(ucb_values)

# 4. Decay epsilon
def decay_epsilon(epsilon, episode, decay_rate=0.995, 
                 min_epsilon=0.01):
    return max(min_epsilon, epsilon * decay_rate)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –û—Ü–µ–Ω–∫–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Q-table</h2>
    <pre><code># –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Q-table
def visualize_q_table(Q, env_shape=(4, 12)):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Q-table –¥–ª—è grid world"""
    fig, axes = plt.subplots(1, 4, figsize=(16, 4))
    actions = ['Up', 'Right', 'Down', 'Left']
    
    for action in range(4):
        q_grid = Q[:, action].reshape(env_shape)
        im = axes[action].imshow(q_grid, cmap='RdYlGn')
        axes[action].set_title(f'Q-values: {actions[action]}')
        plt.colorbar(im, ax=axes[action])
    
    plt.tight_layout()
    plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏
def visualize_policy(Q, env_shape=(4, 12)):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏"""
    policy = np.argmax(Q, axis=1).reshape(env_shape)
    
    arrows = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']
    fig, ax = plt.subplots(figsize=(12, 4))
    
    for i in range(env_shape[0]):
        for j in range(env_shape[1]):
            state = i * env_shape[1] + j
            action = policy[i, j]
            ax.text(j, i, arrows[action],
                   ha='center', va='center', fontsize=20)
    
    ax.set_xlim(-0.5, env_shape[1]-0.5)
    ax.set_ylim(-0.5, env_shape[0]-0.5)
    ax.set_xticks(range(env_shape[1]))
    ax.set_yticks(range(env_shape[0]))
    ax.grid(True)
    ax.set_title('Optimal Policy')
    ax.invert_yaxis()
    plt.show()

# –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
def evaluate_agent(agent, env, n_episodes=100):
    total_rewards = []
    
    for _ in range(n_episodes):
        state = env.reset()
        total_reward = 0
        done = False
        
        while not done:
            # –ñ–∞–¥–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ (–±–µ–∑ exploration)
            action = np.argmax(agent.Q[state])
            state, reward, done, _ = env.step(action)
            total_reward += reward
        
        total_rewards.append(total_reward)
    
    return {
        'mean': np.mean(total_rewards),
        'std': np.std(total_rewards),
        'min': np.min(total_rewards),
        'max': np.max(total_rewards)
    }</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Q-learning vs SARSA</h2>
    <blockquote>
      <strong>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Q-learning –∫–æ–≥–¥–∞:</strong>
      <ul>
        <li>–ù—É–∂–Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞</li>
        <li>–ú–æ–∂–µ—Ç–µ –ø–æ–∑–≤–æ–ª–∏—Ç—å —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è</li>
        <li>–°—Ä–µ–¥–∞ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∞</li>
        <li>–ë—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏</li>
      </ul>
      <strong>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ SARSA –∫–æ–≥–¥–∞:</strong>
      <ul>
        <li>–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞ (cliff walking, —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞)</li>
        <li>–°—Ä–µ–¥–∞ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∞</li>
        <li>–ù—É–∂–Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞</li>
        <li>–í–∞–∂–Ω–æ –∏–∑–±–µ–≥–∞—Ç—å —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏–π</li>
      </ul>
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 12. –†–∞—Å—à–∏—Ä–µ–Ω–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>Expected SARSA</strong>: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ Q(s',a') –≤–º–µ—Å—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ</li>
      <li><strong>Double Q-learning</strong>: –¥–≤–µ Q-table –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∏</li>
      <li><strong>N-step SARSA/Q-learning</strong>: —É—á–∏—Ç—ã–≤–∞–µ—Ç N —à–∞–≥–æ–≤ –≤–ø–µ—Ä–µ–¥</li>
      <li><strong>Eligibility Traces</strong>: SARSA(Œª), Q(Œª)</li>
      <li><strong>Deep Q-Network (DQN)</strong>: –Ω–µ–π—Ä–æ—Å–µ—Ç—å –≤–º–µ—Å—Ç–æ Q-table</li>
    </ul>
    <pre><code># Expected SARSA
def expected_sarsa_update(Q, state, action, reward, 
                         next_state, epsilon, alpha, gamma):
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ Q-value
    n_actions = len(Q[next_state])
    expected_q = 0
    
    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
    for a in range(n_actions):
        if a == np.argmax(Q[next_state]):
            prob = 1 - epsilon + epsilon / n_actions
        else:
            prob = epsilon / n_actions
        expected_q += prob * Q[next_state, a]
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ
    td_target = reward + gamma * expected_q
    Q[state, action] += alpha * (td_target - Q[state, action])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ol>
      <li>‚úÖ –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –¥–µ–π—Å—Ç–≤–∏–π</li>
      <li>‚úÖ –í—ã–±—Ä–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º (Q-learning –∏–ª–∏ SARSA)</li>
      <li>‚úÖ –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (Œ±, Œ≥, Œµ)</li>
      <li>‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Q-table</li>
      <li>‚úÖ –í—ã–±—Ä–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é exploration</li>
      <li>‚úÖ –û–±—É—á–∏—Ç—å –∞–≥–µ–Ω—Ç–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —á–∏—Å–ª–æ —ç–ø–∏–∑–æ–¥–æ–≤</li>
      <li>‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Q-values –∏ –ø–æ–ª–∏—Ç–∏–∫—É</li>
      <li>‚úÖ –û—Ü–µ–Ω–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å</li>
      <li>‚úÖ –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏</li>
      <li>‚úÖ –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è</li>
    </ol>
  </div>

</div>

</div>
</body>
</html>

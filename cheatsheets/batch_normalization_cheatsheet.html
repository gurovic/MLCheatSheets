<!DOCTYPE html><html lang="ru"><head><meta charset="UTF-8"><title>Batch Normalization Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title><style>@media screen{body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Helvetica,Arial,sans-serif;color:#333;background:#fafcff;padding:10px}}@media print{body{background:white;padding:0}@page{size:A4 landscape;margin:10mm}}.container{column-count:3;column-gap:20px;max-width:100%}.block{break-inside:avoid;margin-bottom:1.2em;padding:12px;background:white;border-radius:6px;box-shadow:0 1px 3px rgba(0,0,0,0.05)}h1{font-size:1.6em;font-weight:700;color:#1a5fb4;text-align:center;margin:0 0 8px;column-span:all}.subtitle{text-align:center;color:#666;font-size:0.9em;margin-bottom:12px;column-span:all}h2{font-size:1.15em;font-weight:700;color:#1a5fb4;margin:0 0 8px;padding-bottom:4px;border-bottom:1px solid #e0e7ff}p,ul,ol{font-size:0.92em;margin:0.6em 0}ul,ol{padding-left:18px}li{margin-bottom:4px}code{font-family:'Consolas','Courier New',monospace;background-color:#f0f4ff;padding:1px 4px;border-radius:3px;font-size:0.88em}pre{background-color:#f0f4ff;padding:8px;border-radius:4px;overflow-x:auto;font-size:0.84em;margin:6px 0}pre code{padding:0;background:none;white-space:pre-wrap}table{width:100%;border-collapse:collapse;font-size:0.82em;margin:6px 0}th{background-color:#e6f0ff;text-align:left;padding:4px 6px;font-weight:600}td{padding:4px 6px;border-bottom:1px solid #f0f4ff}tr:nth-child(even){background-color:#f8fbff}.good-vs-bad{display:flex;flex-direction:column;gap:8px}.good-vs-bad div{flex:1;padding:6px 8px;border-radius:4px}.good{background-color:#f0f9f4;border-left:3px solid #2e8b57}.bad{background-color:#fdf0f2;border-left:3px solid #d32f2f}.good h3,.bad h3{margin:0 0 4px;font-size:1em;font-weight:700}.good ul,.bad ul{padding-left:20px;margin:0}.good li::before{content:"‚úÖ ";font-weight:bold}.bad li::before{content:"‚ùå ";font-weight:bold}blockquote{font-style:italic;margin:8px 0;padding:6px 10px;background:#f8fbff;border-left:2px solid #1a5fb4;font-size:0.88em}.formula{text-align:center;font-family:'Cambria Math',serif;font-size:1em;margin:10px 0;padding:8px;background:#f8fbff;border-radius:4px}@media print{.container{column-gap:12px}.block{box-shadow:none}code,pre,table{font-size:0.78em}h1{font-size:1.4em}h2{font-size:1em}}</style></head><body><div class="container"><h1>üîÑ Batch Normalization Cheatsheet</h1><div class="subtitle">–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ‚Ä¢ –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è ‚Ä¢ –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è<br>üìÖ 3 —è–Ω–≤–∞—Ä—è 2026</div><div class="block"><h2>üî∑ 1. –°—É—Ç—å</h2><ul><li><strong>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π</strong>: –ø–æ –±–∞—Ç—á—É –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è</li><li><strong>–£—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ</strong>: –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ learning rate</li><li><strong>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</strong>: —É–º–µ–Ω—å—à–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</li><li><strong>–°—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç</strong>: —É–º–µ–Ω—å—à–∞–µ—Ç internal covariate shift</li></ul><div class="formula">BN(x) = Œ≥ ¬∑ (x - Œº) / ‚àö(œÉ¬≤ + Œµ) + Œ≤</div></div><div class="block"><h2>üî∑ 2. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ (PyTorch)</h2><pre><code>import torch.nn as nn

class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)  # –ø–æ—Å–ª–µ conv
        self.relu = nn.ReLU()
        
        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)      # BN –ø–æ—Å–ª–µ conv
        x = self.relu(x)     # activation –ø–æ—Å–ª–µ BN
        
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        return x</code></pre></div><div class="block"><h2>üî∑ 3. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ (TensorFlow/Keras)</h2><pre><code>from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation

model = tf.keras.Sequential([
    Conv2D(64, 3, padding='same'),
    BatchNormalization(),  # –ø–æ—Å–ª–µ conv
    Activation('relu'),     # activation –ø–æ—Å–ª–µ BN
    
    Conv2D(128, 3, padding='same'),
    BatchNormalization(),
    Activation('relu'),
    
    Dense(10),
    BatchNormalization(),
    Activation('softmax')
])</code></pre></div><div class="block"><h2>üî∑ 4. –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç BN</h2><p><strong>Training —Ä–µ–∂–∏–º:</strong></p><ul><li>1. –í—ã—á–∏—Å–ª—è–µ–º Œº –∏ œÉ –ø–æ –±–∞—Ç—á—É</li><li>2. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º: xÃÇ = (x - Œº) / ‚àö(œÉ¬≤ + Œµ)</li><li>3. –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∏ —Å–¥–≤–∏–≥–∞–µ–º: y = Œ≥¬∑xÃÇ + Œ≤</li><li>4. –û–±–Ω–æ–≤–ª—è–µ–º running statistics</li></ul><p><strong>Inference —Ä–µ–∂–∏–º:</strong></p><ul><li>–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ running mean/var</li><li>–ù–µ –≤—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –±–∞—Ç—á—É</li></ul></div><div class="block"><h2>üî∑ 5. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã BN</h2><table><tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–ó–Ω–∞—á–µ–Ω–∏–µ</th></tr><tr><td><code>Œ≥ (gamma)</code></td><td>–ú–∞—Å—à—Ç–∞–± (learnable)</td><td>–û–±—É—á–∞–µ—Ç—Å—è</td></tr><tr><td><code>Œ≤ (beta)</code></td><td>–°–¥–≤–∏–≥ (learnable)</td><td>–û–±—É—á–∞–µ—Ç—Å—è</td></tr><tr><td><code>Œº (mean)</code></td><td>–°—Ä–µ–¥–Ω–µ–µ (running)</td><td>–û–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏ training</td></tr><tr><td><code>œÉ¬≤ (variance)</code></td><td>–î–∏—Å–ø–µ—Ä—Å–∏—è (running)</td><td>–û–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏ training</td></tr><tr><td><code>Œµ (epsilon)</code></td><td>–î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏</td><td>1e-5 –æ–±—ã—á–Ω–æ</td></tr><tr><td><code>momentum</code></td><td>–î–ª—è running stats</td><td>0.1 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é</td></tr></table></div><div class="block"><h2>üî∑ 6. –ö—É–¥–∞ —Å—Ç–∞–≤–∏—Ç—å BN</h2><pre><code># –í–∞—Ä–∏–∞–Ω—Ç 1: –ü–æ—Å–ª–µ —Å–ª–æ—è, –ø–µ—Ä–µ–¥ –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
x = conv(x)
x = bn(x)
x = relu(x)

# –í–∞—Ä–∏–∞–Ω—Ç 2: –ü–æ—Å–ª–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (—Å—Ç–∞—Ä—ã–π –ø–æ–¥—Ö–æ–¥)
x = conv(x)
x = relu(x)
x = bn(x)

# –î–ª—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ–µ–≤
x = linear(x)
x = bn(x)
x = relu(x)</code></pre></div><div class="block"><h2>üî∑ 7. Training vs Evaluation —Ä–µ–∂–∏–º—ã</h2><pre><code># PyTorch
model.train()  # training —Ä–µ–∂–∏–º
output = model(x)  # –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∞—Ç—á–µ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É

model.eval()   # evaluation —Ä–µ–∂–∏–º
with torch.no_grad():
    output = model(x)  # –∏—Å–ø–æ–ª—å–∑—É–µ—Ç running —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É

# Keras
model.fit(X_train, y_train)  # –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ training=True

model.predict(X_test)  # –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ training=False</code></pre></div><div class="block"><h2>üî∑ 8. BN –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–µ–≤</h2><table><tr><th>–¢–∏–ø —Å–ª–æ—è</th><th>BN —Å–ª–æ–π</th><th>PyTorch</th></tr><tr><td>Fully Connected</td><td>BatchNorm1d</td><td>nn.BatchNorm1d(features)</td></tr><tr><td>Conv2D</td><td>BatchNorm2d</td><td>nn.BatchNorm2d(channels)</td></tr><tr><td>Conv3D</td><td>BatchNorm3d</td><td>nn.BatchNorm3d(channels)</td></tr><tr><td>RNN</td><td>–ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è</td><td>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ LayerNorm</td></tr></table></div><div class="block"><h2>üî∑ 9. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ BN</h2><div class="good-vs-bad"><div class="good"><h3>‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</h3><ul><li>–£—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ (2-10x)</li><li>–ü–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–µ learning rate</li><li>–£–º–µ–Ω—å—à–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏</li><li>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (–º–æ–∂–µ—Ç –∑–∞–º–µ–Ω–∏—Ç—å dropout)</li><li>–°—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ</li></ul></div><div class="bad"><h3>‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h3><ul><li>–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç batch size</li><li>–ü–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –º–∞–ª—ã–º batch</li><li>–ü—Ä–æ–±–ª–µ–º—ã —Å RNN</li><li>–£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –ø–∞–º—è—Ç—å</li><li>–£—Å–ª–æ–∂–Ω—è–µ—Ç distributed training</li></ul></div></div></div><div class="block"><h2>üî∑ 10. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2><div class="good-vs-bad"><div class="good"><h3>‚úÖ –•–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥–∏—Ç</h3><ul><li>CNN –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</li><li>–ì–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏ (>10 —Å–ª–æ–µ–≤)</li><li>–ë–æ–ª—å—à–æ–π batch size (‚â•32)</li><li>Feedforward —Å–µ—Ç–∏</li></ul></div><div class="bad"><h3>‚ùå –ü–ª–æ—Ö–æ –ø–æ–¥—Ö–æ–¥–∏—Ç</h3><ul><li>–ú–∞–ª—ã–π batch size (<16)</li><li>RNN/LSTM (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ LayerNorm)</li><li>Online learning (batch=1)</li><li>Transfer learning (–∏–Ω–æ–≥–¥–∞)</li></ul></div></div></div><div class="block"><h2>üî∑ 11. BN vs Layer Norm vs Instance Norm</h2><table><tr><th>–ú–µ—Ç–æ–¥</th><th>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr><tr><td>Batch Norm</td><td>–ë–∞—Ç—á—É –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É</td><td>CNN, –±–æ–ª—å—à–æ–π batch</td></tr><tr><td>Layer Norm</td><td>–ü—Ä–∏–∑–Ω–∞–∫–∞–º</td><td>RNN, Transformer, –º–∞–ª—ã–π batch</td></tr><tr><td>Instance Norm</td><td>–ö–∞–∂–¥–æ–º—É –ø—Ä–∏–º–µ—Ä—É</td><td>Style transfer, GAN</td></tr><tr><td>Group Norm</td><td>–ì—Ä—É–ø–ø–∞–º –∫–∞–Ω–∞–ª–æ–≤</td><td>–ú–∞–ª—ã–π batch, detection</td></tr></table></div><div class="block"><h2>üî∑ 12. –ü—Ä–æ–±–ª–µ–º–∞ –º–∞–ª–æ–≥–æ batch size</h2><pre><code># –ü–ª–æ—Ö–æ: batch=2, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–∞
batch_size = 2
bn = nn.BatchNorm2d(64)

# –†–µ—à–µ–Ω–∏—è:
# 1. –£–≤–µ–ª–∏—á–∏—Ç—å batch size
batch_size = 32

# 2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Group Norm
gn = nn.GroupNorm(num_groups=8, num_channels=64)

# 3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Layer Norm
ln = nn.LayerNorm(normalized_shape)</code></pre></div><div class="block"><h2>üî∑ 13. BN + Dropout</h2><pre><code># –ú–æ–∂–Ω–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å, –Ω–æ –æ–±—ã—á–Ω–æ BN –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
x = conv(x)
x = bn(x)
x = relu(x)
x = dropout(x, p=0.2)  # –ø–æ—Å–ª–µ BN –∏ activation

# –ò–ª–∏ —Ç–æ–ª—å–∫–æ BN (–æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)
x = conv(x)
x = bn(x)
x = relu(x)</code></pre></div><div class="block"><h2>üî∑ 14. –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ BN –ø—Ä–∏ fine-tuning</h2><pre><code># PyTorch: –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å BN –ø—Ä–∏ transfer learning
for module in model.modules():
    if isinstance(module, nn.BatchNorm2d):
        module.eval()  # –≤—Å–µ–≥–¥–∞ –≤ eval —Ä–µ–∂–∏–º–µ
        # –ò–ª–∏ –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        for param in module.parameters():
            param.requires_grad = False

# TensorFlow/Keras
for layer in model.layers:
    if isinstance(layer, tf.keras.layers.BatchNormalization):
        layer.trainable = False</code></pre></div><div class="block"><h2>üî∑ 15. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2><ul><li><strong>–í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤ CNN</strong>: –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</li><li><strong>–ü–æ—Å–ª–µ conv, –ø–µ—Ä–µ–¥ activation</strong>: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫</li><li><strong>Batch size ‚â• 16</strong>: –º–∏–Ω–∏–º—É–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏</li><li><strong>–ù–µ –∑–∞–±—ã–≤–∞–π—Ç–µ .eval()</strong>: –ø—Ä–∏ inference</li><li><strong>–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–π—Ç–µ –ø—Ä–∏ fine-tuning</strong>: –µ—Å–ª–∏ –Ω—É–∂–Ω–æ</li><li><strong>–î–ª—è RNN –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ LayerNorm</strong></li></ul></div><div class="block"><h2>üî∑ 16. –ß–µ–∫-–ª–∏—Å—Ç</h2><ul><li>[ ] –î–æ–±–∞–≤–∏—Ç—å BN –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ conv/linear —Å–ª–æ—è</li><li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫: layer ‚Üí BN ‚Üí activation</li><li>[ ] –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ batch_size ‚â• 16</li><li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å model.train() –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏</li><li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å model.eval() –ø—Ä–∏ inference</li><li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ learning rate</li><li>[ ] –°—Ä–∞–≤–Ω–∏—Ç—å —Å/–±–µ–∑ BN</li></ul><blockquote>¬´Batch Normalization ‚Äî –æ–¥–∏–Ω –∏–∑ –≤–∞–∂–Ω–µ–π—à–∏—Ö –ø—Ä–æ—Ä—ã–≤–æ–≤ –≤ –≥–ª—É–±–æ–∫–æ–º –æ–±—É—á–µ–Ω–∏–∏. –£—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ, —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –µ–≥–æ –∏ –¥–µ–π—Å—Ç–≤—É–µ—Ç –∫–∞–∫ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä. Must-have –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö CNN¬ª.</blockquote></div><div class="block"><h2>üîó –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏</h2><ul><li><a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html" target="_blank">üìö PyTorch: BatchNorm</a></li><li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization" target="_blank">üìö TensorFlow: BatchNormalization</a></li><li><a href="https://arxiv.org/abs/1502.03167" target="_blank">üìÑ Original Batch Norm Paper</a></li></ul></div></div></body></html>

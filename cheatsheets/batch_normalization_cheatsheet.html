<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Batch Normalization Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ Batch Normalization</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: Internal Covariate Shift ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –º–µ–Ω—è–µ—Ç—Å—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏</li>
      <li><strong>–†–µ—à–µ–Ω–∏–µ</strong>: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –±–∞—Ç—á—É</li>
      <li><strong>–ì–¥–µ</strong>: –º–µ–∂–¥—É —Å–ª–æ—è–º–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏</li>
      <li><strong>–≠—Ñ—Ñ–µ–∫—Ç</strong>: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è, —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è, —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</li>
      <li><strong>–ê–≤—Ç–æ—Ä</strong>: Sergey Ioffe, Christian Szegedy (2015)</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)</h2>
    <p>–î–ª—è –±–∞—Ç—á–∞ —Ä–∞–∑–º–µ—Ä–æ–º m:</p>
    <ol>
      <li><strong>–°—Ä–µ–¥–Ω–µ–µ –ø–æ –±–∞—Ç—á—É</strong>: Œº = (1/m) Œ£ x·µ¢</li>
      <li><strong>–î–∏—Å–ø–µ—Ä—Å–∏—è –ø–æ –±–∞—Ç—á—É</strong>: œÉ¬≤ = (1/m) Œ£ (x·µ¢ - Œº)¬≤</li>
      <li><strong>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è</strong>: xÃÇ·µ¢ = (x·µ¢ - Œº) / ‚àö(œÉ¬≤ + Œµ)</li>
      <li><strong>–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–¥–≤–∏–≥</strong>: y·µ¢ = Œ≥ xÃÇ·µ¢ + Œ≤</li>
    </ol>
    <p><strong>Œ≥, Œ≤</strong> ‚Äî –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã<br><strong>Œµ</strong> ‚Äî –º–∞–ª–µ–Ω—å–∫–∞—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (–æ–±—ã—á–Ω–æ 1e-5)</p>
  </div>

  <div class="block">
    <h2>üî∑ 3. PyTorch (–±–∞–∑–æ–≤—ã–π)</h2>
    <pre><code>import torch.nn as nn

# –î–ª—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ—ë–≤
model = nn.Sequential(
    nn.Linear(784, 256),
    nn.BatchNorm1d(256),  # BatchNorm –ü–û–°–õ–ï –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è
    nn.ReLU(),
    
    nn.Linear(256, 128),
    nn.BatchNorm1d(128),
    nn.ReLU(),
    
    nn.Linear(128, 10)
)

# –î–ª—è —Å–≤—ë—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—ë–≤
cnn_model = nn.Sequential(
    nn.Conv2d(3, 64, 3, padding=1),
    nn.BatchNorm2d(64),  # BatchNorm2d –¥–ª—è 2D
    nn.ReLU(),
    
    nn.Conv2d(64, 128, 3, padding=1),
    nn.BatchNorm2d(128),
    nn.ReLU()
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Keras/TensorFlow</h2>
    <pre><code>from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Dense(256, input_shape=(784,)),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    
    layers.Dense(128),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    
    layers.Dense(10, activation='softmax')
])

# –î–ª—è CNN
cnn_model = models.Sequential([
    layers.Conv2D(64, 3, padding='same', input_shape=(28, 28, 1)),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    
    layers.Conv2D(128, 3, padding='same'),
    layers.BatchNormalization(),
    layers.Activation('relu')
])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ü–æ—Ä—è–¥–æ–∫ —Å–ª–æ—ë–≤ (–¥–µ–±–∞—Ç—ã!)</h2>
    <p><strong>–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—è–¥–æ–∫ (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç—å—è)</strong>:</p>
    <pre><code>Linear ‚Üí BatchNorm ‚Üí Activation</code></pre>
    
    <p><strong>–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ (–∏–Ω–æ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ)</strong>:</p>
    <pre><code>Linear ‚Üí Activation ‚Üí BatchNorm</code></pre>
    
    <p><strong>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è</strong>: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—è–¥–æ–∫, –µ—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω—ã</p>
  </div>

  <div class="block">
    <h2>üî∑ 6. Train vs Eval —Ä–µ–∂–∏–º—ã</h2>
    <p><strong>–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!</strong></p>
    <table>
      <tr><th>–†–µ–∂–∏–º</th><th>–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏</th><th>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</th></tr>
      <tr><td><strong>Train</strong></td><td>Œº, œÉ¬≤ –ø–æ —Ç–µ–∫—É—â–µ–º—É –±–∞—Ç—á—É</td><td>–û–±–Ω–æ–≤–ª—è—é—Ç—Å—è running mean/var</td></tr>
      <tr><td><strong>Eval</strong></td><td>running mean/var</td><td>–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã</td></tr>
    </table>
    <pre><code># PyTorch
model.train()   # –î–ª—è –æ–±—É—á–µ–Ω–∏—è
model.eval()    # –î–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞

# TensorFlow/Keras
model.fit(...)  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ training=True
model.predict(...) # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ training=False

# –Ø–≤–Ω–æ:
output = model(x, training=True)  # Train mode
output = model(x, training=False) # Eval mode</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</h2>
    <ul>
      <li><strong>–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è</strong>: –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–π learning rate</li>
      <li><strong>–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</strong>: –º–µ–Ω—å—à–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li><strong>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</strong>: –¥–æ–±–∞–≤–ª—è–µ—Ç —à—É–º —á–µ—Ä–µ–∑ –±–∞—Ç—á-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏</li>
      <li><strong>–ú–µ–Ω—å—à–µ dropout</strong>: –º–æ–∂–µ—Ç —á–∞—Å—Ç–∏—á–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å dropout</li>
      <li><strong>–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã</strong>: —É–º–µ–Ω—å—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∏—Å—á–µ–∑–∞—é—â–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 8. –í–∞–∂–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã</h2>
    <table>
      <tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é</th></tr>
      <tr><td><code>momentum</code></td><td>–î–ª—è running mean/var</td><td>0.1 (PyTorch), 0.99 (TF)</td></tr>
      <tr><td><code>eps</code></td><td>–î–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏</td><td>1e-5</td></tr>
      <tr><td><code>affine</code></td><td>–û–±—É—á–∞—Ç—å Œ≥ –∏ Œ≤?</td><td>True</td></tr>
      <tr><td><code>track_running_stats</code></td><td>–û—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å running stats?</td><td>True</td></tr>
    </table>
    <pre><code># PyTorch —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
bn = nn.BatchNorm1d(
    num_features=256,
    eps=1e-5,
    momentum=0.1,
    affine=True
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ü—Ä–æ–±–ª–µ–º—ã —Å –º–∞–ª—ã–º–∏ –±–∞—Ç—á–∞–º–∏</h2>
    <p><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: –ø—Ä–∏ –º–∞–ª–æ–º batch size —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–µ—Ç–æ—á–Ω—ã</p>
    <p><strong>–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã</strong>:</p>
    <ul>
      <li><strong>Layer Normalization</strong>: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º, –Ω–µ –ø–æ –±–∞—Ç—á—É</li>
      <li><strong>Group Normalization</strong>: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –≥—Ä—É–ø–ø–∞–º –∫–∞–Ω–∞–ª–æ–≤</li>
      <li><strong>Instance Normalization</strong>: –¥–ª—è style transfer</li>
    </ul>
    <pre><code># PyTorch
nn.LayerNorm(normalized_shape)
nn.GroupNorm(num_groups, num_channels)
nn.InstanceNorm2d(num_features)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Layer Normalization</h2>
    <pre><code># Layer Norm ‚Äî –¥–ª—è RNN, Transformers
import torch.nn as nn

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º (–Ω–µ –ø–æ –±–∞—Ç—á—É!)
layer_norm = nn.LayerNorm(hidden_size)

# –ü—Ä–∏–º–µ—Ä –≤ Transformer
class TransformerBlock(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.attention = MultiHeadAttention(d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.ffn = FeedForward(d_model)
        self.norm2 = nn.LayerNorm(d_model)
    
    def forward(self, x):
        # Residual + Norm
        x = x + self.attention(self.norm1(x))
        x = x + self.ffn(self.norm2(x))
        return x</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–π</h2>
    <table>
      <tr><th>–¢–∏–ø</th><th>–ü–æ —á–µ–º—É –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>Batch Norm</strong></td><td>–ü–æ –±–∞—Ç—á—É –∏ –ø–∏–∫—Å–µ–ª—è–º</td><td>CNN (–±–æ–ª—å—à–∏–µ –±–∞—Ç—á–∏)</td></tr>
      <tr><td><strong>Layer Norm</strong></td><td>–ü–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º</td><td>RNN, Transformers</td></tr>
      <tr><td><strong>Instance Norm</strong></td><td>–ü–æ –∫–∞–∂–¥–æ–º—É –ø—Ä–∏–º–µ—Ä—É</td><td>Style transfer</td></tr>
      <tr><td><strong>Group Norm</strong></td><td>–ü–æ –≥—Ä—É–ø–ø–∞–º –∫–∞–Ω–∞–ª–æ–≤</td><td>–ú–∞–ª—ã–µ –±–∞—Ç—á–∏</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ BatchNorm</h3>
        <ul>
          <li>CNN —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º batch size (‚â•16)</li>
          <li>–ì–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏</li>
          <li>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</li>
          <li>–î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤</li>
          <li>–ù—É–∂–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ BatchNorm</h3>
        <ul>
          <li>–ú–∞–ª—ã–π batch size (&lt;8)</li>
          <li>RNN (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ LayerNorm)</li>
          <li>Online learning (–ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–º–µ—Ä—É)</li>
          <li>–°–∏–ª—å–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –±–∞—Ç—á–∞ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–∞</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 13. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏</h2>
    <ul>
      <li><strong>–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ</strong>: –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞–π—Ç–µ train/eval —Ä–µ–∂–∏–º</li>
      <li><strong>Dropout</strong>: –º–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout —Å BatchNorm</li>
      <li><strong>Learning rate</strong>: –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –≤ 2-10 —Ä–∞–∑</li>
      <li><strong>Batch size</strong>: –º–∏–Ω–∏–º—É–º 8-16 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏</li>
      <li><strong>–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è</strong>: –º–µ–Ω–µ–µ –∫—Ä–∏—Ç–∏—á–Ω–∞ —Å BatchNorm</li>
      <li><strong>–ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å bias</strong>: –≤ —Å–ª–æ–µ –ø–µ—Ä–µ–¥ BatchNorm (–æ–Ω –≤—Å—ë —Ä–∞–≤–Ω–æ –≤—ã—á—Ç–µ—Ç—Å—è)</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 14. –û—Ç–∫–ª—é—á–µ–Ω–∏–µ bias</h2>
    <pre><code># PyTorch: –æ—Ç–∫–ª—é—á–∞–µ–º bias –ø–µ—Ä–µ–¥ BatchNorm
nn.Linear(784, 256, bias=False)  # –ù–µ—Ç bias
nn.BatchNorm1d(256)              # Œ≤ –∑–∞–º–µ–Ω—è–µ—Ç bias

nn.Conv2d(3, 64, 3, bias=False)  # –ù–µ—Ç bias
nn.BatchNorm2d(64)               # Œ≤ –∑–∞–º–µ–Ω—è–µ—Ç bias

# Keras/TensorFlow
layers.Dense(256, use_bias=False)
layers.BatchNormalization()

layers.Conv2D(64, 3, use_bias=False)
layers.BatchNormalization()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –î–æ–±–∞–≤–∏—Ç—å BatchNorm –ø–æ—Å–ª–µ –ª–∏–Ω–µ–π–Ω—ã—Ö/—Å–≤—ë—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—ë–≤</li>
      <li>[ ] –£–±–µ–¥–∏—Ç—å—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ: Linear ‚Üí BN ‚Üí Activation</li>
      <li>[ ] –û—Ç–∫–ª—é—á–∏—Ç—å bias –≤ —Å–ª–æ–µ –ø–µ—Ä–µ–¥ BatchNorm</li>
      <li>[ ] –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å train/eval —Ä–µ–∂–∏–º—ã</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π batch size (‚â•8-16)</li>
      <li>[ ] –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —É–≤–µ–ª–∏—á–∏—Ç—å learning rate</li>
      <li>[ ] –î–ª—è –º–∞–ª—ã—Ö –±–∞—Ç—á–µ–π ‚Äî —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å LayerNorm/GroupNorm</li>
      <li>[ ] –î–ª—è RNN/Transformers ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LayerNorm</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´Batch Normalization ‚Äî —ç—Ç–æ –∫–∞–∫ –µ—Å–ª–∏ –±—ã –º—ã –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–≤–æ–¥–∏–ª–∏ –∏—Ö –∫ –µ–¥–∏–Ω–æ–º—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –≤–∏–¥—É. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –æ–±—É—á–∞—Ç—å—Å—è –±—ã—Å—Ç—Ä–µ–µ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ –Ω–µ —Ç—Ä–∞—Ç–∏—Ç –≤—Ä–µ–º—è –Ω–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏—é –∫ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –º–µ–Ω—è—é—â–µ–º—É—Å—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

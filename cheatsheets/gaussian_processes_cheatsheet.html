<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–ì–∞—É—Å—Å–æ–≤—Å–∫–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.88em;
      margin: 6px 0;
    }

    th, td {
      padding: 6px 8px;
      text-align: left;
      border: 1px solid #e0e7ff;
    }

    th {
      background-color: #1a5fb4;
      color: white;
      font-weight: 700;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ –ì–∞—É—Å—Å–æ–≤—Å–∫–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã Gaussian Processes</h2>
    <p><strong>Gaussian Process (GP)</strong>: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</p>
    <ul>
      <li><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–¥ —Ñ—É–Ω–∫—Ü–∏—è–º–∏, –≥–¥–µ –ª—é–±–æ–µ –∫–æ–Ω–µ—á–Ω–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –∏–º–µ–µ—Ç –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</li>
      <li><strong>Mean function</strong>: m(x) = E[f(x)], —á–∞—Å—Ç–æ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è —Ä–∞–≤–Ω—ã–º 0</li>
      <li><strong>Covariance function (kernel)</strong>: k(x, x') = E[(f(x)-m(x))(f(x')-m(x'))]</li>
      <li><strong>–û–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ</strong>: f(x) ~ GP(m(x), k(x,x'))</li>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ</strong>: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç uncertainty (–¥–∏—Å–ø–µ—Ä—Å–∏—é)</li>
    </ul>
    <blockquote>
      üí° GP - –º–æ—â–Ω—ã–π –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –º–µ—Ç–æ–¥, –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–∏ –º–∞–ª–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö
    </blockquote>

  <div class="block">
    <h2>üî∑ 2. –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä —Ä–µ–≥—Ä–µ—Å—Å–∏–∏</h2>
    <pre><code>from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import numpy as np
import matplotlib.pyplot as plt

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
X_train = np.array([[1], [3], [5], [6], [8]])
y_train = np.sin(X_train).ravel()

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ kernel
kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, 
                                   length_scale_bounds=(1e-2, 1e2))

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
gp = GaussianProcessRegressor(
    kernel=kernel,
    n_restarts_optimizer=10,
    alpha=1e-2  # noise level
)

# –û–±—É—á–µ–Ω–∏–µ
gp.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å uncertainty
X_test = np.linspace(0, 10, 100).reshape(-1, 1)
y_pred, sigma = gp.predict(X_test, return_std=True)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(10, 6))
plt.plot(X_test, y_pred, 'b-', label='Prediction')
plt.fill_between(X_test.ravel(),
                y_pred - 1.96*sigma,
                y_pred + 1.96*sigma,
                alpha=0.3, label='95% confidence')
plt.scatter(X_train, y_train, c='red', s=100, 
           label='Training data')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Gaussian Process Regression')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

print(f"Learned kernel: {gp.kernel_}")
print(f"Log-marginal-likelihood: {gp.log_marginal_likelihood():.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. Kernel functions (—è–¥—Ä–∞)</h2>
    <p><strong>Kernel</strong> –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏:</p>
    <pre><code>from sklearn.gaussian_process.kernels import (
    RBF, Matern, RationalQuadratic, 
    ExpSineSquared, DotProduct, WhiteKernel
)

# 1. RBF (Radial Basis Function) / Squared Exponential
# –ì–ª–∞–¥–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
rbf = RBF(length_scale=1.0)

# 2. Matern kernel
# –ö–æ–Ω—Ç—Ä–æ–ª—å –≥–ª–∞–¥–∫–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä nu
matern = Matern(length_scale=1.0, nu=1.5)
# nu=0.5: –Ω–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞ (Ornstein-Uhlenbeck)
# nu=1.5: –æ–¥–∏–Ω —Ä–∞–∑ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞
# nu=2.5: –¥–≤–∞ —Ä–∞–∑–∞ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞
# nu‚Üí‚àû: —Å—Ö–æ–¥–∏—Ç—Å—è –∫ RBF

# 3. Rational Quadratic
# –°–º–µ—Å—å RBF —Å —Ä–∞–∑–Ω—ã–º–∏ length scales
rq = RationalQuadratic(length_scale=1.0, alpha=1.0)

# 4. Exp-Sine-Squared (–ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏)
periodic = ExpSineSquared(length_scale=1.0, 
                         periodicity=1.0)

# 5. Dot Product (–ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è)
linear = DotProduct(sigma_0=1.0)

# 6. White Kernel (—à—É–º)
white = WhiteKernel(noise_level=0.1)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ö–æ–º–ø–æ–∑–∏—Ü–∏–∏ kernels</h2>
    <pre><code># –°–ª–æ–∂–µ–Ω–∏–µ kernels (—Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π)
kernel_sum = RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)

# –£–º–Ω–æ–∂–µ–Ω–∏–µ kernels (–º–æ–¥—É–ª—è—Ü–∏—è)
kernel_product = C(1.0) * RBF(length_scale=1.0)

# –°–ª–æ–∂–Ω–∞—è –∫–æ–º–ø–æ–∑–∏—Ü–∏—è
# –¢—Ä–µ–Ω–¥ + –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å + —à—É–º
kernel_complex = (
    C(1.0) * RBF(length_scale=10.0)  # –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π —Ç—Ä–µ–Ω–¥
    + C(1.0) * RBF(length_scale=1.0)  # –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è
    * ExpSineSquared(length_scale=1.0, periodicity=1.0)  # –ø–µ—Ä–∏–æ–¥–∏–∫–∞
    + WhiteKernel(noise_level=0.01)  # —à—É–º
)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
gp = GaussianProcessRegressor(kernel=kernel_complex)
gp.fit(X_train, y_train)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

kernels = {
    'RBF': RBF(length_scale=1.0),
    'Matern': Matern(length_scale=1.0, nu=1.5),
    'Periodic': ExpSineSquared(length_scale=1.0, periodicity=1.0),
    'RationalQuadratic': RationalQuadratic(length_scale=1.0)
}

for idx, (name, kernel) in enumerate(kernels.items()):
    gp = GaussianProcessRegressor(kernel=kernel)
    gp.fit(X_train, y_train)
    y_pred, sigma = gp.predict(X_test, return_std=True)
    
    ax = axes[idx//2, idx%2]
    ax.plot(X_test, y_pred, label='Mean')
    ax.fill_between(X_test.ravel(), 
                    y_pred - 1.96*sigma,
                    y_pred + 1.96*sigma,
                    alpha=0.3)
    ax.scatter(X_train, y_train, c='red', s=50)
    ax.set_title(name)
    ax.legend()
    ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è</h2>
    <table>
      <tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</th></tr>
      <tr><td><code>kernel</code></td><td>–§—É–Ω–∫—Ü–∏—è –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏</td><td>RBF –¥–ª—è –Ω–∞—á–∞–ª–∞</td></tr>
      <tr><td><code>alpha</code></td><td>–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞</td><td>1e-10 –¥–æ 1.0</td></tr>
      <tr><td><code>optimizer</code></td><td>–ú–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏</td><td>'fmin_l_bfgs_b'</td></tr>
      <tr><td><code>n_restarts_optimizer</code></td><td>–ß–∏—Å–ª–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–≤</td><td>10-20</td></tr>
      <tr><td><code>normalize_y</code></td><td>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π</td><td>True –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–æ–≤</td></tr>
    </table>
    <pre><code># –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
from sklearn.model_selection import cross_val_score

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö kernels
kernels = [
    RBF(length_scale=1.0),
    Matern(length_scale=1.0, nu=1.5),
    RationalQuadratic(length_scale=1.0)
]

for kernel in kernels:
    gp = GaussianProcessRegressor(kernel=kernel,
                                  n_restarts_optimizer=10)
    scores = cross_val_score(gp, X_train, y_train, 
                            cv=5, scoring='neg_mean_squared_error')
    print(f"{kernel}: MSE = {-scores.mean():.4f} ¬± {scores.std():.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. GP –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</h2>
    <pre><code>from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=100, n_features=2,
                          n_redundant=0, n_informative=2,
                          random_state=42)

# –ú–æ–¥–µ–ª—å
kernel = C(1.0) * RBF(length_scale=1.0)
gpc = GaussianProcessClassifier(kernel=kernel, 
                                n_restarts_optimizer=20)

# –û–±—É—á–µ–Ω–∏–µ
gpc.fit(X, y)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
y_pred = gpc.predict(X_test)
y_proba = gpc.predict_proba(X_test)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–Ω–∏—Ü—ã —Ä–µ—à–µ–Ω–∏—è
xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100),
                     np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 100))

Z = gpc.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
Z = Z.reshape(xx.shape)

plt.figure(figsize=(10, 6))
plt.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.6)
plt.colorbar(label='P(y=1)')
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu',
           edgecolors='black', linewidths=1.5)
plt.title('GP Classification')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è</h2>
    <p><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ GP</strong>: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–æ—Ä–æ–≥–∏—Ö black-box —Ñ—É–Ω–∫—Ü–∏–π</p>
    <pre><code>from sklearn.gaussian_process import GaussianProcessRegressor
from scipy.optimize import minimize
from scipy.stats import norm

# –¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–¥–æ—Ä–æ–≥–∞—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è)
def objective(x):
    return -(x**2 * np.sin(5*x) + 0.5*x)

# Acquisition function (Expected Improvement)
def expected_improvement(X, gp, best_y, xi=0.01):
    mu, sigma = gp.predict(X, return_std=True)
    sigma = sigma.reshape(-1, 1)
    
    imp = mu - best_y - xi
    Z = imp / sigma
    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)
    ei[sigma == 0.0] = 0.0
    
    return ei

# –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
X_init = np.array([[0.0], [0.5], [1.0]])
y_init = objective(X_init).reshape(-1)

kernel = C(1.0) * Matern(length_scale=0.5, nu=2.5)
gp = GaussianProcessRegressor(kernel=kernel, 
                              n_restarts_optimizer=10)

X_sample = X_init.copy()
y_sample = y_init.copy()

n_iter = 10
for i in range(n_iter):
    gp.fit(X_sample, y_sample)
    best_y = np.max(y_sample)
    
    # –ü–æ–∏—Å–∫ —Å–ª–µ–¥—É—é—â–µ–π —Ç–æ—á–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    X_next = None
    max_ei = -np.inf
    
    for x_try in np.linspace(0, 2, 100).reshape(-1, 1):
        ei = expected_improvement(x_try, gp, best_y)
        if ei > max_ei:
            max_ei = ei
            X_next = x_try
    
    # –û—Ü–µ–Ω–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Ç–æ—á–∫–µ
    y_next = objective(X_next)
    
    X_sample = np.vstack([X_sample, X_next])
    y_sample = np.append(y_sample, y_next)
    
    print(f"Iteration {i+1}: x={X_next[0,0]:.3f}, y={y_next[0]:.3f}")

print(f"Optimal: x={X_sample[np.argmax(y_sample)]}, y={np.max(y_sample):.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Sparse GP (–¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö)</h2>
    <pre><code># –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º approximations
# –ù–∞–ø—Ä–∏–º–µ—Ä, —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π GPy –∏–ª–∏ GPflow

# Inducing points approach
# –í—ã–±–∏—Ä–∞–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏

from sklearn.cluster import KMeans

def select_inducing_points(X, n_inducing=50):
    """–í—ã–±–æ—Ä inducing points —Å –ø–æ–º–æ—â—å—é k-means"""
    kmeans = KMeans(n_clusters=n_inducing, random_state=42)
    kmeans.fit(X)
    return kmeans.cluster_centers_

# –ü—Ä–∏–º–µ—Ä —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
X_large = np.random.randn(10000, 5)
y_large = np.sin(X_large).sum(axis=1) + np.random.randn(10000)*0.1

# –í—ã–±–æ—Ä inducing points
X_inducing = select_inducing_points(X_large, n_inducing=100)

# –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å subset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
indices = np.random.choice(len(X_large), 1000, replace=False)
X_subset = X_large[indices]
y_subset = y_large[indices]

kernel = C(1.0) * RBF(length_scale=1.0)
gp = GaussianProcessRegressor(kernel=kernel)
gp.fit(X_subset, y_subset)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Multitask GP</h2>
    <pre><code># GP –¥–ª—è —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏

# –ü—Ä–∏–º–µ—Ä —Å 2 –∑–∞–¥–∞—á–∞–º–∏
def generate_multitask_data(n_samples=50):
    X = np.random.uniform(0, 10, (n_samples, 1))
    
    # –î–≤–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
    y1 = np.sin(X).ravel() + np.random.randn(n_samples)*0.1
    y2 = np.sin(X + 0.5).ravel() + np.random.randn(n_samples)*0.1
    
    return X, y1, y2

X, y1, y2 = generate_multitask_data()

# –û–±—É—á–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
gp1 = GaussianProcessRegressor(kernel=RBF())
gp2 = GaussianProcessRegressor(kernel=RBF())

gp1.fit(X, y1)
gp2.fit(X, y2)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
X_test = np.linspace(0, 10, 100).reshape(-1, 1)
y1_pred, sigma1 = gp1.predict(X_test, return_std=True)
y2_pred, sigma2 = gp2.predict(X_test, return_std=True)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±–µ–∏—Ö –∑–∞–¥–∞—á
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

for ax, y_train, y_pred, sigma, title in [
    (ax1, y1, y1_pred, sigma1, 'Task 1'),
    (ax2, y2, y2_pred, sigma2, 'Task 2')
]:
    ax.plot(X_test, y_pred, 'b-')
    ax.fill_between(X_test.ravel(),
                   y_pred - 1.96*sigma,
                   y_pred + 1.96*sigma,
                   alpha=0.3)
    ax.scatter(X, y_train, c='red', s=30)
    ax.set_title(title)
    ax.grid(alpha=0.3)

plt.tight_layout()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>Uncertainty</th><th>–ì–∏–±–∫–æ—Å—Ç—å</th><th>–°–∫–æ—Ä–æ—Å—Ç—å</th><th>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å</th></tr>
      <tr><td>GP</td><td>‚úÖ –î–∞ (Bayesian)</td><td>–í—ã—Å–æ–∫–∞—è</td><td>O(n¬≥)</td><td>–°—Ä–µ–¥–Ω—è—è</td></tr>
      <tr><td>Random Forest</td><td>‚ö†Ô∏è –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è</td><td>–í—ã—Å–æ–∫–∞—è</td><td>–ë—ã—Å—Ç—Ä–æ</td><td>–ù–∏–∑–∫–∞—è</td></tr>
      <tr><td>Neural Network</td><td>‚ùå –ù–µ—Ç (–±–µ–∑ Bayesian NN)</td><td>–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è</td><td>–°—Ä–µ–¥–Ω–µ</td><td>–ù–∏–∑–∫–∞—è</td></tr>
      <tr><td>SVR</td><td>‚ùå –ù–µ—Ç</td><td>–°—Ä–µ–¥–Ω—è—è</td><td>O(n¬≤-n¬≥)</td><td>–ù–∏–∑–∫–∞—è</td></tr>
      <tr><td>Linear Regression</td><td>‚úÖ –î–∞</td><td>–ù–∏–∑–∫–∞—è</td><td>–ë—ã—Å—Ç—Ä–æ</td><td>–í—ã—Å–æ–∫–∞—è</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GP</h2>
    <blockquote>
      <strong>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GP –∫–æ–≥–¥–∞:</strong>
      <ul>
        <li>–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö (10-1000 —Ç–æ—á–µ–∫)</li>
        <li>–ù—É–∂–Ω–∞ –æ—Ü–µ–Ω–∫–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏</li>
        <li>–í–∞–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å —á–µ—Ä–µ–∑ kernel</li>
        <li>–î–æ—Ä–æ–≥–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã (Bayesian optimization)</li>
        <li>–ì–ª–∞–¥–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏</li>
      </ul>
      <strong>–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–≥–¥–∞:</strong>
      <ul>
        <li>–û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ (>10,000 —Ç–æ—á–µ–∫)</li>
        <li>–í—ã—Å–æ–∫–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (>20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)</li>
        <li>–ù—É–∂–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å inference</li>
        <li>–ù–µ–≥–ª–∞–¥–∫–∏–µ, —Ä–∞–∑—Ä—ã–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏</li>
      </ul>
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 12. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è</h2>
    <pre><code># –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
from sklearn.metrics import mean_squared_error, r2_score

# Cross-validation
from sklearn.model_selection import cross_val_score

scores = cross_val_score(gp, X_train, y_train,
                        cv=5, scoring='neg_mean_squared_error')
print(f"CV MSE: {-scores.mean():.4f} ¬± {scores.std():.4f}")

# –ê–Ω–∞–ª–∏–∑ residuals
y_pred_train = gp.predict(X_train)
residuals = y_train - y_pred_train

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.scatter(y_pred_train, residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.grid(alpha=0.3)

plt.subplot(1, 3, 2)
plt.hist(residuals, bins=20, edgecolor='black')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Residual Distribution')
plt.grid(alpha=0.3)

plt.subplot(1, 3, 3)
from scipy import stats
stats.probplot(residuals, dist="norm", plot=plt)
plt.title('Q-Q Plot')
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Log marginal likelihood
print(f"Log-marginal-likelihood: {gp.log_marginal_likelihood():.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ß–µ–∫-–ª–∏—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è</h2>
    <ol>
      <li>‚úÖ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (<1000 —Ç–æ—á–µ–∫?)</li>
      <li>‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏</li>
      <li>‚úÖ –í—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–π kernel (RBF –¥–ª—è –Ω–∞—á–∞–ª–∞)</li>
      <li>‚úÖ –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å bounds –¥–ª—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ kernel</li>
      <li>‚úÖ –í—ã–±—Ä–∞—Ç—å alpha (—É—Ä–æ–≤–µ–Ω—å —à—É–º–∞)</li>
      <li>‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å n_restarts_optimizer=10-20</li>
      <li>‚úÖ –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å learned kernel</li>
      <li>‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å predictions —Å uncertainty</li>
      <li>‚úÖ –û—Ü–µ–Ω–∏—Ç—å log-marginal-likelihood</li>
      <li>‚úÖ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å residuals –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å</li>
      <li>‚úÖ Cross-validation –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏</li>
    </ol>
  </div>

</div>

</div>
</body>
</html>

<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Contrastive Learning Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üîÑ Contrastive Learning</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã Contrastive Learning</h2>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Ä–∞–∑–ª–∏—á–∞—Ç—å –ø–æ—Ö–æ–∂–∏–µ –∏ –Ω–µ–ø–æ—Ö–æ–∂–∏–µ –æ–±—ä–µ–∫—Ç—ã</li>
      <li><strong>–ò–¥–µ—è</strong>: –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä—ã –±–ª–∏–∑–∫–æ, –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –¥–∞–ª–µ–∫–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
      <li><strong>–ë–µ–∑ –º–µ—Ç–æ–∫</strong>: self-supervised –æ–±—É—á–µ–Ω–∏–µ</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: CV, NLP, audio, multimodal</li>
      <li><strong>–†–µ–∑—É–ª—å—Ç–∞—Ç</strong>: –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ embedding'–∏</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. –ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã</h2>
    <p><strong>–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä—ã</strong>:</p>
    <ul>
      <li>Augmented –≤–µ—Ä—Å–∏–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è</li>
      <li>–†–∞–∑–Ω—ã–µ —Ä–∞–∫—É—Ä—Å—ã –æ–¥–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞</li>
      <li>–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏ –µ–≥–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∞</li>
    </ul>
    <p><strong>–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä—ã</strong>:</p>
    <ul>
      <li>–†–∞–∑–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞</li>
      <li>–î—Ä—É–≥–∏–µ –æ–±—ä–µ–∫—Ç—ã –≤ –±–∞—Ç—á–µ</li>
      <li>–°–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã</li>
    </ul>
    <p><strong>–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å</strong>:</p>
    <ul>
      <li>NT-Xent (Normalized Temperature-scaled Cross Entropy)</li>
      <li>InfoNCE Loss</li>
      <li>Triplet Loss</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. SimCLR Framework</h2>
    <pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms

class SimCLR(nn.Module):
    def __init__(self, base_encoder, projection_dim=128):
        super().__init__()
        self.encoder = base_encoder
        
        # Projection head
        # ResNet-50 –≤—ã—Ö–æ–¥: 2048
        self.projection = nn.Sequential(
            nn.Linear(2048, 2048),
            nn.ReLU(),
            nn.Linear(2048, projection_dim)
        )
    
    def forward(self, x):
        h = self.encoder(x)
        z = self.projection(h)
        return F.normalize(z, dim=1)


# Data augmentation - –ö–õ–Æ–ß–ï–í–û–ô –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
def get_simclr_augmentation():
    return transforms.Compose([
        transforms.RandomResizedCrop(224, scale=(0.2, 1.0)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomApply([
            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)
        ], p=0.8),
        transforms.RandomGrayscale(p=0.2),
        transforms.GaussianBlur(kernel_size=23),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. NT-Xent Loss</h2>
    <pre><code>class NTXentLoss(nn.Module):
    def __init__(self, temperature=0.5):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, z_i, z_j):
        """
        z_i, z_j: (batch_size, projection_dim)
        –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä—ã: (z_i[k], z_j[k])
        """
        batch_size = z_i.shape[0]
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        z = torch.cat([z_i, z_j], dim=0)  # (2*batch, dim)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        sim = F.cosine_similarity(
            z.unsqueeze(1), z.unsqueeze(0), dim=2
        )  # (2*batch, 2*batch)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É
        sim = sim / self.temperature
        
        # –ú–∞—Å–∫–∞ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ i –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π - i+batch –∏–ª–∏ i-batch
        positive_mask = torch.zeros_like(sim, dtype=torch.bool)
        for i in range(batch_size):
            positive_mask[i, i + batch_size] = True
            positive_mask[i + batch_size, i] = True
        
        # –ú–∞—Å–∫–∞ –¥–ª—è —Å–∞–º–∏—Ö —Å–µ–±—è (–¥–∏–∞–≥–æ–Ω–∞–ª—å)
        identity_mask = torch.eye(2 * batch_size, 
                                   device=z.device, 
                                   dtype=torch.bool)
        
        # –õ–æ–≥–∏—Ç—ã –¥–ª—è –≤—Å–µ—Ö –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä
        negatives = sim.masked_fill(identity_mask, -9e15)
        
        # –õ–æ–≥–∏—Ç—ã –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä
        positives = sim[positive_mask].view(2 * batch_size, 1)
        
        # NT-Xent loss
        logits = torch.cat([positives, negatives], dim=1)
        labels = torch.zeros(2 * batch_size, 
                            device=z.device, 
                            dtype=torch.long)
        
        loss = F.cross_entropy(logits, labels)
        return loss</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –û–±—É—á–µ–Ω–∏–µ SimCLR</h2>
    <pre><code>from torchvision import models

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
base_encoder = models.resnet50(pretrained=False)
base_encoder.fc = nn.Identity()  # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π FC —Å–ª–æ–π

model = SimCLR(base_encoder, projection_dim=128)
model = model.cuda()

# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
optimizer = torch.optim.Adam(
    model.parameters(), 
    lr=0.0003, 
    weight_decay=1e-6
)

# Loss
criterion = NTXentLoss(temperature=0.5)

# Augmentation
augment = get_simclr_augmentation()

# Training loop
model.train()
for epoch in range(100):
    for images, _ in dataloader:
        # –°–æ–∑–¥–∞–µ–º –¥–≤–µ augmented –≤–µ—Ä—Å–∏–∏
        images_i = torch.stack([augment(img) for img in images])
        images_j = torch.stack([augment(img) for img in images])
        
        images_i = images_i.cuda()
        images_j = images_j.cuda()
        
        # Forward pass
        z_i = model(images_i)
        z_j = model(images_j)
        
        # Loss
        loss = criterion(z_i, z_j)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f'Epoch {epoch}, Loss: {loss.item():.4f}')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. MoCo (Momentum Contrast)</h2>
    <pre><code>class MoCo(nn.Module):
    def __init__(self, base_encoder, dim=128, 
                 K=65536, m=0.999, T=0.07):
        super().__init__()
        self.K = K  # –†–∞–∑–º–µ—Ä –æ—á–µ—Ä–µ–¥–∏
        self.m = m  # Momentum –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        self.T = T  # Temperature
        
        # Encoder query
        self.encoder_q = base_encoder
        self.encoder_q.fc = nn.Sequential(
            nn.Linear(2048, 2048),
            nn.ReLU(),
            nn.Linear(2048, dim)
        )
        
        # Encoder key (momentum encoder)
        self.encoder_k = base_encoder
        self.encoder_k.fc = nn.Sequential(
            nn.Linear(2048, 2048),
            nn.ReLU(),
            nn.Linear(2048, dim)
        )
        
        # –ö–æ–ø–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        for param_q, param_k in zip(
            self.encoder_q.parameters(),
            self.encoder_k.parameters()
        ):
            param_k.data.copy_(param_q.data)
            param_k.requires_grad = False
        
        # –û—á–µ—Ä–µ–¥—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        self.register_buffer(
            "queue", 
            torch.randn(dim, K)
        )
        self.queue = F.normalize(self.queue, dim=0)
        self.register_buffer("queue_ptr", torch.zeros(1, dtype=torch.long))
    
    @torch.no_grad()
    def _momentum_update_key_encoder(self):
        """Momentum update –¥–ª—è key encoder"""
        for param_q, param_k in zip(
            self.encoder_q.parameters(),
            self.encoder_k.parameters()
        ):
            param_k.data = param_k.data * self.m + \
                          param_q.data * (1. - self.m)
    
    @torch.no_grad()
    def _dequeue_and_enqueue(self, keys):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—á–µ—Ä–µ–¥–∏"""
        batch_size = keys.shape[0]
        ptr = int(self.queue_ptr)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
        self.queue[:, ptr:ptr + batch_size] = keys.T
        ptr = (ptr + batch_size) % self.K
        
        self.queue_ptr[0] = ptr
    
    def forward(self, im_q, im_k):
        # Query
        q = self.encoder_q(im_q)
        q = F.normalize(q, dim=1)
        
        # Key
        with torch.no_grad():
            self._momentum_update_key_encoder()
            k = self.encoder_k(im_k)
            k = F.normalize(k, dim=1)
        
        # Positive logits
        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)
        
        # Negative logits
        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])
        
        # Logits: (N, 1+K)
        logits = torch.cat([l_pos, l_neg], dim=1) / self.T
        
        # Labels
        labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()
        
        # Update queue
        self._dequeue_and_enqueue(k)
        
        return logits, labels</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. BYOL (Bootstrap Your Own Latent)</h2>
    <p>–ë–µ–∑ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤!</p>
    <pre><code>class BYOL(nn.Module):
    def __init__(self, base_encoder, projection_dim=256, 
                 hidden_dim=4096, moving_average_decay=0.996):
        super().__init__()
        self.moving_average_decay = moving_average_decay
        
        # Online network
        self.online_encoder = base_encoder
        self.online_projector = nn.Sequential(
            nn.Linear(2048, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, projection_dim)
        )
        self.online_predictor = nn.Sequential(
            nn.Linear(projection_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, projection_dim)
        )
        
        # Target network (EMA of online)
        self.target_encoder = base_encoder
        self.target_projector = nn.Sequential(
            nn.Linear(2048, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, projection_dim)
        )
        
        # –ö–æ–ø–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.target_encoder.load_state_dict(
            self.online_encoder.state_dict()
        )
        self.target_projector.load_state_dict(
            self.online_projector.state_dict()
        )
        
        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º target
        for param in self.target_encoder.parameters():
            param.requires_grad = False
        for param in self.target_projector.parameters():
            param.requires_grad = False
    
    @torch.no_grad()
    def update_moving_average(self):
        """EMA update –¥–ª—è target network"""
        for online, target in zip(
            self.online_encoder.parameters(),
            self.target_encoder.parameters()
        ):
            target.data = self.moving_average_decay * target.data + \
                         (1 - self.moving_average_decay) * online.data
        
        for online, target in zip(
            self.online_projector.parameters(),
            self.target_projector.parameters()
        ):
            target.data = self.moving_average_decay * target.data + \
                         (1 - self.moving_average_decay) * online.data
    
    def forward(self, x1, x2):
        # Online network
        online_proj_1 = self.online_projector(
            self.online_encoder(x1)
        )
        online_proj_2 = self.online_projector(
            self.online_encoder(x2)
        )
        
        online_pred_1 = self.online_predictor(online_proj_1)
        online_pred_2 = self.online_predictor(online_proj_2)
        
        # Target network
        with torch.no_grad():
            target_proj_1 = self.target_projector(
                self.target_encoder(x1)
            )
            target_proj_2 = self.target_projector(
                self.target_encoder(x2)
            )
        
        # Loss: mean squared error + normalize
        loss_1 = 2 - 2 * F.cosine_similarity(
            online_pred_1, target_proj_2.detach(), dim=-1
        ).mean()
        loss_2 = 2 - 2 * F.cosine_similarity(
            online_pred_2, target_proj_1.detach(), dim=-1
        ).mean()
        
        loss = (loss_1 + loss_2) / 2
        return loss</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–û—Ç—Ä–∏—Ü. –ø—Ä–∏–º–µ—Ä—ã</th><th>Momentum</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å</th></tr>
      <tr><td><strong>SimCLR</strong></td><td>‚úÖ Batch</td><td>‚ùå</td><td>–ë–æ–ª—å—à–æ–π batch size</td></tr>
      <tr><td><strong>MoCo</strong></td><td>‚úÖ Queue</td><td>‚úÖ</td><td>–û—á–µ—Ä–µ–¥—å –ø—Ä–∏–º–µ—Ä–æ–≤</td></tr>
      <tr><td><strong>BYOL</strong></td><td>‚ùå</td><td>‚úÖ</td><td>–¢–æ–ª—å–∫–æ positive pairs</td></tr>
      <tr><td><strong>SwAV</strong></td><td>‚ùå</td><td>‚ùå</td><td>Clustering online</td></tr>
      <tr><td><strong>Barlow Twins</strong></td><td>‚ùå</td><td>‚ùå</td><td>Redundancy reduction</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 9. Linear Evaluation</h2>
    <p>–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ learned representations:</p>
    <pre><code># –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º encoder
for param in model.encoder.parameters():
    param.requires_grad = False

# –î–æ–±–∞–≤–ª—è–µ–º linear classifier
classifier = nn.Linear(2048, num_classes).cuda()

# –û–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ classifier
optimizer = torch.optim.SGD(
    classifier.parameters(), 
    lr=0.1, 
    momentum=0.9
)

# –û–±—É—á–µ–Ω–∏–µ
for epoch in range(100):
    for images, labels in train_loader:
        images = images.cuda()
        labels = labels.cuda()
        
        # –ü–æ–ª—É—á–∞–µ–º features (frozen encoder)
        with torch.no_grad():
            features = model.encoder(images)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        outputs = classifier(features)
        loss = F.cross_entropy(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# –û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏
model.eval()
classifier.eval()
correct = 0
total = 0

with torch.no_grad():
    for images, labels in test_loader:
        images = images.cuda()
        labels = labels.cuda()
        
        features = model.encoder(images)
        outputs = classifier(features)
        _, predicted = outputs.max(1)
        
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100. * correct / total
print(f'Linear Evaluation Accuracy: {accuracy:.2f}%')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Data Augmentation —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏</h2>
    <p>–ö—Ä–∏—Ç–∏—á–Ω—ã –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞!</p>
    <pre><code># –°–∏–ª—å–Ω—ã–µ augmentations –¥–ª—è contrastive learning
strong_augment = transforms.Compose([
    transforms.RandomResizedCrop(224, scale=(0.08, 1.0)),
    transforms.RandomHorizontalFlip(p=0.5),
    
    # Color jittering
    transforms.RandomApply([
        transforms.ColorJitter(
            brightness=0.4,
            contrast=0.4,
            saturation=0.4,
            hue=0.1
        )
    ], p=0.8),
    
    # Grayscale
    transforms.RandomGrayscale(p=0.2),
    
    # Gaussian blur
    transforms.RandomApply([
        transforms.GaussianBlur(
            kernel_size=23, 
            sigma=(0.1, 2.0)
        )
    ], p=0.5),
    
    # Solarization (–¥–ª—è SwAV, BYOL)
    transforms.RandomSolarize(threshold=128, p=0.2),
    
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# –ö–æ–º–ø–æ–∑–∏—Ü–∏—è –¥–≤—É—Ö —Ä–∞–∑–Ω—ã—Ö augmentations
def get_two_augmentations(image):
    return strong_augment(image), strong_augment(image)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç</h3>
        <ul>
          <li>–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ unlabeled data</li>
          <li>Few-shot learning</li>
          <li>Transfer learning</li>
          <li>Image retrieval</li>
          <li>–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π</li>
          <li>Multimodal learning (CLIP)</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –°–ª–æ–∂–Ω–æ—Å—Ç–∏</h3>
        <ul>
          <li>–¢—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ GPU –ø–∞–º—è—Ç–∏</li>
          <li>–ë–æ–ª—å—à–æ–π batch size –∫—Ä–∏—Ç–∏—á–µ–Ω</li>
          <li>–î–æ–ª–≥–æ–µ –æ–±—É—á–µ–Ω–∏–µ (100+ —ç–ø–æ—Ö)</li>
          <li>–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ augmentations</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <ul>
      <li><strong>Batch size</strong>: SimCLR –Ω—É–∂–µ–Ω –±–æ–ª—å—à–æ–π (256-4096)</li>
      <li><strong>Temperature</strong>: 0.1-0.5, –≤–ª–∏—è–µ—Ç –Ω–∞ hard negatives</li>
      <li><strong>Optimizer</strong>: LARS –∏–ª–∏ AdamW —Å weight decay</li>
      <li><strong>Learning rate</strong>: Linear warmup + cosine decay</li>
      <li><strong>Augmentations</strong>: –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã, —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ</li>
      <li><strong>Projection head</strong>: –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è</li>
      <li><strong>–≠–ø–æ—Ö–∏</strong>: 100-1000 –¥–ª—è —Ö–æ—Ä–æ—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–π –º–µ—Ç–æ–¥ (SimCLR, MoCo, BYOL)</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å —Å–∏–ª—å–Ω—ã–µ augmentations</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–æ–π batch size (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å learning rate schedule</li>
      <li>[ ] –û–±—É—á–∞—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–æ–ª–≥–æ (100+ —ç–ø–æ—Ö)</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Å—Ç–∏ linear evaluation</li>
      <li>[ ] –°—Ä–∞–≤–Ω–∏—Ç—å —Å supervised baseline</li>
      <li>[ ] Fine-tune –Ω–∞ downstream –∑–∞–¥–∞—á–µ</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ embeddings</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´Contrastive Learning –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∏—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å –Ω–∞—Ö–æ–¥–∏—Ç—å —Ö–æ—Ä–æ—à–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ ‚Äî –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ –¥–≤–∞ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∫—É—Ä—Å–∞ –æ–¥–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –ø–æ—Ö–æ–∂–∏, –∞ —Ä–∞–∑–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã —Ä–∞–∑–ª–∏—á–Ω—ã¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

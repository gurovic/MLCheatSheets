<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>RNN/LSTM for NLP Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üìù RNN/LSTM for NLP</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. RNN –¥–ª—è NLP: –û—Å–Ω–æ–≤—ã</h2>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ç–µ–∫—Å—Ç–∞</li>
      <li><strong>–ò–¥–µ—è</strong>: hidden state –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –æ—Ç —Å–ª–æ–≤–∞ –∫ —Å–ª–æ–≤—É</li>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ</strong>: —É—á—ë—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ—Ä—è–¥–∫–∞ —Å–ª–æ–≤</li>
      <li><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: –∏—Å—á–µ–∑–∞—é—â–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç</li>
      <li><strong>–†–µ—à–µ–Ω–∏–µ</strong>: LSTM –∏ GRU</li>
    </ul>
    <p><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</strong>:</p>
    <ul>
      <li>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤</li>
      <li>Sentiment analysis</li>
      <li>Named Entity Recognition (NER)</li>
      <li>–ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥</li>
      <li>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞</h2>
    <pre><code>from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# –ö–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤
texts = [
    "I love this movie",
    "This film is terrible",
    "Great acting and plot"
]

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
sequences = tokenizer.texts_to_sequences(texts)
print(sequences)
# [[2, 3, 4, 5], [4, 6, 7, 8], [9, 10, 11, 12]]

# Padding –¥–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã
max_length = 100
padded = pad_sequences(sequences, 
                       maxlen=max_length, 
                       padding='post',
                       truncating='post')

print(f"Padded shape: {padded.shape}")
# (3, 100)

# –°–ª–æ–≤–∞—Ä—å
word_index = tokenizer.word_index
print(f"Vocabulary size: {len(word_index)}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ü—Ä–æ—Å—Ç–∞—è RNN –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</h2>
    <pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Embedding, SimpleRNN, Dense, Dropout
)

vocab_size = 10000
embedding_dim = 128
max_length = 100

model = Sequential([
    # Embedding layer
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    
    # RNN layer
    SimpleRNN(64, return_sequences=False),
    
    # Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    Dropout(0.5),
    
    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. LSTM –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤</h2>
    <pre><code>from tensorflow.keras.layers import LSTM, Bidirectional

model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    
    # LSTM layer
    LSTM(128, return_sequences=True),
    Dropout(0.3),
    
    # –í—Ç–æ—Ä–æ–π LSTM —Å–ª–æ–π
    LSTM(64, return_sequences=False),
    Dropout(0.3),
    
    # Dense layers
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Bidirectional LSTM</h2>
    <p><strong>–ò–¥–µ—è</strong>: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –≤ –æ–±–æ–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö</p>
    <pre><code>model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    
    # Bidirectional LSTM
    Bidirectional(LSTM(128, return_sequences=True)),
    Dropout(0.3),
    
    Bidirectional(LSTM(64, return_sequences=False)),
    Dropout(0.3),
    
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# BiLSTM —É–¥–≤–∞–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
# 128 units ‚Üí 256 outputs (128 forward + 128 backward)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. PyTorch —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è</h2>
    <pre><code>import torch
import torch.nn as nn

class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, 
                 num_layers, num_classes, dropout=0.5):
        super(LSTMClassifier, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        # *2 –∏–∑-–∑–∞ bidirectional
        self.fc = nn.Linear(hidden_dim * 2, num_classes)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # x: (batch, seq_len)
        embedded = self.embedding(x)  # (batch, seq_len, emb_dim)
        
        # LSTM
        lstm_out, (hidden, cell) = self.lstm(embedded)
        # lstm_out: (batch, seq_len, hidden*2)
        
        # –í–∑—è—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π output
        # –î–ª—è bidirectional: –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è forward[-1] –∏ backward[0]
        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
        
        # Dropout –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        output = self.dropout(hidden)
        output = self.fc(output)
        
        return output

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
model = LSTMClassifier(
    vocab_size=10000,
    embedding_dim=128,
    hidden_dim=256,
    num_layers=2,
    num_classes=2,
    dropout=0.5
)

print(model)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Seq2Seq –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞</h2>
    <p><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong>: Encoder-Decoder</p>
    <pre><code>class Encoder(nn.Module):
    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, 
                           batch_first=True)
    
    def forward(self, x):
        embedded = self.embedding(x)
        outputs, (hidden, cell) = self.lstm(embedded)
        return hidden, cell

class Decoder(nn.Module):
    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, 
                           batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, x, hidden, cell):
        embedded = self.embedding(x)
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        prediction = self.fc(output)
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
    
    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        hidden, cell = self.encoder(src)
        
        batch_size = trg.shape[0]
        trg_len = trg.shape[1]
        trg_vocab_size = self.decoder.fc.out_features
        
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size)
        
        input = trg[:, 0:1]  # <SOS> token
        
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[:, t:t+1] = output
            
            # Teacher forcing
            teacher_force = random.random() < teacher_forcing_ratio
            input = trg[:, t:t+1] if teacher_force else output.argmax(2)
        
        return outputs</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Attention –º–µ—Ö–∞–Ω–∏–∑–º</h2>
    <p><strong>–ü—Ä–æ–±–ª–µ–º–∞ Seq2Seq</strong>: –≤—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –æ–¥–Ω–æ–º context vector</p>
    <p><strong>–†–µ—à–µ–Ω–∏–µ</strong>: attention –∫ —Ä–∞–∑–Ω—ã–º —á–∞—Å—Ç—è–º –≤—Ö–æ–¥–∞</p>
    <pre><code>class Attention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)
        self.v = nn.Linear(hidden_dim, 1, bias=False)
    
    def forward(self, hidden, encoder_outputs):
        # hidden: (batch, hidden_dim)
        # encoder_outputs: (batch, seq_len, hidden_dim)
        
        batch_size = encoder_outputs.shape[0]
        src_len = encoder_outputs.shape[1]
        
        # Repeat hidden –¥–ª—è –∫–∞–∂–¥–æ–≥–æ encoder output
        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)
        
        # Concatenate
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))
        
        # Attention scores
        attention = self.v(energy).squeeze(2)
        
        # Softmax
        return torch.softmax(attention, dim=1)

class AttentionDecoder(nn.Module):
    def __init__(self, vocab_size, emb_dim, hidden_dim, attention):
        super().__init__()
        self.attention = attention
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.lstm = nn.LSTM(emb_dim + hidden_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, input, hidden, cell, encoder_outputs):
        # –í—ã—á–∏—Å–ª–∏—Ç—å attention
        a = self.attention(hidden[-1], encoder_outputs)
        a = a.unsqueeze(1)  # (batch, 1, src_len)
        
        # Context vector
        context = torch.bmm(a, encoder_outputs)  # (batch, 1, hidden)
        
        # Embedding
        embedded = self.embedding(input)
        
        # Concatenate —Å context
        lstm_input = torch.cat((embedded, context), dim=2)
        
        # LSTM
        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))
        
        # Prediction
        prediction = self.fc(output)
        
        return prediction, hidden, cell, a</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Pretrained Embeddings</h2>
    <pre><code># –ó–∞–≥—Ä—É–∑–∫–∞ Word2Vec –∏–ª–∏ GloVe
import gensim.downloader as api

# Word2Vec
word2vec = api.load("word2vec-google-news-300")

# GloVe
glove = api.load("glove-wiki-gigaword-100")

# –°–æ–∑–¥–∞–Ω–∏–µ embedding matrix
vocab_size = len(word_index) + 1
embedding_dim = 300
embedding_matrix = np.zeros((vocab_size, embedding_dim))

for word, i in word_index.items():
    try:
        embedding_vector = word2vec[word]
        embedding_matrix[i] = embedding_vector
    except KeyError:
        # –°–ª–æ–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –æ—Å—Ç–∞–≤–∏—Ç—å –Ω—É–ª–∏
        pass

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ Keras
from tensorflow.keras.layers import Embedding

embedding_layer = Embedding(
    vocab_size,
    embedding_dim,
    weights=[embedding_matrix],
    input_length=max_length,
    trainable=False  # –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å –≤–µ—Å–∞
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Named Entity Recognition (NER)</h2>
    <pre><code># –ú–æ–¥–µ–ª—å –¥–ª—è NER (—Ç–æ–∫–µ–Ω-—É—Ä–æ–≤–Ω–µ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    
    # Bidirectional LSTM —Å return_sequences=True
    Bidirectional(LSTM(128, return_sequences=True)),
    Dropout(0.3),
    
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.3),
    
    # TimeDistributed Dense –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
    TimeDistributed(Dense(num_tags, activation='softmax'))
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# –î–ª—è CRF —Å–ª–æ—è (–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ):
from keras_contrib.layers import CRF

crf = CRF(num_tags)
model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    Bidirectional(LSTM(128, return_sequences=True)),
    Dropout(0.3),
    crf
])

model.compile(optimizer='adam', loss=crf.loss_function, 
              metrics=[crf.accuracy])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <ul>
      <li><strong>–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</strong>: 50-200 —Ç–æ–∫–µ–Ω–æ–≤ (–∫–æ–º–ø—Ä–æ–º–∏—Å—Å)</li>
      <li><strong>Batch size</strong>: 32-128 (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø–∞–º—è—Ç–∏)</li>
      <li><strong>Hidden size</strong>: 128-512 –¥–ª—è LSTM</li>
      <li><strong>Layers</strong>: 1-3 LSTM —Å–ª–æ—è –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ</li>
      <li><strong>Dropout</strong>: 0.3-0.5 –º–µ–∂–¥—É —Å–ª–æ—è–º–∏</li>
      <li><strong>Optimizer</strong>: Adam —Å lr=0.001</li>
      <li><strong>Gradient clipping</strong>: clip_value=1.0 –∏–ª–∏ clip_norm=5.0</li>
    </ul>
    <pre><code># Gradient clipping –≤ PyTorch
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)

# –í Keras/TensorFlow
from tensorflow.keras.optimizers import Adam
optimizer = Adam(clipvalue=1.0)
# –∏–ª–∏
optimizer = Adam(clipnorm=5.0)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ RNN vs Transformer</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>RNN/LSTM</th><th>Transformer</th></tr>
      <tr><td><strong>–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è</strong></td><td>–ù–µ—Ç (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ)</td><td>–î–∞</td></tr>
      <tr><td><strong>–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è</strong></td><td>–ú–µ–¥–ª–µ–Ω–Ω–µ–µ</td><td>–ë—ã—Å—Ç—Ä–µ–µ</td></tr>
      <tr><td><strong>–ü–∞–º—è—Ç—å</strong></td><td>–ú–µ–Ω—å—à–µ</td><td>–ë–æ–ª—å—à–µ</td></tr>
      <tr><td><strong>–î–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</strong></td><td>–°–ª–æ–∂–Ω–æ</td><td>–õ–µ–≥–∫–æ</td></tr>
      <tr><td><strong>–ü—Ä–µ—Ç—Ä–µ–π–Ω</strong></td><td>–†–µ–¥–∫–æ</td><td>BERT, GPT</td></tr>
      <tr><td><strong>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</strong></td><td>–ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤</td><td>–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å—Ç—å GPU</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] <strong>–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞</strong>: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, lowercase, —É–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤</li>
      <li>[ ] <strong>Padding</strong>: –¥–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã</li>
      <li>[ ] <strong>Embeddings</strong>: –æ–±—É—á–∞–µ–º—ã–µ –∏–ª–∏ pretrained (Word2Vec, GloVe)</li>
      <li>[ ] <strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong>: LSTM –ª—É—á—à–µ –ø—Ä–æ—Å—Ç–æ–≥–æ RNN</li>
      <li>[ ] <strong>Bidirectional</strong>: –¥–ª—è –∑–∞–¥–∞—á, –≥–¥–µ –¥–æ—Å—Ç—É–ø–µ–Ω –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç</li>
      <li>[ ] <strong>Dropout</strong>: —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –º–µ–∂–¥—É —Å–ª–æ—è–º–∏</li>
      <li>[ ] <strong>Gradient clipping</strong>: –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –≤–∑—Ä—ã–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</li>
      <li>[ ] <strong>Early stopping</strong>: –ø–æ validation loss</li>
      <li>[ ] <strong>–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å Transformer</strong>: –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –∏ —Ä–µ—Å—É—Ä—Å—ã</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´RNN/LSTM –¥–ª—è NLP ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —á–∏—Ç–∞—é—Ç —Ç–µ–∫—Å—Ç —Å–ª–æ–≤–æ –∑–∞ —Å–ª–æ–≤–æ–º, –∑–∞–ø–æ–º–∏–Ω–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–ª–æ–≤. –ö–∞–∫ —á–µ–ª–æ–≤–µ–∫, —á–∏—Ç–∞—é—â–∏–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏ –ø–æ–º–Ω—è—â–∏–π –µ–≥–æ –Ω–∞—á–∞–ª–æ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –∫–æ–Ω—Ü–∞¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

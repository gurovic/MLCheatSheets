<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Learning Rate Scheduling Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üìà Learning Rate Scheduling</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ó–∞—á–µ–º –Ω—É–∂–µ–Ω Scheduling?</h2>
    <p><strong>Learning Rate Scheduling</strong> ‚Äî –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏.</p>
    
    <p><strong>–ü—Ä–æ–±–ª–µ–º–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ lr:</strong></p>
    <ul>
      <li>–ë–æ–ª—å—à–æ–π lr ‚Üí –±—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –Ω–æ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</li>
      <li>–ú–∞–ª–µ–Ω—å–∫–∏–π lr ‚Üí –º–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</li>
      <li>–û–ø—Ç–∏–º—É–º –≥–¥–µ-—Ç–æ –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ –º–µ–Ω—è–µ—Ç—Å—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º</li>
    </ul>
    
    <p><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ scheduling:</strong></p>
    <ul>
      <li>–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–µ</li>
      <li>–°—Ç–∞–±–∏–ª—å–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –∫–æ–Ω—Ü–µ</li>
      <li>–õ—É—á—à–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è</li>
      <li>–í—ã—Ö–æ–¥ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 2. –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã schedulers</h2>
    <table>
      <tr><th>Scheduler</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>StepLR</strong></td><td>–°–Ω–∏–∂–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ N —ç–ø–æ—Ö</td><td>–ë–∞–∑–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥</td></tr>
      <tr><td><strong>ExponentialLR</strong></td><td>–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ</td><td>–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ</td></tr>
      <tr><td><strong>CosineAnnealingLR</strong></td><td>–ö–æ—Å–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ</td><td>–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç</td></tr>
      <tr><td><strong>ReduceLROnPlateau</strong></td><td>–ü—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ —É–ª—É—á—à–µ–Ω–∏—è</td><td>Adaptive</td></tr>
      <tr><td><strong>OneCycleLR</strong></td><td>–û–¥–Ω–∞ —Ñ–∞–∑–∞ –ø–æ–¥—ä–µ–º–∞ –∏ —Å–ø—É—Å–∫–∞</td><td>–ë—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</td></tr>
      <tr><td><strong>WarmupLR</strong></td><td>–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ</td><td>–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. Step Decay (StepLR)</h2>
    <p><strong>–°–Ω–∏–∂–µ–Ω–∏–µ lr –∫–∞–∂–¥—ã–µ N —ç–ø–æ—Ö</strong>:</p>
    <pre><code>lr_new = lr * gamma^(epoch // step_size)</code></pre>
    
    <pre><code>import torch.optim as optim

optimizer = optim.SGD(model.parameters(), lr=0.1)

# –°–Ω–∏–∂–µ–Ω–∏–µ –Ω–∞ 10x –∫–∞–∂–¥—ã–µ 30 —ç–ø–æ—Ö
scheduler = optim.lr_scheduler.StepLR(
    optimizer,
    step_size=30,
    gamma=0.1
)

# Training loop
for epoch in range(num_epochs):
    train_one_epoch()
    scheduler.step()  # –æ–±–Ω–æ–≤–∏—Ç—å lr

# –≠–≤–æ–ª—é—Ü–∏—è lr:
# –≠–ø–æ—Ö–∏ 0-29: lr = 0.1
# –≠–ø–æ—Ö–∏ 30-59: lr = 0.01
# –≠–ø–æ—Ö–∏ 60-89: lr = 0.001</code></pre>
    
    <p><strong>MultiStepLR</strong> ‚Äî —Å–Ω–∏–∂–µ–Ω–∏–µ –Ω–∞ –∑–∞–¥–∞–Ω–Ω—ã—Ö —ç–ø–æ—Ö–∞—Ö:</p>
    <pre><code>scheduler = optim.lr_scheduler.MultiStepLR(
    optimizer,
    milestones=[30, 60, 90],
    gamma=0.1
)

# lr –º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —ç–ø–æ—Ö–∞—Ö 30, 60, 90</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Exponential Decay</h2>
    <p><strong>–ü–ª–∞–≤–Ω–æ–µ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ</strong>:</p>
    <pre><code>lr_new = lr * gamma^epoch</code></pre>
    
    <pre><code># PyTorch
scheduler = optim.lr_scheduler.ExponentialLR(
    optimizer,
    gamma=0.95  # —Å–Ω–∏–∂–µ–Ω–∏–µ –Ω–∞ 5% –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É
)

# –ü—Ä–∏–º–µ—Ä —Å gamma=0.95:
# –≠–ø–æ—Ö–∞ 0: lr = 0.1
# –≠–ø–æ—Ö–∞ 10: lr = 0.0599
# –≠–ø–æ—Ö–∞ 20: lr = 0.0358
# –≠–ø–æ—Ö–∞ 50: lr = 0.0077

# TensorFlow
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.1,
    decay_steps=1000,
    decay_rate=0.96
)

optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Cosine Annealing</h2>
    <p><strong>–ü–ª–∞–≤–Ω–æ–µ –∫–æ—Å–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ</strong>:</p>
    <pre><code>lr_t = lr_min + 0.5 * (lr_max - lr_min) * (1 + cos(œÄ * t / T))</code></pre>
    
    <pre><code># PyTorch
scheduler = optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=100,      # –ø–µ—Ä–∏–æ–¥ –≤ —ç–ø–æ—Ö–∞—Ö
    eta_min=1e-6    # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π lr
)

# –° —Ä–µ—Å—Ç–∞—Ä—Ç–∞–º–∏ (SGDR)
scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer,
    T_0=10,         # –ø–µ—Ä–≤—ã–π –ø–µ—Ä–∏–æ–¥
    T_mult=2,       # —É–º–Ω–æ–∂–µ–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∞ –ø–æ—Å–ª–µ —Ä–µ—Å—Ç–∞—Ä—Ç–∞
    eta_min=1e-6
)

# TensorFlow
lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate=0.1,
    decay_steps=1000,
    alpha=0.0
)</code></pre>
    
    <p><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:</strong></p>
    <ul>
      <li>–ü–ª–∞–≤–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –±–µ–∑ —Å–∫–∞—á–∫–æ–≤</li>
      <li>–•–æ—Ä–æ—à–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –∫–æ–Ω—Ü–µ</li>
      <li>–ü–æ–ø—É–ª—è—Ä–µ–Ω –≤ computer vision</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 6. ReduceLROnPlateau</h2>
    <p><strong>Adaptive scheduler</strong> ‚Äî —Å–Ω–∏–∂–∞–µ—Ç lr –∫–æ–≥–¥–∞ –º–µ—Ç—Ä–∏–∫–∞ –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è:</p>
    <pre><code>scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',          # 'min' –¥–ª—è loss, 'max' –¥–ª—è accuracy
    factor=0.1,          # —É–º–µ–Ω—å—à–µ–Ω–∏–µ –≤ 10 —Ä–∞–∑
    patience=10,         # –∂–¥–∞—Ç—å 10 —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
    verbose=True,
    threshold=1e-4,      # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
    cooldown=0,          # —ç–ø–æ—Ö–∏ –ø–æ—Å–ª–µ —Å–Ω–∏–∂–µ–Ω–∏—è
    min_lr=1e-6          # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π lr
)

# Training loop
for epoch in range(num_epochs):
    train_loss = train_one_epoch()
    val_loss = validate()
    
    # –ü–µ—Ä–µ–¥–∞—Ç—å –º–µ—Ç—Ä–∏–∫—É scheduler
    scheduler.step(val_loss)
    
    print(f"Epoch {epoch}: lr = {optimizer.param_groups[0]['lr']}")</code></pre>
    
    <p><strong>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:</strong></p>
    <ul>
      <li>–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ</li>
      <li>–ù—É–∂–Ω–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –¥–∞–Ω–Ω—ã–º</li>
      <li>–ù–µ –∫—Ä–∏—Ç–∏—á–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 7. OneCycleLR</h2>
    <p><strong>Super-convergence</strong> ‚Äî –±—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞ –æ–¥–∏–Ω —Ü–∏–∫–ª:</p>
    
    <p><strong>–§–∞–∑—ã:</strong></p>
    <ol>
      <li><strong>Warmup (0-30%)</strong>: lr —Ä–∞—Å—Ç–µ—Ç –æ—Ç min –¥–æ max</li>
      <li><strong>Annealing (30-100%)</strong>: lr –ø–∞–¥–∞–µ—Ç –¥–æ min</li>
    </ol>
    
    <pre><code>scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=0.1,             # –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π lr
    epochs=100,
    steps_per_epoch=len(dataloader),
    pct_start=0.3,          # % —ç–ø–æ—Ö –Ω–∞ warmup
    anneal_strategy='cos',  # 'cos' –∏–ª–∏ 'linear'
    div_factor=25,          # initial_lr = max_lr / div_factor
    final_div_factor=1e4    # final_lr = max_lr / final_div_factor
)

# –í—ã–∑—ã–≤–∞—Ç—å –ö–ê–ñ–î–£–Æ –ò–¢–ï–†–ê–¶–ò–Æ, –Ω–µ —ç–ø–æ—Ö—É!
for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        loss = model(batch)
        loss.backward()
        optimizer.step()
        scheduler.step()  # –∫–∞–∂–¥—É—é –∏—Ç–µ—Ä–∞—Ü–∏—é!</code></pre>
    
    <p><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:</strong></p>
    <ul>
      <li>–û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</li>
      <li>–ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –Ω—É–∂–Ω–æ</li>
      <li>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –≤—ã—Å–æ–∫–∏–π lr</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 8. Warmup</h2>
    <p><strong>–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ lr</strong> –≤ –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è:</p>
    
    <p><strong>–ó–∞—á–µ–º –Ω—É–∂–µ–Ω:</strong></p>
    <ul>
      <li>–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –≤ –Ω–∞—á–∞–ª–µ (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è Adam)</li>
      <li>–ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö batch size</li>
      <li>–û–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è Transformers</li>
    </ul>
    
    <pre><code># Linear warmup
class LinearWarmupScheduler:
    def __init__(self, optimizer, warmup_epochs, total_epochs, base_lr):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.base_lr = base_lr
    
    def step(self, epoch):
        if epoch < self.warmup_epochs:
            # Linear warmup
            lr = self.base_lr * (epoch + 1) / self.warmup_epochs
        else:
            # Cosine decay –ø–æ—Å–ª–µ warmup
            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)
            lr = 0.5 * self.base_lr * (1 + math.cos(math.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
scheduler = LinearWarmupScheduler(optimizer, warmup_epochs=5, total_epochs=100, base_lr=0.001)

for epoch in range(100):
    train_one_epoch()
    scheduler.step(epoch)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ schedulers</h2>
    <p><strong>Warmup + Cosine Annealing</strong> ‚Äî –ø–æ–ø—É–ª—è—Ä–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è:</p>
    <pre><code># PyTorch –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π
from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR

# Warmup scheduler
warmup_scheduler = LinearLR(
    optimizer,
    start_factor=0.01,  # –Ω–∞—á–∞—Ç—å —Å lr/100
    end_factor=1.0,
    total_iters=5
)

# Main scheduler
main_scheduler = CosineAnnealingLR(
    optimizer,
    T_max=95,
    eta_min=1e-6
)

# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
scheduler = SequentialLR(
    optimizer,
    schedulers=[warmup_scheduler, main_scheduler],
    milestones=[5]  # –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ 5 —ç–ø–æ—Ö
)

for epoch in range(100):
    train_one_epoch()
    scheduler.step()</code></pre>
    
    <p><strong>Transformers (BERT-style):</strong></p>
    <pre><code>from transformers import get_linear_schedule_with_warmup

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=10000,
    num_training_steps=100000
)

# –í—ã–∑—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π —à–∞–≥
for step in range(num_training_steps):
    optimizer.step()
    scheduler.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Cyclical Learning Rates</h2>
    <p><strong>–¶–∏–∫–ª–∏—á–µ—Å–∫–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ lr</strong> –º–µ–∂–¥—É min –∏ max:</p>
    <pre><code>scheduler = optim.lr_scheduler.CyclicLR(
    optimizer,
    base_lr=0.001,      # –º–∏–Ω–∏–º—É–º
    max_lr=0.1,         # –º–∞–∫—Å–∏–º—É–º
    step_size_up=2000,  # —à–∞–≥–æ–≤ –Ω–∞ –ø–æ–¥—ä–µ–º
    mode='triangular',  # 'triangular', 'triangular2', 'exp_range'
    cycle_momentum=True
)

# –í—ã–∑—ã–≤–∞—Ç—å –∫–∞–∂–¥—É—é –∏—Ç–µ—Ä–∞—Ü–∏—é
for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.step()
        scheduler.step()

# Modes:
# - triangular: –ø–æ—Å—Ç–æ—è–Ω–Ω–∞—è –∞–º–ø–ª–∏—Ç—É–¥–∞
# - triangular2: –ø–æ–ª–æ–≤–∏–Ω–Ω–∞—è –∞–º–ø–ª–∏—Ç—É–¥–∞ –∫–∞–∂–¥—ã–π —Ü–∏–∫–ª
# - exp_range: —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ</code></pre>
    
    <p><strong>Leslie Smith's 1cycle:</strong></p>
    <ul>
      <li>–û–¥–∏–Ω –±–æ–ª—å—à–æ–π —Ü–∏–∫–ª –≤–º–µ—Å—Ç–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–∞–ª—ã—Ö</li>
      <li>Faster training</li>
      <li>Better generalization</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 11. Polynomial Decay</h2>
    <p><strong>–ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ</strong>:</p>
    <pre><code>lr_t = (lr - lr_end) * (1 - t/T)^power + lr_end

# PyTorch —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
class PolynomialLRDecay:
    def __init__(self, optimizer, max_decay_steps, end_lr=0.0001, power=1.0):
        self.optimizer = optimizer
        self.max_decay_steps = max_decay_steps
        self.end_lr = end_lr
        self.power = power
        self.base_lr = optimizer.param_groups[0]['lr']
    
    def step(self, step):
        if step > self.max_decay_steps:
            lr = self.end_lr
        else:
            lr = (self.base_lr - self.end_lr) * \
                 ((1 - step / self.max_decay_steps) ** self.power) + \
                 self.end_lr
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

# TensorFlow
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=0.1,
    decay_steps=10000,
    end_learning_rate=0.0001,
    power=1.0
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Learning Rate Finder</h2>
    <p><strong>–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ lr</strong> –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º:</p>
    <pre><code># pip install torch-lr-finder
from torch_lr_finder import LRFinder

model = YourModel()
optimizer = optim.SGD(model.parameters(), lr=1e-7)
criterion = nn.CrossEntropyLoss()

lr_finder = LRFinder(model, optimizer, criterion)

# Range test
lr_finder.range_test(train_loader, end_lr=1, num_iter=100)

# –ì—Ä–∞—Ñ–∏–∫
lr_finder.plot()  # –ø–æ–∫–∞–∑–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ loss vs lr

# –õ—É—á—à–∏–π lr
lr = lr_finder.suggest_lr()
print(f"Suggested lr: {lr}")

# –°–±—Ä–æ—Å –º–æ–¥–µ–ª–∏
lr_finder.reset()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–π lr
optimizer = optim.SGD(model.parameters(), lr=lr)</code></pre>
    
    <p><strong>–ö–∞–∫ —á–∏—Ç–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫:</strong></p>
    <ul>
      <li>–ù–∞—á–∞–ª–æ: loss —Å–Ω–∏–∂–∞–µ—Ç—Å—è ‚Üí —Ö–æ—Ä–æ—à–æ</li>
      <li>–û–ø—Ç–∏–º—É–º: —Å–∞–º–æ–µ –±—ã—Å—Ç—Ä–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ</li>
      <li>–ö–æ–Ω–µ—Ü: loss —Ä–∞—Å—Ç–µ—Ç ‚Üí —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π lr</li>
      <li><strong>–í—ã–±–æ—Ä</strong>: 1/10 –æ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ lr</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä</h2>
    <pre><code># –ü–æ–ª–Ω—ã–π pipeline —Å warmup + cosine
import torch
import math

class WarmupCosineScheduler:
    def __init__(self, optimizer, warmup_epochs, total_epochs, 
                 min_lr=1e-6, max_lr=0.001):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.min_lr = min_lr
        self.max_lr = max_lr
    
    def step(self, epoch):
        if epoch < self.warmup_epochs:
            # Linear warmup
            lr = self.min_lr + (self.max_lr - self.min_lr) * \
                 epoch / self.warmup_epochs
        else:
            # Cosine annealing
            progress = (epoch - self.warmup_epochs) / \
                      (self.total_epochs - self.warmup_epochs)
            lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * \
                 (1 + math.cos(math.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        return lr

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
model = YourModel()
optimizer = optim.AdamW(model.parameters(), lr=0.001)
scheduler = WarmupCosineScheduler(
    optimizer,
    warmup_epochs=10,
    total_epochs=100,
    max_lr=0.001,
    min_lr=1e-6
)

# Training
for epoch in range(100):
    lr = scheduler.step(epoch)
    
    for batch in train_loader:
        optimizer.zero_grad()
        loss = train_step(batch)
        loss.backward()
        optimizer.step()
    
    print(f"Epoch {epoch}: lr={lr:.6f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏</h3>
        <ul>
          <li><strong>ResNet/CNN</strong>: StepLR –∏–ª–∏ CosineAnnealing</li>
          <li><strong>Transformers</strong>: Warmup + Linear decay</li>
          <li><strong>–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ</strong>: OneCycleLR</li>
          <li><strong>–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∑–∞–¥–∞—á–∞</strong>: ReduceLROnPlateau</li>
          <li><strong>–ë–æ–ª—å—à–æ–π batch</strong>: –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π warmup</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ö†Ô∏è –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏</h3>
        <ul>
          <li>–ó–∞–±—ã–ª–∏ –≤—ã–∑–≤–∞—Ç—å <code>scheduler.step()</code></li>
          <li>OneCycleLR –≤—ã–∑—ã–≤–∞—é—Ç —Ä–∞–∑ –≤ —ç–ø–æ—Ö—É (–Ω—É–∂–Ω–æ –∫–∞–∂–¥—É—é –∏—Ç–µ—Ä–∞—Ü–∏—é!)</li>
          <li>–ù–µ—Ç warmup –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π</li>
          <li>–°–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ lr</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ lr</h2>
    <pre><code># –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ lr –≤ TensorBoard
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter()

for epoch in range(num_epochs):
    # Training
    train_one_epoch()
    
    # –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π lr
    current_lr = optimizer.param_groups[0]['lr']
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å
    writer.add_scalar('Learning Rate', current_lr, epoch)
    
    # Update scheduler
    scheduler.step()

writer.close()

# –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
print(f"Epoch {epoch}: lr = {optimizer.param_groups[0]['lr']:.6f}")

# –î–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö param groups
for i, param_group in enumerate(optimizer.param_groups):
    print(f"Group {i}: lr = {param_group['lr']:.6f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. –†–∞–∑–Ω—ã–µ lr –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–µ–≤</h2>
    <pre><code># –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
optimizer = optim.AdamW([
    {'params': model.backbone.parameters(), 'lr': 1e-5},
    {'params': model.head.parameters(), 'lr': 1e-3}
])

# –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ä–∞–∑–Ω—ã–µ schedulers
backbone_scheduler = optim.lr_scheduler.StepLR(
    optimizer,
    step_size=30,
    gamma=0.1
)

# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Lambda –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è
lambda1 = lambda epoch: 0.95 ** epoch
lambda2 = lambda epoch: 0.90 ** epoch

scheduler = optim.lr_scheduler.LambdaLR(
    optimizer,
    lr_lambda=[lambda1, lambda2]
)

# BERT-style: —Ä–∞–∑–Ω—ã–µ lr –ø–æ —Å–ª–æ—è–º
def get_optimizer_grouped_parameters(model, base_lr=2e-5):
    no_decay = ['bias', 'LayerNorm.weight']
    
    optimizer_grouped_parameters = [
        # Encoder layers —Å decay
        {
            'params': [p for n, p in model.encoder.named_parameters() 
                      if not any(nd in n for nd in no_decay)],
            'lr': base_lr,
            'weight_decay': 0.01
        },
        # Encoder layers –±–µ–∑ decay
        {
            'params': [p for n, p in model.encoder.named_parameters() 
                      if any(nd in n for nd in no_decay)],
            'lr': base_lr,
            'weight_decay': 0.0
        },
        # Head —Å –±–æ–ª—å—à–∏–º lr
        {
            'params': model.head.parameters(),
            'lr': base_lr * 10
        }
    ]
    
    return optimizer_grouped_parameters

params = get_optimizer_grouped_parameters(model)
optimizer = optim.AdamW(params)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. –°–æ–≤–µ—Ç—ã –∏ —Ç—Ä—é–∫–∏</h2>
    <ul>
      <li><strong>Warmup duration</strong>:
        <ul>
          <li>Small models: 5-10 —ç–ø–æ—Ö</li>
          <li>Transformers: 10% –æ—Ç total steps</li>
          <li>Large batch: –±–æ–ª—å—à–µ warmup</li>
        </ul>
      </li>
      <li><strong>Min lr</strong>:
        <ul>
          <li>–û–±—ã—á–Ω–æ 1e-6 –¥–æ 1e-5</li>
          <li>–ù–∏–∫–æ–≥–¥–∞ –Ω–µ 0 (–º–æ–∂–µ—Ç –∑–∞—Å—Ç—Ä—è—Ç—å)</li>
        </ul>
      </li>
      <li><strong>Restart</strong>:
        <ul>
          <li>Cosine with restarts –ø–æ–º–æ–≥–∞–µ—Ç –≤—ã—Ö–æ–¥–∏—Ç—å –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤</li>
          <li>–ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è</li>
        </ul>
      </li>
      <li><strong>–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ</strong>:
        <ul>
          <li>–°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ scheduler!</li>
          <li><code>torch.save(scheduler.state_dict(), 'scheduler.pt')</code></li>
        </ul>
      </li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ LR Finder –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ lr</li>
      <li>[ ] –í—ã–±—Ä–∞–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π scheduler –¥–ª—è –∑–∞–¥–∞—á–∏</li>
      <li>[ ] –î–æ–±–∞–≤–∏–ª–∏ warmup –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π/batch</li>
      <li>[ ] –ü—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç–µ <code>scheduler.step()</code></li>
      <li>[ ] –õ–æ–≥–∏—Ä—É–µ—Ç–µ lr –≤ TensorBoard/Wandb</li>
      <li>[ ] –°–æ—Ö—Ä–∞–Ω—è–µ—Ç–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ scheduler –ø—Ä–∏ checkpoint</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏–ª–∏ —á—Ç–æ lr –Ω–µ —Å–ª–∏—à–∫–æ–º –±—ã—Å—Ç—Ä–æ –ø–∞–¥–∞–µ—Ç</li>
      <li>[ ] –£—á–ª–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (Transformers, ResNet...)</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç–µ loss/accuracy vs lr</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´Learning rate scheduling ‚Äî —ç—Ç–æ –∫–∞–∫ –ø–µ–¥–∞–ª—å –≥–∞–∑–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. –ù–∞—á–∏–Ω–∞–µ–º —Å –±–æ–ª—å—à–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ —á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ –¥–≤–∏–≥–∞—Ç—å—Å—è, –ø–æ—Ç–æ–º –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –∑–∞–º–µ–¥–ª—è–µ–º—Å—è –¥–ª—è —Ç–æ—á–Ω–æ–π "–ø–∞—Ä–∫–æ–≤–∫–∏" –≤ –æ–ø—Ç–∏–º—É–º–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –±—ã—Å—Ç—Ä–µ–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ¬ª.
    </blockquote>
  </div>

</div>

</body>
</html>

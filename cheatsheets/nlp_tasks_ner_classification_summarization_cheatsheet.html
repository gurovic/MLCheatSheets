<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>NLP Tasks: NER, Classification, Summarization ‚Äî Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      font-size: 0.9em;
      color: #666;
      text-align: center;
      margin-bottom: 20px;
      column-span: all;
    }

    h2 {
      font-size: 1.1em;
      font-weight: 700;
      margin-top: 0;
      color: #1a5fb4;
      border-bottom: 2px solid #e0e8f5;
      padding-bottom: 4px;
    }

    h3 {
      font-size: 0.95em;
      font-weight: 600;
      margin: 8px 0 4px;
      color: #26a269;
    }

    p, ul, ol {
      margin: 6px 0;
      font-size: 0.88em;
      line-height: 1.5;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 3px;
    }

    code {
      background: #f6f8fa;
      padding: 1px 4px;
      border-radius: 3px;
      font-family: 'Consolas', 'Monaco', monospace;
      font-size: 0.9em;
      color: #c7254e;
    }

    pre {
      background: #282c34;
      color: #abb2bf;
      padding: 10px;
      border-radius: 6px;
      overflow-x: auto;
      font-size: 0.8em;
      line-height: 1.4;
      margin: 8px 0;
    }

    pre code {
      background: transparent;
      color: inherit;
      padding: 0;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 8px 0;
      font-size: 0.85em;
    }

    th, td {
      border: 1px solid #ddd;
      padding: 6px 8px;
      text-align: left;
    }

    th {
      background: #e0e8f5;
      font-weight: 600;
      color: #1a5fb4;
    }

    tr:nth-child(even) {
      background: #f9fbff;
    }

    blockquote {
      background: #fff9e6;
      border-left: 4px solid #f6d32d;
      padding: 8px 12px;
      margin: 8px 0;
      font-size: 0.88em;
      font-style: italic;
    }

    .formula {
      background: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      margin: 8px 0;
      font-family: 'Cambria', 'Times New Roman', serif;
      font-size: 0.9em;
      text-align: center;
    }
  </style>
</head>
<body>

<h1>üéØ NLP Tasks: NER, Classification, Summarization</h1>
<div class="subtitle"></div>

<div class="container">

  <div class="block">
    <h2>üî∑ 1. Named Entity Recognition</h2>
    <p>–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π: –ø–µ—Ä—Å–æ–Ω—ã, –º–µ—Å—Ç–∞, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, –¥–∞—Ç—ã.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Token Classification (NER) fine-tuning
from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer
from datasets import load_dataset

dataset = load_dataset("conll2003")
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", num_labels=9)

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        label_ids = [-100 if word_id is None else label[word_id] for word_id in word_ids]
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)</code></pre>
  <div class="block">
    <h2>üî∑ 2. NER Approaches</h2>
    <p>CRF, BiLSTM-CRF, BERT-based models (token classification).</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Abstractive Summarization —Å T5
from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained('t5-small')
model = T5ForConditionalGeneration.from_pretrained('t5-small')

article = "summarize: " + """
The Paris Agreement is a legally binding international treaty on climate change. 
It was adopted by 196 Parties at COP 21 in Paris in 2015 and entered into force in 2016.
"""

inputs = tokenizer(article, return_tensors='pt', max_length=512, truncation=True)
summary_ids = model.generate(inputs['input_ids'], max_length=60, min_length=20)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print(f"Summary: {summary}")</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 3. Text Classification</h2>
    <p>–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: sentiment analysis, topic classification, spam detection.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Text Classification —Å Transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import Trainer, TrainingArguments

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞
text = "This movie is amazing! I loved every moment."
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
outputs = model(**inputs)
prediction = outputs.logits.argmax(-1)
print(f"Sentiment: {'Positive' if prediction.item() == 1 else 'Negative'}")</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 4. Classification Models</h2>
    <p>BERT, RoBERTa, DistilBERT –¥–ª—è sequence classification task.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Named Entity Recognition —Å BERT
from transformers import pipeline

ner_pipeline = pipeline("ner", model="dslim/bert-base-NER", aggregation_strategy="simple")

text = "Apple Inc. was founded by Steve Jobs in Cupertino, California."
entities = ner_pipeline(text)

for entity in entities:
    print(f"{entity['word']}: {entity['entity_group']} (score: {entity['score']:.3f})")</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 5. Text Summarization</h2>
    <p>Extractive (–≤—ã–±–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π) vs Abstractive (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞).</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Token Classification (NER) fine-tuning
from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer
from datasets import load_dataset

dataset = load_dataset("conll2003")
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", num_labels=9)

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        label_ids = [-100 if word_id is None else label[word_id] for word_id in word_ids]
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 6. Extractive Methods</h2>
    <p>TextRank, LexRank, scoring –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Abstractive Summarization —Å T5
from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained('t5-small')
model = T5ForConditionalGeneration.from_pretrained('t5-small')

article = "summarize: " + """
The Paris Agreement is a legally binding international treaty on climate change. 
It was adopted by 196 Parties at COP 21 in Paris in 2015 and entered into force in 2016.
"""

inputs = tokenizer(article, return_tensors='pt', max_length=512, truncation=True)
summary_ids = model.generate(inputs['input_ids'], max_length=60, min_length=20)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print(f"Summary: {summary}")</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 7. Abstractive Methods</h2>
    <p>Seq2seq, Transformer models (BART, T5, Pegasus).</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Text Classification —Å Transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import Trainer, TrainingArguments

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞
text = "This movie is amazing! I loved every moment."
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
outputs = model(**inputs)
prediction = outputs.logits.argmax(-1)
print(f"Sentiment: {'Positive' if prediction.item() == 1 else 'Negative'}")</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 8. Evaluation Metrics</h2>
    <p>NER: F1, precision, recall. Classification: accuracy, F1. Summarization: ROUGE, BLEU.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Named Entity Recognition —Å BERT
from transformers import pipeline

ner_pipeline = pipeline("ner", model="dslim/bert-base-NER", aggregation_strategy="simple")

text = "Apple Inc. was founded by Steve Jobs in Cupertino, California."
entities = ner_pipeline(text)

for entity in entities:
    print(f"{entity['word']}: {entity['entity_group']} (score: {entity['score']:.3f})")</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 9. Fine-tuning BERT</h2>
    <p>–î–æ–±–∞–≤–ª–µ–Ω–∏–µ classification head –∏–ª–∏ CRF layer, transfer learning.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Token Classification (NER) fine-tuning
from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer
from datasets import load_dataset

dataset = load_dataset("conll2003")
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", num_labels=9)

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        label_ids = [-100 if word_id is None else label[word_id] for word_id in word_ids]
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 10. Data Preprocessing</h2>
    <p>Tokenization, handling long sequences, batching.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Abstractive Summarization —Å T5
from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained('t5-small')
model = T5ForConditionalGeneration.from_pretrained('t5-small')

article = "summarize: " + """
The Paris Agreement is a legally binding international treaty on climate change. 
It was adopted by 196 Parties at COP 21 in Paris in 2015 and entered into force in 2016.
"""

inputs = tokenizer(article, return_tensors='pt', max_length=512, truncation=True)
summary_ids = model.generate(inputs['input_ids'], max_length=60, min_length=20)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print(f"Summary: {summary}")</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 11. Common Challenges</h2>
    <p>Class imbalance, domain adaptation, multilingual support.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Text Classification —Å Transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import Trainer, TrainingArguments

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞
text = "This movie is amazing! I loved every moment."
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
outputs = model(**inputs)
prediction = outputs.logits.argmax(-1)
print(f"Sentiment: {'Positive' if prediction.item() == 1 else 'Negative'}")</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 12. Production Tips</h2>
    <p>Model distillation, ONNX export, batching for inference.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Named Entity Recognition —Å BERT
from transformers import pipeline

ner_pipeline = pipeline("ner", model="dslim/bert-base-NER", aggregation_strategy="simple")

text = "Apple Inc. was founded by Steve Jobs in Cupertino, California."
entities = ner_pipeline(text)

for entity in entities:
    print(f"{entity['word']}: {entity['entity_group']} (score: {entity['score']:.3f})")</code></pre>
  </div>
</div>

</div>
</body>
</html>
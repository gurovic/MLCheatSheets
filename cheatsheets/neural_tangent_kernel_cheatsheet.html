<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Neural Tangent Kernel Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üî¨ Neural Tangent Kernel</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è</h2>
    <p><strong>NTK (Neural Tangent Kernel):</strong> –ü—Ä–∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π —à–∏—Ä–∏–Ω–µ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –≤–µ–¥—ë—Ç —Å–µ–±—è –∫–∞–∫ –ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å –≤ feature space, –æ–ø—Ä–µ–¥–µ–ª—è–µ–º–æ–º kernel.</p>
    <ul>
      <li><strong>–û—Ç–∫—Ä—ã—Ç–∏–µ</strong>: Jacot et al. (2018)</li>
      <li><strong>–†–µ–∂–∏–º</strong>: Lazy training (–≤–µ—Å–∞ –ø–æ—á—Ç–∏ –Ω–µ –º–µ–Ω—è—é—Ç—Å—è)</li>
      <li><strong>–†–µ–∑—É–ª—å—Ç–∞—Ç</strong>: –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</li>
      <li><strong>–°–≤—è–∑—å</strong>: Neural networks ‚Üî Kernel methods</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</h2>
    <p><strong>–î–ª—è —Å–µ—Ç–∏ f(x; Œ∏):</strong></p>
    <pre><code>NTK: Œò(x, x') = ‚ü®‚àá_Œ∏ f(x; Œ∏), ‚àá_Œ∏ f(x'; Œ∏)‚ü©

–ü—Ä–∏ m ‚Üí ‚àû (—à–∏—Ä–∏–Ω–∞ ‚Üí ‚àû):
- Œò(x, x') ‚Üí Œò_‚àû(x, x') (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–µ–¥–µ–ª)
- Œò –æ—Å—Ç–∞—ë—Ç—Å—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è

–û–±—É—á–µ–Ω–∏–µ:
f_t(x) = f_0(x) + ‚à´_0^t Œò(x, x_train) dy(x_train)/dt dt</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. Lazy training —Ä–µ–∂–∏–º</h2>
    <ul>
      <li><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: –í–µ—Å–∞ Œ∏ –∏–∑–º–µ–Ω—è—é—Ç—Å—è –º–∞–ª–æ: ||Œ∏_t - Œ∏_0|| ‚Üí 0</li>
      <li><strong>–£—Å–ª–æ–≤–∏–µ</strong>: –û—á–µ–Ω—å —à–∏—Ä–æ–∫–∞—è —Å–µ—Ç—å (m ‚Üí ‚àû)</li>
      <li><strong>–°–ª–µ–¥—Å—Ç–≤–∏–µ</strong>: –õ–∏–Ω–µ–∞—Ä–∏–∑–∞—Ü–∏—è –≤–æ–∫—Ä—É–≥ Œ∏_0</li>
      <li><strong>Kernel –ø–æ—Å—Ç–æ—è–Ω–µ–Ω</strong>: Œò_t ‚âà Œò_0</li>
    </ul>
    <blockquote>
      –í lazy —Ä–µ–∂–∏–º–µ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –Ω–µ —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º, –∞ —Ç–æ–ª—å–∫–æ –ª–∏–Ω–µ–π–Ω–æ –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ü—Ä–∏–º–µ—Ä—ã NTK –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä</h2>
    <table>
      <tr><th>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</th><th>NTK —Ñ–æ—Ä–º—É–ª–∞</th></tr>
      <tr><td><strong>Fully connected (1 layer)</strong></td><td>Œò(x,x') = x^T x' œÉ'(w^T x)œÉ'(w^T x')</td></tr>
      <tr><td><strong>Deep network</strong></td><td>–†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ —á–µ—Ä–µ–∑ —Å–ª–æ–∏</td></tr>
      <tr><td><strong>CNN</strong></td><td>–£—á–∏—Ç—ã–≤–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å –∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–æ–Ω–Ω—É—é –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ—Å—Ç—å</td></tr>
      <tr><td><strong>ResNet</strong></td><td>NTK = NNGP + –ø—É—Ç–µ–≤—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ì–∞—Ä–∞–Ω—Ç–∏–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏</h2>
    <p><strong>–¢–µ–æ—Ä–µ–º–∞ (Jacot et al.):</strong></p>
    <pre><code>–î–ª—è —à–∏—Ä–æ–∫–æ–π —Å–µ—Ç–∏ (m ‚Üí ‚àû):

1. –ì–ª–æ–±–∞–ª—å–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –∫ 0 training loss
2. –°–∫–æ—Ä–æ—Å—Ç—å: —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è
3. –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è: –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è
4. –£—Å–ª–æ–≤–∏–µ: Œª_min(Œò) > 0 (kernel –Ω–µ–≤—ã—Ä–æ–∂–¥–µ–Ω)

Loss –¥–∏–Ω–∞–º–∏–∫–∞:
L(t) = L(0) exp(-Œª_min t)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –°–≤—è–∑—å —Å Gaussian Process</h2>
    <p><strong>Neural Network Gaussian Process (NNGP):</strong></p>
    <ul>
      <li><strong>–î–æ –æ–±—É—á–µ–Ω–∏—è</strong>: f(x; Œ∏_0) ~ GP —Å kernel K_NNGP</li>
      <li><strong>–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è</strong>: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç NTK</li>
      <li><strong>–°–≤—è–∑—å</strong>: NTK = ‚àáK_NNGP</li>
    </ul>
    <pre><code># –ü—Ä–∏–º–µ—Ä –≤—ã—á–∏—Å–ª–µ–Ω–∏—è NNGP kernel
import neural_tangents as nt

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
init_fn, apply_fn, kernel_fn = nt.stax.serial(
    nt.stax.Dense(512), 
    nt.stax.Relu(),
    nt.stax.Dense(1)
)

# NTK –∏ NNGP kernels
ntk = kernel_fn(x_train, x_test, 'ntk')
nngp = kernel_fn(x_train, x_test, 'nngp')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</h2>
    <pre><code>import neural_tangents as nt

# –û–±—É—á–µ–Ω–∏–µ —Å NTK (–±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞!)
kernel_fn = nt.empirical_ntk_fn(apply_fn)
ntk_train_train = kernel_fn(x_train, x_train, params)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
from jax.experimental import stax
predict_fn = nt.predict.gradient_descent_mse_ensemble(
    kernel_fn, 
    x_train, 
    y_train
)

# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
y_pred = predict_fn(t=None, x_test=x_test)

# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ: –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ!</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Feature learning vs Lazy</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ Feature Learning —Ä–µ–∂–∏–º</h3>
        <ul>
          <li>–ö–æ–Ω–µ—á–Ω–∞—è —à–∏—Ä–∏–Ω–∞</li>
          <li>–í–µ—Å–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—è—é—Ç—Å—è</li>
          <li>Kernel —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç</li>
          <li>–õ—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å</li>
          <li>–¢–∏–ø–∏—á–Ω—ã–π –¥–ª—è –ø—Ä–∞–∫—Ç–∏–∫–∏</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ö†Ô∏è Lazy —Ä–µ–∂–∏–º (NTK)</h3>
        <ul>
          <li>–ë–µ—Å–∫–æ–Ω–µ—á–Ω–∞—è —à–∏—Ä–∏–Ω–∞</li>
          <li>–í–µ—Å–∞ –ø–æ—á—Ç–∏ –Ω–µ –º–µ–Ω—è—é—Ç—Å—è</li>
          <li>Kernel –ø–æ—Å—Ç–æ—è–Ω–µ–Ω</li>
          <li>–•—É–∂–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏</li>
          <li>–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ö–æ–≥–¥–∞ NTK –ø—Ä–∏–º–µ–Ω–∏–º</h2>
    <table>
      <tr><th>–°—Ü–µ–Ω–∞—Ä–∏–π</th><th>NTK –ø—Ä–∏–º–µ–Ω–∏–º?</th></tr>
      <tr><td>–û—á–µ–Ω—å —à–∏—Ä–æ–∫–∏–µ —Å–µ—Ç–∏</td><td>‚úÖ –î–∞</td></tr>
      <tr><td>–ú–∞–ª—ã–π learning rate</td><td>‚úÖ –ü—Ä–∏–±–ª–∏–∂—ë–Ω–Ω–æ</td></tr>
      <tr><td>–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è</td><td>‚úÖ –•–æ—Ä–æ—à–µ–µ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ</td></tr>
      <tr><td>–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ ResNet</td><td>‚ùå Feature learning –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç</td></tr>
      <tr><td>Transfer learning</td><td>‚ùå –ù–µ lazy</td></tr>
      <tr><td>–ú–∞–ª—ã–µ —Å–µ—Ç–∏</td><td>‚ùå –î–∞–ª–µ–∫–æ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 10. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞</h2>
    <pre><code>import torch

def check_lazy_regime(model, data_loader):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞: –≤ lazy —Ä–µ–∂–∏–º–µ –∏–ª–∏ –Ω–µ—Ç?"""
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
    init_params = {name: p.clone() 
                   for name, p in model.named_parameters()}
    
    # –û–±—É—á–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤
    for _ in range(100):
        train_step(model, data_loader)
    
    # –ò–∑–º–µ—Ä–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏–µ
    total_change = 0
    total_norm = 0
    for name, p in model.named_parameters():
        change = (p - init_params[name]).norm()
        norm = init_params[name].norm()
        total_change += change
        total_norm += norm
    
    relative_change = total_change / total_norm
    
    if relative_change < 0.01:
        print("Lazy regime: –≤–µ—Å–∞ –ø–æ—á—Ç–∏ –Ω–µ –º–µ–Ω—è—é—Ç—Å—è")
    else:
        print("Feature learning: –≤–µ—Å–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—è—é—Ç—Å—è")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è NTK —Ç–µ–æ—Ä–∏–∏</h2>
    <ul>
      <li><strong>–ù–µ –æ–±—ä—è—Å–Ω—è–µ—Ç —É—Å–ø–µ—Ö –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Å–µ—Ç–µ–π</strong>: –†–µ–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –Ω–µ –≤ lazy —Ä–µ–∂–∏–º–µ</li>
      <li><strong>–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ö—É–∂–µ</strong>: Kernel methods < feature learning</li>
      <li><strong>–ù–µ—Ç transfer learning</strong>: –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏</li>
      <li><strong>–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å</strong>: Kernel matrix O(n¬≤)</li>
    </ul>
    <blockquote>
      NTK –≤–∞–∂–µ–Ω –¥–ª—è —Ç–µ–æ—Ä–∏–∏, –Ω–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–µ—Ç–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –∏–Ω–∞—á–µ - –æ–Ω–∏ —É—á–∞—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 12. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>Finite width corrections</strong>: –£—á—ë—Ç –∫–æ–Ω–µ—á–Ω–æ–π —à–∏—Ä–∏–Ω—ã</li>
      <li><strong>Feature learning theory</strong>: –í—ã—Ö–æ–¥ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã lazy</li>
      <li><strong>Mean field theory</strong>: –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥</li>
      <li><strong>Tensor programs</strong>: –û–±—â–∞—è —Ç–µ–æ—Ä–∏—è –¥–ª—è –ª—é–±–æ–π —à–∏—Ä–∏–Ω—ã</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å–∞–π—Ç—ã</h2>
    <ul>
      <li><strong>–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–∞</strong>: NTK –∑–∞–≤–∏—Å–∏—Ç –æ—Ç Œ∏_0</li>
      <li><strong>–®–∏—Ä–∏–Ω–∞ vs –≥–ª—É–±–∏–Ω–∞</strong>: –®–∏—Ä–æ–∫–∏–µ ‚Üí lazy, –≥–ª—É–±–æ–∫–∏–µ ‚Üí feature learning</li>
      <li><strong>Learning rate</strong>: –ú–∞–ª—ã–π LR ‚Üí –±–ª–∏–∂–µ –∫ lazy</li>
      <li><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong>: ResNet –¥–∞–ª—å—à–µ –æ—Ç lazy, —á–µ–º vanilla</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã</h2>
    <ul>
      <li>[ ] NTK –æ–±—ä—è—Å–Ω—è–µ—Ç <strong>–æ—á–µ–Ω—å —à–∏—Ä–æ–∫–∏–µ —Å–µ—Ç–∏</strong></li>
      <li>[ ] –í lazy —Ä–µ–∂–∏–º–µ —Å–µ—Ç—å = <strong>kernel method</strong></li>
      <li>[ ] –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç <strong>–≥–ª–æ–±–∞–ª—å–Ω—É—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</strong></li>
      <li>[ ] –ù–ï –æ–±—ä—è—Å–Ω—è–µ—Ç <strong>–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–µ—Ç–∏</strong></li>
      <li>[ ] Feature learning <strong>–ª—É—á—à–µ</strong> lazy training</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´NTK –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ—á–µ–Ω—å-–æ—á–µ–Ω—å —à–∏—Ä–æ–∫–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ø—Ä–æ—Å—Ç–æ–π ‚Äî –æ–Ω–∞ –Ω–µ —É—á–∏—Ç—Å—è –Ω–æ–≤—ã–º —Å–ø–æ—Å–æ–±–∞–º —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –¥–∞–Ω–Ω—ã–µ, –∞ –ø—Ä–æ—Å—Ç–æ –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Ç–æ, —á—Ç–æ —É–∂–µ —É–º–µ–µ—Ç. –≠—Ç–æ –∫–∞–∫ –Ω–∞–Ω—è—Ç—å –º–∏–ª–ª–∏–æ–Ω —Å—Ç–∞–∂—ë—Ä–æ–≤ –≤–º–µ—Å—Ç–æ –æ–¥–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞ ‚Äî –æ–Ω–∏ —Å–¥–µ–ª–∞—é—Ç —Ä–∞–±–æ—Ç—É, –Ω–æ –Ω–µ –ø—Ä–∏–Ω–µ—Å—É—Ç –Ω–æ–≤—ã—Ö –∏–¥–µ–π¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

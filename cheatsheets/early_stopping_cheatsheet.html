<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>–†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>‚è±Ô∏è –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å Early Stopping</h2>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –¥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è</li>
      <li><strong>–ú–µ—Ç–æ–¥</strong>: –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏</li>
      <li><strong>–ö—Ä–∏—Ç–µ—Ä–∏–π</strong>: –º–µ—Ç—Ä–∏–∫–∞ –ø–µ—Ä–µ—Å—Ç–∞—ë—Ç —É–ª—É—á—à–∞—Ç—å—Å—è</li>
      <li><strong>–≠—Ñ—Ñ–µ–∫—Ç</strong>: —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏</li>
    </ul>
    <blockquote>
      –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ ‚Äî —ç—Ç–æ –∫–∞–∫ –∑–Ω–∞—Ç—å, –∫–æ–≥–¥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ: —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ ‚Äî –Ω–µ –ø–æ–ª—É—á–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ ‚Äî –ø–µ—Ä–µ—Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç–µ—Å—å.
    </blockquote>

    </div>
<div class="block">
    <h2>üî∑ 2. –ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã</h2>
    <ol>
      <li>–†–∞–∑–¥–µ–ª–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/validation/test</li>
      <li>–ù–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏</li>
      <li>–ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –Ω–∞ validation</li>
      <li>–°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å</li>
      <li>–ï—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∞ –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è N —ç–ø–æ—Ö –ø–æ–¥—Ä—è–¥ ‚Äî –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è</li>
      <li>–í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –ª—É—á—à–µ–π —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏</li>
    </ol>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è</h2>
    <pre><code>from sklearn.model_selection import train_test_split

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

# Early stopping
best_val_loss = float('inf')
patience = 10  # —Å–∫–æ–ª—å–∫–æ —ç–ø–æ—Ö –∂–¥–∞—Ç—å
patience_counter = 0
best_model = None

for epoch in range(max_epochs):
    # –û–±—É—á–µ–Ω–∏–µ
    model.fit(X_train, y_train)
    
    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    val_loss = model.evaluate(X_val, y_val)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model = copy.deepcopy(model)
        patience_counter = 0
    else:
        patience_counter += 1
    
    # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
    if patience_counter >= patience:
        print(f"Early stopping at epoch {epoch}")
        break

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
model = best_model</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Keras/TensorFlow</h2>
    <pre><code>from tensorflow.keras.callbacks import EarlyStopping

# –°–æ–∑–¥–∞–Ω–∏–µ callback
early_stop = EarlyStopping(
    monitor='val_loss',      # –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    patience=10,             # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
    restore_best_weights=True,  # –≤–µ—Ä–Ω—É—Ç—å –ª—É—á—à–∏–µ –≤–µ—Å–∞
    verbose=1,               # –≤—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    mode='min'               # min –¥–ª—è loss, max –¥–ª—è accuracy
)

# –û–±—É—á–µ–Ω–∏–µ —Å early stopping
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=1000,
    callbacks=[early_stop],
    batch_size=32
)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∞–∫–∏—Ö —ç–ø–æ—Ö–∞—Ö –æ—Å—Ç–∞–Ω–æ–≤–∏–ª–∏—Å—å
stopped_epoch = early_stop.stopped_epoch
print(f"Training stopped at epoch: {stopped_epoch}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. PyTorch</h2>
    <pre><code>import torch
import copy

class EarlyStopping:
    def __init__(self, patience=10, delta=0, verbose=False):
        self.patience = patience
        self.delta = delta
        self.verbose = verbose
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.best_model = None
    
    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss > self.best_loss - self.delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter}/{self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.save_checkpoint(model)
            self.counter = 0
    
    def save_checkpoint(self, model):
        if self.verbose:
            print(f'Validation loss decreased, saving model...')
        self.best_model = copy.deepcopy(model.state_dict())

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
early_stopping = EarlyStopping(patience=10, verbose=True)

for epoch in range(max_epochs):
    # Train
    model.train()
    train_loss = train_epoch(model, train_loader, optimizer)
    
    # Validate
    model.eval()
    val_loss = validate_epoch(model, val_loader)
    
    # Early stopping
    early_stopping(val_loss, model)
    
    if early_stopping.early_stop:
        print(f"Early stopping at epoch {epoch}")
        break

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
model.load_state_dict(early_stopping.best_model)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã Early Stopping</h2>
    <table>
      <tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</th></tr>
      <tr><td><code>patience</code></td><td>–≠–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è</td><td>5-20 (–±–æ–ª—å—à–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π)</td></tr>
      <tr><td><code>min_delta</code></td><td>–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ</td><td>0.0001-0.001</td></tr>
      <tr><td><code>monitor</code></td><td>–ö–∞–∫—É—é –º–µ—Ç—Ä–∏–∫—É –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å</td><td>val_loss, val_accuracy</td></tr>
      <tr><td><code>mode</code></td><td>min –∏–ª–∏ max</td><td>min –¥–ª—è loss, max –¥–ª—è accuracy</td></tr>
      <tr><td><code>restore_best</code></td><td>–í–µ—Ä–Ω—É—Ç—å –ª—É—á—à–∏–µ –≤–µ—Å–∞</td><td>True (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è</h2>
    <p><strong>Learning Curves –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç:</strong></p>
    <ul>
      <li>–î–∏–Ω–∞–º–∏–∫—É –æ–±—É—á–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏</li>
      <li>–ù–∞–ª–∏—á–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è/–Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏—è</li>
      <li>–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω–∏—Ç—å early stopping</li>
      <li>–ù—É–∂–Ω–æ –ª–∏ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö</li>
    </ul>
    <pre><code>import matplotlib.pyplot as plt

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫—Ä–∏–≤—ã—Ö
plt.figure(figsize=(12, 5))

# Loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.xlabel('–≠–ø–æ—Ö–∞')
plt.ylabel('Loss')
plt.legend()
plt.title('–ö—Ä–∏–≤—ã–µ –ø–æ—Ç–µ—Ä—å')
plt.grid(True)

# Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('–≠–ø–æ—Ö–∞')
plt.ylabel('Accuracy')
plt.legend()
plt.title('–ö—Ä–∏–≤—ã–µ —Ç–æ—á–Ω–æ—Å—Ç–∏')
plt.grid(True)

plt.tight_layout()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫—Ä–∏–≤—ã—Ö</h2>
    <table>
      <tr><th>–ü–∞—Ç—Ç–µ—Ä–Ω</th><th>Train</th><th>Val</th><th>–î–∏–∞–≥–Ω–æ–∑</th><th>–†–µ—à–µ–Ω–∏–µ</th></tr>
      <tr><td>–•–æ—Ä–æ—à–∞—è –º–æ–¥–µ–ª—å</td><td>‚Üì</td><td>‚Üì</td><td>–°—Ö–æ–¥–∏—Ç—Å—è</td><td>‚úÖ –í—Å—ë –æ–∫</td></tr>
      <tr><td>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</td><td>‚Üì‚Üì</td><td>‚Üë</td><td>Overfitting</td><td>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, Early stopping</td></tr>
      <tr><td>–ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ</td><td>–≤—ã—Å–æ–∫–∏–π</td><td>–≤—ã—Å–æ–∫–∏–π</td><td>Underfitting</td><td>–°–ª–æ–∂–Ω–µ–µ –º–æ–¥–µ–ª—å, –±–æ–ª—å—à–µ —ç–ø–æ—Ö</td></tr>
      <tr><td>–†–∞–∑—Ä—ã–≤</td><td>–Ω–∏–∑–∫–∏–π</td><td>–≤—ã—Å–æ–∫–∏–π</td><td>High variance</td><td>–ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö</td></tr>
      <tr><td>Plateau</td><td>‚Üí</td><td>‚Üí</td><td>–ó–∞—Å—Ç—Ä—è–ª–∏</td><td>–ò–∑–º–µ–Ω–∏—Ç—å LR, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 9. –í–∏–∑—É–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –•–æ—Ä–æ—à–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</h3>
        <ul>
          <li>Train –∏ val loss —Å–Ω–∏–∂–∞—é—Ç—Å—è –≤–º–µ—Å—Ç–µ</li>
          <li>–ù–µ–±–æ–ª—å—à–æ–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –Ω–∏–º–∏</li>
          <li>–ü–ª–∞–≤–Ω—ã–µ –∫—Ä–∏–≤—ã–µ –±–µ–∑ —Å–∫–∞—á–∫–æ–≤</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</h3>
        <ul>
          <li>Train loss –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –ø–∞–¥–∞—Ç—å</li>
          <li>Val loss —Ä–∞—Å—Ç—ë—Ç –∏–ª–∏ plateau</li>
          <li>–ë–æ–ª—å—à–æ–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É train –∏ val</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ö—Ä–∏–≤—ã–µ —Ä–∞–∑–º–µ—Ä–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏</h2>
    <pre><code>from sklearn.model_selection import learning_curve
import numpy as np

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫—Ä–∏–≤—ã—Ö
train_sizes, train_scores, val_scores = learning_curve(
    model, X, y,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1
)

# –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
train_mean = -np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = -np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label='Train', marker='o')
plt.plot(train_sizes, val_mean, label='Validation', marker='o')
plt.fill_between(train_sizes, 
                 train_mean - train_std, 
                 train_mean + train_std, 
                 alpha=0.15)
plt.fill_between(train_sizes, 
                 val_mean - val_std, 
                 val_mean + val_std, 
                 alpha=0.15)
plt.xlabel('–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏')
plt.ylabel('MSE')
plt.title('Learning Curves: —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏')
plt.legend()
plt.grid(True)
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. ModelCheckpoint (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ)</h2>
    <pre><code>from tensorflow.keras.callbacks import ModelCheckpoint

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
checkpoint = ModelCheckpoint(
    'best_model.h5',
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

# –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    callbacks=[early_stop, checkpoint]
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
from tensorflow.keras.models import load_model
best_model = load_model('best_model.h5')

# PyTorch
torch.save(model.state_dict(), 'best_model.pt')
model.load_state_dict(torch.load('best_model.pt'))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —Å –¥—Ä—É–≥–∏–º–∏ callbacks</h2>
    <pre><code>from tensorflow.keras.callbacks import (
    EarlyStopping, 
    ModelCheckpoint,
    ReduceLROnPlateau,
    TensorBoard
)

# Early stopping
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True
)

# Model checkpoint
checkpoint = ModelCheckpoint(
    'model_epoch_{epoch:02d}.h5',
    save_best_only=True
)

# Reduce learning rate
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-7,
    verbose=1
)

# TensorBoard
tensorboard = TensorBoard(log_dir='./logs')

# –í—Å–µ –≤–º–µ—Å—Ç–µ
callbacks = [early_stop, checkpoint, reduce_lr, tensorboard]

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=1000,
    callbacks=callbacks
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ê–Ω–∞–ª–∏–∑ –∫—Ä–∏–≤—ã—Ö</h2>
    <pre><code>def analyze_learning_curves(history):
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è"""
    train_loss = history.history['loss']
    val_loss = history.history['val_loss']
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    last_train = np.mean(train_loss[-5:])
    last_val = np.mean(val_loss[-5:])
    
    if last_val > last_train * 1.2:
        print("‚ö†Ô∏è –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
        print("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print("- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é (L1/L2, Dropout)")
        print("- –£–º–µ–Ω—å—à–∏—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏")
        print("- –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö")
        print("- –ü—Ä–∏–º–µ–Ω–∏—Ç—å data augmentation")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏—è
    if last_train > 0.5 and last_val > 0.5:  # –ø–æ—Ä–æ–≥–∏ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –∑–∞–¥–∞—á–∏
        print("‚ö†Ô∏è –ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
        print("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print("- –£–≤–µ–ª–∏—á–∏—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏")
        print("- –£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö")
        print("- –£–º–µ–Ω—å—à–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    recent_change = abs(np.mean(val_loss[-5:]) - np.mean(val_loss[-10:-5]))
    if recent_change < 0.001:
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—à–ª–∞—Å—å")
    else:
        print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –µ—â—ë –º–æ–∂–µ—Ç —É–ª—É—á—à–∞—Ç—å—Å—è")
    
    return {
        'train_loss': last_train,
        'val_loss': last_val,
        'gap': last_val - last_train
    }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
analysis = analyze_learning_curves(history)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –†–∞–∑–¥–µ–ª–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/val/test</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å early stopping (patience=10-20)</li>
      <li>[ ] –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å (restore_best_weights)</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –∏ train, –∏ validation –º–µ—Ç—Ä–∏–∫–∏</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</li>
      <li>[ ] –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Å ReduceLROnPlateau</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TensorBoard –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞</li>
      <li>[ ] –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´Early stopping ‚Äî —ç—Ç–æ –∫–∞–∫ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –Ω–∞ —Ä–µ–ø–µ—Ç–∏—Ü–∏—è—Ö –ø–µ—Ä–µ–¥ –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ–º –≤ –Ω—É–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç: —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ ‚Äî –Ω–µ –±—É–¥–µ—Ç–µ –≥–æ—Ç–æ–≤—ã, —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ ‚Äî –ø–µ—Ä–µ–≥–æ—Ä–∏—Ç–µ –∏ –≤—ã—Å—Ç—É–ø–∏—Ç–µ —Ö—É–∂–µ¬ª.
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <ul>
      <li><strong>Patience</strong>: 5-10 –¥–ª—è –º–∞–ª—ã—Ö –º–æ–¥–µ–ª–µ–π, 20-50 –¥–ª—è –±–æ–ª—å—à–∏—Ö</li>
      <li><strong>–í–∞–ª–∏–¥–∞—Ü–∏—è</strong>: 15-20% –æ—Ç –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li><strong>–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥</strong>: –≤—Å–µ–≥–¥–∞ val_loss, –Ω–µ train_loss</li>
      <li><strong>Min_delta</strong>: 0 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, —É–≤–µ–ª–∏—á—å—Ç–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏</li>
      <li><strong>Restore_best</strong>: –≤—Å–µ–≥–¥–∞ True</li>
      <li><strong>–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ</strong>: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ wandb –∏–ª–∏ tensorboard</li>
    </ul>
    <pre><code># –ü—Ä–∏–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
from sklearn.model_selection import train_test_split

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

# Callbacks
callbacks = [
    EarlyStopping(patience=20, restore_best_weights=True),
    ModelCheckpoint('best_model.h5', save_best_only=True),
    ReduceLROnPlateau(patience=5, factor=0.5)
]

# –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=1000,
    callbacks=callbacks,
    verbose=1
)

# –û—Ü–µ–Ω–∫–∞ –Ω–∞ test
test_loss = model.evaluate(X_test, y_test)</code></pre>
  </div>



</div>
</body>
</html>

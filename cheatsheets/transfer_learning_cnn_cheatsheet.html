<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Transfer Learning –¥–ª—è CNN (Pre-trained –º–æ–¥–µ–ª–∏ –∏ fine-tuning) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.88em;
      margin: 6px 0;
    }

    th, td {
      padding: 6px 8px;
      text-align: left;
      border: 1px solid #e0e7ff;
    }

    th {
      background-color: #1a5fb4;
      color: white;
      font-weight: 700;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ Transfer Learning –¥–ª—è CNN (Pre-trained –º–æ–¥–µ–ª–∏ –∏ fine-tuning)</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. LeNet-5 (1998)</h2>
    <p><strong>–ü–∏–æ–Ω–µ—Ä CNN</strong>: –æ–¥–Ω–∞ –∏–∑ –ø–µ—Ä–≤—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è—ã—Ö —Å–µ—Ç–µ–π, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ Yann LeCun</p>
    <ul>
      <li><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong>: Conv(6) ‚Üí Pool ‚Üí Conv(16) ‚Üí Pool ‚Üí FC(120) ‚Üí FC(84) ‚Üí FC(10)</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ü–∏—Ñ—Ä MNIST</li>
      <li><strong>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏</strong>: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç tanh –∞–∫—Ç–∏–≤–∞—Ü–∏—é, —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞ 32√ó32</li>
      <li><strong>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</strong>: ~60K –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
    </ul>
    <pre><code>import torch.nn as nn

class LeNet5(nn.Module):
    def __init__(self):
        super(LeNet5, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)  # 32x32 -> 28x28
        self.pool = nn.AvgPool2d(2, 2)   # 28x28 -> 14x14
        self.conv2 = nn.Conv2d(6, 16, 5) # 14x14 -> 10x10
        # –ü–æ—Å–ª–µ pool: 10x10 -> 5x5
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
    
    def forward(self, x):
        x = self.pool(torch.tanh(self.conv1(x)))
        x = self.pool(torch.tanh(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = torch.tanh(self.fc1(x))
        x = torch.tanh(self.fc2(x))
        x = self.fc3(x)
        return x</code></pre>

  <div class="block">
    <h2>üî∑ 2. AlexNet (2012)</h2>
    <p><strong>–†–µ–≤–æ–ª—é—Ü–∏—è ImageNet</strong>: –ø–æ–±–µ–¥–∏—Ç–µ–ª—å ILSVRC 2012, —Å–Ω–∏–∑–∏–ª –æ—à–∏–±–∫—É –¥–æ 16.4%</p>
    <ul>
      <li><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong>: 5 conv + 3 FC —Å–ª–æ—è, –≤—Å–µ–≥–æ ~60M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
      <li><strong>–ò–Ω–Ω–æ–≤–∞—Ü–∏–∏</strong>:
        <ul>
          <li>ReLU –∞–∫—Ç–∏–≤–∞—Ü–∏—è (–≤–º–µ—Å—Ç–æ tanh/sigmoid)</li>
          <li>Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</li>
          <li>Data augmentation (–ø–µ—Ä–µ–≤–æ—Ä–æ—Ç—ã, –æ–±—Ä–µ–∑–∫–∏)</li>
          <li>Local Response Normalization (LRN)</li>
          <li>–û–±—É—á–µ–Ω–∏–µ –Ω–∞ GPU</li>
        </ul>
      </li>
      <li><strong>–í—Ö–æ–¥</strong>: 224√ó224√ó3 RGB –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è</li>
    </ul>
    <pre><code>class AlexNet(nn.Module):
    def __init__(self, num_classes=1000):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(256 * 6 * 6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 6 * 6)
        x = self.classifier(x)
        return x</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. VGG (2014)</h2>
    <p><strong>–ì–ª—É–±–∏–Ω–∞ –∏ –ø—Ä–æ—Å—Ç–æ—Ç–∞</strong>: –æ—á–µ–Ω—å –≥–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏ —Å –ø—Ä–æ—Å—Ç–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π</p>
    <ul>
      <li><strong>–í–∞—Ä–∏–∞–Ω—Ç—ã</strong>: VGG-16 (16 —Å–ª–æ—ë–≤), VGG-19 (19 —Å–ª–æ—ë–≤)</li>
      <li><strong>–ü—Ä–∏–Ω—Ü–∏–ø</strong>: —Ç–æ–ª—å–∫–æ 3√ó3 —Å–≤—ë—Ä—Ç–∫–∏, —É–¥–≤–æ–µ–Ω–∏–µ —á–∏—Å–ª–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ pooling</li>
      <li><strong>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</strong>: VGG-16 ~138M, VGG-19 ~144M</li>
      <li><strong>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏</strong>: –Ω–µ—Å–∫–æ–ª—å–∫–æ 3√ó3 —Å–≤—ë—Ä—Ç–æ–∫ –¥–∞—é—Ç —Ç–∞–∫–æ–µ –∂–µ receptive field –∫–∞–∫ –æ–¥–Ω–∞ –±–æ–ª—å—à–∞—è</li>
    </ul>
    <pre><code># VGG-16 –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
class VGG16(nn.Module):
    def __init__(self, num_classes=1000):
        super(VGG16, self).__init__()
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 2
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 3
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 4
            nn.Conv2d(256, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 5
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, num_classes),
        )</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä</h2>
    <table>
      <tr><th>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</th><th>–ì–æ–¥</th><th>–°–ª–æ–∏</th><th>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</th><th>Top-5 Error (ImageNet)</th></tr>
      <tr><td>LeNet-5</td><td>1998</td><td>7</td><td>60K</td><td>N/A (MNIST)</td></tr>
      <tr><td>AlexNet</td><td>2012</td><td>8</td><td>60M</td><td>16.4%</td></tr>
      <tr><td>VGG-16</td><td>2014</td><td>16</td><td>138M</td><td>7.3%</td></tr>
      <tr><td>VGG-19</td><td>2014</td><td>19</td><td>144M</td><td>7.3%</td></tr>
      <tr><td>GoogLeNet</td><td>2014</td><td>22</td><td>6.8M</td><td>6.7%</td></tr>
      <tr><td>ResNet-50</td><td>2015</td><td>50</td><td>25.5M</td><td>3.6%</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 5. GoogLeNet / Inception (2014)</h2>
    <p><strong>Inception –º–æ–¥—É–ª–∏</strong>: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Å–≤—ë—Ä—Ç–∫–∏ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤</p>
    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –≤–º–µ—Å—Ç–æ –≤—ã–±–æ—Ä–∞ —Ä–∞–∑–º–µ—Ä–∞ —Å–≤—ë—Ä—Ç–∫–∏, –ø—Ä–∏–º–µ–Ω–∏—Ç—å –≤—Å–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ</li>
      <li><strong>–ú–æ–¥—É–ª—å Inception</strong>: 1√ó1, 3√ó3, 5√ó5 —Å–≤—ë—Ä—Ç–∫–∏ + 3√ó3 pooling –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ</li>
      <li><strong>1√ó1 —Å–≤—ë—Ä—Ç–∫–∏</strong>: —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥ –¥–æ—Ä–æ–≥–∏–º–∏ –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏</li>
      <li><strong>Auxiliary classifiers</strong>: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤—ã—Ö–æ–¥—ã –¥–ª—è –±–æ—Ä—å–±—ã —Å vanishing gradient</li>
    </ul>
    <pre><code>class InceptionModule(nn.Module):
    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, 
                 red_5x5, out_5x5, out_pool):
        super(InceptionModule, self).__init__()
        
        # 1x1 conv
        self.branch1 = nn.Sequential(
            nn.Conv2d(in_channels, out_1x1, 1),
            nn.ReLU(inplace=True)
        )
        
        # 1x1 -> 3x3 conv
        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, red_3x3, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(red_3x3, out_3x3, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        
        # 1x1 -> 5x5 conv
        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, red_5x5, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(red_5x5, out_5x5, 5, padding=2),
            nn.ReLU(inplace=True)
        )
        
        # 3x3 pool -> 1x1 conv
        self.branch4 = nn.Sequential(
            nn.MaxPool2d(3, stride=1, padding=1),
            nn.Conv2d(in_channels, out_pool, 1),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        return torch.cat([
            self.branch1(x),
            self.branch2(x),
            self.branch3(x),
            self.branch4(x)
        ], dim=1)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π</h2>
    <pre><code># PyTorch
import torchvision.models as models

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
alexnet = models.alexnet(pretrained=True)
vgg16 = models.vgg16(pretrained=True)
vgg19 = models.vgg19(pretrained=True)

# –ü–µ—Ä–µ–≤–æ–¥ –≤ —Ä–µ–∂–∏–º inference
alexnet.eval()

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
import torch
from PIL import Image
from torchvision import transforms

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

img = Image.open('image.jpg')
img_tensor = preprocess(img).unsqueeze(0)

with torch.no_grad():
    output = alexnet(img_tensor)
    probabilities = torch.nn.functional.softmax(output[0], dim=0)
    
# Top-5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(5):
    print(f"{top5_catid[i]}: {top5_prob[i].item():.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Fine-tuning –¥–ª—è —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö</h2>
    <pre><code># –ó–∞–º–æ—Ä–æ–∑–∫–∞ –≤—Å–µ—Ö —Å–ª–æ—ë–≤ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
vgg16 = models.vgg16(pretrained=True)

# –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
for param in vgg16.parameters():
    param.requires_grad = False

# –ó–∞–º–µ–Ω–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π
num_features = vgg16.classifier[6].in_features
vgg16.classifier[6] = nn.Linear(num_features, 10)  # 10 –∫–ª–∞—Å—Å–æ–≤

# –û–±—É—á–µ–Ω–∏–µ
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(vgg16.classifier[6].parameters(), 
                             lr=0.001)

# –û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
for epoch in range(10):
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = vgg16(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# –†–∞–∑–º–æ—Ä–æ–∑–∫–∞ –≤—Å–µ—Ö —Å–ª–æ—ë–≤ –¥–ª—è fine-tuning
for param in vgg16.parameters():
    param.requires_grad = True

optimizer = torch.optim.Adam(vgg16.parameters(), lr=0.0001)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Feature extraction</h2>
    <pre><code># –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CNN –∫–∞–∫ feature extractor
vgg16 = models.vgg16(pretrained=True)
vgg16.eval()

# –£–¥–∞–ª–µ–Ω–∏–µ classifier
feature_extractor = nn.Sequential(*list(vgg16.children())[:-1])

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
features_list = []
with torch.no_grad():
    for inputs, _ in data_loader:
        features = feature_extractor(inputs)
        features = features.view(features.size(0), -1)
        features_list.append(features.cpu().numpy())

features = np.vstack(features_list)
print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features.shape}")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    features, labels, test_size=0.2, random_state=42
)

svm = SVC(kernel='rbf')
svm.fit(X_train, y_train)
accuracy = svm.score(X_test, y_test)
print(f"Accuracy: {accuracy:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Keras/TensorFlow —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è</h2>
    <pre><code>import tensorflow as tf
from tensorflow.keras.applications import VGG16, AlexNet
from tensorflow.keras import layers, models

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
base_model = VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–µ–π –º–æ–¥–µ–ª–∏
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

# –ó–∞–º–æ—Ä–æ–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
base_model.trainable = False

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    train_dataset,
    epochs=10,
    validation_data=val_dataset
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –≠–≤–æ–ª—é—Ü–∏—è –∏–¥–µ–π</h2>
    <ul>
      <li><strong>LeNet ‚Üí AlexNet</strong>: ReLU, dropout, GPU, data augmentation</li>
      <li><strong>AlexNet ‚Üí VGG</strong>: –≥–ª—É–±–∂–µ, –ø—Ä–æ—â–µ (—Ç–æ–ª—å–∫–æ 3√ó3), —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω–æ—Å—Ç—å</li>
      <li><strong>VGG ‚Üí GoogLeNet</strong>: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Å–≤—ë—Ä—Ç–∫–∏, 1√ó1 –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π</li>
      <li><strong>GoogLeNet ‚Üí ResNet</strong>: skip connections, —Å–≤–µ—Ä—Ö–≥–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏ (152+ —Å–ª–æ—è)</li>
      <li><strong>–î–∞–ª–µ–µ</strong>: DenseNet, EfficientNet, Vision Transformers</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∂–¥—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É</h2>
    <table>
      <tr><th>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr>
      <tr><td>LeNet</td><td>–ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏, –º–∞–ª—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (28√ó28)</td></tr>
      <tr><td>AlexNet</td><td>–ë–∞–∑–æ–≤—ã–π transfer learning, –æ–±—É—á–∞—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã</td></tr>
      <tr><td>VGG</td><td>Feature extraction, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è, style transfer</td></tr>
      <tr><td>GoogLeNet</td><td>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å, –Ω—É–∂–Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å</td></tr>
      <tr><td>ResNet</td><td>–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä, –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 12. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏</h2>
    <pre><code># Mixed precision training (PyTorch)
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for epoch in range(epochs):
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        
        # Forward pass —Å autocast
        with autocast():
            outputs = model(inputs)
            loss = criterion(outputs, labels)
        
        # Backward pass
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

# Gradient checkpointing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
import torch.utils.checkpoint as checkpoint

class VGGWithCheckpointing(nn.Module):
    def forward(self, x):
        x = checkpoint.checkpoint(self.block1, x)
        x = checkpoint.checkpoint(self.block2, x)
        # ...
        return x</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ß–µ–∫-–ª–∏—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è</h2>
    <ol>
      <li>‚úÖ –í—ã–±—Ä–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–æ–¥ –∑–∞–¥–∞—á—É –∏ —Ä–µ—Å—É—Ä—Å—ã</li>
      <li>‚úÖ –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (ImageNet)</li>
      <li>‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ (resize, normalize)</li>
      <li>‚úÖ –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å —Å–ª–æ–∏ –¥–ª—è fine-tuning</li>
      <li>‚úÖ –ó–∞–º–µ–Ω–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –ø–æ–¥ —Å–≤–æ–∏ –∫–ª–∞—Å—Å—ã</li>
      <li>‚úÖ –û–±—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Å–ª–æ–∏</li>
      <li>‚úÖ –†–∞–∑–º–æ—Ä–æ–∑–∏—Ç—å –∏ –¥–æ–æ–±—É—á–∏—Ç—å —Å –º–∞–ª—ã–º lr</li>
      <li>‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å data augmentation</li>
      <li>‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å overfitting</li>
      <li>‚úÖ –û—Ü–µ–Ω–∏—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
    </ol>
  </div>

</div>

</div>
</body>
</html>

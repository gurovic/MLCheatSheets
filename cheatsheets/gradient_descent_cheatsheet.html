<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üìâ –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: –Ω–∞–π—Ç–∏ –º–∏–Ω–∏–º—É–º —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å</li>
      <li><strong>–ú–µ—Ç–æ–¥</strong>: –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –¥–≤–∏–≥–∞—Ç—å—Å—è –ø—Ä–æ—Ç–∏–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞</li>
      <li><strong>–ì—Ä–∞–¥–∏–µ–Ω—Ç</strong>: –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–∏–±–æ–ª—å—à–µ–≥–æ —Ä–æ—Å—Ç–∞ —Ñ—É–Ω–∫—Ü–∏–∏</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: –æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö ML-–º–æ–¥–µ–ª–µ–π</li>
    </ul>
    <blockquote>
      –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –≤—ã —Å–ø—É—Å–∫–∞–µ—Ç–µ—Å—å —Å –≥–æ—Ä—ã –≤ —Ç—É–º–∞–Ω–µ: –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–∏–±–æ–ª—å—à–µ–≥–æ –ø–æ–¥—ä—ë–º–∞, –∞ –º—ã –∏–¥—ë–º –≤ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—É—é —Å—Ç–æ—Ä–æ–Ω—É.
    </blockquote>

    </div>
<div class="block">
    <h2>üî∑ 2. –ë–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º—É–ª–∞</h2>
    <p><strong>–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:</strong></p>
    <pre><code>Œ∏ = Œ∏ - Œ± * ‚àáL(Œ∏)

–≥–¥–µ:
Œ∏ ‚Äî –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (–≤–µ—Å–∞)
Œ± ‚Äî learning rate (—Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è)
‚àáL(Œ∏) ‚Äî –≥—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
L(Œ∏) ‚Äî —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –¢–∏–ø—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞</h2>
    <table>
      <tr><th>–¢–∏–ø</th><th>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç</th><th>–ü–ª—é—Å—ã</th><th>–ú–∏–Ω—É—Å—ã</th></tr>
      <tr><td><strong>Batch GD</strong></td><td>–í—Å–µ –¥–∞–Ω–Ω—ã–µ</td><td>–°—Ç–∞–±–∏–ª—å–Ω—ã–π</td><td>–ú–µ–¥–ª–µ–Ω–Ω—ã–π</td></tr>
      <tr><td><strong>Stochastic GD</strong></td><td>1 –æ–±—Ä–∞–∑–µ—Ü</td><td>–ë—ã—Å—Ç—Ä—ã–π</td><td>–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–π</td></tr>
      <tr><td><strong>Mini-Batch GD</strong></td><td>–ë–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö</td><td>–ë–∞–ª–∞–Ω—Å</td><td>–ù—É–∂–µ–Ω —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 4. Batch Gradient Descent</h2>
    <pre><code>import numpy as np

def batch_gradient_descent(X, y, lr=0.01, epochs=1000):
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)
    bias = 0
    
    for epoch in range(epochs):
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        y_pred = X.dot(weights) + bias
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã
        dw = (1/n_samples) * X.T.dot(y_pred - y)
        db = (1/n_samples) * np.sum(y_pred - y)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        weights -= lr * dw
        bias -= lr * db
        
        # –õ–æ—Å—Å (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        loss = np.mean((y_pred - y)**2)
        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss:.4f}")
    
    return weights, bias</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Stochastic Gradient Descent (SGD)</h2>
    <pre><code>def stochastic_gradient_descent(X, y, lr=0.01, epochs=100):
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)
    bias = 0
    
    for epoch in range(epochs):
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        indices = np.random.permutation(n_samples)
        
        for i in indices:
            xi = X[i:i+1]
            yi = y[i:i+1]
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            y_pred = xi.dot(weights) + bias
            
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã
            dw = xi.T.dot(y_pred - yi)
            db = y_pred - yi
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ
            weights -= lr * dw
            bias -= lr * db
    
    return weights, bias</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Mini-Batch Gradient Descent</h2>
    <pre><code>def mini_batch_gd(X, y, lr=0.01, batch_size=32, epochs=100):
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)
    bias = 0
    
    for epoch in range(epochs):
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        indices = np.random.permutation(n_samples)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∏
        for i in range(0, n_samples, batch_size):
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            y_pred = X_batch.dot(weights) + bias
            
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã
            batch_size_actual = len(X_batch)
            dw = (1/batch_size_actual) * X_batch.T.dot(y_pred - y_batch)
            db = (1/batch_size_actual) * np.sum(y_pred - y_batch)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ
            weights -= lr * dw
            bias -= lr * db
    
    return weights, bias</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Learning Rate (Œ±)</h2>
    <p><strong>–ö–ª—é—á–µ–≤–æ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä:</strong></p>
    <ul>
      <li><strong>–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π</strong>: –º–æ–¥–µ–ª—å —Ä–∞—Å—Ö–æ–¥–∏—Ç—Å—è</li>
      <li><strong>–°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π</strong>: –º–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</li>
      <li><strong>–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π</strong>: –±—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</li>
    </ul>
    <table>
      <tr><th>–ó–∞–¥–∞—á–∞</th><th>–¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è</th></tr>
      <tr><td>–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è</td><td>0.001 - 0.1</td></tr>
      <tr><td>–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏</td><td>0.0001 - 0.01</td></tr>
      <tr><td>SGD</td><td>0.01 - 0.1</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ü—Ä–æ–±–ª–µ–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞</h2>
    <table>
      <tr><th>–ü—Ä–æ–±–ª–µ–º–∞</th><th>–ü—Ä–∏—á–∏–Ω–∞</th><th>–†–µ—à–µ–Ω–∏–µ</th></tr>
      <tr><td>–ú–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</td><td>–ú–∞–ª—ã–π LR</td><td>–£–≤–µ–ª–∏—á–∏—Ç—å LR –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Momentum</td></tr>
      <tr><td>–†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ</td><td>–ë–æ–ª—å—à–æ–π LR</td><td>–£–º–µ–Ω—å—à–∏—Ç—å LR</td></tr>
      <tr><td>–õ–æ–∫–∞–ª—å–Ω—ã–π –º–∏–Ω–∏–º—É–º</td><td>–ù–µ–≥–ª–∞–¥–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è</td><td>SGD, Momentum</td></tr>
      <tr><td>–°–µ–¥–ª–æ–≤–∞—è —Ç–æ—á–∫–∞</td><td>–ü–ª–æ—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å</td><td>–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã (Adam)</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 9. Learning Rate Scheduling</h2>
    <pre><code>from sklearn.linear_model import SGDRegressor
from sklearn.model_selection import train_test_split

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π LR
sgd = SGDRegressor(learning_rate='constant', eta0=0.01)

# –ò–Ω–≤–µ—Ä—Å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
sgd = SGDRegressor(learning_rate='invscaling', 
                   eta0=0.01, power_t=0.25)

# –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π LR
sgd = SGDRegressor(learning_rate='adaptive', eta0=0.01)

# –û–±—É—á–µ–Ω–∏–µ
sgd.fit(X_train, y_train)

# –†—É—á–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ
import numpy as np

def lr_schedule(epoch):
    initial_lr = 0.01
    decay = 0.95
    return initial_lr * (decay ** epoch)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏</h2>
    <pre><code>import matplotlib.pyplot as plt

def gradient_descent_with_history(X, y, lr=0.01, epochs=1000):
    weights = np.zeros(X.shape[1])
    bias = 0
    loss_history = []
    
    for epoch in range(epochs):
        y_pred = X.dot(weights) + bias
        
        # –õ–æ—Å—Å
        loss = np.mean((y_pred - y)**2)
        loss_history.append(loss)
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        dw = (1/len(X)) * X.T.dot(y_pred - y)
        db = (1/len(X)) * np.sum(y_pred - y)
        
        weights -= lr * dw
        bias -= lr * db
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(10, 6))
    plt.plot(loss_history)
    plt.xlabel('–≠–ø–æ—Ö–∞')
    plt.ylabel('Loss (MSE)')
    plt.title('–°—Ö–æ–¥–∏–º–æ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞')
    plt.grid(True)
    plt.show()
    
    return weights, bias, loss_history</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã</h2>
    <p><strong>–£–ª—É—á—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ GD:</strong></p>
    <ul>
      <li><strong>Momentum</strong>: —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã</li>
      <li><strong>RMSprop</strong>: –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π learning rate</li>
      <li><strong>Adam</strong>: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è Momentum + RMSprop</li>
      <li><strong>AdaGrad</strong>: –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
    </ul>
    <pre><code>from sklearn.neural_network import MLPRegressor

# SGD —Å momentum
mlp = MLPRegressor(solver='sgd', momentum=0.9, 
                   learning_rate_init=0.01)

# Adam (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
mlp = MLPRegressor(solver='adam', learning_rate_init=0.001)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π —Ç–∏–ø</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>Batch GD</h3>
        <ul>
          <li>–ú–∞–ª—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (< 10000 –æ–±—Ä–∞–∑—Ü–æ–≤)</li>
          <li>–ù—É–∂–Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</li>
          <li>–í—ã–ø—É–∫–ª—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å</li>
        </ul>
      </div>
      <div class="good">
        <h3>Stochastic GD</h3>
        <ul>
          <li>–û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã</li>
          <li>–û–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ</li>
          <li>–ò–∑–±–µ–≥–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤</li>
        </ul>
      </div>
      <div class="good">
        <h3>Mini-Batch GD</h3>
        <ul>
          <li>–°—Ä–µ–¥–Ω–∏–µ –∏ –±–æ–ª—å—à–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã</li>
          <li>–ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏</li>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU (–ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º)</li>
          <li>–°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –≤—ã–±–æ—Ä</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</h2>
    <pre><code>def numerical_gradient(f, x, epsilon=1e-5):
    """–ß–∏—Å–ª–µ–Ω–Ω–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞"""
    grad = np.zeros_like(x)
    
    for i in range(len(x)):
        x_plus = x.copy()
        x_minus = x.copy()
        
        x_plus[i] += epsilon
        x_minus[i] -= epsilon
        
        grad[i] = (f(x_plus) - f(x_minus)) / (2 * epsilon)
    
    return grad

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏ —á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
def check_gradient(analytical_grad, x, f):
    numerical_grad = numerical_gradient(f, x)
    diff = np.linalg.norm(analytical_grad - numerical_grad)
    
    if diff < 1e-7:
        print("‚úÖ –ì—Ä–∞–¥–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–µ: {diff}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å/—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–π learning rate</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mini-batch –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å learning rate scheduling</li>
      <li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã (Adam)</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</li>
      <li>[ ] –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ ‚Äî —ç—Ç–æ —Å–ø–æ—Å–æ–±, –∫–æ—Ç–æ—Ä—ã–º –∫–æ–º–ø—å—é—Ç–µ—Ä —É—á–∏—Ç—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö: –æ–Ω –ø—Ä–æ–±—É–µ—Ç —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã, —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–≤–æ–∏ —Ä–µ—à–µ–Ω–∏—è, –¥–≤–∏–≥–∞—è—Å—å –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤¬ª.
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <ul>
      <li><strong>–ù–∞—á–Ω–∏—Ç–µ —Å Adam</strong>: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä</li>
      <li><strong>Batch size</strong>: –Ω–∞—á–Ω–∏—Ç–µ —Å 32 –∏–ª–∏ 64</li>
      <li><strong>–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥</strong>: —Å–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Ç–µ—Ä—å</li>
      <li><strong>Early stopping</strong>: –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø—Ä–∏ plateau</li>
      <li><strong>Warm restart</strong>: –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–π—Ç–µ —Å –Ω–æ–≤—ã–º LR</li>
      <li><strong>Gradient clipping</strong>: –æ–≥—Ä–∞–Ω–∏—á—å—Ç–µ –±–æ–ª—å—à–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã</li>
    </ul>
    <pre><code># –ü—Ä–∏–º–µ—Ä —Å PyTorch
import torch.optim as optim

optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(num_epochs):
    optimizer.zero_grad()
    loss = compute_loss(model(X), y)
    loss.backward()
    optimizer.step()</code></pre>
  </div>



</div>
</body>
</html>

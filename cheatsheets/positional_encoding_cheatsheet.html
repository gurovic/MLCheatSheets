<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Positional Encoding Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ Positional Encoding</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ü—Ä–æ–±–ª–µ–º–∞ –ø–æ–∑–∏—Ü–∏–π</h2>
    <p><strong>–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä</strong> –Ω–µ –∏–º–µ–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤</p>
    <ul>
      <li><strong>Self-attention</strong>: permutation-invariant</li>
      <li><strong>–ë–µ–∑ –ø–æ–∑–∏—Ü–∏–π</strong>: "cat sat" = "sat cat"</li>
      <li><strong>–†–µ—à–µ–Ω–∏–µ</strong>: –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é</li>
      <li><strong>–°–ø–æ—Å–æ–±—ã</strong>: sinusoidal, learned, relative</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. Sinusoidal Encoding</h2>
    <p>–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –∏–∑ "Attention is All You Need"</p>
    <pre><code>PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

–≥–¥–µ:
- pos = –ø–æ–∑–∏—Ü–∏—è —Ç–æ–∫–µ–Ω–∞ (0, 1, 2, ...)
- i = –∏–Ω–¥–µ–∫—Å –∏–∑–º–µ—Ä–µ–Ω–∏—è (0...d_model/2)
- d_model = —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –°–≤–æ–π—Å—Ç–≤–∞ Sinusoidal PE</h2>
    <ul>
      <li><strong>–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ</strong>: –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –æ–±—É—á–µ–Ω–∏—è</li>
      <li><strong>–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è</strong>: —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö</li>
      <li><strong>–õ–∏–Ω–µ–π–Ω–æ—Å—Ç—å</strong>: PE(pos+k) –ª–∏–Ω–µ–π–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ç PE(pos)</li>
      <li><strong>–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å</strong>: –∫–∞–∂–¥–∞—è –ø–æ–∑–∏—Ü–∏—è –∏–º–µ–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–æ–¥</li>
      <li><strong>–ü–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å</strong>: —Ä–∞–∑–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏–π</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 4. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Sinusoidal PE</h2>
    <pre><code>import torch
import numpy as np

def get_sinusoidal_encoding(max_len, d_model):
    """
    –°–æ–∑–¥–∞—Ç—å sinusoidal positional encoding
    """
    position = torch.arange(max_len).unsqueeze(1)
    div_term = torch.exp(
        torch.arange(0, d_model, 2) * 
        -(np.log(10000.0) / d_model)
    )
    
    pe = torch.zeros(max_len, d_model)
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    return pe

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
max_len = 512
d_model = 512
pe = get_sinusoidal_encoding(max_len, d_model)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º
embeddings = token_embeddings + pe[:seq_len]</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Learned Positional Embeddings</h2>
    <p><strong>–û–±—É—á–∞–µ–º—ã–µ</strong> –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–∫–∞–∫ –≤ BERT)</p>
    <pre><code>import torch.nn as nn

class LearnedPositionalEmbedding(nn.Module):
    def __init__(self, max_len, d_model):
        super().__init__()
        self.embedding = nn.Embedding(max_len, d_model)
    
    def forward(self, x):
        # x: [batch, seq_len, d_model]
        batch_size, seq_len, _ = x.size()
        positions = torch.arange(seq_len, device=x.device)
        positions = positions.unsqueeze(0).expand(batch_size, -1)
        return x + self.embedding(positions)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
pos_emb = LearnedPositionalEmbedding(512, 768)
x_with_pos = pos_emb(token_embeddings)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</th><th>–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</th></tr>
      <tr><td><strong>Sinusoidal</strong></td><td>–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è, –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å</td><td>–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω</td></tr>
      <tr><td><strong>Learned</strong></td><td>–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å –∫ –¥–∞–Ω–Ω—ã–º</td><td>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ max_len</td></tr>
      <tr><td><strong>Relative</strong></td><td>–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏</td><td>–°–ª–æ–∂–Ω–µ–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è</td></tr>
      <tr><td><strong>RoPE</strong></td><td>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è</td><td>–°–ª–æ–∂–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. Relative Positional Encoding</h2>
    <p>–ö–æ–¥–∏—Ä—É–µ—Ç <strong>–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ</strong> —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏</p>
    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –≤–∞–∂–Ω–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ, –∞ –Ω–µ –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è</li>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ</strong>: –ª—É—á—à–µ generalization</li>
      <li><strong>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ</strong>: –≤ Transformer-XL, T5</li>
      <li><strong>Bias –≤ attention</strong>: –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ attention scores</li>
    </ul>
    <pre><code># –í attention
attention_scores = Q @ K.T + relative_pos_bias</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. RoPE (Rotary Position Embedding)</h2>
    <p><strong>RoPE</strong> ‚Äî –º–µ—Ç–æ–¥ –∏–∑ LLaMA, Mistral –∏ –¥—Ä—É–≥–∏—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM</p>
    <ul>
      <li><strong>–†–æ—Ç–∞—Ü–∏—è</strong>: –ø—Ä–∏–º–µ–Ω—è–µ—Ç rotation matrix –∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º</li>
      <li><strong>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å</strong>: —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å Q, K</li>
      <li><strong>–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è</strong>: —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö</li>
      <li><strong>–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å</strong>: –∫–æ–¥–∏—Ä—É–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏</li>
    </ul>
    <blockquote>
      RoPE —Å—Ç–∞–ª —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM (GPT-Neo, LLaMA)
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 9. ALiBi (Attention with Linear Biases)</h2>
    <p>–ü—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥ –∏–∑ T5 –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π</p>
    <pre><code># –õ–∏–Ω–µ–π–Ω—ã–π bias
bias = -m * |i - j|

–≥–¥–µ m ‚Äî slope –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã attention

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫ attention scores
scores = Q @ K.T / sqrt(d_k) + bias
attention = softmax(scores)</code></pre>
    <ul>
      <li><strong>–ü—Ä–æ—Å—Ç–æ—Ç–∞</strong>: –Ω–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
      <li><strong>–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è</strong>: –æ—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–¢–∏–ø PE</th></tr>
      <tr><td><strong>Original Transformer</strong></td><td>Sinusoidal</td></tr>
      <tr><td><strong>BERT</strong></td><td>Learned absolute</td></tr>
      <tr><td><strong>GPT-2</strong></td><td>Learned absolute</td></tr>
      <tr><td><strong>Transformer-XL</strong></td><td>Relative</td></tr>
      <tr><td><strong>T5</strong></td><td>Relative (simplified)</td></tr>
      <tr><td><strong>LLaMA, Mistral</strong></td><td>RoPE</td></tr>
      <tr><td><strong>BLOOM</strong></td><td>ALiBi</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 11. 2D Positional Encoding</h2>
    <p>–î–ª—è <strong>–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</strong> (Vision Transformer)</p>
    <pre><code># –û—Ç–¥–µ–ª—å–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è x –∏ y
PE(x, y) = concat(PE_x(x), PE_y(y))

# –ò–ª–∏ learned 2D embedding
pos_emb_2d = nn.Parameter(
    torch.randn(H, W, d_model)
)</code></pre>
    <ul>
      <li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ ViT, DETR</li>
      <li>–ú–æ–∂–Ω–æ sinusoidal –∏–ª–∏ learned</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 12. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è PE</h2>
    <pre><code>import matplotlib.pyplot as plt
import seaborn as sns

# –°–æ–∑–¥–∞—Ç—å PE
pe = get_sinusoidal_encoding(100, 128)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(12, 6))
sns.heatmap(
    pe.numpy().T, 
    cmap='RdBu', 
    center=0,
    xticklabels=10,
    yticklabels=10
)
plt.xlabel('Position')
plt.ylabel('Embedding Dimension')
plt.title('Sinusoidal Positional Encoding')
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –î–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã</h2>
    <p>–ú–µ—Ç–æ–¥—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ <strong>–¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π</strong>:</p>
    <ul>
      <li><strong>ALiBi</strong>: —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à–æ</li>
      <li><strong>RoPE with scaling</strong>: –ª–∏–Ω–µ–π–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ</li>
      <li><strong>Absolute + Relative</strong>: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è</li>
      <li><strong>Learned extrapolation</strong>: —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</li>
      <li><strong>Position interpolation</strong>: –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –ø–æ–∑–∏—Ü–∏–π</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 14. Best Practices</h2>
    <ol>
      <li><strong>–î–ª—è NLP</strong>: learned –∏–ª–∏ RoPE</li>
      <li><strong>–î–ª—è Vision</strong>: learned 2D</li>
      <li><strong>–î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤</strong>: ALiBi –∏–ª–∏ RoPE</li>
      <li><strong>–î–ª—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏</strong>: sinusoidal –∏–ª–∏ ALiBi</li>
      <li><strong>–î–ª—è transfer learning</strong>: relative PE</li>
      <li><strong>–î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏</strong>: RoPE</li>
    </ol>
  </div>

  <div class="block">
    <h2>üî∑ 15. –í–ª–∏—è–Ω–∏–µ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–°–∫–æ—Ä–æ—Å—Ç—å</th><th>–ü–∞–º—è—Ç—å</th></tr>
      <tr><td><strong>Sinusoidal</strong></td><td>–ë—ã—Å—Ç—Ä–æ</td><td>O(max_len √ó d)</td></tr>
      <tr><td><strong>Learned</strong></td><td>–ë—ã—Å—Ç—Ä–æ</td><td>O(max_len √ó d)</td></tr>
      <tr><td><strong>Relative</strong></td><td>–ú–µ–¥–ª–µ–Ω–Ω–µ–µ</td><td>O(max_len¬≤)</td></tr>
      <tr><td><strong>RoPE</strong></td><td>–ë—ã—Å—Ç—Ä–æ</td><td>O(1)</td></tr>
      <tr><td><strong>ALiBi</strong></td><td>–ë—ã—Å—Ç—Ä–æ</td><td>O(1)</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 16. Debugging PE</h2>
    <pre><code># –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã PE
def check_pe(model, text):
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = tokenizer(text, return_tensors='pt')
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ attention
    outputs = model(**tokens, output_attentions=True)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è attention
    attention = outputs.attentions[0][0].mean(0)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞: –±–ª–∏–∑–∫–∏–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å
    # –≤—ã—Å–æ–∫–æ–µ attention –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É
    plt.imshow(attention.detach())
    plt.colorbar()
    plt.show()

# –¢–µ—Å—Ç: –±–µ–∑ PE attention –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–º</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ PE</h3>
        <ul>
          <li>–î–∞—ë—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞</li>
          <li>–ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è</li>
          <li>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö</li>
          <li>–ú–Ω–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–∞ –≤—ã–±–æ—Ä</li>
        </ul>
      </div>
      <div class="bad">
        <h3>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</h3>
        <ul>
          <li>–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ (learned)</li>
          <li>–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å</li>
          <li>–ù–µ –≤—Å–µ–≥–¥–∞ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ—Ç</li>
          <li>–í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –Ω–µ –æ—á–µ–≤–∏–¥–µ–Ω</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ß–µ–∫-–ª–∏—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è</h2>
    <ol>
      <li>‚úì –í—ã–±—Ä–∞—Ç—å —Ç–∏–ø PE (sinusoidal/learned/relative/RoPE)</li>
      <li>‚úì –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å max_len</li>
      <li>‚úì –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–π</li>
      <li>‚úì –î–æ–±–∞–≤–∏—Ç—å –∫ token embeddings</li>
      <li>‚úì –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏</li>
      <li>‚úì –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—é</li>
      <li>‚úì –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å attention maps</li>
      <li>‚úì –°—Ä–∞–≤–Ω–∏—Ç—å —Å baseline –±–µ–∑ PE</li>
    </ol>
  </div>

</div>

</div>
</body>
</html>

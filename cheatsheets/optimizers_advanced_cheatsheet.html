<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>‚ö° –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã Cheatsheet</h1>
  <div class="subtitle">AdaGrad ‚Ä¢ RMSProp ‚Ä¢ Adadelta ‚Ä¢ Nadam ‚Ä¢ AdamW<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û–±–∑–æ—Ä –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤</h2>
    <p><strong>–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã</strong> ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.</p>
    <table>
      <tr><th>–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr>
      <tr><td><strong>AdaGrad</strong></td><td>–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π learning rate</td><td>–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (NLP)</td></tr>
      <tr><td><strong>RMSProp</strong></td><td>–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç AdaGrad</td><td>RNN, –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–µ –∑–∞–¥–∞—á–∏</td></tr>
      <tr><td><strong>Adadelta</strong></td><td>–ù–µ —Ç—Ä–µ–±—É–µ—Ç lr</td><td>–ö–æ–≥–¥–∞ —Ç—Ä—É–¥–Ω–æ –ø–æ–¥–æ–±—Ä–∞—Ç—å lr</td></tr>
      <tr><td><strong>Nadam</strong></td><td>Adam + Nesterov</td><td>–£–ª—É—á—à–µ–Ω–∏–µ Adam</td></tr>
      <tr><td><strong>AdamW</strong></td><td>Adam + weight decay</td><td>–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 2. AdaGrad</h2>
    <p><strong>Adaptive Gradient Algorithm</strong> ‚Äî –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç learning rate –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞.</p>
    <p><strong>–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è:</strong> –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å —Ä–µ–¥–∫–∏–º–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏ –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª—å—à–∏–µ —à–∞–≥–∏, —á–∞—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è–µ–º—ã–µ ‚Äî –º–∞–ª–µ–Ω—å–∫–∏–µ.</p>
    
    <p><strong>–§–æ—Ä–º—É–ª–∞:</strong></p>
    <pre><code>G_t = G_{t-1} + (‚àáL_t)¬≤
Œ∏_{t+1} = Œ∏_t - (Œ∑ / ‚àö(G_t + Œµ)) ¬∑ ‚àáL_t</code></pre>
    
    <ul>
      <li><code>G_t</code> ‚Äî –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</li>
      <li><code>Œ∑</code> ‚Äî –Ω–∞—á–∞–ª—å–Ω—ã–π learning rate</li>
      <li><code>Œµ</code> ‚Äî –º–∞–ª–∞—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ (1e-8) –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. AdaGrad –≤ –∫–æ–¥–µ</h2>
    <pre><code>import torch.optim as optim

# PyTorch
optimizer = optim.Adagrad(
    model.parameters(),
    lr=0.01,
    lr_decay=0,
    weight_decay=0,
    eps=1e-10
)

# TensorFlow/Keras
from tensorflow.keras.optimizers import Adagrad

optimizer = Adagrad(
    learning_rate=0.01,
    initial_accumulator_value=0.1,
    epsilon=1e-07
)</code></pre>
    
    <p><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:</strong></p>
    <ul>
      <li>–•–æ—Ä–æ—à –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è lr</li>
      <li>–ù–µ —Ç—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ lr</li>
    </ul>
    
    <p><strong>–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:</strong></p>
    <ul>
      <li>Learning rate –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ —É–±—ã–≤–∞–µ—Ç</li>
      <li>–ú–æ–∂–µ—Ç –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è —Å–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ</li>
      <li>–ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ G_t –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –æ—á–µ–Ω—å –º–∞–ª—ã–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 4. RMSProp</h2>
    <p><strong>Root Mean Square Propagation</strong> ‚Äî —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ —É–º–µ–Ω—å—à–µ–Ω–∏—è lr –≤ AdaGrad.</p>
    
    <p><strong>–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è:</strong> –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–π —Å—É–º–º—ã.</p>
    
    <p><strong>–§–æ—Ä–º—É–ª–∞:</strong></p>
    <pre><code>E[g¬≤]_t = Œ≤ ¬∑ E[g¬≤]_{t-1} + (1-Œ≤) ¬∑ (‚àáL_t)¬≤
Œ∏_{t+1} = Œ∏_t - (Œ∑ / ‚àö(E[g¬≤]_t + Œµ)) ¬∑ ‚àáL_t</code></pre>
    
    <ul>
      <li><code>Œ≤</code> ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∑–∞–±—ã–≤–∞–Ω–∏—è (–æ–±—ã—á–Ω–æ 0.9)</li>
      <li><code>E[g¬≤]_t</code> ‚Äî —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 5. RMSProp –≤ –∫–æ–¥–µ</h2>
    <pre><code># PyTorch
optimizer = optim.RMSprop(
    model.parameters(),
    lr=0.01,
    alpha=0.99,      # decay rate
    eps=1e-08,
    weight_decay=0,
    momentum=0,
    centered=False
)

# TensorFlow/Keras
from tensorflow.keras.optimizers import RMSprop

optimizer = RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False
)</code></pre>
    
    <p><strong>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:</strong></p>
    <ul>
      <li>–ù–∞—á–∞–ª—å–Ω—ã–π lr: 0.001</li>
      <li>–•–æ—Ä–æ—à –¥–ª—è RNN –∏ LSTM</li>
      <li>–ú–æ–∂–µ—Ç –ø–æ–º–æ—á—å –ø—Ä–∏ –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö</li>
      <li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 6. Adadelta</h2>
    <p><strong>–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ AdaGrad</strong> ‚Äî –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∑–∞–¥–∞–Ω–∏—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ learning rate.</p>
    
    <p><strong>–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è:</strong> –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.</p>
    
    <p><strong>–§–æ—Ä–º—É–ª—ã:</strong></p>
    <pre><code>E[g¬≤]_t = œÅ ¬∑ E[g¬≤]_{t-1} + (1-œÅ) ¬∑ (‚àáL_t)¬≤
ŒîŒ∏_t = -(‚àö(E[ŒîŒ∏¬≤]_{t-1} + Œµ) / ‚àö(E[g¬≤]_t + Œµ)) ¬∑ ‚àáL_t
E[ŒîŒ∏¬≤]_t = œÅ ¬∑ E[ŒîŒ∏¬≤]_{t-1} + (1-œÅ) ¬∑ (ŒîŒ∏_t)¬≤
Œ∏_{t+1} = Œ∏_t + ŒîŒ∏_t</code></pre>
    
    <ul>
      <li><code>œÅ</code> ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (0.95)</li>
      <li>–ù–µ —Ç—Ä–µ–±—É–µ—Ç –∑–∞–¥–∞–Ω–∏—è lr!</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 7. Adadelta –≤ –∫–æ–¥–µ</h2>
    <pre><code># PyTorch
optimizer = optim.Adadelta(
    model.parameters(),
    lr=1.0,          # scale factor
    rho=0.9,
    eps=1e-06,
    weight_decay=0
)

# TensorFlow/Keras
from tensorflow.keras.optimizers import Adadelta

optimizer = Adadelta(
    learning_rate=0.001,
    rho=0.95,
    epsilon=1e-07
)</code></pre>
    
    <p><strong>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:</strong></p>
    <ul>
      <li>–¢—Ä—É–¥–Ω–æ –ø–æ–¥–æ–±—Ä–∞—Ç—å learning rate</li>
      <li>–ù—É–∂–Ω–∞ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º</li>
      <li>–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ RMSProp</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 8. Nadam</h2>
    <p><strong>Nesterov-accelerated Adaptive Moment Estimation</strong> ‚Äî –∫–æ–º–±–∏–Ω–∞—Ü–∏—è Adam –∏ Nesterov momentum.</p>
    
    <p><strong>–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è:</strong> –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Nesterov momentum –≤–º–µ—Å—Ç–æ –æ–±—ã—á–Ω–æ–≥–æ –≤ Adam.</p>
    
    <p><strong>–§–æ—Ä–º—É–ª—ã:</strong></p>
    <pre><code>m_t = Œ≤‚ÇÅ ¬∑ m_{t-1} + (1-Œ≤‚ÇÅ) ¬∑ ‚àáL_t
v_t = Œ≤‚ÇÇ ¬∑ v_{t-1} + (1-Œ≤‚ÇÇ) ¬∑ (‚àáL_t)¬≤

mÃÇ_t = m_t / (1-Œ≤‚ÇÅ·µó)
vÃÇ_t = v_t / (1-Œ≤‚ÇÇ·µó)

Œ∏_{t+1} = Œ∏_t - Œ∑ ¬∑ (Œ≤‚ÇÅ¬∑mÃÇ_t + (1-Œ≤‚ÇÅ)¬∑‚àáL_t/(1-Œ≤‚ÇÅ·µó)) / (‚àövÃÇ_t + Œµ)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Nadam –≤ –∫–æ–¥–µ</h2>
    <pre><code># PyTorch (–Ω–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ torch-optimizer)
# pip install torch-optimizer
import torch_optimizer as toptim

optimizer = toptim.Nadam(
    model.parameters(),
    lr=2e-3,
    betas=(0.9, 0.999),
    eps=1e-8,
    weight_decay=0
)

# TensorFlow/Keras
from tensorflow.keras.optimizers import Nadam

optimizer = Nadam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07
)</code></pre>
    
    <p><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –Ω–∞–¥ Adam:</strong></p>
    <ul>
      <li>–ë—ã—Å—Ç—Ä–µ–µ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</li>
      <li>–õ—É—á—à–µ –¥–ª—è –∑–∞–¥–∞—á —Å —à—É–º–Ω—ã–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏</li>
      <li>–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 10. AdamW</h2>
    <p><strong>Adam with decoupled Weight decay</strong> ‚Äî –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã weight decay –≤ Adam.</p>
    
    <p><strong>–ü—Ä–æ–±–ª–µ–º–∞ Adam:</strong> L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º lr.</p>
    
    <p><strong>–†–µ—à–µ–Ω–∏–µ AdamW:</strong> —Ä–∞–∑–¥–µ–ª–∏—Ç—å weight decay –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.</p>
    
    <p><strong>–§–æ—Ä–º—É–ª—ã:</strong></p>
    <pre><code>m_t = Œ≤‚ÇÅ ¬∑ m_{t-1} + (1-Œ≤‚ÇÅ) ¬∑ ‚àáL_t
v_t = Œ≤‚ÇÇ ¬∑ v_{t-1} + (1-Œ≤‚ÇÇ) ¬∑ (‚àáL_t)¬≤

Œ∏_{t+1} = Œ∏_t - Œ∑ ¬∑ (m_t/‚àöv_t + Œµ + Œª¬∑Œ∏_t)</code></pre>
    
    <ul>
      <li><code>Œª</code> ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç weight decay (0.01)</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 11. AdamW –≤ –∫–æ–¥–µ</h2>
    <pre><code># PyTorch
optimizer = optim.AdamW(
    model.parameters(),
    lr=1e-3,
    betas=(0.9, 0.999),
    eps=1e-08,
    weight_decay=0.01,  # decoupled!
    amsgrad=False
)

# TensorFlow/Keras (addons)
import tensorflow_addons as tfa

optimizer = tfa.optimizers.AdamW(
    learning_rate=0.001,
    weight_decay=0.01,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07
)</code></pre>
    
    <p><strong>–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:</strong></p>
    <ul>
      <li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ BERT, GPT, ViT</li>
      <li>–õ—É—á—à–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è —á–µ–º Adam</li>
      <li>–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 12. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤</h2>
    <table>
      <tr><th>–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä</th><th>–°–∫–æ—Ä–æ—Å—Ç—å</th><th>–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>SGD</strong></td><td>–ú–µ–¥–ª–µ–Ω–Ω–∞—è</td><td>–í—ã—Å–æ–∫–∞—è</td><td>CV —Å momentum</td></tr>
      <tr><td><strong>Adam</strong></td><td>–ë—ã—Å—Ç—Ä–∞—è</td><td>–°—Ä–µ–¥–Ω—è—è</td><td>–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π</td></tr>
      <tr><td><strong>AdamW</strong></td><td>–ë—ã—Å—Ç—Ä–∞—è</td><td>–í—ã—Å–æ–∫–∞—è</td><td>NLP, Transformers</td></tr>
      <tr><td><strong>RMSProp</strong></td><td>–ë—ã—Å—Ç—Ä–∞—è</td><td>–°—Ä–µ–¥–Ω—è—è</td><td>RNN, RL</td></tr>
      <tr><td><strong>Nadam</strong></td><td>–û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è</td><td>–°—Ä–µ–¥–Ω—è—è</td><td>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã</td></tr>
      <tr><td><strong>AdaGrad</strong></td><td>–°—Ä–µ–¥–Ω—è—è</td><td>–ù–∏–∑–∫–∞—è</td><td>–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 13. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –ù–∞—á–Ω–∏—Ç–µ —Å —ç—Ç–æ–≥–æ</h3>
        <ul>
          <li><strong>AdamW</strong> ‚Äî –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á</li>
          <li>lr=1e-3, weight_decay=0.01</li>
          <li>–û—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è Transformers</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ö†Ô∏è –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏</h3>
        <ul>
          <li><strong>RMSProp</strong> ‚Äî –¥–ª—è RNN/LSTM</li>
          <li><strong>SGD + momentum</strong> ‚Äî –¥–ª—è CNN (ResNet)</li>
          <li><strong>AdaGrad</strong> ‚Äî –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</h2>
    <p><strong>AdamW (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è):</strong></p>
    <ul>
      <li>Learning rate: <code>1e-3</code> –¥–æ <code>5e-4</code></li>
      <li>Œ≤‚ÇÅ (beta_1): <code>0.9</code></li>
      <li>Œ≤‚ÇÇ (beta_2): <code>0.999</code></li>
      <li>Weight decay: <code>0.01</code> –¥–æ <code>0.1</code></li>
      <li>Œµ (epsilon): <code>1e-8</code></li>
    </ul>
    
    <p><strong>RMSProp:</strong></p>
    <ul>
      <li>Learning rate: <code>1e-3</code></li>
      <li>Œ± (alpha/rho): <code>0.99</code></li>
      <li>Momentum: <code>0</code> –∏–ª–∏ <code>0.9</code></li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥</h2>
    <p>–ß–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏:</p>
    <pre><code># PyTorch - clip by norm
torch.nn.utils.clip_grad_norm_(
    model.parameters(),
    max_norm=1.0
)

# PyTorch - clip by value
torch.nn.utils.clip_grad_value_(
    model.parameters(),
    clip_value=0.5
)

# TensorFlow
optimizer = tf.keras.optimizers.AdamW(
    learning_rate=0.001,
    clipnorm=1.0,     # clip by norm
    clipvalue=0.5     # clip by value
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. Lookahead Optimizer</h2>
    <p>–û–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ –ª—é–±—ã–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏:</p>
    <pre><code># –¢—Ä–µ–±—É–µ—Ç torch-optimizer
import torch_optimizer as toptim

base_optimizer = optim.AdamW(
    model.parameters(),
    lr=1e-3
)

optimizer = toptim.Lookahead(
    base_optimizer,
    k=5,        # update every k steps
    alpha=0.5   # slow weights step size
)

# –û–±—É—á–µ–Ω–∏–µ –∫–∞–∫ –æ–±—ã—á–Ω–æ
for batch in dataloader:
    optimizer.zero_grad()
    loss = model(batch)
    loss.backward()
    optimizer.step()</code></pre>
    
    <p><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:</strong></p>
    <ul>
      <li>–£–ª—É—á—à–∞–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ª—é–±–æ–≥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞</li>
      <li>–ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</li>
      <li>–õ—É—á—à–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 17. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π workflow</h2>
    <ol>
      <li><strong>–ë–∞–∑–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞:</strong>
        <ul>
          <li>–ù–∞—á–Ω–∏—Ç–µ —Å AdamW</li>
          <li>lr=1e-3, weight_decay=0.01</li>
        </ul>
      </li>
      <li><strong>–ï—Å–ª–∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ:</strong>
        <ul>
          <li>–£–º–µ–Ω—å—à–∏—Ç–µ lr (–¥–æ 1e-4)</li>
          <li>–î–æ–±–∞–≤—å—Ç–µ gradient clipping</li>
          <li>–ü–æ–ø—Ä–æ–±—É–π—Ç–µ RMSProp</li>
        </ul>
      </li>
      <li><strong>–î–ª—è RNN:</strong>
        <ul>
          <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ RMSProp</li>
          <li>–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ gradient clipping</li>
        </ul>
      </li>
      <li><strong>–î–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π:</strong>
        <ul>
          <li>AdamW —Å weight decay</li>
          <li>Learning rate warmup</li>
          <li>Cosine annealing</li>
        </ul>
      </li>
    </ol>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –í—ã–±—Ä–∞–ª–∏ –∞–¥–µ–∫–≤–∞—Ç–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∑–∞–¥–∞—á–∏</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏–ª–∏ learning rate (–æ–±—ã—á–Ω–æ 1e-3 –¥–ª—è Adam/AdamW)</li>
      <li>[ ] –î–æ–±–∞–≤–∏–ª–∏ weight decay –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏–ª–∏ gradient clipping –¥–ª—è RNN</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç–µ learning rate scheduler</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç–µ loss –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (TensorBoard)</li>
      <li>[ ] –°—Ä–∞–≤–Ω–∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏–ª–∏ –≤–ª–∏—è–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã ‚Äî —ç—Ç–æ "—É–º–Ω—ã–µ" —Å–ø–æ—Å–æ–±—ã –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏. AdamW ‚Äî —Å–∞–º—ã–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π, –æ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –º–æ–¥–µ–ª–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –±—ã—Å—Ç—Ä–µ–µ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ¬ª.
    </blockquote>
  </div>

</div>

</body>
</html>

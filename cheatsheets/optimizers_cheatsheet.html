<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Optimizers (Adam, SGD, etc.) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ Optimizers (Adam, SGD, RMSprop)</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤</h2>
    <ul>
      <li><strong>–ó–∞–¥–∞—á–∞</strong>: –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å</li>
      <li><strong>–ú–µ—Ç–æ–¥</strong>: –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤</li>
      <li><strong>–û—Å–Ω–æ–≤–∞</strong>: –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫</li>
      <li><strong>–¶–µ–ª—å</strong>: –Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –±—ã—Å—Ç—Ä–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ</li>
    </ul>
    <p><strong>–û–±—â–∞—è —Ñ–æ—Ä–º—É–ª–∞</strong>: Œ∏ = Œ∏ - Œ± ¬∑ ‚àáL(Œ∏)</p>
    <p>–≥–¥–µ Œ± ‚Äî learning rate, ‚àáL ‚Äî –≥—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å</p>

    </div>
<div class="block">
    <h2>üî∑ 2. SGD (Stochastic Gradient Descent)</h2>
    <p><strong>–ü—Ä–æ—Å—Ç–µ–π—à–∏–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä</strong></p>
    <p>Œ∏ = Œ∏ - Œ± ¬∑ ‚àáL(Œ∏)</p>
    <pre><code># PyTorch
import torch.optim as optim

optimizer = optim.SGD(
    model.parameters(),
    lr=0.01
)

# Keras/TensorFlow
model.compile(
    optimizer='sgd',
    loss='categorical_crossentropy'
)

# –ò–ª–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
from tensorflow.keras.optimizers import SGD
optimizer = SGD(learning_rate=0.01)</code></pre>
    <p><strong>–ü–ª—é—Å—ã</strong>: –ø—Ä–æ—Å—Ç–æ–π, –ø–æ–Ω—è—Ç–Ω—ã–π<br>
    <strong>–ú–∏–Ω—É—Å—ã</strong>: –º–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –∑–∞—Å—Ç—Ä–µ–≤–∞–µ—Ç –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–∞—Ö</p>
  </div>

  <div class="block">
    <h2>üî∑ 3. SGD with Momentum</h2>
    <p><strong>–î–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω–µ—Ä—Ü–∏—é –¥–≤–∏–∂–µ–Ω–∏—è</strong></p>
    <p>v = Œ≤ ¬∑ v + Œ± ¬∑ ‚àáL(Œ∏)<br>Œ∏ = Œ∏ - v</p>
    <pre><code># PyTorch
optimizer = optim.SGD(
    model.parameters(),
    lr=0.01,
    momentum=0.9  # Œ≤ = 0.9
)

# Keras
optimizer = SGD(
    learning_rate=0.01,
    momentum=0.9
)</code></pre>
    <p><strong>–ü–ª—é—Å—ã</strong>: –±—ã—Å—Ç—Ä–µ–µ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –ø–ª–∞—Ç–æ<br>
    <strong>Œ≤</strong>: –æ–±—ã—á–Ω–æ 0.9 –∏–ª–∏ 0.99</p>
  </div>

  <div class="block">
    <h2>üî∑ 4. RMSprop</h2>
    <p><strong>–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π learning rate –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞</strong></p>
    <p>E[g¬≤] = Œ≤ ¬∑ E[g¬≤] + (1-Œ≤) ¬∑ g¬≤<br>Œ∏ = Œ∏ - Œ± / ‚àö(E[g¬≤] + Œµ) ¬∑ g</p>
    <pre><code># PyTorch
optimizer = optim.RMSprop(
    model.parameters(),
    lr=0.001,
    alpha=0.99,  # decay rate
    eps=1e-8
)

# Keras
from tensorflow.keras.optimizers import RMSprop
optimizer = RMSprop(learning_rate=0.001)</code></pre>
    <p><strong>–•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç</strong>: –¥–ª—è RNN, –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–µ –∑–∞–¥–∞—á–∏</p>
  </div>

  <div class="block">
    <h2>üî∑ 5. Adam (Adaptive Moment Estimation)</h2>
    <p><strong>–°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π! –ö–æ–º–±–∏–Ω–∞—Ü–∏—è Momentum + RMSprop</strong></p>
    <p>m = Œ≤‚ÇÅ ¬∑ m + (1-Œ≤‚ÇÅ) ¬∑ g  (–º–æ–º–µ–Ω—Ç –ø–µ—Ä–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞)<br>
    v = Œ≤‚ÇÇ ¬∑ v + (1-Œ≤‚ÇÇ) ¬∑ g¬≤  (–º–æ–º–µ–Ω—Ç –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞)<br>
    mÃÇ = m / (1 - Œ≤‚ÇÅ·µó)  (bias correction)<br>
    vÃÇ = v / (1 - Œ≤‚ÇÇ·µó)<br>
    Œ∏ = Œ∏ - Œ± ¬∑ mÃÇ / (‚àövÃÇ + Œµ)</p>
    <pre><code># PyTorch
optimizer = optim.Adam(
    model.parameters(),
    lr=0.001,  # –∏–ª–∏ 1e-3
    betas=(0.9, 0.999),  # (Œ≤‚ÇÅ, Œ≤‚ÇÇ)
    eps=1e-8,
    weight_decay=0  # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
)

# Keras
from tensorflow.keras.optimizers import Adam
optimizer = Adam(learning_rate=0.001)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤</h2>
    <table>
      <tr><th>–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä</th><th>LR –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é</th><th>–°–∫–æ—Ä–æ—Å—Ç—å</th><th>–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</th></tr>
      <tr><td><strong>SGD</strong></td><td>0.01</td><td>–ú–µ–¥–ª–µ–Ω–Ω–∞—è</td><td>–°—Ä–µ–¥–Ω—è—è</td></tr>
      <tr><td><strong>SGD + Momentum</strong></td><td>0.01</td><td>–°—Ä–µ–¥–Ω—è—è</td><td>–•–æ—Ä–æ—à–∞—è</td></tr>
      <tr><td><strong>RMSprop</strong></td><td>0.001</td><td>–ë—ã—Å—Ç—Ä–∞—è</td><td>–•–æ—Ä–æ—à–∞—è</td></tr>
      <tr><td><strong>Adam</strong></td><td>0.001</td><td>–û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è</td><td>–û—Ç–ª–∏—á–Ω–∞—è</td></tr>
      <tr><td><strong>AdamW</strong></td><td>0.001</td><td>–û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è</td><td>–û—Ç–ª–∏—á–Ω–∞—è</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. AdamW (Adam with decoupled Weight Decay)</h2>
    <p><strong>–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è Adam</strong></p>
    <pre><code># PyTorch
optimizer = optim.AdamW(
    model.parameters(),
    lr=0.001,
    betas=(0.9, 0.999),
    weight_decay=0.01  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π weight decay
)

# Transformers library
from transformers import AdamW
optimizer = AdamW(
    model.parameters(),
    lr=5e-5,
    weight_decay=0.01
)</code></pre>
    <p><strong>–ü–æ—á–µ–º—É AdamW –ª—É—á—à–µ Adam</strong>: –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</p>
  </div>

  <div class="block">
    <h2>üî∑ 8. –î—Ä—É–≥–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã</h2>
    <ul>
      <li><strong>Adagrad</strong>: –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π LR, —Ö–æ—Ä–æ—à –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li><strong>Adadelta</strong>: —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ Adagrad</li>
      <li><strong>Nadam</strong>: Adam + Nesterov momentum</li>
      <li><strong>RAdam</strong>: Rectified Adam (warm-up –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)</li>
      <li><strong>Lookahead</strong>: –æ–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ –¥—Ä—É–≥–∏–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏</li>
      <li><strong>LAMB</strong>: –¥–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–µ–π</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 9. –¢–∏–ø–∏—á–Ω—ã–µ learning rates</h2>
    <table>
      <tr><th>–ó–∞–¥–∞—á–∞</th><th>–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä</th><th>LR</th></tr>
      <tr><td><strong>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</strong></td><td>Adam</td><td>1e-3 –¥–æ 1e-4</td></tr>
      <tr><td><strong>Fine-tuning (transfer)</strong></td><td>Adam</td><td>1e-5 –¥–æ 1e-4</td></tr>
      <tr><td><strong>Transformers</strong></td><td>AdamW</td><td>5e-5 –¥–æ 3e-4</td></tr>
      <tr><td><strong>RNN/LSTM</strong></td><td>Adam/RMSprop</td><td>1e-3</td></tr>
      <tr><td><strong>GAN</strong></td><td>Adam</td><td>2e-4 (–º–∞–ª—ã–π!)</td></tr>
      <tr><td><strong>–ë–æ–ª—å—à–∏–µ –±–∞—Ç—á–∏</strong></td><td>SGD + momentum</td><td>0.1 (—Å LR schedule)</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 10. Learning Rate Scheduling</h2>
    <p><strong>–ò–∑–º–µ–Ω–µ–Ω–∏–µ LR –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è</strong></p>
    <pre><code># PyTorch - Step LR
from torch.optim.lr_scheduler import StepLR

scheduler = StepLR(
    optimizer,
    step_size=10,  # –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
    gamma=0.1      # —É–º–Ω–æ–∂–∏—Ç—å –Ω–∞ 0.1
)

# –í training loop
for epoch in range(num_epochs):
    train(model, optimizer)
    scheduler.step()  # –û–±–Ω–æ–≤–∏—Ç—å LR

# Cosine Annealing
from torch.optim.lr_scheduler import CosineAnnealingLR
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=50  # –ø–µ—Ä–∏–æ–¥
)

# ReduceLROnPlateau (—É–º–µ–Ω—å—à–∞—Ç—å –ø—Ä–∏ –ø–ª–∞—Ç–æ)
from torch.optim.lr_scheduler import ReduceLROnPlateau
scheduler = ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.1,
    patience=10
)
# –í validation loop
scheduler.step(val_loss)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Keras Learning Rate Schedules</h2>
    <pre><code># Exponential decay
initial_lr = 0.001
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_lr,
    decay_steps=10000,
    decay_rate=0.96
)
optimizer = Adam(learning_rate=lr_schedule)

# Cosine decay
lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
    initial_lr,
    decay_steps=10000
)

# Callback –¥–ª—è ReduceLROnPlateau
from tensorflow.keras.callbacks import ReduceLROnPlateau

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=5,
    min_lr=1e-7
)

model.fit(X, y, callbacks=[reduce_lr])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ Adam/AdamW</h3>
        <ul>
          <li>–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á</li>
          <li>–ë—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</li>
          <li>–ù–µ —Ç—Ä–µ–±—É–µ—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏</li>
          <li>NLP, Computer Vision</li>
          <li>Transformers</li>
        </ul>
      </div>
      <div class="good">
        <h3>‚úÖ SGD + Momentum</h3>
        <ul>
          <li>–õ—É—á—à–∞—è —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è CV</li>
          <li>–õ—É—á—à–∞—è –æ–±–æ–±—â–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å</li>
          <li>–¢—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LR schedule</li>
          <li>ResNet, EfficientNet –æ–±—É—á–µ–Ω–∏–µ</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 13. Training Loop —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º</h2>
    <pre><code># PyTorch –ø–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä
import torch
import torch.nn as nn
import torch.optim as optim

model = MyModel()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

for epoch in range(num_epochs):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        # Forward pass
        output = model(data)
        loss = criterion(output, target)
        
        # Backward pass
        optimizer.zero_grad()  # –í–ê–ñ–ù–û! –û–±–Ω—É–ª–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        loss.backward()
        optimizer.step()       # –û–±–Ω–æ–≤–∏—Ç—å –≤–µ—Å–∞
    
    # –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
    scheduler.step()
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    model.eval()
    with torch.no_grad():
        val_loss = validate(model, val_loader)
    
    print(f"Epoch {epoch}, LR: {scheduler.get_last_lr()[0]:.6f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. Gradient Accumulation</h2>
    <p><strong>–î–ª—è —ç–º—É–ª—è—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–µ–π</strong></p>
    <pre><code># –ï—Å–ª–∏ batch_size=32, –Ω–æ —Ö–æ—Ç–∏–º —ç—Ñ—Ñ–µ–∫—Ç 128
accumulation_steps = 4

for epoch in range(num_epochs):
    for i, (data, target) in enumerate(train_loader):
        output = model(data)
        loss = criterion(output, target)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º loss
        loss = loss / accumulation_steps
        loss.backward()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ –∫–∞–∂–¥—ã–µ accumulation_steps
        if (i + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏</h2>
    <ul>
      <li><strong>–ù–∞—á–Ω–∏—Ç–µ —Å Adam</strong>: —Ä–∞–±–æ—Ç–∞–µ—Ç –∏–∑ –∫–æ—Ä–æ–±–∫–∏</li>
      <li><strong>LR = 1e-3 –∏–ª–∏ 1e-4</strong>: —Ö–æ—Ä–æ—à–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è</li>
      <li><strong>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ LR scheduler</strong>: —É–ª—É—á—à–∞–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</li>
      <li><strong>–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ LR</strong>: –ª–æ–≥–∏—Ä—É–π—Ç–µ —Ç–µ–∫—É—â–∏–π learning rate</li>
      <li><strong>Warm-up</strong>: –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ LR –≤ –Ω–∞—á–∞–ª–µ</li>
      <li><strong>–ù–µ –∑–∞–±—ã–≤–∞–π—Ç–µ optimizer.zero_grad()</strong>: –∏–Ω–∞—á–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç—Å—è!</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é Adam)</li>
      <li>[ ] –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å learning rate (1e-3 –∏–ª–∏ 1e-4)</li>
      <li>[ ] –î–æ–±–∞–≤–∏—Ç—å LR scheduler (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)</li>
      <li>[ ] –ü—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã–∑—ã–≤–∞—Ç—å optimizer.zero_grad()</li>
      <li>[ ] –ü—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã–∑—ã–≤–∞—Ç—å optimizer.step()</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ LR</li>
      <li>[ ] –î–ª—è fine-tuning ‚Äî –º–∞–ª—ã–π LR (1e-5)</li>
      <li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å weight_decay –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</li>
      <li>[ ] –î–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π ‚Äî gradient accumulation</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä ‚Äî —ç—Ç–æ "—à—Ç—É—Ä–º–∞–Ω", –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º—É —Ä–µ—à–µ–Ω–∏—é. Adam ‚Äî —Å–∞–º—ã–π —É–º–Ω—ã–π —à—Ç—É—Ä–º–∞–Ω, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞. SGD –ø—Ä–æ—â–µ, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —Ä—É—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –∑–∞—Ç–æ –∏–Ω–æ–≥–¥–∞ –Ω–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç¬ª.
    </blockquote>
  </div>



</div>
</body>
</html>

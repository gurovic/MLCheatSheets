<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–í–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –≤—ã–≤–æ–¥ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen { body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; color: #333; background: #fafcff; padding: 10px; } }
    @media print { body { background: white; padding: 0; } @page { size: A4 landscape; margin: 10mm; } }
    .container { column-count: 3; column-gap: 20px; max-width: 100%; }
    .block { break-inside: avoid; margin-bottom: 1.2em; padding: 12px; background: white; border-radius: 6px; box-shadow: 0 1px 3px rgba(0,0,0,0.05); }
    h1 { font-size: 1.6em; font-weight: 700; color: #1a5fb4; text-align: center; margin: 0 0 8px; column-span: all; }
    .subtitle { text-align: center; color: #666; font-size: 0.9em; margin-bottom: 12px; column-span: all; }
    h2 { font-size: 1.15em; font-weight: 700; color: #1a5fb4; margin: 0 0 8px; padding-bottom: 4px; border-bottom: 1px solid #e0e7ff; }
    p, ul, ol { font-size: 0.92em; margin: 0.6em 0; }
    ul, ol { padding-left: 18px; }
    li { margin-bottom: 4px; }
    code { font-family: 'Consolas', 'Courier New', monospace; background-color: #f0f4ff; padding: 1px 4px; border-radius: 3px; font-size: 0.88em; }
    pre { background-color: #f0f4ff; padding: 8px; border-radius: 4px; overflow-x: auto; font-size: 0.84em; margin: 6px 0; }
    pre code { padding: 0; background: none; white-space: pre-wrap; }
    blockquote { font-style: italic; margin: 8px 0; padding: 6px 10px; background: #f8fbff; border-left: 2px solid #1a5fb4; font-size: 0.88em; }
    table { width: 100%; border-collapse: collapse; font-size: 0.88em; margin: 6px 0; }
    th, td { padding: 6px 8px; text-align: left; border: 1px solid #e0e7ff; }
    th { background-color: #1a5fb4; color: white; font-weight: 700; }
    tr:nth-child(even) { background-color: #f8fbff; }
    @media print { .container { column-gap: 12px; } .block { box-shadow: none; } code, pre { font-size: 0.78em; } h1 { font-size: 1.4em; } h2 { font-size: 1em; } }
  </style>
</head>
<body>
<div class="container">
  <h1>ÔøΩÔøΩ –í–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –≤—ã–≤–æ–¥</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞</h2>
    <p><strong>Variational Inference (VI)</strong>: –ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω—ã–π –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –≤—ã–≤–æ–¥ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é</p>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: –Ω–∞–π—Ç–∏ q(z) ‚âà p(z|x) —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é</li>
      <li><strong>–í–º–µ—Å—Ç–æ</strong> —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è (MCMC) –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é</li>
      <li><strong>–ë—ã—Å—Ç—Ä–µ–µ</strong> —á–µ–º MCMC, –Ω–æ –º–µ–Ω–µ–µ —Ç–æ—á–Ω–æ</li>
      <li><strong>–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å</strong>: –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li><strong>–î–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º</strong>: –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã</li>
    </ul>
    <blockquote>üí° VI –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –∑–∞–¥–∞—á—É –≤—ã–≤–æ–¥–∞ –≤ –∑–∞–¥–∞—á—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏</blockquote>

  <div class="block">
    <h2>üî∑ 2. ELBO –∏ KL-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è</h2>
    <pre><code><strong>Evidence Lower Bound (ELBO):</strong>

log p(x) ‚â• ELBO = ùîº_q[log p(x,z)] - ùîº_q[log q(z)]
           = ùîº_q[log p(x,z) - log q(z)]

<strong>KL-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è:</strong>
KL(q(z) || p(z|x)) = ùîº_q[log q(z) - log p(z|x)]
                    = -ELBO + log p(x)

<strong>–ó–∞–¥–∞—á–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:</strong>
max ELBO(q) ‚ü∫ min KL(q(z) || p(z|x))
     q              q

<strong>–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è ELBO:</strong>
ELBO = ùîº_q[log p(x|z)] - KL(q(z) || p(z))
       ‚Üë                   ‚Üë
   reconstruction      regularization</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. Mean-field –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ</h2>
    <p><strong>–ü–æ–ª–Ω–∞—è —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è</strong>:</p>
    <pre><code>q(z) = ‚àè·µ¢ q_i(z_i)

# –ü—Ä–∏–º–µ—Ä –¥–ª—è Gaussian
q(z) = ‚àè·µ¢ ùí©(z_i | Œº_i, œÉ_i¬≤)

# –í PyTorch
import torch
import torch.nn as nn

class MeanFieldGaussian(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.mu = nn.Parameter(torch.zeros(dim))
        self.log_sigma = nn.Parameter(torch.zeros(dim))
    
    def sample(self, n_samples=1):
        eps = torch.randn(n_samples, self.mu.shape[0])
        sigma = torch.exp(self.log_sigma)
        return self.mu + sigma * eps
    
    def log_prob(self, z):
        sigma = torch.exp(self.log_sigma)
        log_prob = -0.5 * torch.sum(
            (z - self.mu)**2 / sigma**2 + 
            2 * self.log_sigma + 
            torch.log(torch.tensor(2 * 3.14159))
        , dim=-1)
        return log_prob</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–Ω—ã–π –ø–æ–¥—ä–µ–º (CAVI)</h2>
    <pre><code># Coordinate Ascent Variational Inference
import numpy as np

def cavi_gaussian_mixture(X, K, max_iter=100):
    """CAVI –¥–ª—è Gaussian Mixture Model"""
    N, D = X.shape
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    r = np.random.dirichlet(np.ones(K), N)  # responsibilities
    mu = X[np.random.choice(N, K)]
    
    for iteration in range(max_iter):
        # E-step: –æ–±–Ω–æ–≤–∏—Ç—å q(z)
        for n in range(N):
            for k in range(K):
                r[n, k] = np.exp(
                    -0.5 * np.sum((X[n] - mu[k])**2)
                )
            r[n] /= np.sum(r[n])
        
        # M-step: –æ–±–Ω–æ–≤–∏—Ç—å q(Œº)
        for k in range(K):
            mu[k] = np.sum(r[:, k:k+1] * X, axis=0)
            mu[k] /= np.sum(r[:, k])
        
        # –í—ã—á–∏—Å–ª–∏—Ç—å ELBO
        elbo = compute_elbo(X, mu, r)
        
        if iteration % 10 == 0:
            print(f"Iteration {iteration}, ELBO: {elbo:.3f}")
    
    return mu, r

def compute_elbo(X, mu, r):
    N, K = r.shape
    elbo = 0
    for n in range(N):
        for k in range(K):
            if r[n, k] > 0:
                elbo += r[n, k] * (
                    -0.5 * np.sum((X[n] - mu[k])**2)
                    - r[n, k] * np.log(r[n, k])
                )
    return elbo</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Stochastic Variational Inference</h2>
    <pre><code>import torch
import torch.optim as optim

class StochasticVI:
    def __init__(self, model, q_dist, lr=0.01):
        self.model = model
        self.q = q_dist
        self.optimizer = optim.Adam(q.parameters(), lr=lr)
    
    def elbo_estimate(self, x, n_samples=1):
        """–û—Ü–µ–Ω–∫–∞ ELBO —á–µ—Ä–µ–∑ MC"""
        z = self.q.sample(n_samples)  # [n_samples, dim_z]
        
        # Log likelihood
        log_p_x_z = self.model.log_likelihood(x, z)
        
        # Log prior
        log_p_z = self.model.log_prior(z)
        
        # Log variational
        log_q_z = self.q.log_prob(z)
        
        # ELBO = E[log p(x,z) - log q(z)]
        elbo = torch.mean(
            log_p_x_z + log_p_z - log_q_z
        )
        
        return elbo
    
    def step(self, x, n_samples=1):
        self.optimizer.zero_grad()
        
        # Compute ELBO
        elbo = self.elbo_estimate(x, n_samples)
        
        # Maximize ELBO = minimize -ELBO
        loss = -elbo
        loss.backward()
        
        self.optimizer.step()
        
        return elbo.item()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
svi = StochasticVI(model, q_dist, lr=0.01)
for epoch in range(1000):
    elbo = svi.step(x_batch, n_samples=10)
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, ELBO: {elbo:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Reparametrization trick</h2>
    <pre><code>import torch

class ReparameterizedGaussian:
    """
    –¢—Ä—é–∫ —Ä–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å
    z = Œº + œÉ * Œµ, –≥–¥–µ Œµ ~ N(0,1)
    """
    def __init__(self, mu, log_sigma):
        self.mu = mu
        self.log_sigma = log_sigma
    
    def sample(self, n_samples=1):
        eps = torch.randn(n_samples, *self.mu.shape)
        sigma = torch.exp(self.log_sigma)
        z = self.mu + sigma * eps
        return z
    
    def log_prob(self, z):
        sigma = torch.exp(self.log_sigma)
        log_prob = -0.5 * (
            ((z - self.mu) / sigma) ** 2 +
            2 * self.log_sigma +
            torch.log(torch.tensor(2 * 3.14159))
        )
        return torch.sum(log_prob, dim=-1)

# VAE encoder
class Encoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, 256)
        self.fc_mu = nn.Linear(256, latent_dim)
        self.fc_log_sigma = nn.Linear(256, latent_dim)
    
    def forward(self, x):
        h = torch.relu(self.fc1(x))
        mu = self.fc_mu(h)
        log_sigma = self.fc_log_sigma(h)
        return mu, log_sigma
    
    def encode(self, x):
        mu, log_sigma = self.forward(x)
        q = ReparameterizedGaussian(mu, log_sigma)
        return q</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Variational Autoencoder (VAE)</h2>
    <pre><code>import torch
import torch.nn as nn

class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        
        # Encoder: q(z|x)
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 400),
            nn.ReLU(),
            nn.Linear(400, 2 * latent_dim)  # Œº and log œÉ
        )
        
        # Decoder: p(x|z)
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 400),
            nn.ReLU(),
            nn.Linear(400, input_dim),
            nn.Sigmoid()
        )
        
        self.latent_dim = latent_dim
    
    def encode(self, x):
        h = self.encoder(x)
        mu = h[:, :self.latent_dim]
        log_sigma = h[:, self.latent_dim:]
        return mu, log_sigma
    
    def reparameterize(self, mu, log_sigma):
        std = torch.exp(0.5 * log_sigma)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        return self.decoder(z)
    
    def forward(self, x):
        mu, log_sigma = self.encode(x)
        z = self.reparameterize(mu, log_sigma)
        return self.decode(z), mu, log_sigma
    
    def loss_function(self, x, x_recon, mu, log_sigma):
        # Reconstruction loss
        recon_loss = nn.functional.binary_cross_entropy(
            x_recon, x, reduction='sum'
        )
        
        # KL divergence: KL(q(z|x) || p(z))
        # –≥–¥–µ p(z) = N(0, I)
        kl_loss = -0.5 * torch.sum(
            1 + log_sigma - mu.pow(2) - log_sigma.exp()
        )
        
        return recon_loss + kl_loss

# Training
vae = VAE(input_dim=784, latent_dim=20)
optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)

for epoch in range(num_epochs):
    for batch in dataloader:
        x = batch.view(-1, 784)
        
        # Forward
        x_recon, mu, log_sigma = vae(x)
        loss = vae.loss_function(x, x_recon, mu, log_sigma)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Amortized Inference</h2>
    <p><strong>–ê–º–æ—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥</strong>: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è q(z|x)</p>
    <ul>
      <li><strong>–í–º–µ—Å—Ç–æ</strong>: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ q –¥–ª—è –∫–∞–∂–¥–æ–≥–æ x –æ—Ç–¥–µ–ª—å–Ω–æ</li>
      <li><strong>–û–±—É—á–∞–µ–º</strong>: inference network œÜ: x ‚Üí (Œº, œÉ)</li>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</strong>: –±—ã—Å—Ç—Ä—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –Ω–æ–≤—ã—Ö x</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: VAE, normalizing flows</li>
    </ul>
    <pre><code># –ë–µ–∑ –∞–º–æ—Ä—Ç–∏–∑–∞—Ü–∏–∏ (–º–µ–¥–ª–µ–Ω–Ω–æ)
for x in dataset:
    q_x = optimize_q_for_x(x)  # O(iterations)
    z = q_x.sample()

# –° –∞–º–æ—Ä—Ç–∏–∑–∞—Ü–∏–µ–π (–±—ã—Å—Ç—Ä–æ)
inference_net = train_inference_network(dataset)  # Once
for x in dataset:
    z = inference_net(x).sample()  # O(1)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Black Box Variational Inference</h2>
    <pre><code>import torch

def bbvi_gradient_estimate(model, q, x, n_samples=10):
    """
    Black box variational inference —Å score function estimator
    ‚àáœÜ ELBO = ùîº_q[‚àáœÜ log q(z) (log p(x,z) - log q(z))]
    """
    z_samples = q.sample(n_samples)
    
    # Log joint
    log_p_xz = model.log_joint(x, z_samples)
    
    # Log variational
    log_q_z = q.log_prob(z_samples)
    
    # f(z) = log p(x,z) - log q(z)
    f_z = log_p_xz - log_q_z
    
    # Score function: ‚àáœÜ log q(z; œÜ)
    log_q_z.backward(torch.ones_like(log_q_z), retain_graph=True)
    score = [p.grad.clone() for p in q.parameters()]
    
    # Gradient estimate
    gradients = []
    for param, sc in zip(q.parameters(), score):
        grad = torch.mean(f_z.unsqueeze(-1) * sc, dim=0)
        gradients.append(grad)
    
    return gradients

# –° control variates –¥–ª—è variance reduction
class BBVIWithBaseline:
    def __init__(self, model, q, lr=0.01):
        self.model = model
        self.q = q
        self.optimizer = torch.optim.Adam(q.parameters(), lr=lr)
        self.baseline = 0.0  # Moving average
        self.baseline_decay = 0.9
    
    def step(self, x, n_samples=10):
        z = self.q.sample(n_samples)
        
        log_p_xz = self.model.log_joint(x, z)
        log_q_z = self.q.log_prob(z)
        
        f_z = log_p_xz - log_q_z
        
        # Update baseline (moving average)
        self.baseline = (
            self.baseline_decay * self.baseline +
            (1 - self.baseline_decay) * torch.mean(f_z).item()
        )
        
        # Centered reward
        f_centered = f_z - self.baseline
        
        # Policy gradient with baseline
        loss = -torch.mean(f_centered * log_q_z)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return torch.mean(f_z).item()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –≤—ã–≤–æ–¥–∞</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–°–∫–æ—Ä–æ—Å—Ç—å</th><th>–¢–æ—á–Ω–æ—Å—Ç—å</th><th>–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å</th></tr>
      <tr><td>MCMC</td><td>‚≠ê</td><td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td><td>‚≠ê‚≠ê</td></tr>
      <tr><td>Mean-field VI</td><td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td><td>‚≠ê‚≠ê</td><td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td></tr>
      <tr><td>Stochastic VI</td><td>‚≠ê‚≠ê‚≠ê‚≠ê</td><td>‚≠ê‚≠ê‚≠ê</td><td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td></tr>
      <tr><td>BBVI</td><td>‚≠ê‚≠ê‚≠ê</td><td>‚≠ê‚≠ê‚≠ê</td><td>‚≠ê‚≠ê‚≠ê‚≠ê</td></tr>
      <tr><td>Normalizing Flows</td><td>‚≠ê‚≠ê‚≠ê</td><td>‚≠ê‚≠ê‚≠ê‚≠ê</td><td>‚≠ê‚≠ê‚≠ê</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏</h2>
    <ul>
      <li><strong>Importance Weighted VAE</strong>: —É–ª—É—á—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ ELBO</li>
      <li><strong>Œ≤-VAE</strong>: –∫–æ–Ω—Ç—Ä–æ–ª—å disentanglement —á–µ—Ä–µ–∑ Œ≤¬∑KL</li>
      <li><strong>Normalizing flows</strong>: –±–æ–ª–µ–µ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã–π q(z)</li>
      <li><strong>Hierarchical VI</strong>: –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ latent variables</li>
      <li><strong>Structured VI</strong>: –Ω–µ mean-field, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</li>
      <li><strong>Stein VI</strong>: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Stein's identity</li>
    </ul>
    <pre><code># Œ≤-VAE
def beta_vae_loss(x, x_recon, mu, log_sigma, beta=4.0):
    recon = F.binary_cross_entropy(x_recon, x, reduction='sum')
    kl = -0.5 * torch.sum(1 + log_sigma - mu.pow(2) - log_sigma.exp())
    return recon + beta * kl

# Importance Weighted VAE
def iwae_loss(x, encoder, decoder, k=50):
    mu, log_sigma = encoder(x)
    
    # Sample k latent codes
    z = reparameterize(mu, log_sigma, k)
    
    log_p_x_z = decoder.log_prob(x, z)
    log_p_z = prior.log_prob(z)
    log_q_z_x = gaussian_log_prob(z, mu, log_sigma)
    
    # Importance weights
    log_w = log_p_x_z + log_p_z - log_q_z_x
    
    # IWAE bound
    return -torch.logsumexp(log_w, dim=0) + np.log(k)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <ul>
      <li>‚úÖ <strong>–ù–∞—á–∏–Ω–∞–π—Ç–µ —Å mean-field</strong>: –ø—Ä–æ—Å—Ç–æ—Ç–∞ –≤–∞–∂–Ω–∞</li>
      <li>‚úÖ <strong>–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ELBO</strong>: –¥–æ–ª–∂–µ–Ω —Ä–∞—Å—Ç–∏</li>
      <li>‚úÖ <strong>Reparametrization trick</strong>: –∫–æ–≥–¥–∞ –≤–æ–∑–º–æ–∂–Ω–æ</li>
      <li>‚úÖ <strong>Warm-up –¥–ª—è KL</strong>: Œ≤ –æ—Ç 0 –¥–æ 1</li>
      <li>‚úÖ <strong>Batch normalization</strong>: —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ</li>
      <li>‚úÖ <strong>Learning rate</strong>: –Ω–∞—á–Ω–∏—Ç–µ —Å 1e-3 - 1e-4</li>
      <li>‚úÖ <strong>–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–≤–µ—Ä—Ñ–∏—Ç–∞</strong>: train vs valid ELBO</li>
      <li>‚úÖ <strong>Visualization</strong>: —Å–ª–µ–¥–∏—Ç–µ –∑–∞ latent space</li>
    </ul>
    <blockquote>üí° VI - —ç—Ç–æ approximation, –Ω–µ exact inference. –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è!</blockquote>
  </div>

</div>
</div>
</body>
</html>

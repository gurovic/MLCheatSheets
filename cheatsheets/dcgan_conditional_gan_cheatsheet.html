<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>DCGAN –∏ Conditional GAN ‚Äî Cheatsheet</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      font-size: 0.9em;
      color: #666;
      text-align: center;
      margin-bottom: 20px;
      column-span: all;
    }

    h2 {
      font-size: 1.1em;
      font-weight: 700;
      margin-top: 0;
      color: #1a5fb4;
      border-bottom: 2px solid #e0e8f5;
      padding-bottom: 4px;
    }

    h3 {
      font-size: 0.95em;
      font-weight: 600;
      margin: 8px 0 4px;
      color: #26a269;
    }

    p, ul, ol {
      margin: 6px 0;
      font-size: 0.88em;
      line-height: 1.5;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 3px;
    }

    code {
      background: #f6f8fa;
      padding: 1px 4px;
      border-radius: 3px;
      font-family: 'Consolas', 'Monaco', monospace;
      font-size: 0.9em;
      color: #c7254e;
    }

    pre {
      background: #282c34;
      color: #abb2bf;
      padding: 10px;
      border-radius: 6px;
      overflow-x: auto;
      font-size: 0.8em;
      line-height: 1.4;
      margin: 8px 0;
    }

    pre code {
      background: transparent;
      color: inherit;
      padding: 0;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 8px 0;
      font-size: 0.85em;
    }

    th, td {
      border: 1px solid #ddd;
      padding: 6px 8px;
      text-align: left;
    }

    th {
      background: #e0e8f5;
      font-weight: 600;
      color: #1a5fb4;
    }

    tr:nth-child(even) {
      background: #f9fbff;
    }

    blockquote {
      background: #fff9e6;
      border-left: 4px solid #f6d32d;
      padding: 8px 12px;
      margin: 8px 0;
      font-size: 0.88em;
      font-style: italic;
    }

    .formula {
      background: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      margin: 8px 0;
      font-family: 'Cambria', 'Times New Roman', serif;
      font-size: 0.9em;
      text-align: center;
    }
  </style>
</head>
<body>

<h1>üéØ DCGAN –∏ Conditional GAN</h1>
<div class="subtitle">Deep Convolutional Generative Adversarial Networks</div>

<div class="container">

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã GAN</h2>
    <p>GAN —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —Å–µ—Ç–µ–π: Generator (—Å–æ–∑–¥–∞—ë—Ç –¥–∞–Ω–Ω—ã–µ) –∏ Discriminator (–æ—Ç–ª–∏—á–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –æ—Ç –ø–æ–¥–¥–µ–ª—å–Ω—ã—Ö).</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># –ü—Ä–∏–º–µ—Ä –∫–æ–¥–∞ –¥–ª—è –û—Å–Ω–æ–≤—ã GAN (vanilla GAN –Ω–∞ PyTorch)
import torch
import torch.nn as nn
import torch.optim as optim

# –ü—Ä–æ—Å—Ç—ã–µ —Å–µ—Ç–∏: –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä
latent_dim = 100
data_dim = 784  # –ø—Ä–∏–º–µ—Ä: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è 28x28, —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–µ –≤ –≤–µ–∫—Ç–æ—Ä


class Generator(nn.Module):
    def __init__(self, latent_dim, data_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(True),
            nn.Linear(256, 512),
            nn.ReLU(True),
            nn.Linear(512, data_dim),
            nn.Tanh(),  # –¥–∞–Ω–Ω—ã–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-1, 1]
        )

    def forward(self, z):
        return self.net(z)


class Discriminator(nn.Module):
    def __init__(self, data_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(data_dim, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
        )  # –±–µ–∑ Sigmoid: –∏—Å–ø–æ–ª—å–∑—É–µ–º BCEWithLogitsLoss

    def forward(self, x):
        return self.net(x).view(-1)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
G = Generator(latent_dim, data_dim).to(device)
D = Discriminator(data_dim).to(device)

criterion = nn.BCEWithLogitsLoss()
optimizer_G = optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))
optimizer_D = optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))

# dataloader –¥–æ–ª–∂–µ–Ω –æ—Ç–¥–∞–≤–∞—Ç—å real_images —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ [batch_size, data_dim]
num_epochs = 5
for epoch in range(num_epochs):
    for real_images, _ in dataloader:
        real_images = real_images.view(real_images.size(0), -1).to(device)
        batch_size = real_images.size(0)

        # ----- –û–±—É—á–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞ -----
        z = torch.randn(batch_size, latent_dim, device=device)
        fake_images = G(z).detach()  # –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞

        real_labels = torch.ones(batch_size, device=device)
        fake_labels = torch.zeros(batch_size, device=device)

        D_real = D(real_images)
        D_fake = D(fake_images)

        loss_D_real = criterion(D_real, real_labels)
        loss_D_fake = criterion(D_fake, fake_labels)
        loss_D = (loss_D_real + loss_D_fake) / 2

        optimizer_D.zero_grad()
        loss_D.backward()
        optimizer_D.step()

        # ----- –û–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ -----
        z = torch.randn(batch_size, latent_dim, device=device)
        fake_images = G(z)
        # –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ö–æ—á–µ—Ç, —á—Ç–æ–±—ã D —Å—á–∏—Ç–∞–ª –ø–æ–¥–¥–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–µ–∞–ª—å–Ω—ã–º–∏ (–º–µ—Ç–∫–∏ 1)
        G_labels = torch.ones(batch_size, device=device)
        D_fake_for_G = D(fake_images)
        loss_G = criterion(D_fake_for_G, G_labels)

        optimizer_G.zero_grad()
        loss_G.backward()
        optimizer_G.step()

    print(f"Epoch {epoch+1}: loss_D={loss_D.item():.3f}, loss_G={loss_G.item():.3f}")</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 2. DCGAN Architecture</h2>
    <p>Deep Convolutional GAN ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤—ë—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ –≤–º–µ—Å—Ç–æ fully-connected.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Wasserstein GAN (WGAN) Loss
import torch

def wasserstein_loss_d(real_output, fake_output):
    """Discriminator loss –¥–ª—è WGAN"""
    return -(torch.mean(real_output) - torch.mean(fake_output))

def wasserstein_loss_g(fake_output):
    """Generator loss –¥–ª—è WGAN"""
    return -torch.mean(fake_output)

# Weight clipping –¥–ª—è WGAN
def clip_weights(model, clip_value=0.01):
    for p in model.parameters():
        p.data.clamp_(-clip_value, clip_value)</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 3. Generator Network</h2>
    <p>–ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º z, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ transpose convolutions.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Data Augmentation –¥–ª—è GAN
from torchvision import transforms

transform = transforms.Compose([
    transforms.Resize(64),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 4. Discriminator Network</h2>
    <p>–ü—Ä–∏–Ω–∏–º–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –≤—ã–¥–∞—ë—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —á—Ç–æ –æ–Ω–æ —Ä–µ–∞–ª—å–Ω–æ–µ (0-1).</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Conditional GAN Implementation
import torch
import torch.nn as nn

class ConditionalGenerator(nn.Module):
    def __init__(self, latent_dim, num_classes, img_dim):
        super().__init__()
        self.label_emb = nn.Embedding(num_classes, num_classes)
        
        self.model = nn.Sequential(
            nn.Linear(latent_dim + num_classes, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 256),
            nn.BatchNorm1d(256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.BatchNorm1d(512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, img_dim),
            nn.Tanh()
        )
    
    def forward(self, z, labels):
        label_input = self.label_emb(labels)
        x = torch.cat([z, label_input], dim=1)
        return self.model(x)

# Conditional Discriminator
class ConditionalDiscriminator(nn.Module):
    def __init__(sel<pre><code># Mode Collapse Detection and Prevention
import numpy as np

def check_mode_collapse(generated_samples, num_modes=10):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
    """
    from sklearn.cluster import KMeans
    
    kmeans = KMeans(n_clusters=num_modes)
    kmeans.fit(generated_samples)
    
    # –°—á–∏—Ç–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    unique, counts = np.unique(kmeans.labels_, return_counts=True)
    distribution = counts / len(generated_samples)
    
    # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 80% –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ - –≤–µ—Ä–æ—è—Ç–Ω—ã–π mode collapse
    if max(distribution) > 0.8:
        print("‚ö† Warning: Possible mode collapse detected!")
        return True
    return False

# Minibatch Discrimination –¥–ª—è –±–æ—Ä—å–±—ã —Å mode collapse
class MinibatchDiscrimination(nn.Module):
    def __init__(self, in_features, out_features, kernel_dims):
        super().__init__()
        self.T = nn.Pa<pre><code># GAN Evaluation Metrics
import torch
from scipy.linalg import sqrtm
import numpy as np

def calculate_fid(real_features, generated_features):
    """
    Fr√©chet Inception Distance (FID)
    –ú–µ–Ω—å—à–µ = –ª—É—á—à–µ (–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
    """
    # –í—ã—á–∏—Å–ª—è–µ–º mean –∏ covariance
    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)
    mu2, sigma2 = generated_features.mean(axis=0), np.cov(generated_features, rowvar=False)
    
    # FID —Ñ–æ—Ä–º—É–ª–∞
    diff = mu1 - mu2
    covmean = sqrtm(sigma1.dot(sigma2))
    
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    
    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)
    return fid

def inception_score(generated_images, model, splits=10):
    """
    Inception Score (IS)
    –ë–æ–ª—å—à–µ = –ª—É—á—à–µ (–±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ)
    """
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞<pre><code># Conditional GAN Implementation
import torch
import torch.nn as nn

class ConditionalGenerator(nn.Module):
    def __init__(self, latent_dim, num_classes, img_dim):
        super().__init__()
        self.label_emb = nn.Embedding(num_classes, num_classes)
        
        self.model = nn.Sequential(
            nn.Linear(latent_dim + num_classes, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 256),
            nn.BatchNorm1d(256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.BatchNorm1d(512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, img_dim),
            nn.Tanh()
        )
    
    def forward(self, z, labels):
        label_input = self.label_emb(labels)
        x = torch.cat([z, label_input], dim=1)
        return self.model(x)

# Conditional Discriminator
class ConditionalDiscriminato<pre><code># Mode Collapse Detection and Prevention
import numpy as np

def check_mode_collapse(generated_samples, num_modes=10):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
    """
    from sklearn.cluster import KMeans
    
    kmeans = KMeans(n_clusters=num_modes)
    kmeans.fit(generated_samples)
    
    # –°—á–∏—Ç–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    unique, counts = np.unique(kmeans.labels_, return_counts=True)
    distribution = counts / len(generated_samples)
    
    # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 80% –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ - –≤–µ—Ä–æ—è—Ç–Ω—ã–π mode collapse
    if max(distribution) > 0.8:
        print("‚ö† Warning: Possible mode collapse detected!")
        return True
    return False

# Minibatch Discrimination –¥–ª—è –±–æ—Ä—å–±—ã —Å mode collapse
class MinibatchDiscrimination(nn.Module):
    def __init__(self, in_features, out_features, kernel_dims):
        super().__init__()
        self.T = nn.Parameter(t<pre><code># GAN Evaluation Metrics
import torch
from scipy.linalg import sqrtm
import numpy as np

def calculate_fid(real_features, generated_features):
    """
    Fr√©chet Inception Distance (FID)
    –ú–µ–Ω—å—à–µ = –ª—É—á—à–µ (–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
    """
    # –í—ã—á–∏—Å–ª—è–µ–º mean –∏ covariance
    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)
    mu2, sigma2 = generated_features.mean(axis=0), np.cov(generated_features, rowvar=False)
    
    # FID —Ñ–æ—Ä–º—É–ª–∞
    diff = mu1 - mu2
    covmean = sqrtm(sigma1.dot(sigma2))
    
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    
    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)
    return fid

def inception_score(generated_images, model, splits=10):
    """
    Inception Score (IS)
    –ë–æ–ª—å—à–µ = –ª—É—á—à–µ (–±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ)
    """
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏<pre><code># Conditional GAN Implementation
import torch
import torch.nn as nn

class ConditionalGenerator(nn.Module):
    def __init__(self, latent_dim, num_classes, img_dim):
        super().__init__()
        self.label_emb = nn.Embedding(num_classes, num_classes)
        
        self.model = nn.Sequential(
            nn.Linear(latent_dim + num_classes, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 256),
            nn.BatchNorm1d(256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.BatchNorm1d(512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, img_dim),
            nn.Tanh()
        )
    
    def forward(self, z, labels):
        label_input = self.label_emb(labels)
        x = torch.cat([z, label_input], dim=1)
        return self.model(x)

# Conditional Discriminator
class ConditionalDiscriminator(nn.Module):
 <pre><code># Mode Collapse Detection and Prevention
import numpy as np

def check_mode_collapse(generated_samples, num_modes=10):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
    """
    from sklearn.cluster import KMeans
    
    kmeans = KMeans(n_clusters=num_modes)
    kmeans.fit(generated_samples)
    
    # –°—á–∏—Ç–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    unique, counts = np.unique(kmeans.labels_, return_counts=True)
    distribution = counts / len(generated_samples)
    
    # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 80% –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ - –≤–µ—Ä–æ—è—Ç–Ω—ã–π mode collapse
    if max(distribution) > 0.8:
        print("‚ö† Warning: Possible mode collapse detected!")
        return True
    return False

# Minibatch Discrimination –¥–ª—è –±–æ—Ä—å–±—ã —Å mode collapse
class MinibatchDiscrimination(nn.Module):
    def __init__(self, in_features, out_features, kernel_dims):
        super().__init__(<pre><code># GAN Evaluation Metrics
import torch
from scipy.linalg import sqrtm
import numpy as np

def calculate_fid(real_features, generated_features):
    """
    Fr√©chet Inception Distance (FID)
    –ú–µ–Ω—å—à–µ = –ª—É—á—à–µ (–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
    """
    # –í—ã—á–∏—Å–ª—è–µ–º mean –∏ covariance
    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)
    mu2, sigma2 = generated_features.mean(axis=0), np.cov(generated_features, rowvar=False)
    
    # FID —Ñ–æ—Ä–º—É–ª–∞
    diff = mu1 - mu2
    covmean = sqrtm(sigma1.dot(sigma2))
    
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    
    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)
    return fid

def inception_score(generated_images, model, splits=10):
    """
    Inception Score (IS)
    –ë–æ–ª—å—à–µ = –ª—É—á—à–µ (–±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ)
    """
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Inception –º–æ–¥–µ–ª–∏
    with torch.no_grad():
        preds = model(generated_images)
    
    # –í—ã—á–∏—Å–ª—è–µ–º IS
    split_scores = []
    for k in range(splits):
        part = preds[k * (len(preds) // splits): (k + 1) * (len(preds) // splits)]
        py = part.mean(axis=0)
        scores = []
        for p in part:
            scores.append((p * (np.log(p) - np.log(py))).sum())
        split_scores.append(np.exp(np.mean(scores)))
    
    return np.mean(split_scores), np.std(split_scores)</code></pre>minibatch_features = torch.sum(torch.exp(-abs_diffs), dim=0)
        return torch.cat([x, minibatch_features], dim=1)</code></pre>56, 1),
            nn.Sigmoid()
        )
    
    def forward(self, img, labels):
        label_input = self.label_emb(labels)
        x = torch.cat([img, label_input], dim=1)
        return self.model(x)</code></pre>) - np.log(py))).sum())
        split_scores.append(np.exp(np.mean(scores)))
    
    return np.mean(split_scores), np.std(split_scores)</code></pre>sum(torch.exp(-abs_diffs), dim=0)
        return torch.cat([x, minibatch_features], dim=1)</code></pre>   nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(self, img, labels):
        label_input = self.label_emb(labels)
        x = torch.cat([img, label_input], dim=1)
        return self.model(x)</code></pre>np.log(p) - np.log(py))).sum())
        split_scores.append(np.exp(np.mean(scores)))
    
    return np.mean(split_scores), np.std(split_scores)</code></pre> torch.sum(torch.exp(-abs_diffs), dim=0)
        return torch.cat([x, minibatch_features], dim=1)</code></pre>Sigmoid()
        )
    
    def forward(self, img, labels):
        label_input = self.label_emb(labels)
        x = torch.cat([img, label_input], dim=1)
        return self.model(x)</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 5. Training Process</h2>
    <p>–ú–∏–Ω–∏–º–∞–∫—Å –∏–≥—Ä–∞: G –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç log(D(G(z))), D –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç log(D(x)) + log(1-D(G(z))).</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Progressive Growing GAN
class ProgressiveGenerator(nn.Module):
    def __init__(self):
        super().__init__()
        self.blocks = nn.ModuleList([
            self._block(512, 512),
            self._block(512, 256),
            self._block(256, 128),
        ])
    
    def _block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.Upsample(scale_factor=2),
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.LeakyReLU(0.2)
        )</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 6. Conditional GAN</h2>
    <p>–î–æ–±–∞–≤–ª—è–µ—Ç —É—Å–ª–æ–≤–∏–µ y (–º–µ—Ç–∫—É –∫–ª–∞—Å—Å–∞) –∫ G –∏ D –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Wasserstein GAN (WGAN) Loss
import torch

def wasserstein_loss_d(real_output, fake_output):
    """Discriminator loss –¥–ª—è WGAN"""
    return -(torch.mean(real_output) - torch.mean(fake_output))

def wasserstein_loss_g(fake_output):
    """Generator loss –¥–ª—è WGAN"""
    return -torch.mean(fake_output)

# Weight clipping –¥–ª—è WGAN
def clip_weights(model, clip_value=0.01):
    for p in model.parameters():
        p.data.clamp_(-clip_value, clip_value)</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 7. Loss Functions</h2>
    <p>BCE loss –¥–ª—è D –∏ G. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: Feature matching, Wasserstein loss.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Data Augmentation –¥–ª—è GAN
from torchvision import transforms

transform = transforms.Compose([
    transforms.Resize(64),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 8. Training Tips</h2>
    <p>Batch normalization, LeakyReLU, Adam optimizer, label smoothing.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Progressive Growing GAN
class ProgressiveGenerator(nn.Module):
    def __init__(self):
        super().__init__()
        self.blocks = nn.ModuleList([
            self._block(512, 512),
            self._block(512, 256),
            self._block(256, 128),
        ])
    
    def _block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.Upsample(scale_factor=2),
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.LeakyReLU(0.2)
        )</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 9. Mode Collapse</h2>
    <p>–ü—Ä–æ–±–ª–µ–º–∞ –∫–æ–≥–¥–∞ G –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –†–µ—à–µ–Ω–∏–µ: minibatch discrimination.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Wasserstein GAN (WGAN) Loss
import torch

def wasserstein_loss_d(real_output, fake_output):
    """Discriminator loss –¥–ª—è WGAN"""
    return -(torch.mean(real_output) - torch.mean(fake_output))

def wasserstein_loss_g(fake_output):
    """Generator loss –¥–ª—è WGAN"""
    return -torch.mean(fake_output)

# Weight clipping –¥–ª—è WGAN
def clip_weights(model, clip_value=0.01):
    for p in model.parameters():
        p.data.clamp_(-clip_value, clip_value)</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 10. Evaluation Metrics</h2>
    <p>Inception Score, FID (Fr√©chet Inception Distance), –≤–∏–∑—É–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Data Augmentation –¥–ª—è GAN
from torchvision import transforms

transform = transforms.Compose([
    transforms.Resize(64),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 11. Applications</h2>
    <p>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, data augmentation, style transfer, super-resolution.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Progressive Growing GAN
class ProgressiveGenerator(nn.Module):
    def __init__(self):
        super().__init__()
        self.blocks = nn.ModuleList([
            self._block(512, 512),
            self._block(512, 256),
            self._block(256, 128),
        ])
    
    def _block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.Upsample(scale_factor=2),
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.LeakyReLU(0.2)
        )</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 12. Code Implementation</h2>
    <p>PyTorch/TensorFlow —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∫–æ–¥–∞.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Wasserstein GAN (WGAN) Loss
import torch

def wasserstein_loss_d(real_output, fake_output):
    """Discriminator loss –¥–ª—è WGAN"""
    return -(torch.mean(real_output) - torch.mean(fake_output))

def wasserstein_loss_g(fake_output):
    """Generator loss –¥–ª—è WGAN"""
    return -torch.mean(fake_output)

# Weight clipping –¥–ª—è WGAN
def clip_weights(model, clip_value=0.01):
    for p in model.parameters():
        p.data.clamp_(-clip_value, clip_value)</code></pre>
  </div>
</div>

</body>
</html>
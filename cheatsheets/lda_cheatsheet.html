<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Linear Discriminant Analysis (LDA) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    .formula {
      text-align: center;
      font-family: 'Cambria Math', serif;
      font-size: 1em;
      margin: 10px 0;
      padding: 8px;
      background: #f8fbff;
      border-radius: 4px;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üìä Linear Discriminant Analysis (LDA)</h1>
  <div class="subtitle">üìÖ 3 —è–Ω–≤–∞—Ä—è 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>–î–≤–µ –∑–∞–¥–∞—á–∏</strong>: —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ + –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</li>
      <li><strong>–° —É—á–µ—Ç–æ–º –º–µ—Ç–æ–∫</strong>: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª–∞—Å—Å–∞—Ö (supervised)</li>
      <li><strong>–¶–µ–ª—å</strong>: –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤</li>
      <li><strong>–û—Ç–ª–∏—á–∏–µ –æ—Ç PCA</strong>: PCA –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤</li>
    </ul>
    <div class="formula">
      –ú–∞–∫—Å: –º–µ–∂–¥—É-–∫–ª–∞—Å—Å–æ–≤–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è / –≤–Ω—É—Ç—Ä–∏-–∫–ª–∞—Å—Å–æ–≤–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 2. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ (—Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏)</h2>
    <pre><code>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# –°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(X, y)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import matplotlib.pyplot as plt
plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y)
plt.xlabel('LD1')
plt.ylabel('LD2')
plt.title('LDA –ø—Ä–æ–µ–∫—Ü–∏—è')
plt.show()

print(f"Explained variance: {lda.explained_variance_ratio_}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)</h2>
    <pre><code>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
y_pred = lda.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc:.3f}")

# –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
y_proba = lda.predict_proba(X_test)
print(y_proba[:5])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã</h2>
    <table>
      <tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–°–æ–≤–µ—Ç</th></tr>
      <tr><td><code>n_components</code></td><td>–ß–∏—Å–ª–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç</td><td>–ú–∞–∫—Å: min(n_classes-1, n_features)</td></tr>
      <tr><td><code>solver</code></td><td>–ê–ª–≥–æ—Ä–∏—Ç–º —Ä–µ—à–µ–Ω–∏—è</td><td>'svd', 'lsqr', 'eigen'</td></tr>
      <tr><td><code>shrinkage</code></td><td>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏</td><td>None, 'auto', –∏–ª–∏ float [0, 1]</td></tr>
      <tr><td><code>store_covariance</code></td><td>–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏</td><td>False –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 5. LDA vs PCA</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>PCA</th><th>LDA</th></tr>
      <tr><td>–¢–∏–ø –æ–±—É—á–µ–Ω–∏—è</td><td>Unsupervised</td><td>Supervised</td></tr>
      <tr><td>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–∫–∏</td><td>–ù–µ—Ç</td><td>–î–∞</td></tr>
      <tr><td>–¶–µ–ª—å</td><td>–ú–∞–∫—Å. –¥–∏—Å–ø–µ—Ä—Å–∏—è</td><td>–ú–∞–∫—Å. —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç—å</td></tr>
      <tr><td>–ú–∞–∫—Å. –∫–æ–º–ø–æ–Ω–µ–Ω—Ç</td><td>n_features</td><td>n_classes - 1</td></tr>
      <tr><td>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</td><td>–ù–µ—Ç</td><td>–î–∞</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç</h2>
    <pre><code># LDA –æ–≥—Ä–∞–Ω–∏—á–µ–Ω —á–∏—Å–ª–æ–º –∫–ª–∞—Å—Å–æ–≤
max_components = min(n_classes - 1, n_features)

# –ü—Ä–∏–º–µ—Ä: 3 –∫–ª–∞—Å—Å–∞, 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# max_components = min(3-1, 10) = 2

lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(X, y)

print(f"Shape –¥–æ: {X.shape}")
print(f"Shape –ø–æ—Å–ª–µ: {X_lda.shape}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è</h2>
    <pre><code># –î–æ–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –∫–∞–∂–¥–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–π
explained_var = lda.explained_variance_ratio_
print(f"Explained variance: {explained_var}")

# –ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
import numpy as np
cum_var = np.cumsum(explained_var)
print(f"Cumulative: {cum_var}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.bar(range(len(explained_var)), explained_var)
plt.xlabel('–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞')
plt.ylabel('–û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è')
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Shrinkage (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)</h2>
    <pre><code># –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ shrinkage –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
# –æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ–≥–¥–∞ n_features > n_samples

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä
lda = LinearDiscriminantAnalysis(
    solver='lsqr',
    shrinkage='auto'
)
lda.fit(X_train, y_train)

# –ò–ª–∏ –∑–∞–¥–∞—Ç—å –≤—Ä—É—á–Ω—É—é (0 –¥–æ 1)
lda = LinearDiscriminantAnalysis(
    solver='lsqr',
    shrinkage=0.5
)
lda.fit(X_train, y_train)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö</h2>
    <pre><code>from sklearn.preprocessing import StandardScaler

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# LDA
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(X_train_scaled, y_train)</code></pre>
    <p><strong>–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:</strong> LDA –º–µ–Ω–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –º–∞—Å—à—Ç–∞–±—É, —á–µ–º PCA, –Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ —Ä–∞–≤–Ω–æ –ø–æ–ª–µ–∑–Ω–æ.</p>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</h3>
        <ul>
          <li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª–∞—Å—Å–∞—Ö</li>
          <li>–•–æ—Ä–æ—à –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤</li>
          <li>–ú–æ–∂–µ—Ç –±—ã—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º</li>
          <li>–ë—ã—Å—Ç—Ä–µ–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤</li>
          <li>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h3>
        <ul>
          <li>–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –≥–∞—É—Å—Å–æ–≤–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</li>
          <li>–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤</li>
          <li>–ú–∞–∫—Å. n_classes-1 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç</li>
          <li>–¢–æ–ª—å–∫–æ –ª–∏–Ω–µ–π–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã</li>
          <li>–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±—Ä–æ—Å–∞–º</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –•–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥–∏—Ç</h3>
        <ul>
          <li>–ó–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</li>
          <li>–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤</li>
          <li>–õ–∏–Ω–µ–π–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ</li>
          <li>–ú–∞–ª–æ–µ —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤</li>
          <li>–ì–∞—É—Å—Å–æ–≤–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ü–ª–æ—Ö–æ –ø–æ–¥—Ö–æ–¥–∏—Ç</h3>
        <ul>
          <li>–ù–µ–ª–∏–Ω–µ–π–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Kernel LDA)</li>
          <li>–°–∏–ª—å–Ω–æ —Ä–∞–∑–ª–∏—á–∞—é—â–∏–µ—Å—è –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤ (QDA)</li>
          <li>–ú–Ω–æ–≥–æ –≤—ã–±—Ä–æ—Å–æ–≤</li>
          <li>–ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 12. LDA + Pipeline</h2>
    <pre><code>from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Pipeline: –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ + LDA
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('lda', LinearDiscriminantAnalysis())
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

# –î–æ—Å—Ç—É–ø –∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º
lda_model = pipeline.named_steps['lda']
print(lda_model.explained_variance_ratio_)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ solvers</h2>
    <table>
      <tr><th>Solver</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th></tr>
      <tr><td><code>svd</code></td><td>SVD —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ</td><td>–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é, n_features > n_samples</td></tr>
      <tr><td><code>lsqr</code></td><td>–ú–µ—Ç–æ–¥ –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤</td><td>–° shrinkage</td></tr>
      <tr><td><code>eigen</code></td><td>–°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è</td><td>–ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ, —Å shrinkage</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 14. Quadratic Discriminant Analysis (QDA)</h2>
    <pre><code>from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

# QDA: —Ä–∞–∑–Ω—ã–µ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤
qda = QuadraticDiscriminantAnalysis()
qda.fit(X_train, y_train)
y_pred = qda.predict(X_test)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ LDA vs QDA
lda_acc = lda.score(X_test, y_test)
qda_acc = qda.score(X_test, y_test)
print(f"LDA: {lda_acc:.3f}, QDA: {qda_acc:.3f}")</code></pre>
    <p><strong>QDA</strong>: –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã, –Ω–æ –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</p>
  </div>

  <div class="block">
    <h2>üî∑ 15. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–Ω–∏—Ü —Ä–µ—à–µ–Ω–∏—è</h2>
    <pre><code>import numpy as np
import matplotlib.pyplot as plt

# –û–±—É—á–µ–Ω–∏–µ LDA
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)

# –°–µ—Ç–∫–∞ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
x_min, x_max = X[:, 0].min()-1, X[:, 0].max()+1
y_min, y_max = X[:, 1].min()-1, X[:, 1].max()+1
xx, yy = np.meshgrid(
    np.linspace(x_min, x_max, 100),
    np.linspace(y_min, y_max, 100)
)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Å–µ—Ç–∫–µ
Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')
plt.title('LDA –≥—Ä–∞–Ω–∏—Ü—ã —Ä–µ—à–µ–Ω–∏—è')
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <ul>
      <li><strong>–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ</strong>: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ StandardScaler</li>
      <li><strong>Shrinkage</strong>: –ø—Ä–∏–º–µ–Ω—è–π—Ç–µ –ø—Ä–∏ n_features > n_samples</li>
      <li><strong>–í—ã–±–æ—Ä –∫–æ–º–ø–æ–Ω–µ–Ω—Ç</strong>: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ explained_variance_ratio_</li>
      <li><strong>–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π</strong>: —Ç–µ—Å—Ç –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å, –æ–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç—å –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–π</li>
      <li><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å QDA</strong>: –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –ø–æ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 17. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ</li>
      <li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–∏—Å–ª–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (‚â§ n_classes-1)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å solver</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å shrinkage –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏</li>
      <li>[ ] –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å</li>
      <li>[ ] –û—Ü–µ–Ω–∏—Ç—å explained_variance_ratio_</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã</li>
      <li>[ ] –°—Ä–∞–≤–Ω–∏—Ç—å —Å PCA/QDA –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏</li>
    </ul>
    <blockquote>
      ¬´LDA ‚Äî –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç supervised —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–π –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤. –ò–¥–µ–∞–ª–µ–Ω –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∫–æ–≥–¥–∞ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –µ–≥–æ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è¬ª.
    </blockquote>
  </div>

  <div class="block">
    <h2>üîó –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏</h2>
    <ul>
      <li><a href="https://scikit-learn.org/stable/modules/lda_qda.html" target="_blank">üìö Scikit-learn: LDA and QDA</a></li>
      <li><a href="https://sebastianraschka.com/Articles/2014_python_lda.html" target="_blank">üìñ LDA in Python</a></li>
      <li><a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" target="_blank">üìù Wikipedia: LDA</a></li>
    </ul>
  </div>

</div>
</body>
</html>

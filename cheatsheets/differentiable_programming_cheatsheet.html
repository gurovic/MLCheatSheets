<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>‚àÇ –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ß—Ç–æ —Ç–∞–∫–æ–µ Differentiable Programming?</h2>
    <p><strong>Differentiable Programming</strong> ‚Äî –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –≥–¥–µ –ª—é–±—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã.</p>
    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –ø–∏—Å–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—ã –∫–∞–∫ –æ–±—ã—á–Ω–æ, –ø–æ–ª—É—á–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –±–µ—Å–ø–ª–∞—Ç–Ω–æ</li>
      <li><strong>–û—Å–Ω–æ–≤–∞</strong>: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ (AD)</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è, –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –Ω–∞—É—á–Ω—ã–µ —Ä–∞—Å—á–µ—Ç—ã</li>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ</strong>: –Ω–µ –Ω—É–∂–Ω–æ –ø–∏—Å–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—Ä—É—á–Ω—É—é</li>
    </ul>
    <blockquote>üí° "Deep Learning ‚Äî —ç—Ç–æ —á–∞—Å—Ç–Ω—ã–π —Å–ª—É—á–∞–π –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"</blockquote>

  <div class="block">
    <h2>üî∑ 2. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ (AD)</h2>
    <p>AD –≤—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω—ã–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º.</p>
    
    <p><strong>–¢—Ä–∏ –ø–æ–¥—Ö–æ–¥–∞</strong>:</p>
    <ul>
      <li><strong>–°–∏–º–≤–æ–ª—å–Ω–æ–µ</strong>: –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã (–º–µ–¥–ª–µ–Ω–Ω–æ, —Ç–æ—á–Ω–æ)</li>
      <li><strong>–ß–∏—Å–ª–µ–Ω–Ω–æ–µ</strong>: –∫–æ–Ω–µ—á–Ω—ã–µ —Ä–∞–∑–Ω–æ—Å—Ç–∏ (–±—ã—Å—Ç—Ä–æ, –Ω–µ—Ç–æ—á–Ω–æ)</li>
      <li><strong>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ</strong>: –∫–æ–º–ø–æ–∑–∏—Ü–∏—è –ø—Ä–∞–≤–∏–ª (–±—ã—Å—Ç—Ä–æ, —Ç–æ—á–Ω–æ)</li>
    </ul>

    <p><strong>–ü—Ä–∞–≤–∏–ª–æ —Ü–µ–ø–æ—á–∫–∏</strong>:</p>
    <p>‚àÇz/‚àÇx = (‚àÇz/‚àÇy) √ó (‚àÇy/‚àÇx)</p>

    <p><strong>–†–µ–∂–∏–º—ã AD</strong>:</p>
    <ul>
      <li><strong>Forward mode</strong>: —Å–ª–µ–≤–∞ –Ω–∞–ø—Ä–∞–≤–æ (–¥–ª—è –º–∞–ª–æ–≥–æ —á–∏—Å–ª–∞ –≤—Ö–æ–¥–æ–≤)</li>
      <li><strong>Reverse mode</strong>: —Å–ø—Ä–∞–≤–∞ –Ω–∞–ª–µ–≤–æ (backpropagation, –¥–ª—è ML)</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. PyTorch Autograd</h2>
    <p>PyTorch –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π computational graph:</p>

    <pre><code>import torch

# –í–∫–ª—é—á–∞–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
x = torch.tensor([2.0, 3.0], requires_grad=True)

# –í—ã—á–∏—Å–ª–µ–Ω–∏—è
y = x ** 2 + 3 * x + 1  # y = x¬≤ + 3x + 1

# –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç ‚àÇy/‚àÇx
y.backward(torch.ones_like(y))

print(x.grad)  # [7.0, 9.0] = 2x + 3

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ gradients (inference)
with torch.no_grad():
    z = x ** 2  # –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è</code></pre>

    <p><strong>–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥—Ä–∞—Ñ–æ–º</strong>:</p>
    <pre><code># –û—á–∏—Å—Ç–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
x.grad.zero_()

# –û—Ç—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –æ—Ç –≥—Ä–∞—Ñ–∞
y_detached = y.detach()

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
y_stopped = y.requires_grad_(False)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. JAX: —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ AD</h2>
    <p>JAX ‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –æ—Ç Google –¥–ª—è –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã—Ö —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.</p>

    <pre><code>import jax
import jax.numpy as jnp
from jax import grad, jit, vmap

# –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def f(x):
    return x ** 3 + 2 * x ** 2 - 5 * x + 3

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç
df = grad(f)

x = 2.0
print(f"f({x}) = {f(x)}")
print(f"f'({x}) = {df(x)}")  # 3x¬≤ + 4x - 5

# –í—Ç–æ—Ä–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
d2f = grad(grad(f))
print(f"f''({x}) = {d2f(x)}")  # 6x + 4</code></pre>

    <blockquote>‚ö° JAX –∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –≤ XLA –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 5. JAX transformations</h2>
    <p>JAX –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–æ–∑–∏—Ä—É–µ–º—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏:</p>

    <pre><code>from jax import grad, jit, vmap, pmap

# grad: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ
df = grad(f)

# jit: JIT-–∫–æ–º–ø–∏–ª—è—Ü–∏—è
f_fast = jit(f)

# vmap: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
f_batch = vmap(f)

# pmap: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–∞ GPU/TPU
f_parallel = pmap(f)

# –ö–æ–º–ø–æ–∑–∏—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
fast_grad = jit(grad(f))
batch_grad = vmap(grad(f))</code></pre>

    <p><strong>–ü—Ä–∏–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏</strong>:</p>
    <pre><code># –ë–µ–∑ vmap: —Ü–∏–∫–ª
def apply_to_batch(xs):
    return jnp.array([f(x) for x in xs])

# –° vmap: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
apply_to_batch = vmap(f)

xs = jnp.array([1.0, 2.0, 3.0, 4.0])
print(apply_to_batch(xs))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—Å—à–∏—Ö –ø–æ—Ä—è–¥–∫–æ–≤</h2>
    <p>–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –ª—é–±–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞:</p>

    <pre><code># PyTorch
x = torch.tensor(2.0, requires_grad=True)
y = x ** 4

# –ü–µ—Ä–≤–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
dy_dx = torch.autograd.grad(
    y, x, 
    create_graph=True  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ
)[0]

# –í—Ç–æ—Ä–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]

print(f"dy/dx = {dy_dx}")    # 4x¬≥ = 32
print(f"d¬≤y/dx¬≤ = {d2y_dx2}") # 12x¬≤ = 48

# JAX: –µ—â–µ –ø—Ä–æ—â–µ
from jax import grad

f = lambda x: x ** 4
df = grad(f)          # –ø–µ—Ä–≤–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
d2f = grad(df)        # –≤—Ç–æ—Ä–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
d3f = grad(d2f)       # —Ç—Ä–µ—Ç—å—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è

print(d3f(2.0))       # 24x = 48</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Jacobian –∏ Hessian –º–∞—Ç—Ä–∏—Ü—ã</h2>
    <p>–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö:</p>

    <pre><code>from jax import jacfwd, jacrev, hessian

# –§—É–Ω–∫—Ü–∏—è R^n ‚Üí R^m
def f(x):
    return jnp.array([
        x[0] ** 2 + x[1],
        x[0] * x[1],
        x[1] ** 3
    ])

# Jacobian –º–∞—Ç—Ä–∏—Ü–∞ (‚àÇf_i/‚àÇx_j)
J_forward = jacfwd(f)   # forward mode
J_reverse = jacrev(f)   # reverse mode

x = jnp.array([2.0, 3.0])
print(J_forward(x))

# Hessian –º–∞—Ç—Ä–∏—Ü–∞ (–≤—Ç–æ—Ä—ã–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ)
def g(x):
    return x[0] ** 2 + 2 * x[1] ** 2 + x[0] * x[1]

H = hessian(g)
print(H(x))  # [[2, 1], [1, 4]]</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Custom –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã</h2>
    <p>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è:</p>

    <pre><code># PyTorch: custom autograd Function
class MyFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        ctx.save_for_backward(x)
        return x.clamp(min=0)  # ReLU
    
    @staticmethod
    def backward(ctx, grad_output):
        x, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[x < 0] = 0
        return grad_input

my_relu = MyFunction.apply

# JAX: custom_vjp (vector-Jacobian product)
from jax import custom_vjp

@custom_vjp
def f(x):
    return jnp.sin(x)

def f_fwd(x):
    return f(x), x  # return primal, residuals

def f_bwd(x, g):
    return (g * jnp.cos(x),)  # custom gradient

f.defvjp(f_fwd, f_bwd)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è</h2>
    <p>–í—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏:</p>

    <pre><code>import jax.numpy as jnp
from jax import grad
from jax.scipy.optimize import minimize

# –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –∑–∞–¥–∞—á–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
def inner_loss(x, params):
    return jnp.sum((x - params) ** 2)

# –í–Ω–µ—à–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è (–º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ)
def outer_loss(params, data):
    # –†–µ—à–∞–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –∑–∞–¥–∞—á—É
    result = minimize(
        lambda x: inner_loss(x, params),
        x0=jnp.zeros_like(params),
        method='BFGS'
    )
    x_star = result.x
    
    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö
    return jnp.sum((x_star - data) ** 2)

# –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ params –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é!
grad_outer = grad(outer_loss)

params = jnp.array([1.0, 2.0, 3.0])
data = jnp.array([2.0, 3.0, 4.0])
print(grad_outer(params, data))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Neural ODEs</h2>
    <p>–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–µ –æ–±—ã–∫–Ω–æ–≤–µ–Ω–Ω—ã–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è:</p>

    <pre><code>import jax.numpy as jnp
from jax.experimental.ode import odeint

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º ODE: dy/dt = f(y, t)
def f(y, t, params):
    # –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –∫–∞–∫ f
    return -params[0] * y + params[1] * jnp.sin(t)

# –†–µ—à–∞–µ–º ODE
def solve_ode(params, y0, t):
    return odeint(
        lambda y, t: f(y, t, params),
        y0,
        t
    )

# –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã!
from jax import grad

def loss(params, y0, t, y_target):
    y_pred = solve_ode(params, y0, t)
    return jnp.mean((y_pred - y_target) ** 2)

# –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ODE
grad_loss = grad(loss)

params = jnp.array([1.0, 0.5])
y0 = jnp.array([1.0])
t = jnp.linspace(0, 10, 100)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö</h2>
    <p>–†–∞–±–æ—Ç–∞ —Å –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏:</p>

    <pre><code># JAX –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç pytrees
import jax
from jax import grad

# –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
params = {
    'w1': jnp.array([1.0, 2.0]),
    'b1': jnp.array([0.5]),
    'w2': jnp.array([[1.0, 2.0], [3.0, 4.0]])
}

def model(params, x):
    h = jnp.dot(x, params['w1']) + params['b1']
    h = jnp.tanh(h)
    return jnp.dot(h, params['w2'])

# –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –≤—Å–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
grad_fn = grad(lambda p, x: jnp.sum(model(p, x)))

x = jnp.array([1.0, 2.0])
grads = grad_fn(params, x)

# grads –∏–º–µ–µ—Ç —Ç—É –∂–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —á—Ç–æ params!
print(grads['w1'])
print(grads['b1'])
print(grads['w2'])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Forward vs Reverse mode</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>Forward Mode</th><th>Reverse Mode</th></tr>
      <tr>
        <td><strong>–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ</strong></td>
        <td>–í—Ö–æ–¥—ã ‚Üí –í—ã—Ö–æ–¥—ã</td>
        <td>–í—ã—Ö–æ–¥—ã ‚Üí –í—Ö–æ–¥—ã</td>
      </tr>
      <tr>
        <td><strong>–°–ª–æ–∂–Ω–æ—Å—Ç—å</strong></td>
        <td>O(n √ó m)</td>
        <td>O(n + m)</td>
      </tr>
      <tr>
        <td><strong>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –¥–ª—è</strong></td>
        <td>–ú–∞–ª–æ –≤—Ö–æ–¥–æ–≤</td>
        <td>–ú–∞–ª–æ –≤—ã—Ö–æ–¥–æ–≤</td>
      </tr>
      <tr>
        <td><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong></td>
        <td>Jacobian –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º</td>
        <td>–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã (ML)</td>
      </tr>
      <tr>
        <td><strong>–ü–∞–º—è—Ç—å</strong></td>
        <td>O(1)</td>
        <td>O(–≥—Ä–∞—Ñ)</td>
      </tr>
    </table>

    <pre><code># JAX: –≤—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞
from jax import jacfwd, jacrev

# Forward mode (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –µ—Å–ª–∏ n < m)
J_fwd = jacfwd(f)

# Reverse mode (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –µ—Å–ª–∏ m < n)
J_rev = jacrev(f)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ —Ü–∏–∫–ª—ã</h2>
    <p>JAX –º–æ–∂–µ—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞—Ç—å —Ü–∏–∫–ª—ã:</p>

    <pre><code>from jax import lax, grad

# –û–±—ã—á–Ω—ã–π —Ü–∏–∫–ª (–Ω–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π)
def loop_sum(x, n):
    total = 0.0
    for i in range(n):
        total += x ** i
    return total

# JAX: –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π —Ü–∏–∫–ª
def diff_loop_sum(x, n):
    def body_fn(i, total):
        return total + x ** i
    
    return lax.fori_loop(0, n, body_fn, 0.0)

# –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç!
grad_fn = grad(diff_loop_sum)
print(grad_fn(2.0, 5))

# while_loop: —É—Å–ª–æ–≤–Ω—ã–µ —Ü–∏–∫–ª—ã
def while_example(x):
    def cond_fn(val):
        i, total = val
        return i < 10
    
    def body_fn(val):
        i, total = val
        return (i + 1, total + x ** i)
    
    _, result = lax.while_loop(
        cond_fn, 
        body_fn, 
        (0, 0.0)
    )
    return result</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ —É—Å–ª–æ–≤–∏—è</h2>
    <p>–†–∞–±–æ—Ç–∞ —Å –≤–µ—Ç–≤–ª–µ–Ω–∏—è–º–∏:</p>

    <pre><code>from jax import lax, grad

# –£—Å–ª–æ–≤–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä (–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π)
def conditional(x):
    return lax.cond(
        x > 0,
        lambda x: x ** 2,      # if x > 0
        lambda x: -x,          # else
        x
    )

# –ì—Ä–∞–¥–∏–µ–Ω—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω!
grad_cond = grad(conditional)
print(grad_cond(2.0))   # 4.0 (–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è x¬≤)
print(grad_cond(-2.0))  # -1.0 (–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è -x)

# select: –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤—ã–±–æ—Ä
def select_example(x):
    return lax.select(
        x > 0,
        x ** 2,   # –µ—Å–ª–∏ True
        -x        # –µ—Å–ª–∏ False
    )

# –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ
x = jnp.array([-2.0, -1.0, 0.0, 1.0, 2.0])
print(select_example(x))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. Checkpoint –∏ memory efficiency</h2>
    <p>–£–º–µ–Ω—å—à–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–∏ backprop:</p>

    <pre><code># PyTorch: gradient checkpointing
import torch.utils.checkpoint as checkpoint

class CheckpointedModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Linear(1000, 1000) for _ in range(10)
        ])
    
    def forward(self, x):
        for layer in self.layers:
            # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
            x = checkpoint.checkpoint(layer, x)
        return x

# JAX: jax.checkpoint (remat)
from jax import checkpoint

@checkpoint
def expensive_fn(x):
    # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è
    for _ in range(100):
        x = jnp.sin(x) + jnp.cos(x)
    return x

# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç O(1) –ø–∞–º—è—Ç—å –≤–º–µ—Å—Ç–æ O(n)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è Differentiable Programming</h2>
    <ul>
      <li><strong>–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ</strong>: –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –ª—é–±–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏</li>
      <li><strong>–§–∏–∑–∏—á–µ—Å–∫–∏–µ —Å–∏–º—É–ª—è—Ü–∏–∏</strong>: –æ–±—É—á–∞–µ–º—ã–µ —Å–∏–º—É–ª—è—Ç–æ—Ä—ã</li>
      <li><strong>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è</strong>: gradient-based optimization</li>
      <li><strong>–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –≥—Ä–∞—Ñ–∏–∫–∞</strong>: –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥</li>
      <li><strong>Robotics</strong>: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π</li>
      <li><strong>–ù–∞—É—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è</strong>: PDE solving, inverse problems</li>
      <li><strong>Meta-learning</strong>: MAML, –æ–±—É—á–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤</li>
      <li><strong>Bayesian inference</strong>: –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –≤—ã–≤–æ–¥</li>
    </ul>

    <blockquote>üöÄ –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è ML</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 17. TensorFlow GradientTape</h2>
    <p>TensorFlow –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GradientTape –¥–ª—è AD:</p>

    <pre><code>import tensorflow as tf

# –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–π
x = tf.Variable(3.0)

with tf.GradientTape() as tape:
    y = x ** 2 + 2 * x + 1

# –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç
dy_dx = tape.gradient(y, x)
print(dy_dx)  # 8.0 = 2x + 2

# Persistent tape (–º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ)
x = tf.Variable([1.0, 2.0, 3.0])

with tf.GradientTape(persistent=True) as tape:
    y1 = x ** 2
    y2 = x ** 3

grad_y1 = tape.gradient(y1, x)
grad_y2 = tape.gradient(y2, x)

del tape  # –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 18. Performance tips</h2>
    <p><strong>PyTorch</strong>:</p>
    <ul>
      <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ <code>torch.no_grad()</code> –ø—Ä–∏ inference</li>
      <li>–û—Ç–∫–ª—é—á–∞–π—Ç–µ gradients –¥–ª—è frozen layers</li>
      <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ <code>torch.cuda.amp</code> –¥–ª—è mixed precision</li>
      <li>Gradient checkpointing –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π</li>
    </ul>

    <p><strong>JAX</strong>:</p>
    <ul>
      <li>–í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ <code>jit</code> –¥–ª—è production</li>
      <li><code>vmap</code> –≤–º–µ—Å—Ç–æ —Ü–∏–∫–ª–æ–≤ –ø–æ batch</li>
      <li>–ò–∑–±–µ–≥–∞–π—Ç–µ Python control flow –≤ JIT —Ñ—É–Ω–∫—Ü–∏—è—Ö</li>
      <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ <code>checkpoint</code> –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏</li>
    </ul>

    <pre><code># JAX: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
from jax import jit, vmap, grad

# ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–æ
def slow_fn(params, x_batch):
    return [model(params, x) for x in x_batch]

# ‚úÖ –ë—ã—Å—Ç—Ä–æ
@jit
def fast_fn(params, x_batch):
    return vmap(lambda x: model(params, x))(x_batch)

# –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –±–∞—Ç—á—É
grad_fn = jit(vmap(grad(model), in_axes=(None, 0)))</code></pre>
  </div>

</div>

</div>
</body>
</html>

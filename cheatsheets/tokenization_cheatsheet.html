<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è NLP Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen { body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; color: #333; background: #fafcff; padding: 10px; } }
    @media print { body { background: white; padding: 0; } @page { size: A4 landscape; margin: 10mm; } }
        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }
    .block { break-inside: avoid; margin-bottom: 1.2em; padding: 12px; background: white; border-radius: 6px; box-shadow: 0 1px 3px rgba(0,0,0,0.05); }
    h1 { font-size: 1.6em; font-weight: 700; color: #1a5fb4; text-align: center; margin: 0 0 8px; column-span: all; }
    .subtitle { text-align: center; color: #666; font-size: 0.9em; margin-bottom: 12px; column-span: all; }
    h2 { font-size: 1.15em; font-weight: 700; color: #1a5fb4; margin: 0 0 8px; padding-bottom: 4px; border-bottom: 1px solid #e0e7ff; }
    p, ul, ol { font-size: 0.92em; margin: 0.6em 0; }
    ul, ol { padding-left: 18px; }
    li { margin-bottom: 4px; }
    code { font-family: 'Consolas', 'Courier New', monospace; background-color: #f0f4ff; padding: 1px 4px; border-radius: 3px; font-size: 0.88em; }
    pre { background-color: #f0f4ff; padding: 8px; border-radius: 4px; overflow-x: auto; font-size: 0.84em; margin: 6px 0; }
    pre code { padding: 0; background: none; white-space: pre-wrap; }
    table { width: 100%; border-collapse: collapse; font-size: 0.82em; margin: 6px 0; }
    th { background-color: #e6f0ff; text-align: left; padding: 4px 6px; font-weight: 600; }
    td { padding: 4px 6px; border-bottom: 1px solid #f0f4ff; }
    tr:nth-child(even) { background-color: #f8fbff; }
    blockquote { font-style: italic; margin: 8px 0; padding: 6px 10px; background: #f8fbff; border-left: 2px solid #1a5fb4; font-size: 0.88em; }
    @media print { .container { column-gap: 12px; } .block { box-shadow: none; } code, pre, table { font-size: 0.78em; } h1 { font-size: 1.4em; } h2 { font-size: 1em; } }
  </style>
</head>
<body>

<div class="container">

  <h1>üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è NLP</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ó–∞—á–µ–º –Ω—É–∂–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è</h2>
<ul><li><strong>–¶–µ–ª—å</strong>: —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ (—Ç–æ–∫–µ–Ω—ã) –¥–ª—è –º–æ–¥–µ–ª–∏</li><li><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: –æ–≥—Ä–æ–º–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å vs –ø–æ—Ç–µ—Ä—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</li><li><strong>–†–µ—à–µ–Ω–∏–µ</strong>: subword —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (BPE, WordPiece)</li><li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: BERT, GPT, T5, –≤—Å–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM</li></ul><blockquote>–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ‚Äî —ç—Ç–æ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ (—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ) –∏ —Å–∏–º–≤–æ–ª–∞–º–∏ (—Ç–µ—Ä—è–µ—Ç—Å—è —Å–º—ã—Å–ª).</blockquote>

  <div class="block">
    <h2>üî∑ 2. –í–∏–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏</h2>
<table><tr><th>–¢–∏–ø</th><th>–ü—Ä–∏–º–µ—Ä</th><th>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ</th></tr><tr><td>Word-level</td><td>"Hello world" ‚Üí ["Hello", "world"]</td><td>–ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏</td></tr><tr><td>Character-level</td><td>"cat" ‚Üí ["c", "a", "t"]</td><td>–†–µ–¥–∫–æ (—è–∑—ã–∫–∏ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤)</td></tr><tr><td>Subword (BPE)</td><td>"tokenization" ‚Üí ["token", "ization"]</td><td>GPT, RoBERTa</td></tr><tr><td>WordPiece</td><td>"tokenization" ‚Üí ["token", "##ization"]</td><td>BERT</td></tr></table>
  </div>

  <div class="block">
    <h2>üî∑ 3. BPE (Byte Pair Encoding)</h2>
<pre><code># –ê–ª–≥–æ—Ä–∏—Ç–º BPE
1. –ù–∞—á–∞—Ç—å —Å —Å–∏–º–≤–æ–ª–æ–≤
2. –ù–∞–π—Ç–∏ —Å–∞–º—É—é —á–∞—Å—Ç—É—é –ø–∞—Ä—É —Å–∏–º–≤–æ–ª–æ–≤
3. –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω
4. –ü–æ–≤—Ç–æ—Ä—è—Ç—å –¥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è

–ü—Ä–∏–º–µ—Ä:
"low" + "low" + "lowest"
‚Üí —Å–ª–æ–≤–∞—Ä—å: {l, o, w, e, s, t, lo, low, est}

"lowest" ‚Üí ["low", "est"]</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. BPE —Å Hugging Face</h2>
<pre><code>from transformers import GPT2Tokenizer

# GPT-2 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç BPE
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

text = "Tokenization is awesome!"
tokens = tokenizer.tokenize(text)
print(tokens)
# ['Token', 'ization', ' is', ' awesome', '!']

# Encode –≤ ID
ids = tokenizer.encode(text)
print(ids)
# [15667, 1634, 318, 7427, 0]

# Decode –æ–±—Ä–∞—Ç–Ω–æ
original = tokenizer.decode(ids)
print(original)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. WordPiece (BERT)</h2>
<pre><code>from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

text = "Tokenization"
tokens = tokenizer.tokenize(text)
print(tokens)
# ['token', '##ization']

# ## –æ–∑–Ω–∞—á–∞–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–æ–≤–∞
# –ü–æ–º–æ–≥–∞–µ—Ç —Ä–∞–∑–ª–∏—á–∞—Ç—å "token" –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ
# –∏ "token" –∫–∞–∫ —á–∞—Å—Ç—å "tokenization"</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. SentencePiece</h2>
<pre><code>import sentencepiece as spm

# –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
spm.SentencePieceTrainer.train(
    input="corpus.txt",
    model_prefix="tokenizer",
    vocab_size=32000,
    model_type="bpe"  # –∏–ª–∏ "unigram"
)

# –ó–∞–≥—Ä—É–∑–∫–∞
sp = spm.SentencePieceProcessor()
sp.load("tokenizer.model")

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
tokens = sp.encode_as_pieces("Hello world")
print(tokens)
# ['‚ñÅHello', '‚ñÅworld']
# ‚ñÅ –æ–∑–Ω–∞—á–∞–µ—Ç –Ω–∞—á–∞–ª–æ —Å–ª–æ–≤–∞

ids = sp.encode_as_ids("Hello world")
print(ids)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞</h2>
<pre><code>from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
tokenizer.pre_tokenizer = Whitespace()

# –û–±—É—á–µ–Ω–∏–µ
trainer = BpeTrainer(
    vocab_size=30000,
    special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
)

files = ["train.txt"]
tokenizer.train(files, trainer)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
tokenizer.save("my_tokenizer.json")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã</h2>
<table><tr><th>–¢–æ–∫–µ–Ω</th><th>–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ</th><th>–ú–æ–¥–µ–ª—å</th></tr><tr><td>[CLS]</td><td>–ù–∞—á–∞–ª–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</td><td>BERT</td></tr><tr><td>[SEP]</td><td>–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π</td><td>BERT</td></tr><tr><td>[PAD]</td><td>Padding (–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–ª–∏–Ω—ã)</td><td>–í—Å–µ</td></tr><tr><td>[MASK]</td><td>–ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω</td><td>BERT (MLM)</td></tr><tr><td>[UNK]</td><td>–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ</td><td>–í—Å–µ</td></tr><tr><td>&lt;|endoftext|&gt;</td><td>–ö–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞</td><td>GPT</td></tr></table>
  </div>

  <div class="block">
    <h2>üî∑ 9. Padding –∏ Truncation</h2>
<pre><code># Padding - –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–µ
tokens_batch = tokenizer(
    ["Short text", "Very long text that needs truncation"],
    padding=True,  # –¥–æ–±–∞–≤–∏—Ç—å [PAD]
    truncation=True,  # –æ–±—Ä–µ–∑–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ
    max_length=128,
    return_tensors="pt"  # PyTorch tensors
)

print(tokens_batch["input_ids"])
# tensor([[ 101, 2460, 3793,  102,    0,    0],
#         [ 101, 2200, 2146, ...  102]])

print(tokens_batch["attention_mask"])
# tensor([[1, 1, 1, 1, 0, 0],
#         [1, 1, 1, ..., 1]])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Fast Tokenizers</h2>
<pre><code># HuggingFace Fast Tokenizers (Rust-based)
from transformers import AutoTokenizer

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç fast –≤–µ—Ä—Å–∏—é
tokenizer = AutoTokenizer.from_pretrained(
    "bert-base-uncased",
    use_fast=True
)

# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
# - –í 10x –±—ã—Å—Ç—Ä–µ–µ
# - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ offset_mapping
# - –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

# Offset mapping (–ø–æ–∑–∏—Ü–∏–∏ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ)
encoding = tokenizer(
    "Hello world",
    return_offsets_mapping=True
)
print(encoding.offset_mapping)
# [(0, 5), (6, 11)]  # "Hello" –∏ "world"</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Multilingual —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è</h2>
<pre><code># XLM-RoBERTa - 100 —è–∑—ã–∫–æ–≤
tokenizer = AutoTokenizer.from_pretrained(
    "xlm-roberta-base"
)

texts = [
    "Hello world",  # English
    "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä",   # Russian
    "‰Ω†Â•Ω‰∏ñÁïå"       # Chinese
]

for text in texts:
    tokens = tokenizer.tokenize(text)
    print(f"{text}: {tokens}")

# SentencePiece —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
# BPE/WordPiece –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω—ã</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –û–±—Ä–∞–±–æ—Ç–∫–∞ OOV (Out-of-Vocabulary)</h2>
<pre><code># –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤:

# 1. UNK token
"unknownword" ‚Üí ["[UNK]"]

# 2. Subword decomposition (BPE/WordPiece)
"unknownword" ‚Üí ["unknown", "word"]

# 3. Character fallback
"unknownword" ‚Üí ["u", "n", "k", ...]

# BPE –≤—Å–µ–≥–¥–∞ –º–æ–∂–µ—Ç —Ä–∞–∑–±–∏—Ç—å –¥–æ —Å–∏–º–≤–æ–ª–æ–≤
# –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –±—É–¥–µ—Ç OOV!</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. Byte-level BPE (GPT-2)</h2>
<pre><code># GPT-2/GPT-3 –∏—Å–ø–æ–ª—å–∑—É—é—Ç byte-level BPE
# –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞–π—Ç–æ–≤, –Ω–µ —Å–∏–º–≤–æ–ª–æ–≤

# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
# - –õ—é–±—ã–µ Unicode —Å–∏–º–≤–æ–ª—ã
# - –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π base vocab (256 bytes)
# - –ù–∏–∫–∞–∫–∏—Ö UNK —Ç–æ–∫–µ–Ω–æ–≤

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# –ú–æ–∂–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —á—Ç–æ —É–≥–æ–¥–Ω–æ
text = "Hello ‰∏ñÁïå üåç"
tokens = tokenizer.tokenize(text)
print(tokens)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. Metrics</h2>
<pre><code># –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏

# 1. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
vocab_size = len(tokenizer)
print(f"Vocabulary size: {vocab_size}")

# 2. –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∞
tokens_per_word = len(tokens) / len(text.split())
print(f"Tokens per word: {tokens_per_word:.2f}")
# –ú–µ–Ω—å—à–µ = –ª—É—á—à–µ (–º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ç–æ–π –∂–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)

# 3. Coverage (% –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤)
unk_count = tokens.count("[UNK]")
coverage = 1 - unk_count / len(tokens)
print(f"Coverage: {coverage:.2%}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. Best Practices</h2>
<ul><li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pretrained —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –∫–æ–≥–¥–∞ –≤–æ–∑–º–æ–∂–Ω–æ</li><li>[ ] BPE –¥–ª—è generative (GPT)</li><li>[ ] WordPiece –¥–ª—è BERT-like</li><li>[ ] SentencePiece –¥–ª—è multilingual</li><li>[ ] Fast tokenizers –¥–ª—è production</li><li>[ ] –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ special tokens</li><li>[ ] Padding –∏ truncation –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã</li><li>[ ] –°–ª–æ–≤–∞—Ä—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (30k-50k)</li></ul>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ß–µ–∫-–ª–∏—Å—Ç</h2>
<ul><li>[ ] –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤—ã–±—Ä–∞–Ω (BPE/WordPiece/SentencePiece)</li><li>[ ] –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω</li><li>[ ] –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã</li><li>[ ] Padding/truncation –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã</li><li>[ ] –ü—Ä–æ–≤–µ—Ä–µ–Ω–∞ coverage –Ω–∞ –¥–∞–Ω–Ω—ã—Ö</li><li>[ ] –¢–æ–∫–µ–Ω—ã‚ÜíIDs‚Üí–¢–µ–∫—Å—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç</li></ul><blockquote>¬´–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ‚Äî —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —è–∑—ã–∫–æ–º –∏ —è–∑—ã–∫–æ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π. –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ 5-10% accuracy¬ª.</blockquote>
  </div>

</div>

</div>
</body>
</html>

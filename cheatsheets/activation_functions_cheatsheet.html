<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Activation Functions Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ Activation Functions</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –ó–∞—á–µ–º –Ω—É–∂–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏</h2>
    <ul>
      <li><strong>–ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å</strong>: –ø–æ–∑–≤–æ–ª—è—é—Ç —Å–µ—Ç–∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</li>
      <li><strong>–ë–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏</strong>: —Å–µ—Ç—å = –ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å (–±–µ—Å–ø–æ–ª–µ–∑–Ω–æ!)</li>
      <li><strong>–ì–¥–µ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è</strong>: –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è (–∫—Ä–æ–º–µ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ)</li>
      <li><strong>–í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π</strong>: —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (softmax, sigmoid)</li>
    </ul>

    </div>
<div class="block">
    <h2>üî∑ 2. Sigmoid (œÉ)</h2>
    <p><strong>–§–æ—Ä–º—É–ª–∞</strong>: œÉ(x) = 1 / (1 + e‚ÅªÀ£)</p>
    <p><strong>–î–∏–∞–ø–∞–∑–æ–Ω</strong>: (0, 1)</p>
    <pre><code># PyTorch
import torch.nn as nn
nn.Sigmoid()

# Keras
layers.Activation('sigmoid')</code></pre>
    <p><strong>‚úÖ –ü–ª—é—Å—ã</strong>:</p>
    <ul>
      <li>–í—ã—Ö–æ–¥ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ (0,1) ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å</li>
      <li>–ì–ª–∞–¥–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è</li>
    </ul>
    <p><strong>‚ùå –ú–∏–Ω—É—Å—ã</strong>:</p>
    <ul>
      <li>–ò—Å—á–µ–∑–∞—é—â–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç (vanishing gradient)</li>
      <li>–ù–µ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤–æ–∫—Ä—É–≥ 0</li>
      <li>–ú–µ–¥–ª–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è (exp)</li>
    </ul>
    <p><strong>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</strong>: —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è –≤ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</p>
  </div>

  <div class="block">
    <h2>üî∑ 3. Tanh (–≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏–π —Ç–∞–Ω–≥–µ–Ω—Å)</h2>
    <p><strong>–§–æ—Ä–º—É–ª–∞</strong>: tanh(x) = (eÀ£ - e‚ÅªÀ£) / (eÀ£ + e‚ÅªÀ£)</p>
    <p><strong>–î–∏–∞–ø–∞–∑–æ–Ω</strong>: (-1, 1)</p>
    <pre><code># PyTorch
nn.Tanh()

# Keras
layers.Activation('tanh')</code></pre>
    <p><strong>‚úÖ –ü–ª—é—Å—ã</strong>:</p>
    <ul>
      <li>–¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤–æ–∫—Ä—É–≥ 0</li>
      <li>–õ—É—á—à–µ —á–µ–º sigmoid –¥–ª—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—ë–≤</li>
    </ul>
    <p><strong>‚ùå –ú–∏–Ω—É—Å—ã</strong>:</p>
    <ul>
      <li>–í—Å—ë –µ—â—ë –∏—Å—á–µ–∑–∞—é—â–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç</li>
      <li>–ú–µ–¥–ª–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è</li>
    </ul>
    <p><strong>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</strong>: RNN/LSTM (—Ä–µ–∂–µ –¥–ª—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—ë–≤)</p>
  </div>

  <div class="block">
    <h2>üî∑ 4. ReLU (Rectified Linear Unit) ‚≠ê</h2>
    <p><strong>–§–æ—Ä–º—É–ª–∞</strong>: ReLU(x) = max(0, x)</p>
    <p><strong>–î–∏–∞–ø–∞–∑–æ–Ω</strong>: [0, ‚àû)</p>
    <pre><code># PyTorch
nn.ReLU()

# Keras
layers.Activation('relu')
# –∏–ª–∏
layers.ReLU()</code></pre>
    <p><strong>‚úÖ –ü–ª—é—Å—ã</strong>:</p>
    <ul>
      <li>–û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è (–ø—Ä–æ—Å—Ç–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ)</li>
      <li>–ù–µ—Ç –∏—Å—á–µ–∑–∞—é—â–µ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è x > 0</li>
      <li>–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è (–º–Ω–æ–≥–∏–µ –Ω–µ–π—Ä–æ–Ω—ã = 0)</li>
      <li><strong>–°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è CNN</strong></li>
    </ul>
    <p><strong>‚ùå –ú–∏–Ω—É—Å—ã</strong>:</p>
    <ul>
      <li>"Dying ReLU": –Ω–µ–π—Ä–æ–Ω—ã –º–æ–≥—É—Ç "—É–º–µ—Ä–µ—Ç—å" (–≤—Å–µ–≥–¥–∞ 0)</li>
      <li>–ù–µ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤–æ–∫—Ä—É–≥ 0</li>
    </ul>
    <p><strong>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</strong>: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á!</p>
  </div>

  <div class="block">
    <h2>üî∑ 5. Leaky ReLU</h2>
    <p><strong>–§–æ—Ä–º—É–ª–∞</strong>: LeakyReLU(x) = max(Œ±x, x), –≥–¥–µ Œ± = 0.01</p>
    <p><strong>–î–∏–∞–ø–∞–∑–æ–Ω</strong>: (-‚àû, ‚àû)</p>
    <pre><code># PyTorch
nn.LeakyReLU(negative_slope=0.01)

# Keras
layers.LeakyReLU(alpha=0.01)</code></pre>
    <p><strong>‚úÖ –ü–ª—é—Å—ã</strong>:</p>
    <ul>
      <li>–†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É dying ReLU</li>
      <li>–ú–∞–ª—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è x < 0 (–Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±–Ω—É–ª—è–µ—Ç)</li>
    </ul>
    <p><strong>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</strong>: –µ—Å–ª–∏ ReLU –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –≤ GAN</p>
  </div>

  <div class="block">
    <h2>üî∑ 6. ELU (Exponential Linear Unit)</h2>
    <p><strong>–§–æ—Ä–º—É–ª–∞</strong>:<br>
    ELU(x) = x, –µ—Å–ª–∏ x > 0<br>
    ELU(x) = Œ±(eÀ£ - 1), –µ—Å–ª–∏ x ‚â§ 0</p>
    <pre><code># PyTorch
nn.ELU(alpha=1.0)

# Keras
layers.ELU(alpha=1.0)</code></pre>
    <p><strong>‚úÖ –ü–ª—é—Å—ã</strong>:</p>
    <ul>
      <li>–ù–µ—Ç dying ReLU</li>
      <li>–í—ã—Ö–æ–¥—ã –±–ª–∏–∑–∫–∏ –∫ 0-mean</li>
      <li>–ì–ª–∞–¥–∫–∞—è –≤—Å—é–¥—É</li>
    </ul>
    <p><strong>‚ùå –ú–∏–Ω—É—Å—ã</strong>:</p>
    <ul>
      <li>–ú–µ–¥–ª–µ–Ω–Ω–µ–µ ReLU (exp)</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 7. Swish / SiLU ‚≠ê</h2>
    <p><strong>–§–æ—Ä–º—É–ª–∞</strong>: Swish(x) = x ¬∑ œÉ(x) = x / (1 + e‚ÅªÀ£)</p>
    <p><strong>–û—Ç–∫—Ä—ã—Ç–∞ Google, 2017</strong></p>
    <pre><code># PyTorch
nn.SiLU()  # –¢–æ –∂–µ —á—Ç–æ Swish

# Keras
layers.Activation(tf.nn.swish)</code></pre>
    <p><strong>‚úÖ –ü–ª—é—Å—ã</strong>:</p>
    <ul>
      <li>–ß–∞—Å—Ç–æ –ª—É—á—à–µ ReLU</li>
      <li>–ì–ª–∞–¥–∫–∞—è, –Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è</li>
      <li>Self-gated (x —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ —Å–≤–æ—é –∞–∫—Ç–∏–≤–∞—Ü–∏—é)</li>
      <li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ EfficientNet, MobileNet</li>
    </ul>
    <p><strong>‚ùå –ú–∏–Ω—É—Å—ã</strong>:</p>
    <ul>
      <li>–ù–µ–º–Ω–æ–≥–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ ReLU</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 8. GELU (Gaussian Error Linear Unit) ‚≠ê</h2>
    <p><strong>–°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è Transformers!</strong></p>
    <p><strong>–§–æ—Ä–º—É–ª–∞</strong> (–ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω–æ):<br>GELU(x) ‚âà 0.5x(1 + tanh[‚àö(2/œÄ)(x + 0.044715x¬≥)])</p>
    <pre><code># PyTorch
nn.GELU()

# Keras
layers.Activation(tf.nn.gelu)</code></pre>
    <p><strong>‚úÖ –ü–ª—é—Å—ã</strong>:</p>
    <ul>
      <li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ BERT, GPT</li>
      <li>–°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</li>
      <li>–ì–ª–∞–¥–∫–∞—è</li>
    </ul>
    <p><strong>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</strong>: Transformers, NLP –º–æ–¥–µ–ª–∏</p>
  </div>

  <div class="block">
    <h2>üî∑ 9. Softmax (–≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π)</h2>
    <p><strong>–î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</strong></p>
    <p><strong>–§–æ—Ä–º—É–ª–∞</strong>: Softmax(x·µ¢) = eÀ£‚Å± / Œ£‚±º eÀ£ ≤</p>
    <pre><code># PyTorch
nn.Softmax(dim=-1)

# Keras
layers.Activation('softmax')
# –∏–ª–∏ –≤ Dense
layers.Dense(10, activation='softmax')</code></pre>
    <p><strong>–°–≤–æ–π—Å—Ç–≤–∞</strong>:</p>
    <ul>
      <li>Œ£ –≤—ã—Ö–æ–¥–æ–≤ = 1 (—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π)</li>
      <li>–í—Å–µ –≤—ã—Ö–æ–¥—ã ‚àà (0, 1)</li>
    </ul>
    <p><strong>‚ö†Ô∏è –í–∞–∂–Ω–æ</strong>: —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–æ–µ–Ω–∞ –≤ loss —Ñ—É–Ω–∫—Ü–∏—é (CrossEntropyLoss)</p>
  </div>

  <div class="block">
    <h2>üî∑ 10. –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞</h2>
    <table>
      <tr><th>–§—É–Ω–∫—Ü–∏—è</th><th>–î–∏–∞–ø–∞–∑–æ–Ω</th><th>–°–∫–æ—Ä–æ—Å—Ç—å</th><th>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ</th></tr>
      <tr><td><strong>ReLU</strong></td><td>[0, ‚àû)</td><td>‚ö°‚ö°‚ö°</td><td>–°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è CNN</td></tr>
      <tr><td><strong>Leaky ReLU</strong></td><td>(-‚àû, ‚àû)</td><td>‚ö°‚ö°‚ö°</td><td>–í–º–µ—Å—Ç–æ ReLU, GAN</td></tr>
      <tr><td><strong>ELU</strong></td><td>(-Œ±, ‚àû)</td><td>‚ö°‚ö°</td><td>–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ ReLU</td></tr>
      <tr><td><strong>Swish/SiLU</strong></td><td>(-‚àû, ‚àû)</td><td>‚ö°‚ö°</td><td>MobileNet, EfficientNet</td></tr>
      <tr><td><strong>GELU</strong></td><td>(-‚àû, ‚àû)</td><td>‚ö°‚ö°</td><td>Transformers</td></tr>
      <tr><td><strong>Sigmoid</strong></td><td>(0, 1)</td><td>‚ö°</td><td>–¢–æ–ª—å–∫–æ –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π</td></tr>
      <tr><td><strong>Tanh</strong></td><td>(-1, 1)</td><td>‚ö°</td><td>RNN/LSTM</td></tr>
      <tr><td><strong>Softmax</strong></td><td>(0, 1), Œ£=1</td><td>‚ö°</td><td>–ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è</h2>
    <pre><code># PyTorch CNN
model = nn.Sequential(
    nn.Conv2d(3, 64, 3, padding=1),
    nn.BatchNorm2d(64),
    nn.ReLU(),  # ‚≠ê –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è CNN
    
    nn.Conv2d(64, 128, 3, padding=1),
    nn.BatchNorm2d(128),
    nn.ReLU(),
    
    nn.Flatten(),
    nn.Linear(128 * 8 * 8, 10),
    nn.Softmax(dim=-1)  # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
)

# Transformer block
class TransformerBlock(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.attention = MultiHeadAttention(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),  # ‚≠ê –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è Transformers
            nn.Linear(4 * d_model, d_model)
        )</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫—É—é –∞–∫—Ç–∏–≤–∞—Ü–∏—é</h2>
    <table>
      <tr><th>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞/–ó–∞–¥–∞—á–∞</th><th>–ê–∫—Ç–∏–≤–∞—Ü–∏—è</th></tr>
      <tr><td><strong>CNN (—Å–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏)</strong></td><td>ReLU</td></tr>
      <tr><td><strong>RNN/LSTM</strong></td><td>Tanh</td></tr>
      <tr><td><strong>Transformer</strong></td><td>GELU</td></tr>
      <tr><td><strong>MobileNet, EfficientNet</strong></td><td>Swish/SiLU</td></tr>
      <tr><td><strong>GAN (discriminator)</strong></td><td>Leaky ReLU</td></tr>
      <tr><td><strong>–ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–≤—ã—Ö–æ–¥)</strong></td><td>Sigmoid</td></tr>
      <tr><td><strong>–ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è (–≤—ã—Ö–æ–¥)</strong></td><td>Softmax</td></tr>
      <tr><td><strong>–†–µ–≥—Ä–µ—Å—Å–∏—è (–≤—ã—Ö–æ–¥)</strong></td><td>–ë–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (linear)</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ü—Ä–æ–±–ª–µ–º–∞ –∏—Å—á–µ–∑–∞—é—â–µ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞</h2>
    <p><strong>–ü—Ä–æ–±–ª–µ–º–∞</strong>: –≤ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö —Å sigmoid/tanh –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –æ—á–µ–Ω—å –º–∞–ª—ã–º–∏</p>
    <p><strong>–ü–æ—á–µ–º—É</strong>: –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è sigmoid/tanh ‚â§ 0.25, –ø—Ä–∏ —É–º–Ω–æ–∂–µ–Ω–∏–∏ —á–µ—Ä–µ–∑ —Å–ª–æ–∏ ‚Üí 0</p>
    <p><strong>–†–µ—à–µ–Ω–∏–µ</strong>:</p>
    <ul>
      <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ ReLU/GELU/Swish</li>
      <li>Batch Normalization</li>
      <li>Residual connections (ResNet)</li>
      <li>–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 14. PReLU (Parametric ReLU)</h2>
    <p><strong>–û–±—É—á–∞–µ–º—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä Œ±</strong></p>
    <p>PReLU(x) = max(Œ±x, x), –≥–¥–µ Œ± ‚Äî –æ–±—É—á–∞–µ–º—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä</p>
    <pre><code># PyTorch
nn.PReLU()  # Œ± –æ–±—É—á–∞–µ—Ç—Å—è

# Keras
layers.PReLU()</code></pre>
    <p>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–∫–ª–æ–Ω –¥–ª—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π</p>
  </div>

  <div class="block">
    <h2>üî∑ 15. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏</h2>
    <ul>
      <li><strong>–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é</strong>: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ ReLU</li>
      <li><strong>–î–ª—è Transformers</strong>: GELU</li>
      <li><strong>–ï—Å–ª–∏ ReLU –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç</strong>: –ø–æ–ø—Ä–æ–±—É–π—Ç–µ Leaky ReLU –∏–ª–∏ ELU</li>
      <li><strong>–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã</strong>: Swish/GELU —á–∞—Å—Ç–æ –ª—É—á—à–µ</li>
      <li><strong>–í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π</strong>:
        <ul>
          <li>–ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è ‚Üí Sigmoid</li>
          <li>–ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è ‚Üí Softmax</li>
          <li>–†–µ–≥—Ä–µ—Å—Å–∏—è ‚Üí –±–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏</li>
        </ul>
      </li>
      <li><strong>–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ sigmoid/tanh</strong> –¥–ª—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—ë–≤ –≤ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö!</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –î–ª—è CNN ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ReLU</li>
      <li>[ ] –î–ª—è Transformer ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GELU</li>
      <li>[ ] –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è –Ω–∞ –≤—ã—Ö–æ–¥–Ω–æ–º —Å–ª–æ–µ</li>
      <li>[ ] –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å sigmoid/tanh –¥–ª—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—ë–≤ (–∫—Ä–æ–º–µ RNN)</li>
      <li>[ ] –ï—Å–ª–∏ dying ReLU ‚Äî –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å Leaky ReLU</li>
      <li>[ ] –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å Swish –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏</li>
      <li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å "–º—ë—Ä—Ç–≤—ã–µ" –Ω–µ–π—Ä–æ–Ω—ã (–≤—Å–µ–≥–¥–∞ 0)</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ‚Äî —ç—Ç–æ "—Ä–µ—à–∞—é—â–∏–π –º–µ—Ö–∞–Ω–∏–∑–º" –Ω–µ–π—Ä–æ–Ω–∞: –¥–æ–ª–∂–µ–Ω –ª–∏ –æ–Ω –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å—Å—è (–ø–µ—Ä–µ–¥–∞—Ç—å —Å–∏–≥–Ω–∞–ª –¥–∞–ª—å—à–µ) –∏–ª–∏ –Ω–µ—Ç. ReLU ‚Äî —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è: –µ—Å–ª–∏ —Å–∏–≥–Ω–∞–ª –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π ‚Äî –±–ª–æ–∫–∏—Ä—É–µ–º. –≠—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å, –∫–æ—Ç–æ—Ä—ã–π –≤–∫–ª—é—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∏–ª—å–Ω–æ–º —Å–∏–≥–Ω–∞–ª–µ¬ª.
    </blockquote>
  </div>



</div>
</body>
</html>

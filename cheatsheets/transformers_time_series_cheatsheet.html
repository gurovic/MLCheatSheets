<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Transformers –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üîÆ Transformers –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>


  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã</h2>
    <ul>
      <li><strong>Transformers</strong>: self-attention –¥–ª—è TS</li>
      <li><strong>Attention</strong>: —Ñ–æ–∫—É—Å –Ω–∞ –≤–∞–∂–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–∞—Ö</li>
      <li><strong>Positional</strong>: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—Ä–µ–º–µ–Ω–∏</li>
      <li><strong>Forecast</strong>: –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ</li>
    </ul>
    <p><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:</strong></p>
    <ul>
      <li>–î–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</li>
      <li>–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞</li>
      <li>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å attention</li>
      <li>SOTA —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è TS</h2>
    <pre><code>Input Embedding
    ‚Üì
Positional Encoding
    ‚Üì
Multi-Head Self-Attention
    ‚Üì
Feed-Forward Network
    ‚Üì
Output Projection</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. Temporal Positional Encoding</h2>
    <pre><code>import torch
import math

class TemporalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * 
            -(math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:x.size(1), :]</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Transformer –º–æ–¥–µ–ª—å</h2>
    <pre><code>class TimeSeriesTransformer(nn.Module):
    def __init__(self, input_dim, d_model=128, nhead=8, 
                 num_layers=3, dim_feedforward=512):
        super().__init__()
        self.embedding = nn.Linear(input_dim, d_model)
        self.pos_encoder = TemporalEncoding(d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model, nhead, dim_feedforward,
            dropout=0.1, batch_first=True
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer, num_layers
        )
        self.fc = nn.Linear(d_model, 1)
    
    def forward(self, src):
        src = self.embedding(src)
        src = self.pos_encoder(src)
        output = self.transformer(src)
        output = self.fc(output[:, -1, :])
        return output</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Attention –¥–ª—è TS</h2>
    <p><strong>–§–æ—Ä–º—É–ª–∞:</strong></p>
    <pre><code>Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V

Q = queries (—á—Ç–æ –∏—â–µ–º)
K = keys (–≥–¥–µ –∏—Å–∫–∞—Ç—å)
V = values (—á—Ç–æ –ø–æ–ª—É—á–∏—Ç—å)</code></pre>
    <p><strong>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:</strong></p>
    <ul>
      <li>–í—ã—Å–æ–∫–∏–π attention ‚Üí –≤–∞–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç</li>
      <li>–ù–∏–∑–∫–∏–π attention ‚Üí –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 6. Informer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</h2>
    <pre><code># Efficient long-sequence forecasting
from pyts.transformation import ROCKET

class Informer(nn.Module):
    def __init__(self, enc_in, dec_in, c_out, seq_len):
        super().__init__()
        # ProbSparse Self-Attention
        self.encoder = InformerEncoder(enc_in, seq_len)
        self.decoder = InformerDecoder(dec_in, c_out)
    
    def forward(self, x_enc, x_dec):
        enc_out = self.encoder(x_enc)
        dec_out = self.decoder(x_dec, enc_out)
        return dec_out</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏</h2>
    <pre><code>model = TimeSeriesTransformer(input_dim=1, d_model=64)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

for epoch in range(100):
    model.train()
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        
        # Forward
        predictions = model(X_batch)
        loss = criterion(predictions, y_batch)
        
        # Backward
        loss.backward()
        optimizer.step()
    
    # Validation
    model.eval()
    val_loss = evaluate(model, val_loader)
    print(f'Epoch {epoch}: Loss={loss:.4f}, Val={val_loss:.4f}')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–ì–æ–¥</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏</th></tr>
      <tr><td><strong>Informer</strong></td><td>2020</td><td>ProbSparse attention</td></tr>
      <tr><td><strong>Autoformer</strong></td><td>2021</td><td>Auto-correlation</td></tr>
      <tr><td><strong>FEDformer</strong></td><td>2022</td><td>Frequency enhanced</td></tr>
      <tr><td><strong>PatchTST</strong></td><td>2023</td><td>–ü–∞—Ç—á–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>Electricity</strong>: –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è</li>
      <li><strong>Traffic</strong>: –ø—Ä–æ–≥–Ω–æ–∑ —Ç—Ä–∞—Ñ–∏–∫–∞</li>
      <li><strong>Weather</strong>: –º–µ—Ç–µ–æ–ø—Ä–æ–≥–Ω–æ–∑</li>
      <li><strong>Stock</strong>: —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä—ã–Ω–∫–∏</li>
    </ul>
    <pre><code># –ü—Ä–∏–º–µ—Ä —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
from gluonts.dataset.repository import get_dataset

dataset = get_dataset("electricity")
train_data = dataset.train
test_data = dataset.test</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –•–æ—Ä–æ—à–æ</h3>
        <ul>
          <li>–î–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (>100)</li>
          <li>–ú—É–ª—å—Ç–∏–≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã–µ —Ä—è–¥—ã</li>
          <li>–°–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</li>
          <li>–ë–æ–ª—å—à–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã</li>
          <li>–ù—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ü–ª–æ—Ö–æ</h3>
        <ul>
          <li>–ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ (< 10k)</li>
          <li>–ö–æ—Ä–æ—Ç–∫–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</li>
          <li>–û–≥—Ä–∞–Ω–∏—á–µ–Ω—ã GPU</li>
          <li>–ü—Ä–æ—Å—Ç—ã–µ –ª–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å d_model (64-256)</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ heads (4-8)</li>
      <li>[ ] –î–æ–±–∞–≤–∏—Ç—å positional encoding</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å layer normalization</li>
      <li>[ ] –ü—Ä–∏–º–µ–Ω–∏—Ç—å dropout (0.1)</li>
      <li>[ ] Warm-up learning rate</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å attention weights</li>
      <li>[ ] –°—Ä–∞–≤–Ω–∏—Ç—å —Å LSTM/TCN</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´Transformer –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ ‚Äî —ç—Ç–æ –∫–∞–∫ —É–º–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫, –∫–æ—Ç–æ—Ä—ã–π —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –≤—Å—é –∏—Å—Ç–æ—Ä–∏—é –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∏ —Å–∞–º —Ä–µ—à–∞–µ—Ç, –∫–∞–∫–∏–µ –º–æ–º–µ–Ω—Ç—ã –≤–∞–∂–Ω—ã –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞. –ú–æ–¥–µ–ª—å "–æ–±—Ä–∞—â–∞–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ" –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è –≤ –ø—Ä–æ—à–ª–æ–º¬ª.
    </blockquote>
  </div>


</div>

</div>
</body>
</html>

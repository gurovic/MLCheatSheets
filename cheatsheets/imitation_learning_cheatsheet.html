<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Imitation Learning (–ò–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ) ‚Äî Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      font-size: 0.9em;
      color: #666;
      text-align: center;
      margin-bottom: 20px;
      column-span: all;
    }

    h2 {
      font-size: 1.1em;
      font-weight: 700;
      margin-top: 0;
      color: #1a5fb4;
      border-bottom: 2px solid #e0e8f5;
      padding-bottom: 4px;
    }

    h3 {
      font-size: 0.95em;
      font-weight: 600;
      margin: 8px 0 4px;
      color: #26a269;
    }

    p, ul, ol {
      margin: 6px 0;
      font-size: 0.88em;
      line-height: 1.5;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 3px;
    }

    code {
      background: #f6f8fa;
      padding: 1px 4px;
      border-radius: 3px;
      font-family: 'Consolas', 'Monaco', monospace;
      font-size: 0.9em;
      color: #c7254e;
    }

    pre {
      background: #282c34;
      color: #abb2bf;
      padding: 10px;
      border-radius: 6px;
      overflow-x: auto;
      font-size: 0.8em;
      line-height: 1.4;
      margin: 8px 0;
    }

    pre code {
      background: transparent;
      color: inherit;
      padding: 0;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 8px 0;
      font-size: 0.85em;
    }

    th, td {
      border: 1px solid #ddd;
      padding: 6px 8px;
      text-align: left;
    }

    th {
      background: #e0e8f5;
      font-weight: 600;
      color: #1a5fb4;
    }

    tr:nth-child(even) {
      background: #f9fbff;
    }

    blockquote {
      background: #fff9e6;
      border-left: 4px solid #f6d32d;
      padding: 8px 12px;
      margin: 8px 0;
      font-size: 0.88em;
      font-style: italic;
    }

    .formula {
      background: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      margin: 8px 0;
      font-family: 'Cambria', 'Times New Roman', serif;
      font-size: 0.9em;
      text-align: center;
    }
  </style>
</head>
<body>

<h1>üéØ Imitation Learning (–ò–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)</h1>
<div class="subtitle"></div>

<div class="container">

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è</h2>
    <p>–ê–≥–µ–Ω—Ç —É—á–∏—Ç—Å—è –ø–æ–ª–∏—Ç–∏–∫–µ, –Ω–∞–±–ª—é–¥–∞—è –∑–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–º, –±–µ–∑ —è–≤–Ω–æ–π reward function.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># GAIL Discriminator Training
import torch
import torch.nn as nn

class GailDiscriminator(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim + action_dim, 100),
            nn.Tanh(),
            nn.Linear(100, 1),
            nn.Sigmoid()
        )
    
    def forward(self, state, action):
        return self.net(torch.cat([state, action], dim=1))

# –û–±—É—á–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞
discriminator = GailDiscriminator(4, 2)
optimizer = torch.optim.Adam(discriminator.parameters())

for _ in range(1000):
    expert_sa = sample_expert_trajectories()
    policy_sa = sample_policy_trajectories()
    
    loss = -(torch.log(discriminator(*expert_sa)).mean() + 
             torch.log(1 - discriminator(*policy_sa)).mean())
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()</code></pre>
  <div class="block">
    <h2>üî∑ 2. Behavioral Cloning</h2>
    <p>–ü—Ä–æ—Å—Ç–µ–π—à–∏–π –ø–æ–¥—Ö–æ–¥: supervised learning –Ω–∞ –ø–∞—Ä–∞—Ö (—Å–æ—Å—Ç–æ—è–Ω–∏–µ, –¥–µ–π—Å—Ç–≤–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–∞).</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># DAgger Implementation
import gym
import numpy as np

env = gym.make('CartPole-v1')
policy = initialize_policy()
dataset = []

for iteration in range(10):
    states, actions = [], []
    for _ in range(20):  # 20 —ç–ø–∏–∑–æ–¥–æ–≤
        state = env.reset()
        done = False
        while not done:
            # –ê–≥–µ–Ω—Ç –¥–µ–π—Å—Ç–≤—É–µ—Ç
            agent_action = policy.predict(state)
            # –≠–∫—Å–ø–µ—Ä—Ç –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç
            expert_action = expert_policy(state)
            
            states.append(state)
            actions.append(expert_action)
            
            state, _, done, _ = env.step(agent_action)
    
    dataset.extend(list(zip(states, actions)))
    policy.fit([s for s, a in dataset], [a for s, a in dataset])</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 3. Dataset Aggregation (DAgger)</h2>
    <p>–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö: –∞–≥–µ–Ω—Ç –¥–µ–π—Å—Ç–≤—É–µ—Ç, —ç–∫—Å–ø–µ—Ä—Ç –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Behavioral Cloning
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# –ü—Ä–æ—Å—Ç–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏
class Policy(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, state):
        return self.net(state)

# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è—Ö —ç–∫—Å–ø–µ—Ä—Ç–∞
policy = Policy(state_dim=4, action_dim=2)
optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)
criterion = nn.MSELoss()

for epoch in range(100):
    for states, actions in expert_dataloader:
        pred_actions = policy(states)
        loss = criterion(pred_actions, actions)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 4. Inverse Reinforcement Learning</h2>
    <p>–í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º reward function –∏–∑ –¥–µ–π—Å—Ç–≤–∏–π —ç–∫—Å–ø–µ—Ä—Ç–∞.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># GAIL Discriminator Training
import torch
import torch.nn as nn

class GailDiscriminator(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim + action_dim, 100),
            nn.Tanh(),
            nn.Linear(100, 1),
            nn.Sigmoid()
        )
    
    def forward(self, state, action):
        return self.net(torch.cat([state, action], dim=1))

# –û–±—É—á–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞
discriminator = GailDiscriminator(4, 2)
optimizer = torch.optim.Adam(discriminator.parameters())

for _ in range(1000):
    expert_sa = sample_expert_trajectories()
    policy_sa = sample_policy_trajectories()
    
    loss = -(torch.log(discriminator(*expert_sa)).mean() + 
             torch.log(1 - discriminator(*policy_sa)).mean())
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 5. Generative Adversarial Imitation Learning (GAIL)</h2>
    <p>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç GAN-–ø–æ–¥—Ö–æ–¥ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏: discriminator –æ—Ç–ª–∏—á–∞–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–∞ –æ—Ç –∞–≥–µ–Ω—Ç–∞.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># DAgger Implementation
import gym
import numpy as np

env = gym.make('CartPole-v1')
policy = initialize_policy()
dataset = []

for iteration in range(10):
    states, actions = [], []
    for _ in range(20):  # 20 —ç–ø–∏–∑–æ–¥–æ–≤
        state = env.reset()
        done = False
        while not done:
            # –ê–≥–µ–Ω—Ç –¥–µ–π—Å—Ç–≤—É–µ—Ç
            agent_action = policy.predict(state)
            # –≠–∫—Å–ø–µ—Ä—Ç –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç
            expert_action = expert_policy(state)
            
            states.append(state)
            actions.append(expert_action)
            
            state, _, done, _ = env.step(agent_action)
    
    dataset.extend(list(zip(states, actions)))
    policy.fit([s for s, a in dataset], [a for s, a in dataset])</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 6. Challenges</h2>
    <p>Distribution mismatch, compounding errors, expert suboptimality.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Behavioral Cloning
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# –ü—Ä–æ—Å—Ç–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏
class Policy(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, state):
        return self.net(state)

# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è—Ö —ç–∫—Å–ø–µ—Ä—Ç–∞
policy = Policy(state_dim=4, action_dim=2)
optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)
criterion = nn.MSELoss()

for epoch in range(100):
    for states, actions in expert_dataloader:
        pred_actions = policy(states)
        loss = criterion(pred_actions, actions)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 7. Applications</h2>
    <p>–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞, –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏, –∏–≥—Ä—ã, –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è –æ–±—ä–µ–∫—Ç–∞–º–∏.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># GAIL Discriminator Training
import torch
import torch.nn as nn

class GailDiscriminator(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim + action_dim, 100),
            nn.Tanh(),
            nn.Linear(100, 1),
            nn.Sigmoid()
        )
    
    def forward(self, state, action):
        return self.net(torch.cat([state, action], dim=1))

# –û–±—É—á–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞
discriminator = GailDiscriminator(4, 2)
optimizer = torch.optim.Adam(discriminator.parameters())

for _ in range(1000):
    expert_sa = sample_expert_trajectories()
    policy_sa = sample_policy_trajectories()
    
    loss = -(torch.log(discriminator(*expert_sa)).mean() + 
             torch.log(1 - discriminator(*policy_sa)).mean())
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 8. Comparison with RL</h2>
    <p>–ù–µ —Ç—Ä–µ–±—É–µ—Ç reward engineering, –Ω–æ –Ω—É–∂–Ω—ã —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># DAgger Implementation
import gym
import numpy as np

env = gym.make('CartPole-v1')
policy = initialize_policy()
dataset = []

for iteration in range(10):
    states, actions = [], []
    for _ in range(20):  # 20 —ç–ø–∏–∑–æ–¥–æ–≤
        state = env.reset()
        done = False
        while not done:
            # –ê–≥–µ–Ω—Ç –¥–µ–π—Å—Ç–≤—É–µ—Ç
            agent_action = policy.predict(state)
            # –≠–∫—Å–ø–µ—Ä—Ç –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç
            expert_action = expert_policy(state)
            
            states.append(state)
            actions.append(expert_action)
            
            state, _, done, _ = env.step(agent_action)
    
    dataset.extend(list(zip(states, actions)))
    policy.fit([s for s, a in dataset], [a for s, a in dataset])</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 9. Data Requirements</h2>
    <p>–ö–∞—á–µ—Å—Ç–≤–æ > –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ. –ù—É–∂–Ω—ã —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Behavioral Cloning
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# –ü—Ä–æ—Å—Ç–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏
class Policy(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, state):
        return self.net(state)

# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è—Ö —ç–∫—Å–ø–µ—Ä—Ç–∞
policy = Policy(state_dim=4, action_dim=2)
optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)
criterion = nn.MSELoss()

for epoch in range(100):
    for states, actions in expert_dataloader:
        pred_actions = policy(states)
        loss = criterion(pred_actions, actions)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 10. Evaluation</h2>
    <p>Success rate –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —ç–∫—Å–ø–µ—Ä—Ç–æ–º.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># GAIL Discriminator Training
import torch
import torch.nn as nn

class GailDiscriminator(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim + action_dim, 100),
            nn.Tanh(),
            nn.Linear(100, 1),
            nn.Sigmoid()
        )
    
    def forward(self, state, action):
        return self.net(torch.cat([state, action], dim=1))

# –û–±—É—á–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞
discriminator = GailDiscriminator(4, 2)
optimizer = torch.optim.Adam(discriminator.parameters())

for _ in range(1000):
    expert_sa = sample_expert_trajectories()
    policy_sa = sample_policy_trajectories()
    
    loss = -(torch.log(discriminator(*expert_sa)).mean() + 
             torch.log(1 - discriminator(*policy_sa)).mean())
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 11. Best Practices</h2>
    <p>Augmentation –¥–∞–Ω–Ω—ã—Ö, ensemble of policies, uncertainty estimation.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># DAgger Implementation
import gym
import numpy as np

env = gym.make('CartPole-v1')
policy = initialize_policy()
dataset = []

for iteration in range(10):
    states, actions = [], []
    for _ in range(20):  # 20 —ç–ø–∏–∑–æ–¥–æ–≤
        state = env.reset()
        done = False
        while not done:
            # –ê–≥–µ–Ω—Ç –¥–µ–π—Å—Ç–≤—É–µ—Ç
            agent_action = policy.predict(state)
            # –≠–∫—Å–ø–µ—Ä—Ç –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç
            expert_action = expert_policy(state)
            
            states.append(state)
            actions.append(expert_action)
            
            state, _, done, _ = env.step(agent_action)
    
    dataset.extend(list(zip(states, actions)))
    policy.fit([s for s, a in dataset], [a for s, a in dataset])</code></pre>
  </div>
  <div class="block">
    <h2>üî∑ 12. Code Examples</h2>
    <p>Implementations in stable-baselines3, imitation library.</p>
    <h3>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</h3>
    <ul>
      <li>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã</li>
      <li>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞</li>
      <li>–ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏</li>
      <li>–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</li>
      <li>–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python</li>
      <li>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li>–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</li>
      <li>–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞</li>
    </ul>
    <pre><code># Behavioral Cloning
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# –ü—Ä–æ—Å—Ç–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏
class Policy(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, state):
        return self.net(state)

# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è—Ö —ç–∫—Å–ø–µ—Ä—Ç–∞
policy = Policy(state_dim=4, action_dim=2)
optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)
criterion = nn.MSELoss()

for epoch in range(100):
    for states, actions in expert_dataloader:
        pred_actions = policy(states)
        loss = criterion(pred_actions, actions)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()</code></pre>
  </div>
</div>

</div>
</body>
</html>
<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>GAN (Generative Adversarial Networks) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ GAN (Generative Adversarial Networks) Cheatsheet</h1>
  <div class="subtitle">–î–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö ‚Ä¢ –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å–µ—Ç–∏ ‚Ä¢ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ñ–æ–∫—É—Å<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å GAN</h2>
    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –¥–≤–µ —Å–µ—Ç–∏ —Å–æ—Ä–µ–≤–Ω—É—é—Ç—Å—è –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º</li>
      <li><strong>Generator (G)</strong>: —Å–æ–∑–¥–∞—ë—Ç –ø–æ–¥–¥–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ</li>
      <li><strong>Discriminator (D)</strong>: –æ—Ç–ª–∏—á–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –æ—Ç –ø–æ–¥–¥–µ–ª—å–Ω—ã—Ö</li>
      <li><strong>–ü—Ä–æ—Ü–µ—Å—Å</strong>: G —É–ª—É—á—à–∞–µ—Ç—Å—è –≤ –æ–±–º–∞–Ω–µ D, D ‚Äî –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏</li>
      <li><strong>–†–µ–∑—É–ª—å—Ç–∞—Ç</strong>: G —Å–æ–∑–¥–∞—ë—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ</li>
      <li><strong>–ê–≤—Ç–æ—Ä</strong>: Ian Goodfellow, 2014</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 2. –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)</h2>
    <p><strong>Minimax –∏–≥—Ä–∞</strong>:</p>
    <p>min_G max_D V(D, G) = ùîº[log D(x)] + ùîº[log(1 - D(G(z)))]</p>
    <ul>
      <li><strong>D(x)</strong>: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —á—Ç–æ x —Ä–µ–∞–ª—å–Ω—ã–π</li>
      <li><strong>G(z)</strong>: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑ —à—É–º–∞ z</li>
      <li><strong>D –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç</strong>: –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é</li>
      <li><strong>G –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç</strong>: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ (PyTorch)</h2>
    <pre><code>import torch
import torch.nn as nn

# Generator
class Generator(nn.Module):
    def __init__(self, latent_dim=100, img_shape=(1,28,28)):
        super().__init__()
        self.img_shape = img_shape
        
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(256),
            
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(512),
            
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.BatchNorm1d(1024),
            
            nn.Linear(1024, int(np.prod(img_shape))),
            nn.Tanh()  # –í—ã—Ö–æ–¥ –≤ [-1, 1]
        )
    
    def forward(self, z):
        img = self.model(z)
        img = img.view(img.size(0), *self.img_shape)
        return img

# Discriminator
class Discriminator(nn.Module):
    def __init__(self, img_shape=(1,28,28)):
        super().__init__()
        
        self.model = nn.Sequential(
            nn.Linear(int(np.prod(img_shape)), 512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            
            nn.Linear(256, 1),
            nn.Sigmoid()  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏
        )
    
    def forward(self, img):
        img_flat = img.view(img.size(0), -1)
        validity = self.model(img_flat)
        return validity</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Training Loop</h2>
    <pre><code># –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
generator = Generator()
discriminator = Discriminator()

# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã (–º–∞–ª—ã–π LR!)
optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

criterion = nn.BCELoss()

# Training loop
for epoch in range(num_epochs):
    for i, (real_imgs, _) in enumerate(dataloader):
        batch_size = real_imgs.size(0)
        
        # –ú–µ—Ç–∫–∏
        real_labels = torch.ones(batch_size, 1)
        fake_labels = torch.zeros(batch_size, 1)
        
        # -----------------
        #  Train Discriminator
        # -----------------
        optimizer_D.zero_grad()
        
        # –†–µ–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        real_loss = criterion(discriminator(real_imgs), real_labels)
        
        # –ü–æ–¥–¥–µ–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        z = torch.randn(batch_size, latent_dim)
        fake_imgs = generator(z)
        fake_loss = criterion(discriminator(fake_imgs.detach()), fake_labels)
        
        # –û–±—â–∏–π loss
        d_loss = (real_loss + fake_loss) / 2
        d_loss.backward()
        optimizer_D.step()
        
        # -----------------
        #  Train Generator
        # -----------------
        optimizer_G.zero_grad()
        
        # Generator —Ö–æ—á–µ—Ç –æ–±–º–∞–Ω—É—Ç—å discriminator
        z = torch.randn(batch_size, latent_dim)
        gen_imgs = generator(z)
        g_loss = criterion(discriminator(gen_imgs), real_labels)
        
        g_loss.backward()
        optimizer_G.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ü—Ä–æ–±–ª–µ–º—ã GAN</h2>
    <table>
      <tr><th>–ü—Ä–æ–±–ª–µ–º–∞</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th></tr>
      <tr><td><strong>Mode Collapse</strong></td><td>G –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–∏–ø–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</td></tr>
      <tr><td><strong>Vanishing Gradients</strong></td><td>G –ø–µ—Ä–µ—Å—Ç–∞—ë—Ç –æ–±—É—á–∞—Ç—å—Å—è</td></tr>
      <tr><td><strong>–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</strong></td><td>–°–ª–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É G –∏ D</td></tr>
      <tr><td><strong>Convergence</strong></td><td>–ù–µ–ø–æ–Ω—è—Ç–Ω–æ –∫–æ–≥–¥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 6. DCGAN (Deep Convolutional GAN)</h2>
    <p><strong>–£–ª—É—á—à–µ–Ω–∏—è –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</strong>:</p>
    <ul>
      <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Conv2d –≤–º–µ—Å—Ç–æ Linear</li>
      <li>BatchNorm –≤ –æ–±–µ–∏—Ö —Å–µ—Ç—è—Ö (–∫—Ä–æ–º–µ –≤—ã—Ö–æ–¥–æ–≤)</li>
      <li>LeakyReLU –≤ D, ReLU –≤ G</li>
      <li>Tanh –≤ –≤—ã—Ö–æ–¥–µ G</li>
      <li>Transposed Convolutions –¥–ª—è –∞–ø—Å–µ–º–ø–ª–∏–Ω–≥–∞</li>
      <li>–£–±—Ä–∞—Ç—å Pooling –∏ Fully Connected —Å–ª–æ–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 7. DCGAN Generator</h2>
    <pre><code>class DCGANGenerator(nn.Module):
    def __init__(self, latent_dim=100, channels=3):
        super().__init__()
        
        self.model = nn.Sequential(
            # –í—Ö–æ–¥: z (latent_dim)
            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(True),
            # 4x4
            
            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            # 8x8
            
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            # 16x16
            
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            # 32x32
            
            nn.ConvTranspose2d(64, channels, 4, 2, 1, bias=False),
            nn.Tanh()
            # 64x64
        )
    
    def forward(self, z):
        z = z.view(z.size(0), z.size(1), 1, 1)
        return self.model(z)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏</h2>
    <ul>
      <li><strong>Learning Rate</strong>: –Ω–∏–∑–∫–∏–π (0.0002)</li>
      <li><strong>Optimizer</strong>: Adam —Å Œ≤‚ÇÅ=0.5, Œ≤‚ÇÇ=0.999</li>
      <li><strong>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è</strong>: –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ [-1, 1]</li>
      <li><strong>LeakyReLU</strong>: –≤ Discriminator (alpha=0.2)</li>
      <li><strong>Label Smoothing</strong>: real = 0.9 –≤–º–µ—Å—Ç–æ 1.0</li>
      <li><strong>Noisy Labels</strong>: –∏–Ω–æ–≥–¥–∞ –ø–µ—Ä–µ–≤–æ—Ä–∞—á–∏–≤–∞—Ç—å –º–µ—Ç–∫–∏</li>
      <li><strong>Separate Batches</strong>: –æ—Ç–¥–µ–ª—å–Ω—ã–µ –±–∞—Ç—á–∏ –¥–ª—è real/fake</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 9. –í–∞—Ä–∏–∞–Ω—Ç—ã GAN</h2>
    <ul>
      <li><strong>Conditional GAN (cGAN)</strong>: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —É—Å–ª–æ–≤–∏–µ–º (–º–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞)</li>
      <li><strong>WGAN</strong>: Wasserstein distance (—Å—Ç–∞–±–∏–ª—å–Ω–µ–µ)</li>
      <li><strong>WGAN-GP</strong>: —Å gradient penalty</li>
      <li><strong>StyleGAN</strong>: –∫–æ–Ω—Ç—Ä–æ–ª—å —Å—Ç–∏–ª—è, –æ—á–µ–Ω—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ</li>
      <li><strong>CycleGAN</strong>: –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç–∏–ª—è –±–µ–∑ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li><strong>Pix2Pix</strong>: image-to-image translation</li>
      <li><strong>ProGAN</strong>: –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 10. Conditional GAN</h2>
    <pre><code># –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∫—É –∫–ª–∞—Å—Å–∞
class ConditionalGenerator(nn.Module):
    def __init__(self, latent_dim=100, n_classes=10):
        super().__init__()
        self.label_emb = nn.Embedding(n_classes, n_classes)
        
        self.model = nn.Sequential(
            nn.Linear(latent_dim + n_classes, 256),
            # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏
        )
    
    def forward(self, z, labels):
        label_embedding = self.label_emb(labels)
        gen_input = torch.cat((z, label_embedding), -1)
        return self.model(gen_input)

# –ü—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É–∫–∞–∑—ã–≤–∞–µ–º –∫–ª–∞—Å—Å
z = torch.randn(batch_size, latent_dim)
labels = torch.randint(0, 10, (batch_size,))
fake_imgs = generator(z, labels)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞</h2>
    <ul>
      <li><strong>Inception Score (IS)</strong>: —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ –∫–∞—á–µ—Å—Ç–≤–æ</li>
      <li><strong>Fr√©chet Inception Distance (FID)</strong>: –ø–æ—Ö–æ–∂–µ—Å—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ (‚Üì –ª—É—á—à–µ)</li>
      <li><strong>Precision & Recall</strong>: –∫–∞—á–µ—Å—Ç–≤–æ vs —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ</li>
      <li><strong>Visual Inspection</strong>: —Ä—É—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞</li>
    </ul>
    <pre><code># –ü—Ä–∏–º–µ—Ä –≤—ã—á–∏—Å–ª–µ–Ω–∏—è FID (pytorch-fid)
from pytorch_fid import fid_score

fid_value = fid_score.calculate_fid_given_paths(
    [path_real, path_generated],
    batch_size=50,
    device='cuda',
    dims=2048
)
print(f"FID: {fid_value}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è GAN</h2>
    <ul>
      <li><strong>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–∏—Ü</strong>: ThisPersonDoesNotExist</li>
      <li><strong>Super-Resolution</strong>: —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è</li>
      <li><strong>Style Transfer</strong>: –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç–∏–ª—è</li>
      <li><strong>Text-to-Image</strong>: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é</li>
      <li><strong>Data Augmentation</strong>: —Å–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li><strong>Inpainting</strong>: –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤</li>
      <li><strong>Video Generation</strong>: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 13. –°–æ–≤–µ—Ç—ã –ø–æ –æ—Ç–ª–∞–¥–∫–µ</h2>
    <ul>
      <li><strong>D —Å–ª–∏—à–∫–æ–º —Å–∏–ª—ë–Ω</strong>: —É–º–µ–Ω—å—à–∏—Ç—å capacity –∏–ª–∏ —É–≤–µ–ª–∏—á–∏—Ç—å dropout</li>
      <li><strong>Mode collapse</strong>: –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (WGAN, minibatch discrimination)</li>
      <li><strong>–õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å</strong>: D_loss, G_loss, D(x), D(G(z))</li>
      <li><strong>–°–æ—Ö—Ä–∞–Ω—è—Ç—å</strong>: –ø—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É</li>
      <li><strong>–ë–∞–ª–∞–Ω—Å</strong>: D_loss ‚âà G_loss ‚Äî —Ö–æ—Ä–æ—à–∏–π –ø—Ä–∏–∑–Ω–∞–∫</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ [-1, 1]</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LeakyReLU –≤ Discriminator</li>
      <li>[ ] BatchNorm –≤ –æ–±–µ–∏—Ö —Å–µ—Ç—è—Ö</li>
      <li>[ ] –ú–∞–ª—ã–π learning rate (0.0002)</li>
      <li>[ ] Adam —Å Œ≤‚ÇÅ=0.5</li>
      <li>[ ] –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</li>
      <li>[ ] –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å losses</li>
      <li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å DCGAN –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</li>
      <li>[ ] –ü—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö ‚Äî –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å WGAN</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´GAN ‚Äî —ç—Ç–æ –∫–∞–∫ –¥–≤–æ–µ –ª—é–¥–µ–π: –æ–¥–∏–Ω (–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä) –ø—ã—Ç–∞–µ—Ç—Å—è –ø–æ–¥–¥–µ–ª–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω—ã, –¥—Ä—É–≥–æ–π (–¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä) ‚Äî —ç–∫—Å–ø–µ—Ä—Ç, –∫–æ—Ç–æ—Ä—ã–π –∏—Ö –ø—Ä–æ–≤–µ—Ä—è–µ—Ç. –ü–æ–¥–¥–µ–ª—ã–≤–∞—Ç–µ–ª—å —É—á–∏—Ç—Å—è –≤—Å—ë –ª—É—á—à–µ –æ–±–º–∞–Ω—ã–≤–∞—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–∞, –∞ —ç–∫—Å–ø–µ—Ä—Ç —É—á–∏—Ç—Å—è –ª—É—á—à–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –ø–æ–¥–¥–µ–ª–∫–∏. –í –∏—Ç–æ–≥–µ –ø–æ–¥–¥–µ–ª–∫–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –Ω–µ–æ—Ç–ª–∏—á–∏–º—ã –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–∞¬ª.
    </blockquote>
  </div>

</div>

</body>
</html>

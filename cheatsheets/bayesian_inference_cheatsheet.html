<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –≤—ã–≤–æ–¥ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.88em;
      margin: 6px 0;
    }

    th, td {
      padding: 6px 8px;
      text-align: left;
      border: 1px solid #e0e7ff;
    }

    th {
      background-color: #1a5fb4;
      color: white;
      font-weight: 700;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üîÆ –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –≤—ã–≤–æ–¥</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞</h2>
    <p><strong>–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –≤—ã–≤–æ–¥</strong>: –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π</p>
    <ul>
      <li><strong>–¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞</strong>: P(Œ∏|D) = P(D|Œ∏) √ó P(Œ∏) / P(D)</li>
      <li><strong>Prior</strong> P(Œ∏): –∞–ø—Ä–∏–æ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π</li>
      <li><strong>Likelihood</strong> P(D|Œ∏): –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö</li>
      <li><strong>Posterior</strong> P(Œ∏|D): –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π</li>
      <li><strong>Evidence</strong> P(D): –Ω–æ—Ä–º–∏—Ä–æ–≤–æ—á–Ω–∞—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞</li>
    </ul>
    <blockquote>
      üí° –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –≤—ã–≤–æ–¥ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∞ –Ω–µ —Ç–æ—á–µ—á–Ω—É—é –æ—Ü–µ–Ω–∫—É
    </blockquote>

  <div class="block">
    <h2>üî∑ 2. –§–æ—Ä–º—É–ª–∞ –ë–∞–π–µ—Å–∞</h2>
    <pre><code><strong>–¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞:</strong>

P(Œ∏|D) = P(D|Œ∏) √ó P(Œ∏) / P(D)

–≥–¥–µ:
- Œ∏ (theta) - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
- D - –Ω–∞–±–ª—é–¥–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ
- P(Œ∏) - –∞–ø—Ä–∏–æ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
- P(D|Œ∏) - —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è
- P(Œ∏|D) - –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
- P(D) = ‚à´ P(D|Œ∏) P(Œ∏) dŒ∏ - evidence

<strong>–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞:</strong>
Posterior ‚àù Likelihood √ó Prior</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –í—ã–±–æ—Ä –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è</h2>
    <table>
      <tr><th>–¢–∏–ø</th><th>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</th><th>–ü—Ä–∏–º–µ—Ä</th></tr>
      <tr><td>Uninformative</td><td>–ù–µ—Ç –∑–Ω–∞–Ω–∏–π</td><td>Uniform, Jeffreys</td></tr>
      <tr><td>Weakly informative</td><td>–û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è</td><td>Normal(0, 10)</td></tr>
      <tr><td>Informative</td><td>–°–∏–ª—å–Ω—ã–µ –∑–Ω–∞–Ω–∏—è</td><td>Normal(Œº_expert, œÉ¬≤)</td></tr>
      <tr><td>Conjugate</td><td>–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ</td><td>Beta –¥–ª—è Bernoulli</td></tr>
      <tr><td>Hierarchical</td><td>–ì—Ä—É–ø–ø–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ</td><td>Multi-level priors</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 4. –°–æ–ø—Ä—è–∂–µ–Ω–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è</h2>
    <table>
      <tr><th>Likelihood</th><th>Conjugate Prior</th><th>Posterior</th></tr>
      <tr><td>Bernoulli</td><td>Beta(Œ±, Œ≤)</td><td>Beta(Œ±+k, Œ≤+n-k)</td></tr>
      <tr><td>Binomial</td><td>Beta(Œ±, Œ≤)</td><td>Beta(Œ±+k, Œ≤+n-k)</td></tr>
      <tr><td>Multinomial</td><td>Dirichlet(Œ±)</td><td>Dirichlet(Œ±+counts)</td></tr>
      <tr><td>Poisson</td><td>Gamma(Œ±, Œ≤)</td><td>Gamma(Œ±+Œ£x, Œ≤+n)</td></tr>
      <tr><td>Normal (Œº)</td><td>Normal(Œº‚ÇÄ, œÉ‚ÇÄ¬≤)</td><td>Normal(Œº_new, œÉ_new¬≤)</td></tr>
      <tr><td>Normal (œÉ¬≤)</td><td>Inverse-Gamma</td><td>Inverse-Gamma</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è</h2>
    <pre><code>import numpy as np
from scipy import stats

class BayesianLinearRegression:
    def __init__(self, alpha=1.0, beta=1.0):
        """
        alpha: —Ç–æ—á–Ω–æ—Å—Ç—å prior –Ω–∞ –≤–µ—Å–∞ (1/œÉ¬≤)
        beta: —Ç–æ—á–Ω–æ—Å—Ç—å noise (1/œÉ¬≤_noise)
        """
        self.alpha = alpha
        self.beta = beta
        self.m_N = None  # Posterior mean
        self.S_N = None  # Posterior covariance
    
    def fit(self, X, y):
        N, M = X.shape
        
        # Prior: w ~ N(0, Œ±‚Åª¬πI)
        S_0 = (1/self.alpha) * np.eye(M)
        m_0 = np.zeros(M)
        
        # Posterior: w ~ N(m_N, S_N)
        S_N_inv = self.alpha * np.eye(M) + self.beta * X.T @ X
        self.S_N = np.linalg.inv(S_N_inv)
        self.m_N = self.beta * self.S_N @ X.T @ y
        
        return self
    
    def predict(self, X, return_std=False):
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: y ~ N(m_N^T x, œÉ¬≤)
        y_pred = X @ self.m_N
        
        if return_std:
            # Uncertainty: œÉ¬≤ = 1/Œ≤ + x^T S_N x
            y_var = 1/self.beta + np.sum(X @ self.S_N * X, axis=1)
            y_std = np.sqrt(y_var)
            return y_pred, y_std
        
        return y_pred
    
    def sample_weights(self, n_samples=100):
        """–°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤ –∏–∑ posterior"""
        return np.random.multivariate_normal(
            self.m_N, self.S_N, size=n_samples
        )

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
model = BayesianLinearRegression(alpha=1.0, beta=25.0)
model.fit(X_train, y_train)
y_pred, y_std = model.predict(X_test, return_std=True)

# Confidence intervals
y_lower = y_pred - 1.96 * y_std
y_upper = y_pred + 1.96 * y_std</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è</h2>
    <pre><code>import numpy as np
from scipy import stats

class BayesianLogisticRegression:
    """–ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å Laplace approximation"""
    
    def __init__(self, alpha=1.0, max_iter=100):
        self.alpha = alpha  # Prior precision
        self.max_iter = max_iter
        self.w_map = None  # MAP estimate
        self.H_inv = None  # Inverse Hessian
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y):
        N, M = X.shape
        
        # Initialize weights
        w = np.zeros(M)
        
        # Newton-Raphson optimization for MAP
        for _ in range(self.max_iter):
            # Predictions
            pred = self.sigmoid(X @ w)
            
            # Gradient
            grad = X.T @ (pred - y) + self.alpha * w
            
            # Hessian
            R = np.diag(pred * (1 - pred))
            H = X.T @ R @ X + self.alpha * np.eye(M)
            
            # Update
            w = w - np.linalg.solve(H, grad)
        
        self.w_map = w
        self.H_inv = np.linalg.inv(H)
        
        return self
    
    def predict_proba(self, X):
        # MAP prediction
        z = X @ self.w_map
        p = self.sigmoid(z)
        
        # Uncertainty (approximate)
        var = np.sum(X @ self.H_inv * X, axis=1)
        
        return p, var
    
    def predict(self, X, threshold=0.5):
        p, _ = self.predict_proba(X)
        return (p >= threshold).astype(int)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
model = BayesianLogisticRegression(alpha=1.0)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
y_proba, uncertainty = model.predict_proba(X_test)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Bayesian Model Selection</h2>
    <pre><code>import numpy as np
from scipy import stats

def bayesian_model_comparison(models, X, y):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ Bayes Factor
    """
    log_evidences = []
    
    for model in models:
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ log evidence (marginal likelihood)
        # P(D|M) = ‚à´ P(D|Œ∏,M) P(Œ∏|M) dŒ∏
        
        # Laplace approximation
        model.fit(X, y)
        
        # Log likelihood at MAP
        if hasattr(model, 'log_likelihood'):
            log_like = model.log_likelihood(X, y)
        else:
            # Approximate for sklearn models
            y_pred = model.predict(X)
            log_like = -0.5 * np.sum((y - y_pred)**2)
        
        # Penalty for complexity (BIC approximation)
        n_params = len(model.w_map) if hasattr(model, 'w_map') else X.shape[1]
        n_samples = len(y)
        log_evidence = log_like - 0.5 * n_params * np.log(n_samples)
        
        log_evidences.append(log_evidence)
    
    # Bayes Factors
    log_evidences = np.array(log_evidences)
    bayes_factors = np.exp(log_evidences - log_evidences[0])
    
    # Posterior probabilities
    posterior_probs = bayes_factors / np.sum(bayes_factors)
    
    return {
        'log_evidences': log_evidences,
        'bayes_factors': bayes_factors,
        'posterior_probs': posterior_probs
    }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
results = bayesian_model_comparison([model1, model2, model3], X, y)
print(f"Posterior probabilities: {results['posterior_probs']}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Bayesian A/B Testing</h2>
    <pre><code>import numpy as np
from scipy import stats

def bayesian_ab_test(conversions_A, trials_A, 
                     conversions_B, trials_B,
                     prior_alpha=1, prior_beta=1):
    """
    –ë–∞–π–µ—Å–æ–≤—Å–∫–æ–µ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å Beta-Binomial –º–æ–¥–µ–ª—å—é
    """
    # Posterior distributions
    posterior_A = stats.beta(
        prior_alpha + conversions_A,
        prior_beta + trials_A - conversions_A
    )
    posterior_B = stats.beta(
        prior_alpha + conversions_B,
        prior_beta + trials_B - conversions_B
    )
    
    # Monte Carlo –¥–ª—è P(B > A)
    samples_A = posterior_A.rvs(size=10000)
    samples_B = posterior_B.rvs(size=10000)
    
    prob_B_better = np.mean(samples_B > samples_A)
    
    # Expected loss
    expected_loss_A = np.mean(np.maximum(samples_B - samples_A, 0))
    expected_loss_B = np.mean(np.maximum(samples_A - samples_B, 0))
    
    # Posterior means and credible intervals
    mean_A = posterior_A.mean()
    mean_B = posterior_B.mean()
    ci_A = posterior_A.interval(0.95)
    ci_B = posterior_B.interval(0.95)
    
    return {
        'prob_B_better': prob_B_better,
        'expected_loss_A': expected_loss_A,
        'expected_loss_B': expected_loss_B,
        'mean_A': mean_A,
        'mean_B': mean_B,
        'ci_A': ci_A,
        'ci_B': ci_B
    }

# –ü—Ä–∏–º–µ—Ä
results = bayesian_ab_test(
    conversions_A=100, trials_A=1000,
    conversions_B=120, trials_B=1000
)

print(f"P(B > A) = {results['prob_B_better']:.3f}")
print(f"Expected loss if choose A: {results['expected_loss_A']:.4f}")
print(f"Expected loss if choose B: {results['expected_loss_B']:.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h2>
    <p><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</strong>:</p>
    <ul>
      <li>‚úÖ –ü–æ–ª–Ω–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å (uncertainty quantification)</li>
      <li>‚úÖ –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ prior</li>
      <li>‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç —Å –º–∞–ª—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏</li>
      <li>‚úÖ Incorporates prior knowledge</li>
      <li>‚úÖ –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ</li>
      <li>‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π model selection</li>
    </ul>
    <p><strong>–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</strong>:</p>
    <ul>
      <li>‚ùå –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –∑–∞—Ç—Ä–∞—Ç–Ω–æ</li>
      <li>‚ùå –¢—Ä–µ–±—É–µ—Ç –≤—ã–±–æ—Ä–∞ prior</li>
      <li>‚ùå –°–ª–æ–∂–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</li>
      <li>‚ùå –ú–æ–∂–µ—Ç –±—ã—Ç—å —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ prior</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞</h2>
    <pre><code># PyMC3
import pymc3 as pm

with pm.Model() as model:
    # Priors
    alpha = pm.Normal('alpha', mu=0, sd=10)
    beta = pm.Normal('beta', mu=0, sd=10, shape=X.shape[1])
    sigma = pm.HalfNormal('sigma', sd=1)
    
    # Linear model
    mu = alpha + pm.math.dot(X, beta)
    
    # Likelihood
    y_obs = pm.Normal('y_obs', mu=mu, sd=sigma, observed=y)
    
    # Inference
    trace = pm.sample(2000, tune=1000)

# Arviz –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
import arviz as az
az.plot_trace(trace)
az.summary(trace)

# Stan —á–µ—Ä–µ–∑ PyStan
import pystan

stan_code = """
data {
    int&lt;lower=0&gt; N;
    vector[N] x;
    vector[N] y;
}
parameters {
    real alpha;
    real beta;
    real&lt;lower=0&gt; sigma;
}
model {
    y ~ normal(alpha + beta * x, sigma);
}
"""

sm = pystan.StanModel(model_code=stan_code)
fit = sm.sampling(data={'N': len(y), 'x': X, 'y': y})</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏</h2>
    <ul>
      <li><strong>–í—ã–±–æ—Ä prior</strong>: –Ω–∞—á–Ω–∏—Ç–µ —Å weakly informative priors</li>
      <li><strong>Prior predictive checks</strong>: —Å–∏–º—É–ª–∏—Ä—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ prior</li>
      <li><strong>Posterior predictive checks</strong>: –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å</li>
      <li><strong>Sensitivity analysis</strong>: —Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ priors</li>
      <li><strong>Convergence diagnostics</strong>: R-hat, effective sample size</li>
      <li><strong>–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è</strong>: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ trace plots, posterior plots</li>
      <li><strong>–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è</strong>: –æ–ø–∏—Å—ã–≤–∞–π—Ç–µ –≤—ã–±–æ—Ä priors</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</h2>
    <p><strong>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –≤—ã–≤–æ–¥</strong>:</p>
    <ul>
      <li>üéØ –ú–∞–ª–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö</li>
      <li>üéØ –ù—É–∂–Ω–∞ –æ—Ü–µ–Ω–∫–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏</li>
      <li>üéØ –ï—Å—Ç—å prior knowledge</li>
      <li>üéØ –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö</li>
      <li>üéØ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –º–∞–ª—ã–º–∏ –≤—ã–±–æ—Ä–∫–∞–º–∏</li>
      <li>üéØ –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏</li>
      <li>üéØ Robustness –∫ outliers</li>
      <li>üéØ Causal inference</li>
    </ul>
    <blockquote>
      üí° –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –æ—Å–æ–±–µ–Ω–Ω–æ —Ü–µ–Ω–µ–Ω –∫–æ–≥–¥–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

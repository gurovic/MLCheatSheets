<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Bias-Variance Decomposition Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen { body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; color: #333; background: #fafcff; padding: 10px; 
        min-width: 900px;
      } }
    @media print { body { background: white; padding: 0; } @page { size: A4 landscape; margin: 10mm; } }
        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }
    .block { break-inside: avoid; margin-bottom: 1.2em; padding: 12px; background: white; border-radius: 6px; box-shadow: 0 1px 3px rgba(0,0,0,0.05); }
    h1 { font-size: 1.6em; font-weight: 700; color: #1a5fb4; text-align: center; margin: 0 0 8px; column-span: all; }
    .subtitle { text-align: center; color: #666; font-size: 0.9em; margin-bottom: 12px; column-span: all; }
    h2 { font-size: 1.15em; font-weight: 700; color: #1a5fb4; margin: 0 0 8px; padding-bottom: 4px; border-bottom: 1px solid #e0e7ff; }
    p, ul, ol { font-size: 0.92em; margin: 0.6em 0; }
    ul, ol { padding-left: 18px; }
    li { margin-bottom: 4px; }
    code { font-family: 'Consolas', 'Courier New', monospace; background-color: #f0f4ff; padding: 1px 4px; border-radius: 3px; font-size: 0.88em; }
    pre { background-color: #f0f4ff; padding: 8px; border-radius: 4px; overflow-x: auto; font-size: 0.84em; margin: 6px 0; }
    pre code { padding: 0; background: none; white-space: pre-wrap; }
    table { width: 100%; border-collapse: collapse; font-size: 0.82em; margin: 6px 0; }
    th { background-color: #e6f0ff; text-align: left; padding: 4px 6px; font-weight: 600; }
    td { padding: 4px 6px; border-bottom: 1px solid #f0f4ff; }
    tr:nth-child(even) { background-color: #f8fbff; }
    .good-vs-bad { display: flex; flex-direction: column; gap: 8px; }
    .good-vs-bad div { flex: 1; padding: 6px 8px; border-radius: 4px; }
    .good { background-color: #f0f9f4; border-left: 3px solid #2e8b57; }
    .bad { background-color: #fdf0f2; border-left: 3px solid #d32f2f; }
    .good h3, .bad h3 { margin: 0 0 4px; font-size: 1em; font-weight: 700; }
    .good ul, .bad ul { padding-left: 20px; margin: 0; }
    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }
    blockquote { font-style: italic; margin: 8px 0; padding: 6px 10px; background: #f8fbff; border-left: 2px solid #1a5fb4; font-size: 0.88em; }
    @media print { .container { column-gap: 12px; } .block { box-shadow: none; } code, pre, table { font-size: 0.78em; } h1 { font-size: 1.4em; } h2 { font-size: 1em; } }
  </style>
</head>
<body>
<div class="container">
  <h1>‚öñÔ∏è Bias-Variance Decomposition</h1>
  <div class="subtitle">üìÖ 4 —è–Ω–≤–∞—Ä—è 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <div style="text-align: center; margin: 10px 0;">
      <!-- Duplicate image removed (hash: eb6424da...) -->
    </div>

    <ul>
      <li><strong>–¶–µ–ª—å</strong>: –ø–æ–Ω—è—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ—à–∏–±–∫–∏ –º–æ–¥–µ–ª–∏</li>
      <li><strong>–†–∞–∑–ª–æ–∂–µ–Ω–∏–µ</strong>: Error = Bias¬≤ + Variance + Irreducible Error</li>
      <li><strong>Bias (—Å–º–µ—â–µ–Ω–∏–µ)</strong>: —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞</li>
      <li><strong>Variance (–¥–∏—Å–ø–µ—Ä—Å–∏—è)</strong>: —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –¥–∞–Ω–Ω—ã–º</li>
      <li><strong>Trade-off</strong>: —É–º–µ–Ω—å—à–∞—è bias, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º variance</li>
    </ul>

    </div>
<div class="block">
    <h2>üî∑ 2. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞</h2>
    <p><strong>–û–∂–∏–¥–∞–µ–º–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è</strong>:</p>
    <pre><code>E[(y - fÃÇ(x))¬≤] = Bias¬≤(fÃÇ(x)) + Var(fÃÇ(x)) + œÉ¬≤

–≥–¥–µ:
- y: –∏—Å—Ç–∏–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
- fÃÇ(x): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
- Bias¬≤: (E[fÃÇ(x)] - f(x))¬≤
- Variance: E[(fÃÇ(x) - E[fÃÇ(x)])¬≤]
- œÉ¬≤: irreducible error (—à—É–º)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. Bias (–°–º–µ—â–µ–Ω–∏–µ)</h2>
    <p><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: –æ—à–∏–±–∫–∞ –æ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ</p>
    <p><strong>–ü—Ä–∏—á–∏–Ω—ã –≤—ã—Å–æ–∫–æ–≥–æ bias</strong>:</p>
    <ul>
      <li>–°–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å</li>
      <li>–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å</li>
      <li>–ú–∞–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</li>
      <li>–õ–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
    </ul>
    <p><strong>–ü—Ä–∏–∑–Ω–∞–∫–∏</strong>:</p>
    <ul>
      <li>–ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train</li>
      <li>–ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ test</li>
      <li>–ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ (underfitting)</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 4. Variance (–î–∏—Å–ø–µ—Ä—Å–∏—è)</h2>
    <p><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ —Ñ–ª—É–∫—Ç—É–∞—Ü–∏—è–º –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö</p>
    <p><strong>–ü—Ä–∏—á–∏–Ω—ã –≤—ã—Å–æ–∫–æ–π variance</strong>:</p>
    <ul>
      <li>–°–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞—è –º–æ–¥–µ–ª—å</li>
      <li>–ú–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
      <li>–ú–∞–ª—ã–π —Ä–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏</li>
      <li>–ù–µ—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</li>
    </ul>
    <p><strong>–ü—Ä–∏–∑–Ω–∞–∫–∏</strong>:</p>
    <ul>
      <li>–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ train</li>
      <li>–ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ test</li>
      <li>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (overfitting)</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 5. Bias-Variance Trade-off</h2>
    <table>
      <tr><th>–°–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏</th><th>Bias</th><th>Variance</th><th>–ò—Ç–æ–≥–æ–≤–∞—è –æ—à–∏–±–∫–∞</th></tr>
      <tr><td>–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è</td><td>–í—ã—Å–æ–∫–∏–π ‚Üë</td><td>–ù–∏–∑–∫–∞—è ‚Üì</td><td>–í—ã—Å–æ–∫–∞—è (underfitting)</td></tr>
      <tr><td>–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è</td><td>–°—Ä–µ–¥–Ω–∏–π</td><td>–°—Ä–µ–¥–Ω—è—è</td><td><strong>–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è ‚úì</strong></td></tr>
      <tr><td>–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è</td><td>–ù–∏–∑–∫–∏–π ‚Üì</td><td>–í—ã—Å–æ–∫–∞—è ‚Üë</td><td>–í—ã—Å–æ–∫–∞—è (overfitting)</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 6. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è</h2>
    <pre><code>import matplotlib.pyplot as plt
import numpy as np

# –°–∏–º—É–ª—è—Ü–∏—è bias-variance trade-off
complexity = np.linspace(0, 10, 100)
bias = 10 / (1 + complexity)  # —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è
variance = complexity / 2      # —Ä–∞—Å—Ç–µ—Ç
total_error = bias**2 + variance

plt.figure(figsize=(10, 6))
plt.plot(complexity, bias**2, label='Bias¬≤', linewidth=2)
plt.plot(complexity, variance, label='Variance', linewidth=2)
plt.plot(complexity, total_error, label='Total Error', 
         linewidth=3, linestyle='--')
plt.axvline(complexity[np.argmin(total_error)], 
            color='r', linestyle=':', label='Optimal')
plt.xlabel('Model Complexity')
plt.ylabel('Error')
plt.legend()
plt.title('Bias-Variance Trade-off')
plt.grid(True, alpha=0.3)
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ</h2>
    <pre><code>from mlxtend.evaluate import bias_variance_decomp
from sklearn.tree import DecisionTreeRegressor

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ bias –∏ variance
mse, bias, var = bias_variance_decomp(
    DecisionTreeRegressor(max_depth=3),
    X_train, y_train,
    X_test, y_test,
    loss='mse',
    num_rounds=100,
    random_seed=42
)

print(f"MSE: {mse:.4f}")
print(f"Bias¬≤: {bias:.4f}")
print(f"Variance: {var:.4f}")
print(f"Bias¬≤ + Variance: {bias + var:.4f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. –°–Ω–∏–∂–µ–Ω–∏–µ Bias</h2>
    <p><strong>–ú–µ—Ç–æ–¥—ã</strong>:</p>
    <ul>
      <li>–£—Å–ª–æ–∂–Ω–∏—Ç—å –º–æ–¥–µ–ª—å (–±–æ–ª—å—à–µ —Å–ª–æ–µ–≤, –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)</li>
      <li>–î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏, –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏</li>
      <li>–£–º–µ–Ω—å—à–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é</li>
      <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏</li>
      <li>–£–≤–µ–ª–∏—á–∏—Ç—å –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è</li>
      <li>–ê–Ω—Å–∞–º–±–ª–∏ (boosting)</li>
    </ul>
    <pre><code># –ü—Ä–∏–º–µ—Ä: —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã –¥–µ—Ä–µ–≤–∞
shallow = DecisionTreeRegressor(max_depth=2)  # high bias
deep = DecisionTreeRegressor(max_depth=10)    # low bias</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –°–Ω–∏–∂–µ–Ω–∏–µ Variance</h2>
    <p><strong>–ú–µ—Ç–æ–¥—ã</strong>:</p>
    <ul>
      <li>–£–ø—Ä–æ—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å (–º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)</li>
      <li>–ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è</li>
      <li>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (L1, L2, dropout)</li>
      <li>Feature selection</li>
      <li>Early stopping</li>
      <li>–ê–Ω—Å–∞–º–±–ª–∏ (bagging, random forest)</li>
      <li>Cross-validation</li>
    </ul>
    <pre><code># –ü—Ä–∏–º–µ—Ä: —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
from sklearn.linear_model import Ridge

# –í—ã—Å–æ–∫–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è -> –Ω–∏–∑–∫–∞—è variance
model = Ridge(alpha=10.0)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π</h2>
    <div style="text-align: center; margin: 10px 0;">
      <!-- Duplicate image removed (hash: 7fbfdf00...) -->
    </div>

    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>Bias</th><th>Variance</th><th>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π</th></tr>
      <tr><td>Linear Regression</td><td>–í—ã—Å–æ–∫–∏–π</td><td>–ù–∏–∑–∫–∞—è</td><td>–ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å</td></tr>
      <tr><td>Polynomial (degree=10)</td><td>–ù–∏–∑–∫–∏–π</td><td>–í—ã—Å–æ–∫–∞—è</td><td>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</td></tr>
      <tr><td>Decision Tree (max_depth=20)</td><td>–ù–∏–∑–∫–∏–π</td><td>–í—ã—Å–æ–∫–∞—è</td><td>–ì–∏–±–∫–∞—è, –Ω–æ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–∞—è</td></tr>
      <tr><td>Random Forest</td><td>–°—Ä–µ–¥–Ω–∏–π</td><td>–ù–∏–∑–∫–∞—è</td><td>Bagging —Å–Ω–∏–∂–∞–µ—Ç variance</td></tr>
      <tr><td>Gradient Boosting</td><td>–ù–∏–∑–∫–∏–π</td><td>–°—Ä–µ–¥–Ω—è—è</td><td>Boosting —Å–Ω–∏–∂–∞–µ—Ç bias</td></tr>
      <tr><td>k-NN (k=1)</td><td>–ù–∏–∑–∫–∏–π</td><td>–í—ã—Å–æ–∫–∞—è</td><td>–û—á–µ–Ω—å –≥–∏–±–∫–∞—è</td></tr>
      <tr><td>k-NN (k=100)</td><td>–í—ã—Å–æ–∫–∏–π</td><td>–ù–∏–∑–∫–∞—è</td><td>–£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 11. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å –ø–æ–º–æ—â—å—é –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è</h2>
    <pre><code>from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    model, X, y, cv=5,
    train_sizes=np.linspace(0.1, 1.0, 10)
)

train_mean = train_scores.mean(axis=1)
val_mean = val_scores.mean(axis=1)

# –ê–Ω–∞–ª–∏–∑
gap = train_mean - val_mean

if gap[-1] > 0.1:
    print("High variance (overfitting)")
    print("Solution: —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö")
elif train_mean[-1] < 0.8:
    print("High bias (underfitting)")
    print("Solution: —É—Å–ª–æ–∂–Ω–∏—Ç—å –º–æ–¥–µ–ª—å")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ê–Ω—Å–∞–º–±–ª–∏ –∏ Bias-Variance</h2>
    <p><strong>Bagging</strong> (Random Forest):</p>
    <ul>
      <li>–°–Ω–∏–∂–∞–µ—Ç variance</li>
      <li>–ù–µ –º–µ–Ω—è–µ—Ç bias</li>
      <li>–£—Å—Ä–µ–¥–Ω—è–µ—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –º–æ–¥–µ–ª–∏</li>
    </ul>
    <p><strong>Boosting</strong> (XGBoost, AdaBoost):</p>
    <ul>
      <li>–°–Ω–∏–∂–∞–µ—Ç bias</li>
      <li>–ú–æ–∂–µ—Ç —É–≤–µ–ª–∏—á–∏—Ç—å variance</li>
      <li>–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –º–æ–¥–µ–ª–∏</li>
    </ul>
    <pre><code># Bagging —Å–Ω–∏–∂–∞–µ—Ç variance
from sklearn.ensemble import BaggingRegressor
bagging = BaggingRegressor(n_estimators=50)

# Boosting —Å–Ω–∏–∂–∞–µ—Ç bias
from sklearn.ensemble import GradientBoostingRegressor
boosting = GradientBoostingRegressor(n_estimators=100)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π workflow</h2>
    <ol>
      <li><strong>Baseline</strong>: –ø—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å (high bias)</li>
      <li><strong>–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞</strong>: learning curves</li>
      <li><strong>–ï—Å–ª–∏ underfitting</strong>: —É—Å–ª–æ–∂–Ω–∏—Ç—å –º–æ–¥–µ–ª—å</li>
      <li><strong>–ï—Å–ª–∏ overfitting</strong>: —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö</li>
      <li><strong>–ü–æ–≤—Ç–æ—Ä–∏—Ç—å</strong> –¥–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞</li>
    </ol>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ü–æ—Å—Ç—Ä–æ–∏—Ç—å learning curves</li>
      <li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å: high bias –∏–ª–∏ high variance?</li>
      <li>[ ] –î–ª—è high bias: —É—Å–ª–æ–∂–Ω–∏—Ç—å –º–æ–¥–µ–ª—å, –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏</li>
      <li>[ ] –î–ª—è high variance: —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö</li>
      <li>[ ] –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å cross-validation</li>
      <li>[ ] –ò–∑–º–µ—Ä–∏—Ç—å bias –∏ variance —á–∏—Å–ª–µ–Ω–Ω–æ</li>
      <li>[ ] –ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ó–∞–∫–ª—é—á–µ–Ω–∏–µ</h2>
    <blockquote>
      ¬´Bias-variance trade-off ‚Äî —ç—Ç–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è –≤ ML. –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å 
      –¥–µ–ª–∞–µ—Ç —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è (high bias), —Å–ª–æ–∂–Ω–∞—è ‚Äî —Å–ª–∏—à–∫–æ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞ 
      –∫ –¥–∞–Ω–Ω—ã–º (high variance). –ò—Å–∫—É—Å—Å—Ç–≤–æ ML ‚Äî –Ω–∞–π—Ç–∏ –∑–æ–ª–æ—Ç—É—é —Å–µ—Ä–µ–¥–∏–Ω—É. –≠—Ç–æ –∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ 
      –º–∏–∫—Ä–æ—Å–∫–æ–ø–∞: —Å–ª–∏—à–∫–æ–º –≥—Ä—É–±–æ ‚Äî –Ω–µ –≤–∏–¥–Ω–æ –¥–µ—Ç–∞–ª–µ–π, —Å–ª–∏—à–∫–æ–º —Ç–æ—á–Ω–æ ‚Äî –≤–∏–¥–Ω–æ —Ç–æ–ª—å–∫–æ —à—É–º¬ª.
    </blockquote>
  </div>


</div>
</body>
</html>

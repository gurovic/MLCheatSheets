<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–°—Ç–µ–º–º–∏–Ω–≥ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen { body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; color: #333; background: #fafcff; padding: 10px; } }
    @media print { body { background: white; padding: 0; } @page { size: A4 landscape; margin: 10mm; } }
    .container { column-count: 3; column-gap: 20px; max-width: 100%; }
    .block { break-inside: avoid; margin-bottom: 1.2em; padding: 12px; background: white; border-radius: 6px; box-shadow: 0 1px 3px rgba(0,0,0,0.05); }
    h1 { font-size: 1.6em; font-weight: 700; color: #1a5fb4; text-align: center; margin: 0 0 8px; column-span: all; }
    .subtitle { text-align: center; color: #666; font-size: 0.9em; margin-bottom: 12px; column-span: all; }
    h2 { font-size: 1.15em; font-weight: 700; color: #1a5fb4; margin: 0 0 8px; padding-bottom: 4px; border-bottom: 1px solid #e0e7ff; }
    p, ul, ol { font-size: 0.92em; margin: 0.6em 0; }
    ul, ol { padding-left: 18px; }
    li { margin-bottom: 4px; }
    code { font-family: 'Consolas', 'Courier New', monospace; background-color: #f0f4ff; padding: 1px 4px; border-radius: 3px; font-size: 0.88em; }
    pre { background-color: #f0f4ff; padding: 8px; border-radius: 4px; overflow-x: auto; font-size: 0.84em; margin: 6px 0; }
    pre code { padding: 0; background: none; white-space: pre-wrap; }
    table { width: 100%; border-collapse: collapse; font-size: 0.82em; margin: 6px 0; }
    th { background-color: #e6f0ff; text-align: left; padding: 4px 6px; font-weight: 600; }
    td { padding: 4px 6px; border-bottom: 1px solid #f0f4ff; }
    tr:nth-child(even) { background-color: #f8fbff; }
    blockquote { font-style: italic; margin: 8px 0; padding: 6px 10px; background: #f8fbff; border-left: 2px solid #1a5fb4; font-size: 0.88em; }
  </style>
</head>
<body>
<div class="container">
  <h1>‚úÇÔ∏è –°—Ç–µ–º–º–∏–Ω–≥ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è</h1>
  <div class="subtitle">üìÖ 5 —è–Ω–≤–∞—Ä—è 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã</h2>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å–ª–æ–≤ –∫ –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º–µ</li>
      <li><strong>Stemming</strong>: –æ—Ç—Å–µ—á–µ–Ω–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏–π (–≥—Ä—É–±–æ)</li>
      <li><strong>Lemmatization</strong>: –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ —Å–ª–æ–≤–∞—Ä–Ω–æ–π —Ñ–æ—Ä–º–µ (—Ç–æ—á–Ω–æ)</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: –ø–æ–∏—Å–∫, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, topic modeling</li>
      <li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ</strong>: —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è, –æ–±–æ–±—â–µ–Ω–∏–µ</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. Stemming vs Lemmatization</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>Stemming</th><th>Lemmatization</th></tr>
      <tr><td>–ú–µ—Ç–æ–¥</td><td>–ü—Ä–∞–≤–∏–ª–∞, —ç–≤—Ä–∏—Å—Ç–∏–∫–∏</td><td>–°–ª–æ–≤–∞—Ä—å, –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è</td></tr>
      <tr><td>–°–∫–æ—Ä–æ—Å—Ç—å</td><td>–ë—ã—Å—Ç—Ä–æ</td><td>–ú–µ–¥–ª–µ–Ω–Ω–µ–µ</td></tr>
      <tr><td>–¢–æ—á–Ω–æ—Å—Ç—å</td><td>–ù–∏–∂–µ</td><td>–í—ã—à–µ</td></tr>
      <tr><td>–†–µ–∑—É–ª—å—Ç–∞—Ç</td><td>–ú–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —Å–ª–æ–≤–æ–º</td><td>–í—Å–µ–≥–¥–∞ —Å–ª–æ–≤–æ</td></tr>
      <tr><td>–ü—Ä–∏–º–µ—Ä</td><td>running ‚Üí run</td><td>running ‚Üí run</td></tr>
      <tr><td>–ü—Ä–∏–º–µ—Ä</td><td>better ‚Üí better</td><td>better ‚Üí good</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. Porter Stemmer</h2>
    <p><strong>–°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π stemmer –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ:</strong></p>
    <pre><code>from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

words = [
    "running", "runs", "ran",
    "easily", "fairly",
    "maximum", "maximal"
]

for word in words:
    stem = stemmer.stem(word)
    print(f"{word:15s} ‚Üí {stem}")

# –†–µ–∑—É–ª—å—Ç–∞—Ç:
# running         ‚Üí run
# runs            ‚Üí run
# ran             ‚Üí ran
# easily          ‚Üí easili
# fairly          ‚Üí fairli
# maximum         ‚Üí maximum
# maximal         ‚Üí maxim</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Snowball Stemmer</h2>
    <p><strong>–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è Porter, –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è:</strong></p>
    <pre><code>from nltk.stem import SnowballStemmer

# English
en_stemmer = SnowballStemmer('english')
print(en_stemmer.stem("generously"))  # generous

# Russian
ru_stemmer = SnowballStemmer('russian')
words_ru = ["–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç", "–ø—Ä–æ–≥—Ä–∞–º–º–∞"]
for word in words_ru:
    print(f"{word} ‚Üí {ru_stemmer.stem(word)}")

# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏
print(SnowballStemmer.languages)
# ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french',
#  'german', 'hungarian', 'italian', 'norwegian', 'porter',
#  'portuguese', 'romanian', 'russian', 'spanish', 'swedish')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Lancaster Stemmer</h2>
    <p><strong>–ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π stemmer:</strong></p>
    <pre><code>from nltk.stem import LancasterStemmer

lancaster = LancasterStemmer()
porter = PorterStemmer()

words = ["maximum", "presumably", "multiply", "provision"]

for word in words:
    p = porter.stem(word)
    l = lancaster.stem(word)
    print(f"{word:15s} Porter: {p:10s} Lancaster: {l}")

# Lancaster –æ–±—Ä–µ–∑–∞–µ—Ç –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–µ–µ
# maximum         Porter: maximum     Lancaster: maxim
# presumably      Porter: presum      Lancaster: presum
# multiply        Porter: multipli    Lancaster: mult
# provision       Porter: provis      Lancaster: provid</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Lemmatization —Å NLTK</h2>
    <pre><code>from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

# –ë–µ–∑ POS tag
print(lemmatizer.lemmatize("running"))  # running
print(lemmatizer.lemmatize("better"))   # better
print(lemmatizer.lemmatize("rocks"))    # rock

# –° POS tag (–≤–∞–∂–Ω–æ!)
from nltk.corpus import wordnet

print(lemmatizer.lemmatize("running", pos='v'))  # run
print(lemmatizer.lemmatize("better", pos='a'))   # good
print(lemmatizer.lemmatize("rocks", pos='n'))    # rock
print(lemmatizer.lemmatize("rocks", pos='v'))    # rock

# POS tags: 'n'=noun, 'v'=verb, 'a'=adjective, 'r'=adverb</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. POS Tagging –¥–ª—è lemmatization</h2>
    <pre><code>from nltk import pos_tag, word_tokenize
from nltk.corpus import wordnet

def get_wordnet_pos(treebank_tag):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è POS tag –≤ WordNet —Ñ–æ—Ä–º–∞—Ç"""
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

def lemmatize_sentence(sentence):
    tokens = word_tokenize(sentence)
    pos_tags = pos_tag(tokens)
    
    lemmatizer = WordNetLemmatizer()
    lemmas = []
    
    for word, pos in pos_tags:
        wordnet_pos = get_wordnet_pos(pos)
        lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)
        lemmas.append(lemma)
    
    return lemmas

sentence = "The cats are running and jumping better"
lemmas = lemmatize_sentence(sentence)
print(lemmas)
# ['The', 'cat', 'be', 'run', 'and', 'jump', 'well']</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. spaCy Lemmatization</h2>
    <p><strong>–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –ª—É—á—à–∏–º quality:</strong></p>
    <pre><code>import spacy

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
nlp = spacy.load('en_core_web_sm')

text = "The striped bats are hanging better on the wires"
doc = nlp(text)

for token in doc:
    print(f"{token.text:15s} ‚Üí {token.lemma_:15s} POS: {token.pos_}")

# –†–µ–∑—É–ª—å—Ç–∞—Ç:
# The             ‚Üí the              POS: DET
# striped         ‚Üí strip            POS: VERB
# bats            ‚Üí bat              POS: NOUN
# are             ‚Üí be               POS: AUX
# hanging         ‚Üí hang             POS: VERB
# better          ‚Üí well             POS: ADV
# on              ‚Üí on               POS: ADP
# the             ‚Üí the              POS: DET
# wires           ‚Üí wire             POS: NOUN</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Russian Lemmatization</h2>
    <pre><code># pymorphy2 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
import pymorphy2

morph = pymorphy2.MorphAnalyzer()

words = ["–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–ª", "–∫—Ä–∞—Å–∏–≤–µ–µ"]

for word in words:
    parsed = morph.parse(word)[0]
    lemma = parsed.normal_form
    print(f"{word:20s} ‚Üí {lemma}")

# –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ    ‚Üí –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
# –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç         ‚Üí –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç
# –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–ª      ‚Üí –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞—Ç—å
# –∫—Ä–∞—Å–∏–≤–µ–µ            ‚Üí –∫—Ä–∞—Å–∏–≤—ã–π

# spaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
nlp_ru = spacy.load('ru_core_news_sm')
doc = nlp_ru("–ü—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç—ã –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–ª–∏ –ø—Ä–æ–≥—Ä–∞–º–º—É")
for token in doc:
    print(f"{token.text} ‚Üí {token.lemma_}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–°–∫–æ—Ä–æ—Å—Ç—å (1M words)</th><th>–ö–∞—á–µ—Å—Ç–≤–æ</th></tr>
      <tr><td>Porter Stemmer</td><td>~2 —Å–µ–∫</td><td>–ù–∏–∑–∫–æ–µ</td></tr>
      <tr><td>Snowball Stemmer</td><td>~3 —Å–µ–∫</td><td>–°—Ä–µ–¥–Ω–µ–µ</td></tr>
      <tr><td>NLTK Lemmatizer</td><td>~30 —Å–µ–∫</td><td>–•–æ—Ä–æ—à–µ–µ</td></tr>
      <tr><td>spaCy</td><td>~60 —Å–µ–∫</td><td>–û—Ç–ª–∏—á–Ω–æ–µ</td></tr>
    </table>
    <pre><code>import time

text_list = documents * 1000  # –±–æ–ª—å—à–æ–π —Å–ø–∏—Å–æ–∫

start = time.time()
stemmed = [porter.stem(word) for doc in text_list 
           for word in doc.split()]
print(f"Porter: {time.time() - start:.2f}s")

start = time.time()
lemmatized = [lemmatizer.lemmatize(word) 
              for doc in text_list for word in doc.split()]
print(f"Lemmatizer: {time.time() - start:.2f}s")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –í pipeline preprocessing</h2>
    <pre><code>import re
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

def preprocess_text(text, method='lemma'):
    # Lowercase
    text = text.lower()
    
    # Remove punctuation and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    
    # Tokenize
    tokens = text.split()
    
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [t for t in tokens if t not in stop_words]
    
    # Stemming or Lemmatization
    if method == 'stem':
        stemmer = SnowballStemmer('english')
        tokens = [stemmer.stem(t) for t in tokens]
    elif method == 'lemma':
        lemmatizer = WordNetLemmatizer()
        tokens = [lemmatizer.lemmatize(t) for t in tokens]
    
    return ' '.join(tokens)

text = "The cats were running and jumping better than before!"
print(preprocess_text(text, 'stem'))
print(preprocess_text(text, 'lemma'))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á—Ç–æ</h2>
    <table>
      <tr><th>–ó–∞–¥–∞—á–∞</th><th>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è</th></tr>
      <tr><td>–ü–æ–∏—Å–∫</td><td>Stemming (—Å–∫–æ—Ä–æ—Å—Ç—å)</td></tr>
      <tr><td>Sentiment Analysis</td><td>Lemmatization (—Ç–æ—á–Ω–æ—Å—Ç—å)</td></tr>
      <tr><td>Topic Modeling</td><td>Lemmatization</td></tr>
      <tr><td>Text Classification</td><td>–û–±–∞ —Ä–∞–±–æ—Ç–∞—é—Ç, —Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ</td></tr>
      <tr><td>Named Entity Recognition</td><td>–ù–∏ —Ç–æ –Ω–∏ –¥—Ä—É–≥–æ–µ</td></tr>
      <tr><td>Machine Translation</td><td>Lemmatization</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç—å</h2>
    <pre><code># Snowball –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤
languages = {
    'en': 'english',
    'ru': 'russian',
    'de': 'german',
    'fr': 'french',
    'es': 'spanish'
}

def multilingual_stem(text, lang='en'):
    stemmer = SnowballStemmer(languages[lang])
    return ' '.join([
        stemmer.stem(word) 
        for word in text.split()
    ])

# spaCy –¥–ª—è lemmatization
models = {
    'en': 'en_core_web_sm',
    'ru': 'ru_core_news_sm',
    'de': 'de_core_news_sm'
}

def multilingual_lemma(text, lang='en'):
    nlp = spacy.load(models[lang])
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏</h2>
    <ul>
      <li><strong>Irregular verbs</strong>: stemming –Ω–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è (went ‚Üí went)</li>
      <li><strong>–û–º–æ–Ω–∏–º—ã</strong>: –Ω—É–∂–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç (rock-–∫–∞–º–µ–Ω—å vs rock-–∫–∞—á–∞—Ç—å—Å—è)</li>
      <li><strong>Compound words</strong>: –º–æ–≥—É—Ç —Ä–∞–∑–±–∏–≤–∞—Ç—å—Å—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ</li>
      <li><strong>Neologisms</strong>: –Ω–æ–≤—ã–µ —Å–ª–æ–≤–∞ –º–æ–≥—É—Ç –Ω–µ –±—ã—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ</li>
      <li><strong>Domain-specific</strong>: –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä–µ–π</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <ul>
      <li>–ù–∞—á–Ω–∏—Ç–µ —Å–æ stemming –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞</li>
      <li>Lemmatization –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏</li>
      <li>–í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ POS tags –¥–ª—è lemmatization</li>
      <li>spaCy –ª—É—á—à–µ NLTK –¥–ª—è production</li>
      <li>–ö—ç—à–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏</li>
      <li>–ù–µ –ø—Ä–∏–º–µ–Ω—è–π—Ç–µ –∫ –∏–º–µ–Ω–∞–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º</li>
      <li>–¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ –≤–∞—à–µ–º –¥–æ–º–µ–Ω–µ</li>
    </ul>
    <blockquote>
      üí° <strong>–û–±—ä—è—Å–Ω–µ–Ω–∏–µ:</strong> ¬´–ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ —Å–ª–æ–≤–∞ –∫ –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º–µ: "–±–µ–≥–∞–ª", "–±–µ–≥–∞–µ—Ç", "–±–µ–∂–∏—Ç" ‚Üí "–±–µ–∂–∞—Ç—å". –≠—Ç–æ —É–º–µ–Ω—å—à–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –∏ —É–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–µ–Ω–∏–µ –≤ –º–æ–¥–µ–ª—è—Ö NLP¬ª.
    </blockquote>
  </div>

</div>
</div>
</body>
</html>

<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>GPT –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ GPT –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. GPT: –æ—Å–Ω–æ–≤—ã</h2>
    <p><strong>GPT</strong> (Generative Pre-trained Transformer) ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Ç OpenAI</p>
    <ul>
      <li><strong>Decoder-only</strong>: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ decoder —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞</li>
      <li><strong>–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è</strong>: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º</li>
      <li><strong>–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ</strong>: –Ω–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ—Ä–ø—É—Å–∞—Ö</li>
      <li><strong>Fine-tuning</strong>: –∞–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 2. –≠–≤–æ–ª—é—Ü–∏—è GPT</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–ì–æ–¥</th><th>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å</th></tr>
      <tr><td><strong>GPT-1</strong></td><td>2018</td><td>117M</td><td>Proof of concept</td></tr>
      <tr><td><strong>GPT-2</strong></td><td>2019</td><td>1.5B</td><td>Zero-shot learning</td></tr>
      <tr><td><strong>GPT-3</strong></td><td>2020</td><td>175B</td><td>Few-shot in-context</td></tr>
      <tr><td><strong>ChatGPT</strong></td><td>2022</td><td>~175B</td><td>RLHF, –¥–∏–∞–ª–æ–≥–∏</td></tr>
      <tr><td><strong>GPT-4</strong></td><td>2023</td><td>~1.7T*</td><td>Multimodal</td></tr>
    </table>
    <p style="font-size: 0.8em;">*–æ—Ü–µ–Ω–∫–∞, —Ç–æ—á–Ω–æ–µ —á–∏—Å–ª–æ –Ω–µ —Ä–∞—Å–∫—Ä—ã—Ç–æ</p>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GPT</h2>
    <p>–û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ <strong>Transformer Decoder</strong> —Å causal masking</p>
    <ul>
      <li><strong>Self-attention</strong>: —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ç–æ–∫–µ–Ω—ã</li>
      <li><strong>Causal mask</strong>: –∑–∞–ø—Ä–µ—â–∞–µ—Ç —Å–º–æ—Ç—Ä–µ—Ç—å –≤ –±—É–¥—É—â–µ–µ</li>
      <li><strong>Layer Norm</strong>: –¥–æ self-attention</li>
      <li><strong>Residual connections</strong>: –º–µ–∂–¥—É —Å–ª–æ—è–º–∏</li>
      <li><strong>Position encoding</strong>: learned positional embeddings</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ GPT</h2>
    <p><strong>–ó–∞–¥–∞—á–∞</strong>: –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω (next token prediction)</p>
    <pre><code># –¶–µ–ª—å –æ–±—É—á–µ–Ω–∏—è
P(x_t | x_1, x_2, ..., x_{t-1})

# Loss —Ñ—É–Ω–∫—Ü–∏—è
L = -‚àë log P(x_t | x_<t)</code></pre>
    <p>–ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π:</p>
    <ul>
      <li>–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç</li>
      <li>–ü–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç</li>
      <li>–í—ã–ø–æ–ª–Ω—è—Ç—å zero-shot –∑–∞–¥–∞—á–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ —Å GPT-2</h2>
    <pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
prompt = "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç ‚Äî —ç—Ç–æ"
inputs = tokenizer.encode(prompt, return_tensors='pt')

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
outputs = model.generate(
    inputs,
    max_length=100,
    num_return_sequences=1,
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    do_sample=True
)

text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(text)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ú–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>Greedy</strong></td><td>–í—ã–±–æ—Ä —Å–∞–º–æ–≥–æ –≤–µ—Ä–æ—è—Ç–Ω–æ–≥–æ</td><td>–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ</td></tr>
      <tr><td><strong>Beam Search</strong></td><td>–ü–æ–∏—Å–∫ –ª—É—á—à–∏—Ö K –ø—É—Ç–µ–π</td><td>–ë–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ</td></tr>
      <tr><td><strong>Sampling</strong></td><td>–°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏</td><td>–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ</td></tr>
      <tr><td><strong>Top-K</strong></td><td>Sampling –∏–∑ K –ª—É—á—à–∏—Ö</td><td>–ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞</td></tr>
      <tr><td><strong>Top-P (nucleus)</strong></td><td>Sampling –∏–∑ —Ç–æ–ø-P –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏</td><td>–ë–∞–ª–∞–Ω—Å</td></tr>
      <tr><td><strong>Temperature</strong></td><td>–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ logits</td><td>–ö–æ–Ω—Ç—Ä–æ–ª—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. Temperature –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏</h2>
    <p><strong>Temperature</strong> –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏</p>
    <pre><code># –§–æ—Ä–º—É–ª–∞
P(x) = exp(logits / T) / sum(exp(logits / T))

# T = 0.1 ‚Üí –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ
# T = 1.0 ‚Üí —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
# T = 2.0 ‚Üí –±–æ–ª–µ–µ —Å–ª—É—á–∞–π–Ω–æ</code></pre>
    <ul>
      <li><strong>T ‚Üí 0</strong>: –ø–æ—á—Ç–∏ greedy, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —Ç–µ–∫—Å—Ç</li>
      <li><strong>T = 0.7-0.9</strong>: –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ, –Ω–æ —Å–≤—è–∑–Ω–æ</li>
      <li><strong>T > 1.0</strong>: –æ—á–µ–Ω—å —Å–ª—É—á–∞–π–Ω–æ, –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–≤—è–∑–Ω–æ</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 8. Zero-shot, Few-shot, Fine-tuning</h2>
    <table>
      <tr><th>–ü–æ–¥—Ö–æ–¥</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–ü—Ä–∏–º–µ—Ä—ã</th></tr>
      <tr><td><strong>Zero-shot</strong></td><td>–ë–µ–∑ –ø—Ä–∏–º–µ—Ä–æ–≤, —Ç–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç</td><td>0</td></tr>
      <tr><td><strong>One-shot</strong></td><td>–û–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –≤ –ø—Ä–æ–º–ø—Ç–µ</td><td>1</td></tr>
      <tr><td><strong>Few-shot</strong></td><td>–ù–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (3-10)</td><td>3-10</td></tr>
      <tr><td><strong>Fine-tuning</strong></td><td>–î–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö</td><td>1000+</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 9. In-context Learning</h2>
    <p>GPT-3 —É–º–µ–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è <strong>–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ</strong> –±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤</p>
    <pre><code># Few-shot –ø—Ä–∏–º–µ—Ä
prompt = """
–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π:

–ü—Ä–∏–≤–µ—Ç ‚Üí Hello
–°–ø–∞—Å–∏–±–æ ‚Üí Thank you
–ö–∞–∫ –¥–µ–ª–∞? ‚Üí How are you?
–ü–æ–∫–∞ ‚Üí """

# GPT –ø—Ä–µ–¥—Å–∫–∞–∂–µ—Ç: Goodbye</code></pre>
    <p>–ú–æ–¥–µ–ª—å "–ø–æ–Ω–∏–º–∞–µ—Ç" –ø–∞—Ç—Ç–µ—Ä–Ω –∏–∑ –ø—Ä–∏–º–µ—Ä–æ–≤</p>
  </div>

  <div class="block">
    <h2>üî∑ 10. ChatGPT –∏ RLHF</h2>
    <p><strong>RLHF</strong> (Reinforcement Learning from Human Feedback)</p>
    <ol>
      <li><strong>Supervised Fine-tuning</strong>: –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∏–∞–ª–æ–≥–∞—Ö</li>
      <li><strong>Reward Model</strong>: –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤</li>
      <li><strong>PPO</strong>: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ —Å –Ω–∞–≥—Ä–∞–¥–∞–º–∏</li>
    </ol>
    <blockquote>
      RLHF –¥–µ–ª–∞–µ—Ç –º–æ–¥–µ–ª—å –±–æ–ª–µ–µ –ø–æ–ª–µ–∑–Ω–æ–π, –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –∏ —Å–ª–µ–¥—É—é—â–µ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 11. GPT-4: —É–ª—É—á—à–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>Multimodal</strong>: –ø–æ–Ω–∏–º–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è + —Ç–µ–∫—Å—Ç</li>
      <li><strong>–ë–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</strong>: –ª—É—á—à–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ</li>
      <li><strong>–î–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç</strong>: –¥–æ 32K —Ç–æ–∫–µ–Ω–æ–≤</li>
      <li><strong>–ú–µ–Ω—å—à–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π</strong>: –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã</li>
      <li><strong>–°–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º</strong>: –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞–µ—Ç –∑–∞–ø—Ä–æ—Å—ã</li>
      <li><strong>–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å</strong>: –º–µ–Ω—å—à–µ —Ç–æ–∫—Å–∏—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 12. Fine-tuning GPT</h2>
    <pre><code>from transformers import GPT2LMHeadModel, Trainer
from transformers import TrainingArguments

model = GPT2LMHeadModel.from_pretrained('gpt2')

training_args = TrainingArguments(
    output_dir='./gpt2-finetuned',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=5e-5,
    warmup_steps=500,
    save_steps=1000,
    logging_steps=100
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

trainer.train()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è GPT</h2>
    <ul>
      <li><strong>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞</strong>: —Å—Ç–∞—Ç—å–∏, –∏—Å—Ç–æ—Ä–∏–∏, –∫–æ–¥</li>
      <li><strong>–î–∏–∞–ª–æ–≥–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã</strong>: —á–∞—Ç–±–æ—Ç—ã, –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã</li>
      <li><strong>–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è</strong>: –∫—Ä–∞—Ç–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ</li>
      <li><strong>–ü–µ—Ä–µ–≤–æ–¥</strong>: –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥</li>
      <li><strong>Q&A</strong>: –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã</li>
      <li><strong>Code generation</strong>: Copilot, CodeGen</li>
      <li><strong>Creative writing</strong>: —Å—Ç–∏—Ö–∏, —Å—Ü–µ–Ω–∞—Ä–∏–∏</li>
      <li><strong>Data augmentation</strong>: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ü—Ä–æ–±–ª–µ–º—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>–ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏</strong>: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–æ–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</li>
      <li><strong>Bias</strong>: –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li><strong>Context window</strong>: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã</li>
      <li><strong>–í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã</strong>: –¥–æ—Ä–æ–≥–æ –æ–±—É—á–∞—Ç—å –∏ –∑–∞–ø—É—Å–∫–∞—Ç—å</li>
      <li><strong>–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ reasoning</strong>: —Å–ª–æ–∂–Ω–æ —Å –ª–æ–≥–∏–∫–æ–π</li>
      <li><strong>–ù–µ –æ–±—É—á–∞–µ—Ç—Å—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏</strong>: –∑–Ω–∞–Ω–∏—è –¥–æ –¥–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã GPT</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å</th></tr>
      <tr><td><strong>LLaMA</strong></td><td>Meta</td><td>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è, open-source</td></tr>
      <tr><td><strong>Claude</strong></td><td>Anthropic</td><td>Constitutional AI</td></tr>
      <tr><td><strong>PaLM</strong></td><td>Google</td><td>Pathways architecture</td></tr>
      <tr><td><strong>Mistral</strong></td><td>Mistral AI</td><td>–û—Ç–∫—Ä—ã—Ç–∞—è, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è</td></tr>
      <tr><td><strong>Gemini</strong></td><td>Google</td><td>Multimodal, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç GPT-4</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 16. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è production</h2>
    <ul>
      <li><strong>Quantization</strong>: 8-bit, 4-bit (GPTQ, AWQ)</li>
      <li><strong>LoRA</strong>: efficient fine-tuning</li>
      <li><strong>Flash Attention</strong>: —É—Å–∫–æ—Ä–µ–Ω–∏–µ attention</li>
      <li><strong>KV cache</strong>: –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏</li>
      <li><strong>Model pruning</strong>: —É–¥–∞–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤</li>
      <li><strong>Distillation</strong>: –≤ –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å</li>
    </ul>
    <pre><code># –ó–∞–≥—Ä—É–∑–∫–∞ —Å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º
from transformers import GPT2LMHeadModel

model = GPT2LMHeadModel.from_pretrained(
    'gpt2',
    load_in_8bit=True,
    device_map='auto'
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. Prompt Engineering</h2>
    <p>–ò—Å–∫—É—Å—Å—Ç–≤–æ —Å–æ–∑–¥–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤</p>
    <ul>
      <li><strong>Clear instructions</strong>: —á–µ—Ç–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏</li>
      <li><strong>Context</strong>: –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç</li>
      <li><strong>Examples</strong>: few-shot –ø—Ä–∏–º–µ—Ä—ã</li>
      <li><strong>Format</strong>: —É–∫–∞–∑–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞</li>
      <li><strong>Constraints</strong>: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</li>
      <li><strong>Reasoning</strong>: "think step by step"</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</h3>
        <ul>
          <li>–ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞</li>
          <li>Zero-shot –∏ few-shot —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏</li>
          <li>–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á</li>
          <li>–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏</li>
        </ul>
      </div>
      <div class="bad">
        <h3>–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h3>
        <ul>
          <li>–ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –∏ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏</li>
          <li>–î–æ—Ä–æ–≥–æ –æ–±—É—á–∞—Ç—å –∏ –∑–∞–ø—É—Å–∫–∞—Ç—å</li>
          <li>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞</li>
          <li>Bias –∏ —ç—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 19. –ß–µ–∫-–ª–∏—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è</h2>
    <ol>
      <li>‚úì –í—ã–±—Ä–∞—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (GPT-2, GPT-3, GPT-4)</li>
      <li>‚úì –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∑–∞–¥–∞—á—É (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è, Q&A, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)</li>
      <li>‚úì –°–æ–∑–¥–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç</li>
      <li>‚úì –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (temperature, top-p)</li>
      <li>‚úì –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö</li>
      <li>‚úì –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏</li>
      <li>‚úì –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è production (quantization)</li>
      <li>‚úì –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∏ –∑–∞—Ç—Ä–∞—Ç—ã</li>
    </ol>
  </div>

</div>

</body>
</html>

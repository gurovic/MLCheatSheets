<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>–ù–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ –ù–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å –º–µ—Ç–æ–¥–∞</h2>
    <ul>
      <li><strong>–ò–¥–µ—è</strong>: –Ω–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—Ç—å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ–æ—Ä–º—É –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</li>
      <li><strong>–ì–∏–±–∫–æ—Å—Ç—å</strong>: –º–æ–¥–µ–ª—å –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –¥–∞–Ω–Ω—ã–º</li>
      <li><strong>–ë–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</strong>: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ä–∞—Å—Ç–µ—Ç —Å –¥–∞–Ω–Ω—ã–º–∏</li>
      <li><strong>–õ–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å</strong>: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –±–ª–∏–∑–∫–∏—Ö —Ç–æ—á–∫–∞—Ö</li>
      <li><strong>–°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ</strong>: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –≥–ª–∞–¥–∫–æ—Å—Ç—å—é</li>
    </ul>

    </div>
<div class="block">
    <h2>üî∑ 2. Vs –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>–ü–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è</th><th>–ù–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è</th></tr>
      <tr><td>–§–æ—Ä–º–∞</td><td>–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è (y=wx+b)</td><td>–ü—Ä–æ–∏–∑–≤–æ–ª—å–Ω–∞—è</td></tr>
      <tr><td>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</td><td>–ü–æ—Å—Ç–æ—è–Ω–Ω–æ–µ —á–∏—Å–ª–æ</td><td>–†–∞—Å—Ç–µ—Ç —Å –¥–∞–Ω–Ω—ã–º–∏</td></tr>
      <tr><td>–ì–∏–±–∫–æ—Å—Ç—å</td><td>–ù–∏–∑–∫–∞—è</td><td>–í—ã—Å–æ–∫–∞—è</td></tr>
      <tr><td>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</td><td>–ü—Ä–æ—Å—Ç–∞—è</td><td>–°–ª–æ–∂–Ω–∞—è</td></tr>
      <tr><td>–ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ</td><td>–õ—É—á—à–µ</td><td>–•—É–∂–µ</td></tr>
      <tr><td>–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</td><td>–ù–∏–∂–µ —Ä–∏—Å–∫</td><td>–í—ã—à–µ —Ä–∏—Å–∫</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. k-NN —Ä–µ–≥—Ä–µ—Å—Å–∏—è</h2>
    <p>–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ = —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–π k –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π</p>
    <pre><code>from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∞–∂–Ω–æ!
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# k-NN —Ä–µ–≥—Ä–µ—Å—Å–∏—è
knn_reg = KNeighborsRegressor(
    n_neighbors=5,
    weights='distance',  # –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é
    metric='euclidean'
)

knn_reg.fit(X_train_scaled, y_train)
y_pred = knn_reg.predict(X_test_scaled)

from sklearn.metrics import mean_squared_error, r2_score
print(f"MSE: {mean_squared_error(y_test, y_pred):.3f}")
print(f"R¬≤: {r2_score(y_test, y_pred):.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Kernel Ridge Regression</h2>
    <p>–ö–æ–º–±–∏–Ω–∞—Ü–∏—è —è–¥–µ—Ä–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏</p>
    <pre><code>from sklearn.kernel_ridge import KernelRidge

# –†–∞–∑–ª–∏—á–Ω—ã–µ —è–¥—Ä–∞
kr_rbf = KernelRidge(
    alpha=1.0,      # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    kernel='rbf',   # –ì–∞—É—Å—Å–æ–≤–æ —è–¥—Ä–æ
    gamma=0.1       # –®–∏—Ä–∏–Ω–∞ —è–¥—Ä–∞
)

kr_poly = KernelRidge(
    alpha=1.0,
    kernel='poly',
    degree=3
)

kr_rbf.fit(X_train, y_train)
y_pred = kr_rbf.predict(X_test)

# –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
from sklearn.model_selection import GridSearchCV

params = {
    'alpha': [0.1, 1.0, 10.0],
    'gamma': [0.01, 0.1, 1.0]
}

grid = GridSearchCV(KernelRidge(kernel='rbf'), 
                   params, cv=5)
grid.fit(X_train, y_train)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. LOESS/LOWESS</h2>
    <p>Locally Weighted Scatterplot Smoothing</p>
    <pre><code>from scipy.signal import savgol_filter
import statsmodels.api as sm

# LOWESS —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ (–¥–ª—è 1D)
lowess = sm.nonparametric.lowess

# frac = –¥–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
smoothed = lowess(y, X, frac=0.3)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.scatter(X, y, alpha=0.5, label='–î–∞–Ω–Ω—ã–µ')
plt.plot(smoothed[:, 0], smoothed[:, 1], 
         'r-', linewidth=2, label='LOWESS')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()</code></pre>

    <p><strong>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</strong>:</p>
    <ul>
      <li><code>frac</code>: –¥–æ–ª—è —Ç–æ—á–µ–∫ –≤ –æ–∫–Ω–µ (0.3 = 30%)</li>
      <li><code>it</code>: —á–∏—Å–ª–æ —Ä–æ–±–∞—Å—Ç–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π</li>
      <li>–ú–µ–Ω—å—à–µ frac ‚Üí –±–æ–ª–µ–µ –∏–∑–ª–æ–º–∞–Ω–Ω–∞—è –∫—Ä–∏–≤–∞—è</li>
      <li>–ë–æ–ª—å—à–µ frac ‚Üí –±–æ–ª–µ–µ –≥–ª–∞–¥–∫–∞—è –∫—Ä–∏–≤–∞—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 6. Gaussian Process Regression</h2>
    <p>–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥</p>
    <pre><code>from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel

# –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–¥—Ä–æ
kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)

# –ú–æ–¥–µ–ª—å
gp = GaussianProcessRegressor(
    kernel=kernel,
    n_restarts_optimizer=10,
    alpha=0.1,
    normalize_y=True
)

gp.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
y_pred, sigma = gp.predict(X_test, return_std=True)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, label='–û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ')
plt.plot(X_test, y_pred, 'r-', label='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ')
plt.fill_between(X_test.ravel(), 
                 y_pred - 1.96*sigma, 
                 y_pred + 1.96*sigma,
                 alpha=0.2, label='95% CI')
plt.legend()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –Ø–¥—Ä–∞ –¥–ª—è GP</h2>
    <pre><code>from sklearn.gaussian_process.kernels import (
    RBF, Matern, RationalQuadratic, 
    ExpSineSquared, DotProduct, WhiteKernel
)

# RBF (–≥–∞—É—Å—Å–æ–≤–æ) - –≥–ª–∞–¥–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
kernel_rbf = RBF(length_scale=1.0)

# Mat√©rn - –∫–æ–Ω—Ç—Ä–æ–ª—å –≥–ª–∞–¥–∫–æ—Å—Ç–∏
kernel_matern = Matern(length_scale=1.0, nu=1.5)

# –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
kernel_periodic = ExpSineSquared(
    length_scale=1.0,
    periodicity=2.0
)

# –ö–æ–º–±–∏–Ω–∞—Ü–∏–∏ —è–¥–µ—Ä
kernel_combined = RBF(1.0) * Matern(1.0) + WhiteKernel(0.1)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
gp = GaussianProcessRegressor(kernel=kernel_combined)
gp.fit(X_train, y_train)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Nadaraya-Watson —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä</h2>
    <p>–Ø–¥–µ—Ä–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –≤–µ—Å–∞–º–∏</p>
    <pre><code>from sklearn.neighbors import KernelDensity
import numpy as np

def nadaraya_watson(X_train, y_train, X_test, bandwidth=1.0):
    """–Ø–¥–µ—Ä–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è Nadaraya-Watson"""
    predictions = []
    
    for x_test in X_test:
        # –í—ã—á–∏—Å–ª–∏—Ç—å –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
        distances = np.linalg.norm(X_train - x_test, axis=1)
        
        # –ì–∞—É—Å—Å–æ–≤–æ —è–¥—Ä–æ
        weights = np.exp(-(distances**2) / (2 * bandwidth**2))
        weights /= weights.sum()
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ
        y_pred = np.dot(weights, y_train)
        predictions.append(y_pred)
    
    return np.array(predictions)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
y_pred = nadaraya_watson(X_train, y_train, X_test, bandwidth=0.5)

# –ü–æ–¥–±–æ—Ä bandwidth —á–µ—Ä–µ–∑ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é
from sklearn.model_selection import cross_val_score

bandwidths = np.logspace(-2, 1, 10)
scores = []

for bw in bandwidths:
    y_cv_pred = nadaraya_watson(X_train, y_train, X_val, bandwidth=bw)
    mse = mean_squared_error(y_val, y_cv_pred)
    scores.append(mse)

best_bandwidth = bandwidths[np.argmin(scores)]</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –°–ø–ª–∞–π–Ω—ã (Splines)</h2>
    <p>–ö—É—Å–æ—á–Ω–æ-–ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è</p>
    <pre><code>from scipy.interpolate import UnivariateSpline, BSpline
import numpy as np

# B-splines —á–µ—Ä–µ–∑ scipy
spline = UnivariateSpline(X.ravel(), y, s=1.0)  # s = smoothing factor
y_pred = spline(X_test.ravel())

# Cubic spline (—á–µ—Ä–µ–∑ numpy)
from scipy.interpolate import CubicSpline
cs = CubicSpline(X.ravel(), y)
y_pred = cs(X_test.ravel())

# Penalized B-splines —á–µ—Ä–µ–∑ patsy
import patsy
from sklearn.linear_model import Ridge

# –°–æ–∑–¥–∞—Ç—å –±–∞–∑–∏—Å —Å–ø–ª–∞–π–Ω–æ–≤
bs_basis = patsy.dmatrix("bs(x, df=10)", {"x": X.ravel()})
bs_test = patsy.dmatrix("bs(x, df=10)", {"x": X_test.ravel()})

# –†–µ–≥—Ä–µ—Å—Å–∏—è –Ω–∞ –±–∞–∑–∏—Å–µ
ridge = Ridge(alpha=1.0)
ridge.fit(bs_basis, y)
y_pred = ridge.predict(bs_test)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –õ–æ–∫–∞–ª—å–Ω–∞—è –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è</h2>
    <pre><code>def local_polynomial_regression(X_train, y_train, x_test, 
                               degree=2, bandwidth=1.0):
    """–õ–æ–∫–∞–ª—å–Ω–∞—è –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è"""
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    
    predictions = []
    
    for x in x_test:
        # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ –≤—Å–µ—Ö —Ç–æ—á–µ–∫
        distances = np.abs(X_train - x)
        
        # –í—ã—á–∏—Å–ª–∏—Ç—å –≤–µ—Å–∞ (–≥–∞—É—Å—Å–æ–≤–æ —è–¥—Ä–æ)
        weights = np.exp(-(distances**2) / (2 * bandwidth**2))
        weights = weights.ravel()
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
        poly = PolynomialFeatures(degree=degree)
        X_poly = poly.fit_transform(X_train)
        
        # Weighted least squares
        lr = LinearRegression()
        lr.fit(X_poly, y_train, sample_weight=weights)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        x_poly = poly.transform([[x]])
        y_pred = lr.predict(x_poly)
        predictions.append(y_pred[0])
    
    return np.array(predictions)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
y_pred = local_polynomial_regression(
    X_train, y_train, X_test, 
    degree=2, bandwidth=0.5
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Decision Tree Regression</h2>
    <p>–ù–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –∫—É—Å–æ—á–Ω–æ-–∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è</p>
    <pre><code>from sklearn.tree import DecisionTreeRegressor

# –î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π
tree_reg = DecisionTreeRegressor(
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10
)

tree_reg.fit(X_train, y_train)
y_pred = tree_reg.predict(X_test)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ä–µ–≤–∞
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(tree_reg, filled=True, feature_names=['X'])
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 12. Random Forest Regression</h2>
    <p>–ê–Ω—Å–∞–º–±–ª—å –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤</p>
    <pre><code>from sklearn.ensemble import RandomForestRegressor

rf_reg = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    random_state=42
)

rf_reg.fit(X_train, y_train)
y_pred = rf_reg.predict(X_test)

# Feature importance
importances = rf_reg.feature_importances_
print("Feature importances:", importances)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –í—ã–±–æ—Ä bandwidth/smoothing</h2>
    <p><strong>–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è bandwidth:</strong></p>
    <pre><code>from sklearn.model_selection import KFold

def cv_bandwidth_selection(X, y, bandwidths, n_splits=5):
    """–í—ã–±—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π bandwidth —á–µ—Ä–µ–∑ CV"""
    kf = KFold(n_splits=n_splits)
    cv_scores = {bw: [] for bw in bandwidths}
    
    for train_idx, val_idx in kf.split(X):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        for bw in bandwidths:
            y_pred = nadaraya_watson(X_train, y_train, X_val, bandwidth=bw)
            mse = mean_squared_error(y_val, y_pred)
            cv_scores[bw].append(mse)
    
    # –£—Å—Ä–µ–¥–Ω–∏—Ç—å –ø–æ —Ñ–æ–ª–¥–∞–º
    avg_scores = {bw: np.mean(scores) for bw, scores in cv_scores.items()}
    best_bw = min(avg_scores, key=avg_scores.get)
    
    return best_bw, avg_scores

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
bandwidths = np.logspace(-2, 1, 20)
best_bw, scores = cv_bandwidth_selection(X_train, y_train, bandwidths)
print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π bandwidth: {best_bw:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. Bias-Variance Trade-off</h2>
    <p>–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ–º –∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º</p>
    <ul>
      <li><strong>–ë–æ–ª—å—à–æ–π bandwidth/k</strong>: –≤—ã—Å–æ–∫–æ–µ —Å–º–µ—â–µ–Ω–∏–µ, –Ω–∏–∑–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è (–≥–ª–∞–¥–∫–∞—è, –Ω–æ –Ω–µ—Ç–æ—á–Ω–∞—è)</li>
      <li><strong>–ú–∞–ª—ã–π bandwidth/k</strong>: –Ω–∏–∑–∫–æ–µ —Å–º–µ—â–µ–Ω–∏–µ, –≤—ã—Å–æ–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è (—Ç–æ—á–Ω–∞—è, –Ω–æ –∏–∑—Ä–µ–∑–∞–Ω–Ω–∞—è)</li>
    </ul>
    <pre><code># –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–Ω—ã—Ö bandwidth
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

bandwidths = [0.05, 0.1, 0.5, 1.0, 2.0, 5.0]

for idx, bw in enumerate(bandwidths):
    ax = axes[idx // 3, idx % 3]
    
    y_pred = nadaraya_watson(X_train, y_train, X_plot, bandwidth=bw)
    
    ax.scatter(X_train, y_train, alpha=0.3)
    ax.plot(X_plot, y_pred, 'r-', linewidth=2)
    ax.set_title(f'Bandwidth = {bw}')
    ax.set_xlabel('X')
    ax.set_ylabel('y')

plt.tight_layout()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–°–ª–æ–∂–Ω–æ—Å—Ç—å</th><th>–ì–ª–∞–¥–∫–æ—Å—Ç—å</th><th>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</th></tr>
      <tr><td><strong>k-NN</strong></td><td>O(n)</td><td>–ö—É—Å–æ—á–Ω–æ-–∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–∞—è</td><td>–ü—Ä–æ—Å—Ç–∞—è</td></tr>
      <tr><td><strong>Kernel Ridge</strong></td><td>O(n¬≥)</td><td>–ì–ª–∞–¥–∫–∞—è</td><td>–°—Ä–µ–¥–Ω—è—è</td></tr>
      <tr><td><strong>GP</strong></td><td>O(n¬≥)</td><td>–û—á–µ–Ω—å –≥–ª–∞–¥–∫–∞—è</td><td>–°–ª–æ–∂–Ω–∞—è</td></tr>
      <tr><td><strong>LOWESS</strong></td><td>O(n¬≤)</td><td>–õ–æ–∫–∞–ª—å–Ω–æ –≥–ª–∞–¥–∫–∞—è</td><td>–ü—Ä–æ—Å—Ç–∞—è</td></tr>
      <tr><td><strong>Splines</strong></td><td>O(n)</td><td>–ö—É—Å–æ—á–Ω–æ-–≥–ª–∞–¥–∫–∞—è</td><td>–°—Ä–µ–¥–Ω—è—è</td></tr>
      <tr><td><strong>Trees</strong></td><td>O(n log n)</td><td>–ö—É—Å–æ—á–Ω–æ-–∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–∞—è</td><td>–ü—Ä–æ—Å—Ç–∞—è</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ú–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è</h2>
    <p>–î–ª—è –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (X ‚àà ‚Ñù·µà)</p>
    <pre><code># k-NN —Ä–∞–±–æ—Ç–∞–µ—Ç out-of-the-box
from sklearn.neighbors import KNeighborsRegressor

knn = KNeighborsRegressor(n_neighbors=10, weights='distance')
knn.fit(X_train, y_train)  # X_train –º–æ–∂–µ—Ç –±—ã—Ç—å –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–º

# Kernel Ridge —Ç–∞–∫–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
from sklearn.kernel_ridge import KernelRidge

kr = KernelRidge(alpha=1.0, kernel='rbf', gamma=0.1)
kr.fit(X_train, y_train)

# –î–ª—è LOWESS –Ω—É–∂–Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GAM (Generalized Additive Models)
from pygam import LinearGAM, s

gam = LinearGAM(s(0) + s(1) + s(2))  # –ü–æ –æ–¥–Ω–æ–º—É —Å–ø–ª–∞–π–Ω—É –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫
gam.fit(X_train, y_train)
y_pred = gam.predict(X_test)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. –ì–µ–Ω–µ—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∞–¥–¥–∏—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ (GAM)</h2>
    <p>–ê–¥–¥–∏—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å —Å –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏</p>
    <pre><code>from pygam import LinearGAM, s, f, te

# y = f‚ÇÅ(x‚ÇÅ) + f‚ÇÇ(x‚ÇÇ) + ... + f‚Çö(x‚Çö)

# –ü—Ä–æ—Å—Ç–∞—è GAM
gam = LinearGAM(s(0) + s(1) + s(2))
gam.fit(X, y)

# –° –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è–º–∏
gam_interaction = LinearGAM(
    s(0) + s(1) + te(0, 1)  # te = tensor product (–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ)
)
gam_interaction.fit(X, y)

# –° —Ñ–∞–∫—Ç–æ—Ä–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
gam_factor = LinearGAM(s(0) + f(1))  # f = factor (–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π)
gam_factor.fit(X, y)

# –ü–æ–¥–±–æ—Ä –ª—è–º–±–¥—ã (smoothing penalty)
gam.gridsearch(X, y)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
import matplotlib.pyplot as plt
fig, axes = plt.subplots(1, X.shape[1], figsize=(15, 5))

for i, ax in enumerate(axes):
    XX = gam.generate_X_grid(term=i)
    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))
    ax.set_title(f'Feature {i}')

plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –•–æ—Ä–æ—à–æ</h3>
        <ul>
          <li>–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ñ–æ—Ä–º–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</li>
          <li>–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ, —Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã</li>
          <li>–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö (n > 100)</li>
          <li>–ù–∏–∑–∫–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (d < 10)</li>
          <li>–ù—É–∂–Ω–∞ –≥–∏–±–∫–æ—Å—Ç—å –º–æ–¥–µ–ª–∏</li>
          <li>–õ–æ–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–∞–∂–Ω—ã</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ü–ª–æ—Ö–æ</h3>
        <ul>
          <li>–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö (n < 50)</li>
          <li>–í—ã—Å–æ–∫–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (d > 20)</li>
          <li>–ù—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</li>
          <li>–õ–∏–Ω–µ–π–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å</li>
          <li>–¢—Ä–µ–±—É–µ—Ç—Å—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è</li>
          <li>–ù—É–∂–Ω–∞ –±—ã—Å—Ç—Ä–∞—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 19. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è</h2>
    <pre><code># –û—Å—Ç–∞—Ç–∫–∏ (residuals)
residuals = y_test - y_pred

# –ì—Ä–∞—Ñ–∏–∫ –æ—Å—Ç–∞—Ç–∫–æ–≤
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(y_pred, residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
plt.ylabel('–û—Å—Ç–∞—Ç–∫–∏')
plt.title('Residual Plot')

plt.subplot(1, 2, 2)
plt.hist(residuals, bins=30, edgecolor='black')
plt.xlabel('–û—Å—Ç–∞—Ç–∫–∏')
plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤')

plt.tight_layout()
plt.show()

# Q-Q plot –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏
from scipy import stats
stats.probplot(residuals, dist="norm", plot=plt)
plt.title('Q-Q Plot')
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 20. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ (–¥–ª—è –º–µ—Ç–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–π –º–µ—Ç–æ–¥ (k-NN, GP, splines, GAM)</li>
      <li>[ ] –ü–æ–¥–æ–±—Ä–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (bandwidth, k, alpha) —á–µ—Ä–µ–∑ CV</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö (< 10 –ª—É—á—à–µ)</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –æ—Å—Ç–∞—Ç–∫–∏</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (train vs test)</li>
      <li>[ ] –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ (MSE, R¬≤, MAE)</li>
      <li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∫–æ–º–±–∏–Ω–∞—Ü–∏—é —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏</li>
      <li>[ ] –î–ª—è –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GAM</li>
      <li>[ ] –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–ù–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è ‚Äî —ç—Ç–æ –∫–∞–∫ —Ä–∏—Å–æ–≤–∞–Ω–∏–µ –∫—Ä–∏–≤–æ–π "–Ω–∞ –≥–ª–∞–∑" —á–µ—Ä–µ–∑ —Ç–æ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –Ω–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—è –∑–∞—Ä–∞–Ω–µ–µ, —á—Ç–æ —ç—Ç–æ –ø—Ä—è–º–∞—è –ª–∏–Ω–∏—è –∏–ª–∏ –ø–∞—Ä–∞–±–æ–ª–∞. –ú–æ–¥–µ–ª—å –≥–∏–±–∫–æ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –ª—é–±–æ–π —Ñ–æ—Ä–º–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—É—é –≤–∏–¥–∏—Ç –≤ –¥–∞–Ω–Ω—ã—Ö, —É—á–∏—Ç—ã–≤–∞—è –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã¬ª.
    </blockquote>
  </div>



</div>
</body>
</html>

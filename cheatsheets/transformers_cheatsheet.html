<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>ü§ñ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>–†–µ–≤–æ–ª—é—Ü–∏—è</strong>: –∑–∞–º–µ–Ω–∏–ª–∏ RNN –≤ NLP</li>
      <li><strong>Attention</strong>: —Ñ–æ–∫—É—Å –Ω–∞ –≤–∞–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö</li>
      <li><strong>–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º</strong>: –±—ã—Å—Ç—Ä–µ–µ RNN</li>
      <li><strong>SOTA</strong>: –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ NLP, CV</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã</h2>
    <ul>
      <li><strong>Self-Attention</strong>: —Å–≤—è–∑—å –º–µ–∂–¥—É –≤—Å–µ–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏</li>
      <li><strong>Multi-Head Attention</strong>: –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö attention</li>
      <li><strong>Positional Encoding</strong>: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–∑–∏—Ü–∏–∏</li>
      <li><strong>Feed-Forward</strong>: –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏</li>
      <li><strong>Layer Normalization</strong>: —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è</li>
      <li><strong>Residual Connections</strong>: skip connections</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. Hugging Face Transformers</h2>
    <pre><code>from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    Trainer, 
    TrainingArguments
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name, 
    num_labels=2
)

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
texts = ["This is great!", "This is bad."]
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
outputs = model(**inputs)
predictions = outputs.logits.argmax(dim=-1)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Self-Attention –º–µ—Ö–∞–Ω–∏–∑–º</h2>
    <p><strong>–§–æ—Ä–º—É–ª–∞:</strong></p>
    <pre><code>Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) V

–≥–¥–µ:
Q = Query (–∑–∞–ø—Ä–æ—Å)
K = Key (–∫–ª—é—á)
V = Value (–∑–Ω–∞—á–µ–Ω–∏–µ)
d_k = —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–ª—é—á–∞</code></pre>

    <p><strong>–ò–Ω—Ç—É–∏—Ü–∏—è:</strong></p>
    <ul>
      <li>Q —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç: "–ß—Ç–æ –º–Ω–µ –≤–∞–∂–Ω–æ?"</li>
      <li>K –æ—Ç–≤–µ—á–∞–µ—Ç: "–Ø —Å–æ–¥–µ—Ä–∂—É —ç—Ç–æ"</li>
      <li>V –¥–∞–µ—Ç: "–í–æ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏</h2>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–ì–æ–¥</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏</th></tr>
      <tr><td><strong>BERT</strong></td><td>2018</td><td>Bidirectional, –ø–æ–Ω–∏–º–∞–Ω–∏–µ</td></tr>
      <tr><td><strong>GPT-2/3</strong></td><td>2019/2020</td><td>Autoregressive, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è</td></tr>
      <tr><td><strong>T5</strong></td><td>2019</td><td>Text-to-text framework</td></tr>
      <tr><td><strong>RoBERTa</strong></td><td>2019</td><td>–£–ª—É—á—à–µ–Ω–Ω—ã–π BERT</td></tr>
      <tr><td><strong>ALBERT</strong></td><td>2019</td><td>–õ–µ–≥–∫–∏–π BERT</td></tr>
      <tr><td><strong>DistilBERT</strong></td><td>2019</td><td>60% –±—ã—Å—Ç—Ä–µ–µ, 95% —Ç–æ—á–Ω–æ—Å—Ç–∏</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 6. Fine-tuning BERT</h2>
    <pre><code>from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    learning_rate=2e-5,
    save_strategy="epoch",
    evaluation_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

trainer.train()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞</h2>
    <pre><code>from transformers import pipeline

# Sentiment analysis
classifier = pipeline("sentiment-analysis")
result = classifier("I love transformers!")
print(result)  # [{'label': 'POSITIVE', 'score': 0.99}]

# Custom model
classifier = pipeline(
    "text-classification",
    model="distilbert-base-uncased-finetuned-sst-2-english"
)

results = classifier([
    "This is wonderful!",
    "This is terrible."
])
print(results)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Named Entity Recognition</h2>
    <pre><code># NER pipeline
ner = pipeline("ner", grouped_entities=True)

text = "Hugging Face is based in New York City"
entities = ner(text)

for entity in entities:
    print(f"{entity['word']}: {entity['entity_group']} ({entity['score']:.2f})")

# Output:
# Hugging Face: ORG (0.99)
# New York City: LOC (0.99)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞</h2>
    <pre><code># GPT-2 –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
generator = pipeline("text-generation", model="gpt2")

result = generator(
    "Once upon a time",
    max_length=50,
    num_return_sequences=1,
    temperature=0.7
)

print(result[0]['generated_text'])

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
# max_length: –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
# temperature: –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (0.7-1.0)
# num_return_sequences: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Vision Transformer (ViT)</h2>
    <pre><code>from transformers import ViTForImageClassification, ViTFeatureExtractor

model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')
feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
from PIL import Image
image = Image.open('cat.jpg')

inputs = feature_extractor(images=image, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits

predicted_class = logits.argmax(-1).item()
print(model.config.id2label[predicted_class])</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –•–æ—Ä–æ—à–æ</h3>
        <ul>
          <li>NLP –∑–∞–¥–∞—á–∏ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, NER, Q&A)</li>
          <li>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞</li>
          <li>–ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥</li>
          <li>Computer Vision (ViT)</li>
          <li>–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ü–ª–æ—Ö–æ</h3>
        <ul>
          <li>–û–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã</li>
          <li>–ú–∞–ª—ã–π –¥–∞—Ç–∞—Å–µ—Ç –±–µ–∑ transfer learning</li>
          <li>–ù—É–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å</li>
          <li>Real-time inference –Ω–∞ CPU</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (BERT, GPT)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏</li>
      <li>[ ] Fine-tune –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Hugging Face –±–∏–±–ª–∏–æ—Ç–µ–∫—É</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å learning rate (2e-5 - 5e-5)</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å gradient accumulation –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π</li>
      <li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å DistilBERT –¥–ª—è production</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU/TPU</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä ‚Äî —ç—Ç–æ –∫–∞–∫ –æ—á–µ–Ω—å –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π —á–∏—Ç–∞—Ç–µ–ª—å: –æ–Ω –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤–∏–¥–∏—Ç –≤—Å—ë –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏ –ø–æ–Ω–∏–º–∞–µ—Ç, –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –≤–∞–∂–Ω—ã –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–º—ã—Å–ª–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —á–∏—Ç–∞–µ—Ç —Å–ª–µ–≤–∞ –Ω–∞–ø—Ä–∞–≤–æ, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –≤–∏–¥–∏—Ç –≤—Å—ë —Å—Ä–∞–∑—É¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

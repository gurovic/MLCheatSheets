<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–¢–µ–æ—Ä–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ ML Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>ÔøΩÔøΩ –¢–µ–æ—Ä–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ Machine Learning</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –í–≤–µ–¥–µ–Ω–∏–µ</h2>
    <p><strong>–¢–µ–æ—Ä–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</strong> ‚Äî –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞ –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —Å–∂–∞—Ç–∏—è –¥–∞–Ω–Ω—ã—Ö.</p>
    <ul>
      <li><strong>–û—Å–Ω–æ–≤–∞—Ç–µ–ª—å</strong>: Claude Shannon (1948)</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ ML</strong>:
        <ul>
          <li>–§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å (Cross-Entropy)</li>
          <li>–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Mutual Information)</li>
          <li>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Information Bottleneck)</li>
          <li>–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ (VAE, GAN)</li>
        </ul>
      </li>
    </ul>
    <blockquote>üí° "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è ‚Äî —ç—Ç–æ —Ç–æ, —á—Ç–æ —É–º–µ–Ω—å—à–∞–µ—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å"</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 2. –≠–Ω—Ç—Ä–æ–ø–∏—è –®–µ–Ω–Ω–æ–Ω–∞</h2>
    <p><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: –º–µ—Ä–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ —Å–ª—É—á–∞–π–Ω–æ–π –≤–µ–ª–∏—á–∏–Ω—ã</p>
    <p style="text-align: center; font-size: 1.1em;">H(X) = -Œ£ p(x) log p(x)</p>
    
    <p><strong>–°–≤–æ–π—Å—Ç–≤–∞</strong>:</p>
    <ul>
      <li>H(X) ‚â• 0 (–≤—Å–µ–≥–¥–∞ –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞)</li>
      <li>H(X) = 0 –∫–æ–≥–¥–∞ X –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∞</li>
      <li>H(X) –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è</li>
      <li>–î–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π X: H(X) ‚â§ log |X|</li>
    </ul>

    <pre><code>import numpy as np

def entropy(p):
    """–í—ã—á–∏—Å–ª–∏—Ç—å —ç–Ω—Ç—Ä–æ–ø–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è p"""
    # –£–±—Ä–∞—Ç—å –Ω—É–ª–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è log(0)
    p = p[p > 0]
    return -np.sum(p * np.log2(p))

# –ü—Ä–∏–º–µ—Ä—ã
# –ú–æ–Ω–µ—Ç–∞: p(–æ—Ä–µ–ª)=0.5, p(—Ä–µ—à–∫–∞)=0.5
p_fair = np.array([0.5, 0.5])
print(f"–ß–µ—Å—Ç–Ω–∞—è –º–æ–Ω–µ—Ç–∞: {entropy(p_fair):.3f} bits")  # 1.0

# –ù–µ—á–µ—Å—Ç–Ω–∞—è –º–æ–Ω–µ—Ç–∞: p(–æ—Ä–µ–ª)=0.9, p(—Ä–µ—à–∫–∞)=0.1
p_biased = np.array([0.9, 0.1])
print(f"–ù–µ—á–µ—Å—Ç–Ω–∞—è –º–æ–Ω–µ—Ç–∞: {entropy(p_biased):.3f} bits")  # 0.469

# –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è: p(–æ—Ä–µ–ª)=1.0
p_det = np.array([1.0, 0.0])
print(f"–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è: {entropy(p_det):.3f} bits")  # 0.0

# –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è)
n = 8
p_uniform = np.ones(n) / n
print(f"–†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ (8 –∏—Å—Ö–æ–¥–æ–≤): {entropy(p_uniform):.3f} bits")  # 3.0</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –£—Å–ª–æ–≤–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è</h2>
    <p><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: —ç–Ω—Ç—Ä–æ–ø–∏—è Y –ø—Ä–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ–º X</p>
    <p>H(Y|X) = Œ£ p(x) H(Y|X=x)</p>
    <p>H(Y|X) = H(X,Y) - H(X)</p>
    
    <p><strong>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</strong>: –æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å Y –ø–æ—Å–ª–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è X</p>

    <pre><code>def conditional_entropy(joint_p):
    """
    –£—Å–ª–æ–≤–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è H(Y|X)
    joint_p: —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ p(x,y)
    """
    # –ú–∞—Ä–≥–∏–Ω–∞–ª—å–Ω–æ–µ p(x)
    p_x = joint_p.sum(axis=1, keepdims=True)
    
    # H(X,Y)
    h_xy = entropy(joint_p.flatten())
    
    # H(X)
    h_x = entropy(p_x.flatten())
    
    # H(Y|X) = H(X,Y) - H(X)
    return h_xy - h_x

# –ü—Ä–∏–º–µ—Ä: X –∏ Y –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã
joint_indep = np.array([
    [0.25, 0.25],
    [0.25, 0.25]
])
print(f"H(Y|X) (–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ): {conditional_entropy(joint_indep):.3f}")  # 1.0

# X –∏ Y –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–∏—Å–∏–º—ã
joint_dep = np.array([
    [0.5, 0.0],
    [0.0, 0.5]
])
print(f"H(Y|X) (–∑–∞–≤–∏—Å–∏–º—ã–µ): {conditional_entropy(joint_dep):.3f}")  # 0.0</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –í–∑–∞–∏–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (Mutual Information)</h2>
    <p><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ—Å—É—Ç –¥—Ä—É–≥ –æ –¥—Ä—É–≥–µ</p>
    <p>I(X;Y) = H(X) + H(Y) - H(X,Y)</p>
    <p>I(X;Y) = H(Y) - H(Y|X) = H(X) - H(X|Y)</p>
    
    <p><strong>–°–≤–æ–π—Å—Ç–≤–∞</strong>:</p>
    <ul>
      <li>I(X;Y) ‚â• 0 (–Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞)</li>
      <li>I(X;Y) = 0 ‚ü∫ X –∏ Y –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã</li>
      <li>I(X;Y) = I(Y;X) (—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞)</li>
      <li>I(X;X) = H(X) (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ–±–µ = —ç–Ω—Ç—Ä–æ–ø–∏—è)</li>
    </ul>

    <pre><code>from sklearn.feature_selection import mutual_info_classif, mutual_info_regression

# –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
mi_scores = mutual_info_classif(X, y, random_state=42)

# –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ MI
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'mi_score': mi_scores
}).sort_values('mi_score', ascending=False)

print(feature_importance)

# –û—Ç–æ–±—Ä–∞—Ç—å —Ç–æ–ø-K –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
from sklearn.feature_selection import SelectKBest

selector = SelectKBest(
    mutual_info_classif, 
    k=10
)
X_selected = selector.fit_transform(X, y)

# –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
mi_reg_scores = mutual_info_regression(X, y)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ö—É–ª—å–±–∞–∫–∞-–õ–µ–π–±–ª–µ—Ä–∞ (KL Divergence)</h2>
    <p><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: –º–µ—Ä–∞ —Ä–∞–∑–ª–∏—á–∏—è –¥–≤—É—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π</p>
    <p>D_KL(P||Q) = Œ£ p(x) log(p(x)/q(x))</p>
    
    <p><strong>–°–≤–æ–π—Å—Ç–≤–∞</strong>:</p>
    <ul>
      <li>D_KL(P||Q) ‚â• 0 (–Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞)</li>
      <li>D_KL(P||Q) = 0 ‚ü∫ P = Q</li>
      <li>‚ùå –ù–µ—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞: D_KL(P||Q) ‚â† D_KL(Q||P)</li>
      <li>‚ùå –ù–µ —è–≤–ª—è–µ—Ç—Å—è –º–µ—Ç—Ä–∏–∫–æ–π</li>
    </ul>

    <pre><code>def kl_divergence(p, q, epsilon=1e-10):
    """
    KL –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è –º–µ–∂–¥—É p –∏ q
    epsilon: –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
    """
    p = p + epsilon
    q = q + epsilon
    return np.sum(p * np.log(p / q))

# –ü—Ä–∏–º–µ—Ä
p = np.array([0.7, 0.2, 0.1])
q = np.array([0.3, 0.5, 0.2])

print(f"D_KL(P||Q) = {kl_divergence(p, q):.3f}")
print(f"D_KL(Q||P) = {kl_divergence(q, p):.3f}")  # —Ä–∞–∑–Ω—ã–µ!

# PyTorch
import torch
import torch.nn.functional as F

p_tensor = torch.tensor([0.7, 0.2, 0.1])
q_tensor = torch.tensor([0.3, 0.5, 0.2])

kl_loss = F.kl_div(
    q_tensor.log(), 
    p_tensor, 
    reduction='sum'
)
print(f"KL (PyTorch): {kl_loss:.3f}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Cross-Entropy Loss</h2>
    <p><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏—Ç –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–±—ã—Ç–∏–π –∏–∑ P –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–¥ –¥–ª—è Q</p>
    <p>H(P,Q) = -Œ£ p(x) log q(x)</p>
    <p>H(P,Q) = H(P) + D_KL(P||Q)</p>
    
    <p><strong>–í ML</strong>: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</p>

    <pre><code># –ë–∏–Ω–∞—Ä–Ω–∞—è cross-entropy
from sklearn.metrics import log_loss

y_true = [1, 0, 1, 1, 0]
y_pred_proba = [0.9, 0.1, 0.8, 0.7, 0.2]

bce = log_loss(y_true, y_pred_proba)
print(f"Binary Cross-Entropy: {bce:.4f}")

# Categorical cross-entropy (multi-class)
from tensorflow.keras.losses import CategoricalCrossentropy

y_true = [[0, 1, 0], [1, 0, 0], [0, 0, 1]]
y_pred = [[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.2, 0.2, 0.6]]

cce = CategoricalCrossentropy()
loss = cce(y_true, y_pred)
print(f"Categorical Cross-Entropy: {loss:.4f}")

# –í—Ä—É—á–Ω—É—é
def cross_entropy(y_true, y_pred, epsilon=1e-10):
    """Cross-entropy –¥–ª—è one-hot encoded labels"""
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.sum(y_true * np.log(y_pred))

# PyTorch
import torch.nn as nn

criterion = nn.CrossEntropyLoss()
# –ü—Ä–∏–Ω–∏–º–∞–µ—Ç logits –∏ class indices
logits = torch.randn(3, 5)  # batch=3, classes=5
targets = torch.tensor([1, 0, 4])
loss = criterion(logits, targets)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. Information Gain (–¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ —Ä–µ—à–µ–Ω–∏–π)</h2>
    <p><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: —É–º–µ–Ω—å—à–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –ø–æ—Å–ª–µ split</p>
    <p>IG(S, A) = H(S) - Œ£ (|S_v|/|S|) √ó H(S_v)</p>
    
    <p>–ì–¥–µ S ‚Äî –¥–∞—Ç–∞—Å–µ—Ç, A ‚Äî –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è split, S_v ‚Äî –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø–æ—Å–ª–µ split</p>

    <pre><code>def information_gain(y, splits):
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å information gain –¥–ª—è split
    y: —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
    splits: list –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤ –ø–æ—Å–ª–µ split
    """
    # –≠–Ω—Ç—Ä–æ–ø–∏—è –¥–æ split
    h_before = entropy_from_labels(y)
    
    # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è –ø–æ—Å–ª–µ split
    n_total = len(y)
    h_after = 0
    
    for split in splits:
        n_split = len(split)
        if n_split > 0:
            weight = n_split / n_total
            h_after += weight * entropy_from_labels(split)
    
    return h_before - h_after

def entropy_from_labels(y):
    """–≠–Ω—Ç—Ä–æ–ø–∏—è –¥–ª—è –º–∞—Å—Å–∏–≤–∞ –º–µ—Ç–æ–∫"""
    _, counts = np.unique(y, return_counts=True)
    p = counts / len(y)
    return entropy(p)

# –ü—Ä–∏–º–µ—Ä: –≤—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–∞ –¥–ª—è split
from sklearn.tree import DecisionTreeClassifier

# DecisionTrees –∏—Å–ø–æ–ª—å–∑—É—é—Ç information gain (Gini –∏–ª–∏ entropy)
tree = DecisionTreeClassifier(
    criterion='entropy',  # –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å entropy (info gain)
    max_depth=5
)
tree.fit(X_train, y_train)

# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ~ accumulated information gain
feature_importance = tree.feature_importances_</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Gini Impurity</h2>
    <p><strong>–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏</strong> –¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ (–±—ã—Å—Ç—Ä–µ–µ –≤—ã—á–∏—Å–ª—è—Ç—å)</p>
    <p>Gini(S) = 1 - Œ£ p_i¬≤</p>
    
    <p><strong>–°–≤—è–∑—å —Å —ç–Ω—Ç—Ä–æ–ø–∏–µ–π</strong>:</p>
    <ul>
      <li>–û–±–µ –º–µ—Ä—ã –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏</li>
      <li>Gini –±—ã—Å—Ç—Ä–µ–µ (–Ω–µ—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º–æ–≤)</li>
      <li>–ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –¥–∞—é—Ç –ø–æ—Ö–æ–∂–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã</li>
      <li>Entropy –±–æ–ª–µ–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–∞</li>
    </ul>

    <pre><code>def gini_impurity(y):
    """–í—ã—á–∏—Å–ª–∏—Ç—å Gini impurity"""
    _, counts = np.unique(y, return_counts=True)
    p = counts / len(y)
    return 1 - np.sum(p**2)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —ç–Ω—Ç—Ä–æ–ø–∏–µ–π
y = np.array([0, 0, 0, 1, 1, 1])

h = entropy_from_labels(y)
g = gini_impurity(y)

print(f"Entropy: {h:.4f}")
print(f"Gini: {g:.4f}")

# Decision tree —Å Gini
tree_gini = DecisionTreeClassifier(criterion='gini')
tree_gini.fit(X_train, y_train)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Information Bottleneck</h2>
    <p><strong>–ü—Ä–∏–Ω—Ü–∏–ø</strong>: —Å–∂–∞—Ç—å X –¥–æ T, —Å–æ—Ö—Ä–∞–Ω—è—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ Y</p>
    <p>–ú–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å: I(X;T) - Œ≤√óI(T;Y)</p>
    
    <ul>
      <li>I(X;T): —Å–∂–∞—Ç–∏–µ (–Ω–∞—Å–∫–æ–ª—å–∫–æ T –ø—Ä–æ—â–µ X)</li>
      <li>I(T;Y): —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (–Ω–∞—Å–∫–æ–ª—å–∫–æ T –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç Y)</li>
      <li>Œ≤: trade-off –ø–∞—Ä–∞–º–µ—Ç—Ä</li>
    </ul>

    <p><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è</p>

    <blockquote>üí° –¢–µ–æ—Ä–∏—è IB –æ–±—ä—è—Å–Ω—è–µ—Ç, –ø–æ—á–µ–º—É –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å–Ω–∞—á–∞–ª–∞ –∑–∞–ø–æ–º–∏–Ω–∞—é—Ç (—É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç I(X;T)), –∞ –ø–æ—Ç–æ–º —Å–∂–∏–º–∞—é—Ç (—É–º–µ–Ω—å—à–∞—é—Ç I(X;T) –ø—Ä–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º I(T;Y))</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 10. Variational Autoencoder (VAE) –∏ ELBO</h2>
    <p><strong>VAE –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç Evidence Lower Bound (ELBO)</strong>:</p>
    <p>ELBO = E[log p(x|z)] - D_KL(q(z|x)||p(z))</p>
    
    <ul>
      <li>–ü–µ—Ä–≤—ã–π —á–ª–µ–Ω: reconstruction loss</li>
      <li>–í—Ç–æ—Ä–æ–π —á–ª–µ–Ω: KL regularization (–ø—Ä–∏–±–ª–∏–∑–∏—Ç—å prior)</li>
    </ul>

    <pre><code>import torch
import torch.nn as nn

class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        # Encoder
        self.fc1 = nn.Linear(input_dim, 400)
        self.fc_mu = nn.Linear(400, latent_dim)
        self.fc_logvar = nn.Linear(400, latent_dim)
        
        # Decoder
        self.fc3 = nn.Linear(latent_dim, 400)
        self.fc4 = nn.Linear(400, input_dim)
    
    def encode(self, x):
        h = torch.relu(self.fc1(x))
        return self.fc_mu(h), self.fc_logvar(h)
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        h = torch.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h))
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def vae_loss(recon_x, x, mu, logvar):
    """VAE loss = reconstruction + KL divergence"""
    # Reconstruction loss (binary cross-entropy)
    BCE = nn.functional.binary_cross_entropy(
        recon_x, x, reduction='sum'
    )
    
    # KL divergence: D_KL(q(z|x) || p(z))
    # –≥–¥–µ p(z) = N(0,I)
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    
    return BCE + KLD

# –û–±—É—á–µ–Ω–∏–µ
model = VAE(input_dim=784, latent_dim=20)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(10):
    for batch in dataloader:
        x = batch[0]
        
        recon_x, mu, logvar = model(x)
        loss = vae_loss(recon_x, x, mu, logvar)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Jensen-Shannon Divergence</h2>
    <p><strong>–°–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –≤–µ—Ä—Å–∏—è KL divergence</strong>:</p>
    <p>JSD(P||Q) = ¬ΩD_KL(P||M) + ¬ΩD_KL(Q||M)</p>
    <p>–ì–¥–µ M = ¬Ω(P + Q)</p>
    
    <p><strong>–°–≤–æ–π—Å—Ç–≤–∞</strong>:</p>
    <ul>
      <li>‚úÖ –°–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞: JSD(P||Q) = JSD(Q||P)</li>
      <li>‚úÖ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∞: 0 ‚â§ JSD ‚â§ log(2)</li>
      <li>‚úÖ –ö–≤–∞–¥—Ä–∞—Ç–Ω—ã–π –∫–æ—Ä–µ–Ω—å JSD ‚Äî –º–µ—Ç—Ä–∏–∫–∞</li>
    </ul>

    <pre><code>def js_divergence(p, q):
    """Jensen-Shannon Divergence"""
    m = 0.5 * (p + q)
    return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)

# –ü—Ä–∏–º–µ—Ä
p = np.array([0.7, 0.2, 0.1])
q = np.array([0.3, 0.5, 0.2])

jsd = js_divergence(p, q)
print(f"JSD(P||Q) = {jsd:.4f}")
print(f"JSD(Q||P) = {js_divergence(q, p):.4f}")  # —Ç–æ –∂–µ —Å–∞–º–æ–µ!

# Scipy
from scipy.spatial.distance import jensenshannon

jsd_scipy = jensenshannon(p, q)**2  # scipy –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç sqrt(JSD)
print(f"JSD (scipy): {jsd_scipy:.4f}")</code></pre>

    <p><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ GAN</strong>: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GAN –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç JSD –º–µ–∂–¥—É —Ä–µ–∞–ª—å–Ω—ã–º –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏</p>
  </div>

  <div class="block">
    <h2>üî∑ 12. Perplexity</h2>
    <p><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: –º–µ—Ä–∞ "—É–¥–∏–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏" –º–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã–º–∏</p>
    <p>Perplexity = 2^H(P) = 2^(-Œ£ p(x) log‚ÇÇ p(x))</p>
    
    <p><strong>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</strong>: —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–≤–Ω–æ–≤–µ—Ä–æ—è—Ç–Ω—ã—Ö –∏—Å—Ö–æ–¥–æ–≤</p>

    <pre><code>def perplexity(p):
    """–í—ã—á–∏—Å–ª–∏—Ç—å perplexity —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
    h = entropy(p)  # –≤ –±–∏—Ç–∞—Ö (log2)
    return 2 ** h

# –ü—Ä–∏–º–µ—Ä
# –ß–µ—Å—Ç–Ω–∞—è –º–æ–Ω–µ—Ç–∞
p_fair = np.array([0.5, 0.5])
print(f"Perplexity (fair): {perplexity(p_fair):.1f}")  # 2.0

# 6-–≥—Ä–∞–Ω–Ω–∞—è –∫–æ—Å—Ç—å
p_dice = np.ones(6) / 6
print(f"Perplexity (dice): {perplexity(p_dice):.1f}")  # 6.0

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö
def language_model_perplexity(model, text):
    """Perplexity —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ–∫—Å—Ç–µ"""
    log_probs = []
    
    for token in text:
        prob = model.predict_proba(token)
        log_probs.append(np.log2(prob))
    
    avg_log_prob = np.mean(log_probs)
    return 2 ** (-avg_log_prob)

# –ß–µ–º –Ω–∏–∂–µ perplexity, —Ç–µ–º –ª—É—á—à–µ –º–æ–¥–µ–ª—å</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. Fisher Information</h2>
    <p><strong>–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ</strong>: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –∫–æ—Ç–æ—Ä—É—é –¥–∞–Ω–Ω—ã–µ –Ω–µ—Å—É—Ç –æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –º–æ–¥–µ–ª–∏</p>
    <p>I(Œ∏) = E[(‚àÇlog p(x|Œ∏)/‚àÇŒ∏)¬≤]</p>
    
    <p><strong>–°–≤–æ–π—Å—Ç–≤–∞</strong>:</p>
    <ul>
      <li>–ö—Ä–∞–º–µ—Ä-–†–∞–æ –Ω–∏–∂–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞: Var(Œ∏ÃÇ) ‚â• 1/I(Œ∏)</li>
      <li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Natural Gradient Descent</li>
      <li>–°–≤—è–∑–∞–Ω–∞ —Å –∫—Ä–∏–≤–∏–∑–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å</li>
    </ul>

    <pre><code># Natural Gradient (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç Fisher Information)
# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ: Œ∏ ‚Üê Œ∏ - Œ± √ó F^(-1) √ó ‚àáL
# –≥–¥–µ F ‚Äî Fisher Information Matrix

import torch

def natural_gradient_step(model, loss, fisher_matrix, lr=0.01):
    """
    Natural gradient descent —à–∞–≥
    """
    # –û–±—ã—á–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç
    grads = torch.autograd.grad(loss, model.parameters())
    
    # Natural gradient: F^(-1) √ó g
    nat_grads = []
    for grad in grads:
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—É—é –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—é F
        nat_grad = grad / (fisher_matrix + 1e-8)
        nat_grads.append(nat_grad)
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    with torch.no_grad():
        for param, nat_grad in zip(model.parameters(), nat_grads):
            param -= lr * nat_grad
    
    return nat_grads</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. Differential Entropy (–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è)</h2>
    <p><strong>–î–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–µ–ª–∏—á–∏–Ω</strong>:</p>
    <p>h(X) = -‚à´ p(x) log p(x) dx</p>
    
    <p><strong>–ü—Ä–∏–º–µ—Ä—ã</strong>:</p>
    <ul>
      <li>Normal(Œº, œÉ¬≤): h = ¬Ωlog(2œÄeœÉ¬≤)</li>
      <li>Uniform[a,b]: h = log(b-a)</li>
      <li>Exponential(Œª): h = 1 - log(Œª)</li>
    </ul>

    <pre><code>from scipy.stats import norm, uniform, expon
import scipy.stats as stats

# –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ
h_normal = norm.entropy()
print(f"h(N(0,1)): {h_normal:.3f}")  # 1.419 nats

# –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ –Ω–∞ [0, 5]
h_uniform = uniform.entropy(scale=5)
print(f"h(U[0,5]): {h_uniform:.3f}")

# –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å Œª=2
h_expon = expon.entropy(scale=1/2)
print(f"h(Exp(2)): {h_expon:.3f}")

# –í—ã—á–∏—Å–ª–∏—Ç—å –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (—á–∏—Å–ª–µ–Ω–Ω–æ)
def differential_entropy_numerical(samples):
    """
    –û—Ü–µ–Ω–∫–∞ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏ –∏–∑ —Å—ç–º–ø–ª–æ–≤
    """
    from scipy.stats import gaussian_kde
    
    # Kernel Density Estimation
    kde = gaussian_kde(samples)
    
    # –û—Ü–µ–Ω–∏—Ç—å –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –≤ —Ç–æ—á–∫–∞—Ö —Å—ç–º–ø–ª–æ–≤
    densities = kde(samples)
    
    # h ‚âà -E[log p(x)]
    return -np.mean(np.log(densities))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è (MaxEnt)</h2>
    <p><strong>–ü—Ä–∏–Ω—Ü–∏–ø</strong>: –≤—ã–±—Ä–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–µ–π –ø—Ä–∏ –∑–∞–¥–∞–Ω–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö</p>
    
    <p><strong>–ü—Ä–∏–º–µ—Ä—ã</strong>:</p>
    <ul>
      <li>–ù–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π ‚Üí –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ</li>
      <li>–ó–∞–¥–∞–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∏ –¥–∏—Å–ø–µ—Ä—Å–∏—è ‚Üí –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ</li>
      <li>–ó–∞–¥–∞–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ) ‚Üí –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ</li>
    </ul>

    <pre><code># Maximum Entropy Classifier (logistic regression)
from sklearn.linear_model import LogisticRegression

# LogReg = MaxEnt classifier —Å –ª–∏–Ω–µ–π–Ω—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
maxent = LogisticRegression(
    penalty='l2',
    C=1.0,  # —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    solver='lbfgs'
)

maxent.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö)
y_pred = maxent.predict(X_test)

# –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
y_proba = maxent.predict_proba(X_test)</code></pre>

    <blockquote>üí° MaxEnt –º–æ–¥–µ–ª–∏ –Ω–µ –¥–µ–ª–∞—é—Ç –Ω–µ–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π ‚Äî –æ–Ω–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ "–Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã" –ø—Ä–∏ –¥–∞–Ω–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤ ML</h2>
    <table>
      <tr><th>–ö–æ–Ω—Ü–µ–ø—Ü–∏—è</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td>Entropy</td><td>Decision Trees (splitting criterion)</td></tr>
      <tr><td>Cross-Entropy</td><td>Loss function –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</td></tr>
      <tr><td>KL Divergence</td><td>VAE, Variational Inference, Distillation</td></tr>
      <tr><td>Mutual Information</td><td>Feature selection, ICA</td></tr>
      <tr><td>Information Gain</td><td>Decision Trees, Feature importance</td></tr>
      <tr><td>Perplexity</td><td>Language models, t-SNE</td></tr>
      <tr><td>Fisher Information</td><td>Natural gradient descent</td></tr>
      <tr><td>MaxEnt</td><td>Logistic regression, RL</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 17. –°–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏</h2>
    <p><strong>Cross-Entropy –∏ Log Loss</strong>:</p>
    <p>Cross-Entropy Loss = -Œ£ y_true √ó log(y_pred)</p>
    
    <p><strong>KL Divergence –∏ Cross-Entropy</strong>:</p>
    <p>H(P,Q) = H(P) + D_KL(P||Q)</p>
    <p>–ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è cross-entropy ‚ü∫ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è KL divergence</p>
    
    <p><strong>Mutual Information –∏ Correlation</strong>:</p>
    <ul>
      <li>MI = 0 ‚ü∫ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å</li>
      <li>Correlation = 0 ‚ü∫ –ª–∏–Ω–µ–π–Ω–∞—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å</li>
      <li>MI —É–ª–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –î–µ–ª–∞—Ç—å</h3>
        <ul>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MI –¥–ª—è feature selection</li>
          <li>Cross-entropy –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</li>
          <li>–õ–æ–≥–∞—Ä–∏—Ñ–º—ã –≤ base 2 –¥–ª—è –±–∏—Ç–æ–≤, e –¥–ª—è nats</li>
          <li>–ü—Ä–æ–≤–µ—Ä—è—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å KL (–º–æ–∂–µ—Ç –±—ã—Ç—å ‚àû)</li>
          <li>–í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ò–∑–±–µ–≥–∞—Ç—å</h3>
        <ul>
          <li>–î–µ–ª–µ–Ω–∏–µ –Ω–∞ 0 –≤ log (–¥–æ–±–∞–≤–ª—è—Ç—å epsilon)</li>
          <li>–ü—É—Ç–∞—Ç—å KL(P||Q) –∏ KL(Q||P)</li>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å entropy –Ω–∞ –Ω–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è—Ö</li>
          <li>–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —á–∏—Å–ª–µ–Ω–Ω—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 19. –ü–æ–ª–µ–∑–Ω—ã–µ —Ñ–æ—Ä–º—É–ª—ã</h2>
    <pre><code># –í–∞–∂–Ω—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
# H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
# I(X;Y) = H(X) + H(Y) - H(X,Y)
# I(X;Y) = H(Y) - H(Y|X)
# H(Y|X) = H(Y) - I(X;Y)
# H(X,Y) ‚â§ H(X) + H(Y)  (—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –ø—Ä–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)

# Chain rule –¥–ª—è —ç–Ω—Ç—Ä–æ–ø–∏–∏
# H(X‚ÇÅ,...,X‚Çô) = Œ£ H(X·µ¢|X‚ÇÅ,...,X·µ¢‚Çã‚ÇÅ)

# Data Processing Inequality
# X ‚Üí Y ‚Üí Z  ‚üπ  I(X;Z) ‚â§ I(X;Y)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 20. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ–º—ã</h2>
    <ul>
      <li><strong>Kolmogorov Complexity</strong>: —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–µ–¥–µ–ª —Å–∂–∞—Ç–∏—è</li>
      <li><strong>Rate-Distortion Theory</strong>: trade-off –º–µ–∂–¥—É —Å–∂–∞—Ç–∏–µ–º –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º</li>
      <li><strong>Information Geometry</strong>: –≥–µ–æ–º–µ—Ç—Ä–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π</li>
      <li><strong>Quantum Information</strong>: –æ–±–æ–±—â–µ–Ω–∏–µ –Ω–∞ –∫–≤–∞–Ω—Ç–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã</li>
      <li><strong>Causality</strong>: –ø—Ä–∏—á–∏–Ω–Ω–∞—è —Ç–µ–æ—Ä–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</li>
    </ul>

    <blockquote>üìö –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã:
      <br>‚Ä¢ Cover & Thomas - "Elements of Information Theory"
      <br>‚Ä¢ MacKay - "Information Theory, Inference, and Learning"
      <br>‚Ä¢ Murphy - "Machine Learning: A Probabilistic Perspective"
    </blockquote>
  </div>

</div>

</body>
</html>

<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Efficient Transformers ‚Äî Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      font-size: 0.9em;
      color: #666;
      text-align: center;
      margin-bottom: 20px;
      column-span: all;
    }

    h2 {
      font-size: 1.1em;
      font-weight: 700;
      margin-top: 0;
      color: #1a5fb4;
      border-bottom: 2px solid #e0e8f5;
      padding-bottom: 4px;
    }

    h3 {
      font-size: 0.95em;
      font-weight: 600;
      margin: 8px 0 4px;
      color: #26a269;
    }

    p, ul, ol {
      margin: 6px 0;
      font-size: 0.88em;
      line-height: 1.5;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 3px;
    }

    code {
      background: #f6f8fa;
      padding: 1px 4px;
      border-radius: 3px;
      font-family: 'Consolas', 'Monaco', monospace;
      font-size: 0.9em;
      color: #c7254e;
    }

    pre {
      background: #282c34;
      color: #abb2bf;
      padding: 10px;
      border-radius: 6px;
      overflow-x: auto;
      font-size: 0.8em;
      line-height: 1.4;
      margin: 8px 0;
    }

    pre code {
      background: transparent;
      color: inherit;
      padding: 0;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 8px 0;
      font-size: 0.85em;
    }

    th, td {
      border: 1px solid #ddd;
      padding: 6px 8px;
      text-align: left;
    }

    th {
      background: #e0e8f5;
      font-weight: 600;
      color: #1a5fb4;
    }

    tr:nth-child(even) {
      background: #f9fbff;
    }

    blockquote {
      background: #fff9e6;
      border-left: 4px solid #f6d32d;
      padding: 8px 12px;
      margin: 8px 0;
      font-size: 0.88em;
      font-style: italic;
    }

    .formula {
      background: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      margin: 8px 0;
      font-family: 'Cambria', 'Times New Roman', serif;
      font-size: 0.9em;
      text-align: center;
    }
  </style>
</head>
<body>

<h1>‚ö° Efficient Transformers</h1>
<div class="subtitle"></div>

<div class="container">

  <div class="block">
    <h2>üî∑ 1. –ü—Ä–æ–±–ª–µ–º–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤</h2>
    <p><strong>–°–ª–æ–∂–Ω–æ—Å—Ç—å self-attention:</strong> O(n¬≤) –ø–æ –ø–∞–º—è—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏, –≥–¥–µ n ‚Äî –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.</p>

    <h3>–£–∑–∫–∏–µ –º–µ—Å—Ç–∞:</h3>
    <ul>
      <li><strong>–ü–∞–º—è—Ç—å</strong>: –ú–∞—Ç—Ä–∏—Ü–∞ attention —Ä–∞–∑–º–µ—Ä–æ–º [n √ó n] –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã</li>
      <li><strong>–í—ã—á–∏—Å–ª–µ–Ω–∏—è</strong>: –ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–ø–µ—Ä–∞—Ü–∏–π</li>
      <li><strong>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è</strong>: –°–ª–æ–∂–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (>2048 —Ç–æ–∫–µ–Ω–æ–≤)</li>
    </ul>

    <div class="formula">
      Attention(Q, K, V) = softmax(QK<sup>T</sup>/‚àöd<sub>k</sub>)V
      <br>–°–ª–æ–∂–Ω–æ—Å—Ç—å: O(n<sup>2</sup> √ó d)

    <h3>–î–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω–æ–π 10,000:</h3>
    <ul>
      <li>–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π BERT: ~3.8GB –ø–∞–º—è—Ç–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è attention</li>
      <li>Efficient models: 10-100x –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 2. –õ–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã (Linear Attention)</h2>
    <p>–°–Ω–∏–∂–∞—é—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –¥–æ O(n) –∑–∞ —Å—á—ë—Ç –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–ø–µ—Ä–∞—Ü–∏–π.</p>

    <h3>–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è:</h3>
    <div class="formula">
      Attention = softmax(QK<sup>T</sup>)V ‚âà œÜ(Q)(œÜ(K)<sup>T</sup>V)
    </div>

    <pre><code>import torch
import torch.nn as nn

class LinearAttention(nn.Module):
    def __init__(self, dim, heads=8):
        super().__init__()
        self.heads = heads
        self.to_qkv = nn.Linear(dim, dim * 3)
        self.to_out = nn.Linear(dim, dim)
    
    def forward(self, x):
        # x: [batch, seq_len, dim]
        b, n, d = x.shape
        h = self.heads
        
        qkv = self.to_qkv(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: t.reshape(b, n, h, -1), qkv)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º feature map (–Ω–∞–ø—Ä–∏–º–µ—Ä, ELU + 1)
        q = torch.nn.functional.elu(q) + 1
        k = torch.nn.functional.elu(k) + 1
        
        # –õ–∏–Ω–µ–π–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å O(n)
        # –í–º–µ—Å—Ç–æ (Q @ K.T) @ V –¥–µ–ª–∞–µ–º Q @ (K.T @ V)
        context = torch.einsum('bnhd,bnhe->bhde', k, v)
        out = torch.einsum('bnhd,bhde->bnhe', q, context)
        
        out = out.reshape(b, n, -1)
        return self.to_out(out)</code></pre>

    <h3>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:</h3>
    <ul>
      <li>O(n √ó d¬≤) –≤–º–µ—Å—Ç–æ O(n¬≤ √ó d)</li>
      <li>–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π</li>
      <li>–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 3. Sparse Attention (–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ)</h2>
    <p>–ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω —Å–º–æ—Ç—Ä–∏—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥—Ä—É–≥–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤.</p>

    <h3>–¢–∏–ø—ã —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏:</h3>
    <table>
      <tr><th>–¢–∏–ø</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–°–ª–æ–∂–Ω–æ—Å—Ç—å</th></tr>
      <tr><td><strong>Local</strong></td><td>–¢–æ–ª—å–∫–æ —Å–æ—Å–µ–¥–Ω–∏–µ —Ç–æ–∫–µ–Ω—ã</td><td>O(n √ó w)</td></tr>
      <tr><td><strong>Strided</strong></td><td>–ö–∞–∂–¥—ã–π k-–π —Ç–æ–∫–µ–Ω</td><td>O(n √ó n/k)</td></tr>
      <tr><td><strong>Fixed</strong></td><td>–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏</td><td>O(n √ó c)</td></tr>
      <tr><td><strong>Random</strong></td><td>–°–ª—É—á–∞–π–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã</td><td>O(n √ó r)</td></tr>
    </table>

    <h3>–†–µ–∞–ª–∏–∑–∞—Ü–∏—è Local + Strided:</h3>
    <pre><code>def create_sparse_mask(seq_len, window=128, stride=64):
    """
    –°–æ–∑–¥–∞—ë—Ç –º–∞—Å–∫—É –¥–ª—è sparse attention
    """
    mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)
    
    for i in range(seq_len):
        # Local window
        start = max(0, i - window // 2)
        end = min(seq_len, i + window // 2 + 1)
        mask[i, start:end] = True
        
        # Strided positions
        strided_positions = range(0, seq_len, stride)
        mask[i, strided_positions] = True
    
    return mask

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
mask = create_sparse_mask(seq_len=1024, window=128, stride=64)
attention_scores = attention_scores.masked_fill(~mask, float('-inf'))</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Longformer</h2>
    <p>–ö–æ–º–±–∏–Ω–∞—Ü–∏—è local + global attention patterns.</p>

    <h3>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:</h3>
    <ul>
      <li><strong>Local attention</strong>: –û–∫–Ω–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞</li>
      <li><strong>Global attention</strong>: –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —Å–º–æ—Ç—Ä—è—Ç –Ω–∞ –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å</li>
      <li>–°–ª–æ–∂–Ω–æ—Å—Ç—å: O(n √ó w) –≥–¥–µ w ‚Äî —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞</li>
    </ul>

    <pre><code>from transformers import LongformerModel, LongformerTokenizer

tokenizer = LongformerTokenizer.from_pretrained(
    'allenai/longformer-base-4096'
)
model = LongformerModel.from_pretrained(
    'allenai/longformer-base-4096'
)

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–¥–æ 4096 —Ç–æ–∫–µ–Ω–æ–≤)
text = "Very long document..." * 1000
inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=4096)

# Global attention –Ω–∞ CLS —Ç–æ–∫–µ–Ω–µ
inputs['global_attention_mask'] = torch.zeros_like(inputs['input_ids'])
inputs['global_attention_mask'][:, 0] = 1  # CLS token

outputs = model(**inputs)
last_hidden = outputs.last_hidden_state</code></pre>

    <h3>–ù–∞—Å—Ç—Ä–æ–π–∫–∞ attention_window:</h3>
    <pre><code># –†–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –æ–∫–Ω–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ—ë–≤
model.config.attention_window = [64, 128, 256, 512, 512, 512]</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. BigBird</h2>
    <p>Sparse attention —Å random connections –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏.</p>

    <h3>–ü–∞—Ç—Ç–µ—Ä–Ω –≤–Ω–∏–º–∞–Ω–∏—è:</h3>
    <ol>
      <li><strong>Local</strong>: –û–∫–Ω–æ —Å–æ—Å–µ–¥–µ–π (window_size)</li>
      <li><strong>Global</strong>: –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (–≤—Å–µ –≤–∏–¥—è—Ç –∏—Ö)</li>
      <li><strong>Random</strong>: –°–ª—É—á–∞–π–Ω—ã–µ —Å–≤—è–∑–∏ (–¥–ª—è long-range dependencies)</li>
    </ol>

    <pre><code>from transformers import BigBirdModel, BigBirdTokenizer

tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')
model = BigBirdModel.from_pretrained('google/bigbird-roberta-base')

# BigBird –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–æ 4096 —Ç–æ–∫–µ–Ω–æ–≤
text = "Long document..." * 1000
inputs = tokenizer(
    text, 
    return_tensors='pt',
    max_length=4096,
    truncation=True
)

outputs = model(**inputs)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
model.config.attention_type = 'block_sparse'  # –∏–ª–∏ 'original_full'
model.config.block_size = 64
model.config.num_random_blocks = 3</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Reformer</h2>
    <p>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LSH (Locality-Sensitive Hashing) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤.</p>

    <h3>–û—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏:</h3>
    <ul>
      <li><strong>LSH attention</strong>: –¢–æ–∫–µ–Ω—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑—É—é—Ç—Å—è –ø–æ —Ö—ç—à–∞–º</li>
      <li><strong>Reversible layers</strong>: –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏ backward</li>
      <li><strong>Chunking</strong>: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ —á–∞—Å—Ç—è–º</li>
    </ul>

    <pre><code>from transformers import ReformerModel, ReformerTokenizer

tokenizer = ReformerTokenizer.from_pretrained('google/reformer-enwik8')
model = ReformerModel.from_pretrained('google/reformer-enwik8')

# Reformer –¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
text = "Super long text..." * 5000
inputs = tokenizer(
    text,
    return_tensors='pt',
    max_length=65536,  # –¥–æ 64K —Ç–æ–∫–µ–Ω–æ–≤!
    truncation=True
)

outputs = model(**inputs)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã LSH
model.config.num_hashes = 2  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ö—ç—à-—Ñ—É–Ω–∫—Ü–∏–π
model.config.num_buckets = [64, 128]  # —Ä–∞–∑–º–µ—Ä –±–∞–∫–µ—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ö—ç—à–∞</code></pre>

    <h3>–ü—Ä–∏–Ω—Ü–∏–ø LSH attention:</h3>
    <div class="formula">
      hash(query<sub>i</sub>) ‚âà hash(key<sub>j</sub>) ‚Üí attend(i, j)
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 7. Linformer</h2>
    <p>–ü—Ä–æ–µ—Ü–∏—Ä—É–µ—Ç K –∏ V –≤ –Ω–∏–∑–∫–æ—Ä–∞–∑–º–µ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ.</p>

    <h3>–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è:</h3>
    <div class="formula">
      Attention(Q, K, V) = softmax(Q(EK)<sup>T</sup>/‚àöd<sub>k</sub>)(FV)
      <br>–≥–¥–µ E, F ‚àà ‚Ñù<sup>k√ón</sup>, k << n
    </div>

    <pre><code>import torch.nn as nn

class LinformerAttention(nn.Module):
    def __init__(self, dim, seq_len, heads=8, k=256):
        super().__init__()
        self.heads = heads
        self.k = k  # projected dimension
        
        self.to_qkv = nn.Linear(dim, dim * 3)
        
        # –ü—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã
        self.proj_k = nn.Linear(seq_len, k, bias=False)
        self.proj_v = nn.Linear(seq_len, k, bias=False)
        
        self.to_out = nn.Linear(dim, dim)
    
    def forward(self, x):
        b, n, d = x.shape
        h = self.heads
        
        qkv = self.to_qkv(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: t.reshape(b, n, h, -1), qkv)
        
        # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º K –∏ V: [b, n, h, d] -> [b, k, h, d]
        k = self.proj_k(k.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        v = self.proj_v(v.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π attention, –Ω–æ K –∏ V —Ä–∞–∑–º–µ—Ä–∞ k –≤–º–µ—Å—Ç–æ n
        dots = torch.einsum('bnhd,bkhd->bnhk', q, k) / (d ** 0.5)
        attn = dots.softmax(dim=-1)
        
        out = torch.einsum('bnhk,bkhd->bnhd', attn, v)
        out = out.reshape(b, n, -1)
        
        return self.to_out(out)</code></pre>

    <h3>–°–ª–æ–∂–Ω–æ—Å—Ç—å:</h3>
    <p>O(n √ó k) –≤–º–µ—Å—Ç–æ O(n¬≤), –≥–¥–µ –æ–±—ã—á–Ω–æ k = 256</p>
  </div>

  <div class="block">
    <h2>üî∑ 8. Performer (Fast Attention)</h2>
    <p>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç random features –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ softmax kernel.</p>

    <h3>FAVOR+ –∞–ª–≥–æ—Ä–∏—Ç–º:</h3>
    <div class="formula">
      softmax(QK<sup>T</sup>) ‚âà œÜ(Q)œÜ(K)<sup>T</sup>
      <br>–≥–¥–µ œÜ ‚Äî random Fourier features
    </div>

    <pre><code>import torch
import math

def orthogonal_random_features(q, k, num_features=256):
    """
    Performer's FAVOR+ algorithm
    """
    b, h, n, d = q.shape
    
    # –°–æ–∑–¥–∞—ë–º –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ random features
    device = q.device
    projection = torch.randn(num_features // 2, d, device=device)
    projection, _ = torch.qr(projection.T)
    projection = projection.T
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º random features
    def apply_feature_map(x):
        x_proj = torch.einsum('bhnd,fd->bhnf', x, projection)
        
        # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ random features
        return torch.cat([
            torch.exp(x_proj - x.pow(2).sum(dim=-1, keepdim=True) / 2),
            torch.exp(-x_proj - x.pow(2).sum(dim=-1, keepdim=True) / 2)
        ], dim=-1) / math.sqrt(num_features)
    
    q_prime = apply_feature_map(q)
    k_prime = apply_feature_map(k)
    
    return q_prime, k_prime

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
q_prime, k_prime = orthogonal_random_features(Q, K)

# –õ–∏–Ω–µ–π–Ω—ã–π attention
kv = torch.einsum('bhnd,bhnk->bhdk', k_prime, V)
out = torch.einsum('bhnd,bhdk->bhnk', q_prime, kv)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Flash Attention</h2>
    <p>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ GPU, –Ω–µ –º–µ–Ω—è—é—â–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫—É attention.</p>

    <h3>–ö–ª—é—á–µ–≤—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:</h3>
    <ul>
      <li><strong>Tiling</strong>: –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–ª–æ–∫–∞–º–∏, –ø–æ–º–µ—â–∞—é—â–∏–º–∏—Å—è –≤ SRAM</li>
      <li><strong>Recomputation</strong>: –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã</li>
      <li><strong>Fused kernels</strong>: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–π</li>
    </ul>

    <pre><code># –£—Å—Ç–∞–Ω–æ–≤–∫–∞
# pip install flash-attn

import torch
from flash_attn import flash_attn_func

# –û–±—ã—á–Ω—ã–π attention
q = torch.randn(32, 12, 2048, 64, device='cuda', dtype=torch.float16)
k = torch.randn(32, 12, 2048, 64, device='cuda', dtype=torch.float16)
v = torch.randn(32, 12, 2048, 64, device='cuda', dtype=torch.float16)

# Flash Attention - —Ç–∞–∫–æ–π –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –Ω–æ –≤ 3-5x –±—ã—Å—Ç—Ä–µ–µ
out = flash_attn_func(q, k, v, causal=False)

# –î–ª—è causal (GPT-style) attention
out_causal = flash_attn_func(q, k, v, causal=True)</code></pre>

    <h3>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:</h3>
    <ul>
      <li>2-4x –±—ã—Å—Ç—Ä–µ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ attention</li>
      <li>10-20x –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏</li>
      <li>–¢–æ—á–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ (–Ω–µ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è)</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 10. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–°–ª–æ–∂–Ω–æ—Å—Ç—å</th><th>–¢–æ—á–Ω–æ—Å—Ç—å</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr>
        <td><strong>Standard</strong></td>
        <td>O(n¬≤)</td>
        <td>100%</td>
        <td>–î–æ 512 —Ç–æ–∫–µ–Ω–æ–≤</td>
      </tr>
      <tr>
        <td><strong>Linear</strong></td>
        <td>O(n)</td>
        <td>~95%</td>
        <td>–õ—é–±–∞—è –¥–ª–∏–Ω–∞</td>
      </tr>
      <tr>
        <td><strong>Longformer</strong></td>
        <td>O(n√ów)</td>
        <td>~98%</td>
        <td>–î–æ 4K —Ç–æ–∫–µ–Ω–æ–≤</td>
      </tr>
      <tr>
        <td><strong>BigBird</strong></td>
        <td>O(n√ów)</td>
        <td>~98%</td>
        <td>–î–æ 4K —Ç–æ–∫–µ–Ω–æ–≤</td>
      </tr>
      <tr>
        <td><strong>Reformer</strong></td>
        <td>O(n log n)</td>
        <td>~96%</td>
        <td>–î–æ 64K —Ç–æ–∫–µ–Ω–æ–≤</td>
      </tr>
      <tr>
        <td><strong>Linformer</strong></td>
        <td>O(n√ók)</td>
        <td>~96%</td>
        <td>–§–∏–∫—Å. –¥–ª–∏–Ω–∞</td>
      </tr>
      <tr>
        <td><strong>Performer</strong></td>
        <td>O(n)</td>
        <td>~94%</td>
        <td>–õ—é–±–∞—è –¥–ª–∏–Ω–∞</td>
      </tr>
      <tr>
        <td><strong>Flash Attn</strong></td>
        <td>O(n¬≤)*</td>
        <td>100%</td>
        <td>GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è</td>
      </tr>
    </table>
    <p><small>*–¢–∞ –∂–µ —Å–ª–æ–∂–Ω–æ—Å—Ç—å, –Ω–æ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏</small></p>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</h2>
    <h3>–î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ (< 512 —Ç–æ–∫–µ–Ω–æ–≤):</h3>
    <ul>
      <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π BERT/RoBERTa</li>
      <li>Flash Attention –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è</li>
    </ul>

    <h3>–î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (512-4K —Ç–æ–∫–µ–Ω–æ–≤):</h3>
    <ul>
      <li><strong>Longformer</strong> ‚Äî –µ—Å–ª–∏ –≤–∞–∂–Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å</li>
      <li><strong>BigBird</strong> ‚Äî –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ —Å–∫–æ—Ä–æ—Å—Ç—å/–∫–∞—á–µ—Å—Ç–≤–æ</li>
      <li>Flash Attention + gradient checkpointing</li>
    </ul>

    <h3>–î–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (>4K):</h3>
    <ul>
      <li><strong>Reformer</strong> ‚Äî –¥–æ 64K —Ç–æ–∫–µ–Ω–æ–≤</li>
      <li><strong>Linear Transformers</strong> ‚Äî –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞</li>
      <li>Hierarchical models</li>
    </ul>

    <h3>–î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:</h3>
    <ul>
      <li>Performer –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞</li>
      <li>Flash Attention –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –¥–ª–∏–Ω—ã</li>
      <li>Linformer –¥–ª—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 12. Gradient Checkpointing</h2>
    <p>–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ –¥–ª—è –ª—é–±–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.</p>

    <pre><code>from transformers import BertModel

model = BertModel.from_pretrained('bert-base-uncased')

# –í–∫–ª—é—á–∞–µ–º gradient checkpointing
model.gradient_checkpointing_enable()

# –¢–µ–ø–µ—Ä—å –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ~30% –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏
# –ù–æ –Ω–∞ ~20% –º–µ–¥–ª–µ–Ω–Ω–µ–µ (–ø–µ—Ä–µ—Å—á—ë—Ç –ø—Ä–∏ backward)

# –î–ª—è –æ–±—É—á–µ–Ω–∏—è
outputs = model(input_ids, labels=labels)
loss = outputs.loss
loss.backward()  # –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. Mixed Precision Training</h2>
    <pre><code>import torch
from torch.cuda.amp import autocast, GradScaler

model = LongformerModel.from_pretrained('allenai/longformer-base-4096')
model = model.cuda()

scaler = GradScaler()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

for batch in dataloader:
    optimizer.zero_grad()
    
    # Forward –≤ float16
    with autocast():
        outputs = model(**batch)
        loss = outputs.loss
    
    # Backward —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()

# –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏: ~50%
# –£—Å–∫–æ—Ä–µ–Ω–∏–µ: 2-3x –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö GPU</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏–∫</h2>
    <pre><code>from transformers import AutoModel, TrainingArguments, Trainer

model = AutoModel.from_pretrained('allenai/longformer-base-4096')

# –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
training_args = TrainingArguments(
    output_dir='./results',
    
    # Mixed precision
    fp16=True,
    
    # Gradient checkpointing
    gradient_checkpointing=True,
    
    # Gradient accumulation (–≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π batch)
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,  # —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ batch=16
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
    optim='adafactor',  # memory-efficient optimizer
    
    # –î—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    learning_rate=1e-5,
    num_train_epochs=3,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

trainer.train()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ß–µ–∫-–ª–∏—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏</h2>
    <ul>
      <li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É (—Å–º. —Ç–∞–±–ª–∏—Ü—É)</li>
      <li>[ ] –í–∫–ª—é—á–∏—Ç—å Flash Attention –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ (GPU)</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å gradient checkpointing –ø—Ä–∏ OOM</li>
      <li>[ ] –ü—Ä–∏–º–µ–Ω–∏—Ç—å mixed precision (FP16/BF16)</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å gradient accumulation –¥–ª—è –±–æ–ª—å—à–∏—Ö batch</li>
      <li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å distillation –¥–ª—è inference</li>
      <li>[ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ)</li>
      <li>[ ] –ò–∑–º–µ—Ä–∏—Ç—å —Ä–µ–∞–ª—å–Ω–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏</li>
      <li>[ ] –°—Ä–∞–≤–Ω–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å —Å baseline</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´Efficient Transformers ‚Äî —ç—Ç–æ –∫–∞–∫ —Å–∫–æ—Ä–æ—Å—Ç–Ω—ã–µ –∞–≤—Ç–æ–º–∞–≥–∏—Å—Ç—Ä–∞–ª–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞. –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä—è—Ç—å –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ —Å–æ –≤—Å–µ–º–∏ –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏ (—á—Ç–æ –º–µ–¥–ª–µ–Ω–Ω–æ), –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–º–Ω—ã–µ –º–∞—Ä—à—Ä—É—Ç—ã –∏ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ 10-100 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

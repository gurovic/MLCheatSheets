<!DOCTYPE html><html lang="ru"><head><meta charset="UTF-8"><title>Variational Autoencoders (VAE) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title><style>@media screen{body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Helvetica,Arial,sans-serif;color:#333;background:#fafcff;padding:10px}}@media print{body{background:white;padding:0}@page{size:A4 landscape;margin:10mm}}.container{column-count:3;column-gap:20px;max-width:100%}.block{break-inside:avoid;margin-bottom:1.2em;padding:12px;background:white;border-radius:6px;box-shadow:0 1px 3px rgba(0,0,0,0.05)}h1{font-size:1.6em;font-weight:700;color:#1a5fb4;text-align:center;margin:0 0 8px;column-span:all}.subtitle{text-align:center;color:#666;font-size:0.9em;margin-bottom:12px;column-span:all}h2{font-size:1.15em;font-weight:700;color:#1a5fb4;margin:0 0 8px;padding-bottom:4px;border-bottom:1px solid #e0e7ff}p,ul,ol{font-size:0.92em;margin:0.6em 0}ul,ol{padding-left:18px}li{margin-bottom:4px}code{font-family:'Consolas','Courier New',monospace;background-color:#f0f4ff;padding:1px 4px;border-radius:3px;font-size:0.88em}pre{background-color:#f0f4ff;padding:8px;border-radius:4px;overflow-x:auto;font-size:0.84em;margin:6px 0}pre code{padding:0;background:none;white-space:pre-wrap}table{width:100%;border-collapse:collapse;font-size:0.82em;margin:6px 0}th{background-color:#e6f0ff;text-align:left;padding:4px 6px;font-weight:600}td{padding:4px 6px;border-bottom:1px solid #f0f4ff}tr:nth-child(even){background-color:#f8fbff}.good-vs-bad{display:flex;flex-direction:column;gap:8px}.good-vs-bad div{flex:1;padding:6px 8px;border-radius:4px}.good{background-color:#f0f9f4;border-left:3px solid #2e8b57}.bad{background-color:#fdf0f2;border-left:3px solid #d32f2f}.good h3,.bad h3{margin:0 0 4px;font-size:1em;font-weight:700}.good ul,.bad ul{padding-left:20px;margin:0}.good li::before{content:"‚úÖ ";font-weight:bold}.bad li::before{content:"‚ùå ";font-weight:bold}blockquote{font-style:italic;margin:8px 0;padding:6px 10px;background:#f8fbff;border-left:2px solid #1a5fb4;font-size:0.88em}.formula{text-align:center;font-family:'Cambria Math',serif;font-size:1em;margin:10px 0;padding:8px;background:#f8fbff;border-radius:4px}@media print{.container{column-gap:12px}.block{box-shadow:none}code,pre,table{font-size:0.78em}h1{font-size:1.4em}h2{font-size:1em}}</style></head><body><div class="container"><h1>üé® Variational Autoencoders (VAE) Cheatsheet</h1><div class="subtitle">–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ ‚Ä¢ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ ‚Ä¢ –õ–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ<br>üìÖ 3 —è–Ω–≤–∞—Ä—è 2026</div><div class="block"><h2>üî∑ 1. –°—É—Ç—å</h2><ul><li><strong>–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å</strong>: —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ</li><li><strong>–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π encoder</strong>: –∫–æ–¥–∏—Ä—É–µ—Ç –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (Œº, œÉ)</li><li><strong>Reparameterization trick</strong>: –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è</li><li><strong>–î–≤–∞ loss</strong>: reconstruction + KL divergence</li></ul><div class="formula">Loss = Reconstruction Loss + Œ≤¬∑KL Divergence</div></div><div class="block"><h2>üî∑ 2. –ë–∞–∑–æ–≤—ã–π –∫–æ–¥ (PyTorch)</h2><pre><code>import torch
import torch.nn as nn

class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU()
        )
        self.fc_mu = nn.Linear(256, latent_dim)
        self.fc_logvar = nn.Linear(256, latent_dim)
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, input_dim),
            nn.Sigmoid()
        )
    
    def encode(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        return self.decoder(z)
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        return recon, mu, logvar</code></pre></div><div class="block"><h2>üî∑ 3. Loss —Ñ—É–Ω–∫—Ü–∏—è</h2><pre><code>def vae_loss(recon_x, x, mu, logvar):
    # Reconstruction loss (BCE –∏–ª–∏ MSE)
    recon_loss = F.binary_cross_entropy(
        recon_x, x, reduction='sum'
    )
    
    # KL divergence: -0.5 * Œ£(1 + log(œÉ¬≤) - Œº¬≤ - œÉ¬≤)
    kl_loss = -0.5 * torch.sum(
        1 + logvar - mu.pow(2) - logvar.exp()
    )
    
    return recon_loss + kl_loss

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
recon, mu, logvar = model(x)
loss = vae_loss(recon, x, mu, logvar)
loss.backward()
optimizer.step()</code></pre></div><div class="block"><h2>üî∑ 4. Reparameterization Trick</h2><p><strong>–ü—Ä–æ–±–ª–µ–º–∞:</strong> –Ω–µ–ª—å–∑—è –¥–µ–ª–∞—Ç—å backprop —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω–æ–µ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ</p><p><strong>–†–µ—à–µ–Ω–∏–µ:</strong> z = Œº + œÉ¬∑Œµ, –≥–¥–µ Œµ ~ N(0,1)</p><pre><code># –ü–ª–æ—Ö–æ: –Ω–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ
z = sample_from_distribution(mu, sigma)

# –•–æ—Ä–æ—à–æ: reparameterization trick
def reparameterize(mu, logvar):
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)  # Œµ ~ N(0,1)
    z = mu + eps * std           # z = Œº + œÉ¬∑Œµ
    return z</code></pre></div><div class="block"><h2>üî∑ 5. VAE –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (CNN)</h2><pre><code>class ConvVAE(nn.Module):
    def __init__(self, latent_dim=128):
        super().__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 4, 2, 1),  # 32x32 -> 16x16
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, 2, 1), # 16x16 -> 8x8
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1),# 8x8 -> 4x4
            nn.ReLU(),
            nn.Flatten()
        )
        self.fc_mu = nn.Linear(128*4*4, latent_dim)
        self.fc_logvar = nn.Linear(128*4*4, latent_dim)
        
        # Decoder
        self.fc_decode = nn.Linear(latent_dim, 128*4*4)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 3, 4, 2, 1),
            nn.Sigmoid()
        )</code></pre></div><div class="block"><h2>üî∑ 6. –û–±—É—á–µ–Ω–∏–µ VAE</h2><pre><code>model = VAE(input_dim=784, latent_dim=20)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(num_epochs):
    for batch in dataloader:
        x = batch[0]
        
        # Forward
        recon, mu, logvar = model(x)
        loss = vae_loss(recon, x, mu, logvar)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f"Epoch {epoch}, Loss: {loss.item():.4f}")</code></pre></div><div class="block"><h2>üî∑ 7. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö</h2><pre><code># –ú–µ—Ç–æ–¥ 1: —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ prior N(0, I)
z = torch.randn(batch_size, latent_dim)
generated = model.decode(z)

# –ú–µ—Ç–æ–¥ 2: –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏
z1 = torch.randn(1, latent_dim)
z2 = torch.randn(1, latent_dim)
alphas = torch.linspace(0, 1, 10)
interpolated = []
for alpha in alphas:
    z = (1-alpha) * z1 + alpha * z2
    img = model.decode(z)
    interpolated.append(img)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(15, 2))
for i, img in enumerate(interpolated):
    plt.subplot(1, 10, i+1)
    plt.imshow(img.squeeze().detach().numpy())
    plt.axis('off')
plt.show()</code></pre></div><div class="block"><h2>üî∑ 8. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞</h2><pre><code># –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
latents = []
labels = []
model.eval()
with torch.no_grad():
    for x, y in dataloader:
        mu, logvar = model.encode(x)
        latents.append(mu)
        labels.append(y)

latents = torch.cat(latents).numpy()
labels = torch.cat(labels).numpy()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ latent_dim=2 –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ t-SNE)
plt.figure(figsize=(10, 8))
plt.scatter(latents[:, 0], latents[:, 1], c=labels, cmap='tab10')
plt.colorbar()
plt.title('–õ–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ VAE')
plt.show()</code></pre></div><div class="block"><h2>üî∑ 9. Beta-VAE</h2><pre><code># Beta-VAE: –∫–æ–Ω—Ç—Ä–æ–ª—å –±–∞–ª–∞–Ω—Å–∞ reconstruction/KL
def beta_vae_loss(recon_x, x, mu, logvar, beta=1.0):
    recon_loss = F.binary_cross_entropy(
        recon_x, x, reduction='sum'
    )
    kl_loss = -0.5 * torch.sum(
        1 + logvar - mu.pow(2) - logvar.exp()
    )
    return recon_loss + beta * kl_loss

# beta > 1: —Å–∏–ª—å–Ω–µ–µ disentanglement
# beta < 1: –ª—É—á—à–µ reconstruction
loss = beta_vae_loss(recon, x, mu, logvar, beta=4.0)</code></pre></div><div class="block"><h2>üî∑ 10. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h2><div class="good-vs-bad"><div class="good"><h3>‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞</h3><ul><li>–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ</li><li>–ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ</li><li>–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</li><li>–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ</li><li>Disentangled representations</li></ul></div><div class="bad"><h3>‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏</h3><ul><li>–†–∞–∑–º—ã—Ç—ã–µ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏</li><li>–°–ª–æ–∂–Ω–µ–µ –æ–±—É—á–∞—Ç—å —á–µ–º AE</li><li>–ë–∞–ª–∞–Ω—Å reconstruction/KL</li><li>–ú–µ–¥–ª–µ–Ω–Ω–µ–µ GAN</li><li>–¢—Ä–µ–±—É–µ—Ç –ø–æ–¥–±–æ—Ä–∞ Œ≤</li></ul></div></div></div><div class="block"><h2>üî∑ 11. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2><div class="good-vs-bad"><div class="good"><h3>‚úÖ –•–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥–∏—Ç</h3><ul><li>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö</li><li>–ê–Ω–æ–º–∞–ª–∏–∏ (reconstruction error)</li><li>–ü–æ–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏</li><li>–ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö</li><li>–ù—É–∂–Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å</li></ul></div><div class="bad"><h3>‚ùå –ü–ª–æ—Ö–æ –ø–æ–¥—Ö–æ–¥–∏—Ç</h3><ul><li>–ù—É–∂–Ω—ã —á–µ—Ç–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ GAN)</li><li>–¢–æ–ª—å–∫–æ —Å–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ AE)</li><li>–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏</li></ul></div></div></div><div class="block"><h2>üî∑ 12. VAE vs Autoencoder vs GAN</h2><table><tr><th>–ú–æ–¥–µ–ª—å</th><th>–¢–∏–ø</th><th>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è</th><th>–ö–∞—á–µ—Å—Ç–≤–æ</th></tr><tr><td>Autoencoder</td><td>–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π</td><td>–ù–µ—Ç</td><td>–•–æ—Ä–æ—à–µ–µ —Å–∂–∞—Ç–∏–µ</td></tr><tr><td>VAE</td><td>–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π</td><td>–î–∞</td><td>–†–∞–∑–º—ã—Ç—ã–µ –æ–±—Ä–∞–∑—Ü—ã</td></tr><tr><td>GAN</td><td>–°–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–π</td><td>–î–∞</td><td>–ß–µ—Ç–∫–∏–µ –æ–±—Ä–∞–∑—Ü—ã</td></tr></table></div><div class="block"><h2>üî∑ 13. Conditional VAE (CVAE)</h2><pre><code># –£—Å–ª–æ–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ –∫–ª–∞—Å—Å—É)
class CVAE(nn.Module):
    def __init__(self, input_dim, latent_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(num_classes, 10)
        
        # Encoder: x + label
        self.encoder = nn.Linear(input_dim + 10, 256)
        
        # Decoder: z + label
        self.decoder = nn.Linear(latent_dim + 10, 256)
    
    def forward(self, x, label):
        label_emb = self.embedding(label)
        # Encoder
        x_cond = torch.cat([x, label_emb], dim=1)
        mu, logvar = self.encode(x_cond)
        z = self.reparameterize(mu, logvar)
        # Decoder
        z_cond = torch.cat([z, label_emb], dim=1)
        recon = self.decode(z_cond)
        return recon, mu, logvar

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
label = torch.tensor([5])  # –∫–ª–∞—Å—Å 5
z = torch.randn(1, latent_dim)
generated = model.decode(torch.cat([z, model.embedding(label)], dim=1))</code></pre></div><div class="block"><h2>üî∑ 14. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π —Å VAE</h2><pre><code># –ê–Ω–æ–º–∞–ª–∏–∏ –∏–º–µ—é—Ç –≤—ã—Å–æ–∫–∏–π reconstruction error
model.eval()
with torch.no_grad():
    recon, mu, logvar = model(x_test)
    
    # Reconstruction error
    recon_error = F.mse_loss(
        recon, x_test, reduction='none'
    ).sum(dim=1)
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞
    threshold = recon_error.mean() + 2*recon_error.std()
    
    # –ê–Ω–æ–º–∞–ª–∏–∏
    anomalies = recon_error > threshold
    print(f"–ù–∞–π–¥–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {anomalies.sum()}")</code></pre></div><div class="block"><h2>üî∑ 15. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2><ul><li><strong>Latent dim</strong>: 2-128, –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö</li><li><strong>Beta</strong>: –Ω–∞—á–Ω–∏—Ç–µ —Å 1.0, —É–≤–µ–ª–∏—á—å—Ç–µ –¥–ª—è disentanglement</li><li><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong>: encoder/decoder —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã</li><li><strong>Activation</strong>: Sigmoid –¥–ª—è [0,1], Tanh –¥–ª—è [-1,1]</li><li><strong>Warm-up</strong>: –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ –≤–µ—Å KL loss</li><li><strong>–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥</strong>: —Å–ª–µ–¥–∏—Ç–µ –∑–∞ reconstruction –∏ KL –æ—Ç–¥–µ–ª—å–Ω–æ</li></ul></div><div class="block"><h2>üî∑ 16. –ß–µ–∫-–ª–∏—Å—Ç</h2><ul><li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞</li><li>[ ] –í—ã–±—Ä–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É encoder/decoder</li><li>[ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å reparameterization trick</li><li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å loss (reconstruction + KL)</li><li>[ ] –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å</li><li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ</li><li>[ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –Ω–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤</li><li>[ ] –ü–æ–¥–æ–±—Ä–∞—Ç—å beta –¥–ª—è –±–∞–ª–∞–Ω—Å–∞</li></ul><blockquote>¬´VAE ‚Äî –º–æ—â–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π. –ò–¥–µ–∞–ª—å–Ω–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏, —Ö–æ—Ç—è –∏ —É—Å—Ç—É–ø–∞–µ—Ç GAN –ø–æ —á–µ—Ç–∫–æ—Å—Ç–∏ –æ–±—Ä–∞–∑—Ü–æ–≤¬ª.</blockquote></div><div class="block"><h2>üîó –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏</h2><ul><li><a href="https://arxiv.org/abs/1312.6114" target="_blank">üìÑ Original VAE Paper</a></li><li><a href="https://arxiv.org/abs/1606.05579" target="_blank">üìÑ Tutorial on VAE</a></li><li><a href="https://github.com/AntixK/PyTorch-VAE" target="_blank">üíª PyTorch VAE Examples</a></li><li><a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" target="_blank">üìñ Understanding VAEs</a></li></ul></div></div></body></html>

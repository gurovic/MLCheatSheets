<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen { body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; color: #333; background: #fafcff; padding: 10px; 
        min-width: 900px;
      } }
    @media print { body { background: white; padding: 0; } @page { size: A4 landscape; margin: 10mm; } }
        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }
    .block { break-inside: avoid; margin-bottom: 1.2em; padding: 12px; background: white; border-radius: 6px; box-shadow: 0 1px 3px rgba(0,0,0,0.05); }
    h1 { font-size: 1.6em; font-weight: 700; color: #1a5fb4; text-align: center; margin: 0 0 8px; column-span: all; }
    .subtitle { text-align: center; color: #666; font-size: 0.9em; margin-bottom: 12px; column-span: all; }
    h2 { font-size: 1.15em; font-weight: 700; color: #1a5fb4; margin: 0 0 8px; padding-bottom: 4px; border-bottom: 1px solid #e0e7ff; }
    p, ul, ol { font-size: 0.92em; margin: 0.6em 0; }
    ul, ol { padding-left: 18px; }
    li { margin-bottom: 4px; }
    code { font-family: 'Consolas', 'Courier New', monospace; background-color: #f0f4ff; padding: 1px 4px; border-radius: 3px; font-size: 0.88em; }
    pre { background-color: #f0f4ff; padding: 8px; border-radius: 4px; overflow-x: auto; font-size: 0.84em; margin: 6px 0; }
    pre code { padding: 0; background: none; white-space: pre-wrap; }
    table { width: 100%; border-collapse: collapse; font-size: 0.82em; margin: 6px 0; }
    th { background-color: #e6f0ff; text-align: left; padding: 4px 6px; font-weight: 600; }
    td { padding: 4px 6px; border-bottom: 1px solid #f0f4ff; }
    tr:nth-child(even) { background-color: #f8fbff; }
    blockquote { font-style: italic; margin: 8px 0; padding: 6px 10px; background: #f8fbff; border-left: 2px solid #1a5fb4; font-size: 0.88em; }
  </style>
</head>
<body>
<div class="container">
  <h1>üìö –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ</h1>
  <div class="subtitle">üìÖ 5 —è–Ω–≤–∞—Ä—è 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤—ã</h2>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—è–≤–ª–µ–Ω–∏–µ —Ç–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤</li>
      <li><strong>Topic</strong>: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –Ω–∞–¥ —Å–ª–æ–≤–∞–º–∏</li>
      <li><strong>Document</strong>: —Å–º–µ—Å—å —Ç–µ–º</li>
      <li><strong>Unsupervised</strong>: –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –º–µ—Ç–æ–∫</li>
      <li><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –∞–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤</li>
    </ul>

    </div>
<div class="block">
    <h2>üî∑ 2. –ú–µ—Ç–æ–¥—ã</h2>
    <table>
      <tr><th>–ú–µ—Ç–æ–¥</th><th>–ü–æ–¥—Ö–æ–¥</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏</th></tr>
      <tr><td><strong>LDA</strong></td><td>–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π</td><td>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å</td></tr>
      <tr><td><strong>NMF</strong></td><td>–ú–∞—Ç—Ä–∏—á–Ω–∞—è —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è</td><td>–ë—ã—Å—Ç—Ä—ã–π</td></tr>
      <tr><td><strong>LSA/LSI</strong></td><td>SVD</td><td>–ü—Ä–æ—Å—Ç–æ–π</td></tr>
      <tr><td><strong>Top2Vec</strong></td><td>Embeddings</td><td>–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π</td></tr>
      <tr><td><strong>BERTopic</strong></td><td>Transformers</td><td>State-of-the-art</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. LDA (Latent Dirichlet Allocation)</h2>
    <p><strong>–ò–¥–µ—è:</strong> –¥–æ–∫—É–º–µ–Ω—Ç - —Å–º–µ—Å—å —Ç–µ–º, —Ç–µ–º–∞ - —Å–º–µ—Å—å —Å–ª–æ–≤</p>
    <pre><code>from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

# Vectorization
vectorizer = CountVectorizer(
    max_features=1000,
    max_df=0.8,
    min_df=5,
    stop_words='english'
)
X = vectorizer.fit_transform(documents)

# LDA model
lda = LatentDirichletAllocation(
    n_components=10,  # —á–∏—Å–ª–æ —Ç–µ–º
    random_state=42,
    max_iter=20
)
doc_topics = lda.fit_transform(X)

# –¢–æ–ø —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã
feature_names = vectorizer.get_feature_names_out()
for topic_idx, topic in enumerate(lda.components_):
    top_words_idx = topic.argsort()[-10:][::-1]
    top_words = [feature_names[i] for i in top_words_idx]
    print(f"Topic {topic_idx}: {', '.join(top_words)}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. NMF (Non-negative Matrix Factorization)</h2>
    <pre><code>from sklearn.decomposition import NMF
from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF
tfidf = TfidfVectorizer(
    max_features=1000,
    max_df=0.8,
    min_df=5
)
X = tfidf.fit_transform(documents)

# NMF
nmf = NMF(
    n_components=10,
    random_state=42,
    init='nndsvda',  # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    max_iter=500
)
doc_topics = nmf.fit_transform(X)
word_topics = nmf.components_

# –í—ã–≤–æ–¥ —Ç–µ–º
feature_names = tfidf.get_feature_names_out()
for topic_idx, topic in enumerate(word_topics):
    top_idx = topic.argsort()[-10:][::-1]
    top_words = [feature_names[i] for i in top_idx]
    print(f"Topic {topic_idx}: {', '.join(top_words)}")</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ LDA vs NMF</h2>
    <table>
      <tr><th>–ê—Å–ø–µ–∫—Ç</th><th>LDA</th><th>NMF</th></tr>
      <tr><td>–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ</td><td>Count matrix</td><td>TF-IDF</td></tr>
      <tr><td>–°–∫–æ—Ä–æ—Å—Ç—å</td><td>–ú–µ–¥–ª–µ–Ω–Ω–µ–µ</td><td>–ë—ã—Å—Ç—Ä–µ–µ</td></tr>
      <tr><td>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</td><td>–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞—è</td><td>–ß–∞—Å—Ç–∏—á–Ω–∞—è</td></tr>
      <tr><td>–ö–∞—á–µ—Å—Ç–≤–æ</td><td>–õ—É—á—à–µ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤</td><td>–•–æ—Ä–æ—à–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö</td></tr>
      <tr><td>Sparse data</td><td>–•–æ—Ä–æ—à–æ</td><td>–û—Ç–ª–∏—á–Ω–æ</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 6. –ü–æ–¥–±–æ—Ä —á–∏—Å–ª–∞ —Ç–µ–º</h2>
    <pre><code>from sklearn.metrics import silhouette_score

# –ü–µ—Ä–µ–±–æ—Ä –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ–º
coherence_scores = []
perplexity_scores = []

for n_topics in range(5, 51, 5):
    lda = LatentDirichletAllocation(
        n_components=n_topics,
        random_state=42
    )
    doc_topics = lda.fit_transform(X)
    
    # Perplexity
    perplexity = lda.perplexity(X)
    perplexity_scores.append(perplexity)
    
    # Coherence (—É–ø—Ä–æ—â—ë–Ω–Ω—ã–π)
    coherence = compute_coherence(lda, X, vectorizer)
    coherence_scores.append(coherence)

# Elbow method
import matplotlib.pyplot as plt
plt.plot(range(5, 51, 5), coherence_scores)
plt.xlabel('Number of topics')
plt.ylabel('Coherence score')
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–º</h2>
    <pre><code>import pyLDAvis.sklearn

# pyLDAvis –¥–ª—è LDA
prepared = pyLDAvis.sklearn.prepare(
    lda, X, vectorizer
)
pyLDAvis.save_html(prepared, 'lda_viz.html')

# t-SNE –¥–ª—è NMF
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=42)
doc_topics_2d = tsne.fit_transform(doc_topics)

plt.scatter(
    doc_topics_2d[:, 0], 
    doc_topics_2d[:, 1],
    c=doc_topics.argmax(axis=1),
    cmap='tab10'
)
plt.title('Document Topics Visualization')
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Gensim LDA</h2>
    <pre><code>from gensim import corpora
from gensim.models import LdaModel
import gensim

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
texts = [doc.split() for doc in documents]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# LDA model
lda_model = LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=10,
    random_state=42,
    passes=10,
    alpha='auto'
)

# –¢–æ–ø —Å–ª–æ–≤–∞ —Ç–µ–º
for idx, topic in lda_model.print_topics(-1):
    print(f'Topic {idx}: {topic}')

# Coherence score
coherence_model = gensim.models.CoherenceModel(
    model=lda_model,
    texts=texts,
    dictionary=dictionary,
    coherence='c_v'
)
coherence_score = coherence_model.get_coherence()
print(f'Coherence Score: {coherence_score}')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. BERTopic - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥</h2>
    <pre><code>from bertopic import BERTopic

# –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å
topic_model = BERTopic(language="russian")
topics, probs = topic_model.fit_transform(documents)

# –í—ã–≤–æ–¥ —Ç–µ–º
topic_model.get_topic_info()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
topic_model.visualize_topics()
topic_model.visualize_barchart()

# –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
similar_topics, similarity = topic_model.find_topics(
    "machine learning", 
    top_n=5
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞</h2>
    <ul>
      <li><strong>Tokenization</strong>: —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å–ª–æ–≤–∞</li>
      <li><strong>Lowercase</strong>: –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É</li>
      <li><strong>Stop words</strong>: —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤</li>
      <li><strong>Lemmatization</strong>: –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ</li>
      <li><strong>Punctuation removal</strong>: —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏</li>
      <li><strong>Min/Max length</strong>: —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–ª–∏–Ω–µ</li>
    </ul>
    <pre><code>import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

def preprocess(text):
    # Lowercase
    text = text.lower()
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Tokenize
    tokens = text.split()
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [t for t in tokens if t not in stop_words]
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    return ' '.join(tokens)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <table>
      <tr><th>–û–±–ª–∞—Å—Ç—å</th><th>–ó–∞–¥–∞—á–∞</th></tr>
      <tr><td>E-commerce</td><td>–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–æ–≤–∞—Ä–æ–≤, –∞–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤</td></tr>
      <tr><td>–ù–æ–≤–æ—Å—Ç–∏</td><td>–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å—Ç–∞—Ç–µ–π, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</td></tr>
      <tr><td>–ê–∫–∞–¥–µ–º–∏—è</td><td>–ê–Ω–∞–ª–∏–∑ –ø—É–±–ª–∏–∫–∞—Ü–∏–π, —Ç—Ä–µ–Ω–¥—ã</td></tr>
      <tr><td>–°–æ—Ü—Å–µ—Ç–∏</td><td>Trending topics, –º–æ–¥–µ—Ä–∞—Ü–∏—è</td></tr>
      <tr><td>–ë–∏–∑–Ω–µ—Å</td><td>–ê–Ω–∞–ª–∏–∑ feedback, customer insights</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞</h2>
    <ul>
      <li><strong>Coherence</strong>: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å —Ç–µ–º</li>
      <li><strong>Perplexity</strong>: –¥–ª—è LDA (–º–µ–Ω—å—à–µ - –ª—É—á—à–µ)</li>
      <li><strong>Topic diversity</strong>: —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–º</li>
      <li><strong>Human evaluation</strong>: —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞</li>
    </ul>
    <pre><code>from gensim.models import CoherenceModel

# Coherence score
coherence_model = CoherenceModel(
    model=lda_model,
    texts=texts,
    dictionary=dictionary,
    coherence='c_v'  # –∏–ª–∏ 'u_mass', 'c_uci'
)
coherence = coherence_model.get_coherence()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ</h2>
    <p><strong>Dynamic Topic Models:</strong> —Ç–µ–º—ã –º–µ–Ω—è—é—Ç—Å—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏</p>
    <pre><code>from gensim.models import LdaSeqModel

# –î–æ–∫—É–º–µ–Ω—Ç—ã —Ä–∞–∑–±–∏—Ç—ã –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –æ—Ç—Ä–µ–∑–∫–∞–º
time_slices = [100, 150, 200]  # –∫–æ–ª-–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

ldaseq = LdaSeqModel(
    corpus=corpus,
    id2word=dictionary,
    time_slice=time_slices,
    num_topics=10
)

# –≠–≤–æ–ª—é—Ü–∏—è —Ç–µ–º—ã 0
ldaseq.print_topic_times(topic=0, top_terms=10)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <ul>
      <li>–ù–∞—á–Ω–∏—Ç–µ —Å 10-20 —Ç–µ–º, –∑–∞—Ç–µ–º –ø–æ–¥–±–∏—Ä–∞–π—Ç–µ</li>
      <li>–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–∏—Ç–∏—á–Ω–∞</li>
      <li>LDA –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤, NMF –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö</li>
      <li>BERTopic –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á</li>
      <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ domain stopwords</li>
      <li>Coherence score –≤–∞–∂–Ω–µ–µ perplexity</li>
      <li>–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏</li>
    </ul>
    <blockquote>
      üí° <strong>–û–±—ä—è—Å–Ω–µ–Ω–∏–µ:</strong> ¬´–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏–º —Ç–µ–º—ã –≤ –±–æ–ª—å—à–∏—Ö –∫–æ–ª–ª–µ–∫—Ü–∏—è—Ö —Ç–µ–∫—Å—Ç–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ 10000 –Ω–æ–≤–æ—Å—Ç–µ–π –≤—ã—è–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã: –ø–æ–ª–∏—Ç–∏–∫–∞, —Å–ø–æ—Ä—Ç, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ ‚Äî –±–µ–∑ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏¬ª.
    </blockquote>
  </div>


</div>
</body>
</html>

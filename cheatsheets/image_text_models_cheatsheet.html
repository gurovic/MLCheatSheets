<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>–ú–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞ Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

    .container {
      column-count: 3;
      column-gap: 20px;
      max-width: 100%;
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üñºÔ∏èüìù –ú–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞ Cheatsheet</h1>
  <div class="subtitle">–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ ‚Ä¢ Vision-Language ‚Ä¢ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>–¶–µ–ª—å</strong>: —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞</li>
      <li><strong>–ó–∞–¥–∞—á–∏</strong>: image captioning, VQA, image-text retrieval</li>
      <li><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong>: dual-encoder –∏–ª–∏ encoder-decoder</li>
      <li><strong>–û–±—É—á–µ–Ω–∏–µ</strong>: contrastive learning, alignment</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 2. –û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏</h2>
    <table>
      <tr><th>–ó–∞–¥–∞—á–∞</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>Image Captioning</strong></td><td>–û–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è</td><td>–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å, SEO</td></tr>
      <tr><td><strong>VQA</strong></td><td>–û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é</td><td>–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã, –ø–æ–∏—Å–∫</td></tr>
      <tr><td><strong>Image-Text Retrieval</strong></td><td>–ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É</td><td>–ü–æ–∏—Å–∫–æ–≤–∏–∫–∏</td></tr>
      <tr><td><strong>Visual Grounding</strong></td><td>–õ–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é</td><td>–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞</td></tr>
      <tr><td><strong>Text-to-Image</strong></td><td>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞</td><td>–ö—Ä–µ–∞—Ç–∏–≤, –¥–∏–∑–∞–π–Ω</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ö–ª—é—á–µ–≤—ã–µ –º–æ–¥–µ–ª–∏</h2>
    <p><strong>CLIP (OpenAI)</strong></p>
    <ul>
      <li>Contrastive pre-training –Ω–∞ 400M –ø–∞—Ä</li>
      <li>Zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</li>
      <li>Image –∏ text encoder –æ—Ç–¥–µ–ª—å–Ω–æ</li>
    </ul>
    <p><strong>ALIGN (Google)</strong></p>
    <ul>
      <li>–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —à—É–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (1.8B –ø–∞—Ä)</li>
      <li>–ü–æ—Ö–æ–∂ –Ω–∞ CLIP, –Ω–æ –±–æ–ª–µ–µ –º–∞—Å—à—Ç–∞–±–Ω—ã–π</li>
    </ul>
    <p><strong>BLIP / BLIP-2</strong></p>
    <ul>
      <li>Bootstrapping –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö</li>
      <li>Image captioning –∏ retrieval</li>
      <li>Q-Former –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 4. CLIP ‚Äî –±–∞–∑–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä</h2>
    <pre><code>from transformers import CLIPProcessor, CLIPModel
from PIL import Image

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
image = Image.open("photo.jpg")
texts = ["a dog", "a cat", "a bird"]

inputs = processor(
    text=texts, 
    images=image, 
    return_tensors="pt", 
    padding=True
)

# –ü–æ–ª—É—á–µ–Ω–∏–µ similarity scores
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image
probs = logits_per_image.softmax(dim=1)
print(probs)  # [prob_dog, prob_cat, prob_bird]</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã</h2>
    <p><strong>Dual-Encoder (CLIP-style)</strong></p>
    <ul>
      <li>–û—Ç–¥–µ–ª—å–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç–∞</li>
      <li>–û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ contrastive loss</li>
      <li>–ë—ã—Å—Ç—Ä—ã–π inference, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –ø–æ–∏—Å–∫</li>
    </ul>
    <pre><code># –ü—Å–µ–≤–¥–æ–∫–æ–¥ CLIP
image_features = image_encoder(image)  # [B, D]
text_features = text_encoder(text)     # [B, D]
# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
image_features = F.normalize(image_features)
text_features = F.normalize(text_features)
# Similarity matrix
logits = image_features @ text_features.T
# Contrastive loss
labels = torch.arange(B)
loss_i = F.cross_entropy(logits, labels)
loss_t = F.cross_entropy(logits.T, labels)
loss = (loss_i + loss_t) / 2</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Encoder-Decoder –º–æ–¥–µ–ª–∏</h2>
    <p><strong>–î–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á</strong></p>
    <table>
      <tr><th>–ú–æ–¥–µ–ª—å</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏</th></tr>
      <tr><td><strong>ViT-GPT2</strong></td><td>Vision Transformer + GPT-2 –¥–ª—è captions</td></tr>
      <tr><td><strong>BLIP</strong></td><td>Encoder-decoder + contrastive learning</td></tr>
      <tr><td><strong>GIT</strong></td><td>Generative Image-to-text Transformer</td></tr>
      <tr><td><strong>Flamingo</strong></td><td>Few-shot vision-language (DeepMind)</td></tr>
      <tr><td><strong>LLaVA</strong></td><td>Visual instruction tuning —Å LLM</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 7. Image Captioning</h2>
    <pre><code>from transformers import BlipProcessor, BlipForConditionalGeneration

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained(
    "Salesforce/blip-image-captioning-base"
)

image = Image.open("photo.jpg")
inputs = processor(image, return_tensors="pt")

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è
out = model.generate(**inputs, max_length=50)
caption = processor.decode(out[0], skip_special_tokens=True)
print(caption)</code></pre>
    <p><strong>–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞</strong>:</p>
    <ul>
      <li>Beam search –¥–ª—è –ª—É—á—à–∏—Ö –æ–ø–∏—Å–∞–Ω–∏–π</li>
      <li>Nucleus sampling –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è</li>
      <li>Temperature –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 8. Visual Question Answering (VQA)</h2>
    <pre><code>from transformers import ViltProcessor, ViltForQuestionAnswering

processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
model = ViltForQuestionAnswering.from_pretrained(
    "dandelin/vilt-b32-finetuned-vqa"
)

image = Image.open("room.jpg")
question = "What color is the sofa?"

inputs = processor(image, question, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
idx = logits.argmax(-1).item()
answer = model.config.id2label[idx]
print(answer)  # "blue"</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Cross-modal Retrieval</h2>
    <p><strong>–ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É</strong></p>
    <pre><code>import torch
import torch.nn.functional as F

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
def get_embeddings(model, processor, images, texts):
    inputs = processor(text=texts, images=images, 
                      return_tensors="pt", padding=True)
    outputs = model(**inputs)
    image_embeds = outputs.image_embeds
    text_embeds = outputs.text_embeds
    return image_embeds, text_embeds

# –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
text_query = "sunset on the beach"
text_emb = get_text_embedding(text_query)
similarities = F.cosine_similarity(text_emb, image_embeds_db)
top_k_indices = similarities.topk(k=5).indices</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –û–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è</h2>
    <p><strong>Contrastive Learning</strong></p>
    <pre><code>def contrastive_loss(image_features, text_features, temperature=0.07):
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    image_features = F.normalize(image_features, dim=-1)
    text_features = F.normalize(text_features, dim=-1)
    
    # Similarity matrix
    logits = (image_features @ text_features.T) / temperature
    
    # Labels (diagonal)
    labels = torch.arange(len(image_features))
    
    # Symmetric loss
    loss_i2t = F.cross_entropy(logits, labels)
    loss_t2i = F.cross_entropy(logits.T, labels)
    
    return (loss_i2t + loss_t2i) / 2</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è</h3>
        <ul>
          <li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pre-trained CLIP/BLIP</li>
          <li>Fine-tuning –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö</li>
          <li>Data augmentation –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</li>
          <li>Prompt engineering –¥–ª—è —Ç–µ–∫—Å—Ç–∞</li>
          <li>–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ò–∑–±–µ–≥–∞—Ç—å</h3>
        <ul>
          <li>–û–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è –±–µ–∑ –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞</li>
          <li>–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞</li>
          <li>–°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π batch size (&lt;256)</li>
          <li>–û–±—É—á–µ–Ω–∏–µ –±–µ–∑ temperature scaling</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏</h2>
    <table>
      <tr><th>–ú–µ—Ç—Ä–∏–∫–∞</th><th>–ó–∞–¥–∞—á–∞</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th></tr>
      <tr><td><strong>BLEU</strong></td><td>Captioning</td><td>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å reference captions</td></tr>
      <tr><td><strong>CIDEr</strong></td><td>Captioning</td><td>Consensus-based –æ—Ü–µ–Ω–∫–∞</td></tr>
      <tr><td><strong>SPICE</strong></td><td>Captioning</td><td>Semantic —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ</td></tr>
      <tr><td><strong>Recall@K</strong></td><td>Retrieval</td><td>–î–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –≤ —Ç–æ–ø-K</td></tr>
      <tr><td><strong>MRR</strong></td><td>Retrieval</td><td>Mean Reciprocal Rank</td></tr>
      <tr><td><strong>VQA Accuracy</strong></td><td>VQA</td><td>–î–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 13. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏</h2>
    <p><strong>Attention Visualization</strong></p>
    <ul>
      <li>–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è cross-attention –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏</li>
      <li>–ü–æ–Ω–∏–º–∞–Ω–∏–µ, –Ω–∞ —á—Ç–æ –º–æ–¥–µ–ª—å "—Å–º–æ—Ç—Ä–∏—Ç"</li>
    </ul>
    <p><strong>Hard Negative Mining</strong></p>
    <ul>
      <li>–í—ã–±–æ—Ä —Å–ª–æ–∂–Ω—ã—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤</li>
      <li>–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ contrastive learning</li>
    </ul>
    <p><strong>Prompt Tuning</strong></p>
    <ul>
      <li>–û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–º–ø—Ç–æ–≤</li>
      <li>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è</h2>
    <ul>
      <li><strong>E-commerce</strong>: –ø–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é</li>
      <li><strong>–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏</strong>: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–¥–ø–∏—Å—å —Ñ–æ—Ç–æ</li>
      <li><strong>–ú–µ–¥–∏—Ü–∏–Ω–∞</strong>: –∞–Ω–∞–ª–∏–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Ç–µ–∫—Å—Ç–æ–º</li>
      <li><strong>–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å</strong>: –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Å–ª–∞–±–æ–≤–∏–¥—è—â–∏—Ö</li>
      <li><strong>–ö–æ–Ω—Ç–µ–Ω—Ç-–º–æ–¥–µ—Ä–∞—Ü–∏—è</strong>: –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π + –ø–æ–¥–ø–∏—Å–µ–π</li>
      <li><strong>–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ</strong>: –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ —É—á–µ–±–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–ú–æ–¥–µ–ª—å –ø–æ–Ω–∏–º–∞–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏ –∏ —Ç–µ–∫—Å—Ç–æ–º. –ú–æ–∂–Ω–æ –∏—Å–∫–∞—Ç—å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é 
      ("–∑–∞–∫–∞—Ç –Ω–∞ –ø–ª—è–∂–µ") –∏–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–ø–∏—Å–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. 
      –≠—Ç–æ –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ —Ñ–æ—Ç–æ –∏ –º–æ–∂–µ—Ç –µ–≥–æ –æ–ø–∏—Å–∞—Ç—å¬ª.
    </blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∑–∞–¥–∞—á—É (captioning/VQA/retrieval)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É (dual-encoder/encoder-decoder)</li>
      <li>[ ] –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç (–ø–∞—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å pre-trained –º–æ–¥–µ–ª—å (CLIP/BLIP/GIT)</li>
      <li>[ ] Fine-tuning –∏–ª–∏ zero-shot</li>
      <li>[ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å data augmentation</li>
      <li>[ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Å—Ç–∏ ablation studies</li>
      <li>[ ] –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å inference (–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è, ONNX)</li>
    </ul>
  </div>

</div>

</body>
</html>

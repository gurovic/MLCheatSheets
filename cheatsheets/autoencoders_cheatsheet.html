<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã (Autoencoders) Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üéØ –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã (Autoencoders)</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å</h2>
    <ul>
      <li><strong>–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä</strong> ‚Äî –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π</li>
      <li><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong>: Encoder ‚Üí Bottleneck ‚Üí Decoder</li>
      <li><strong>–¶–µ–ª—å</strong>: –≤–æ—Å—Å–æ–∑–¥–∞—Ç—å –≤—Ö–æ–¥ –Ω–∞ –≤—ã—Ö–æ–¥–µ</li>
      <li><strong>–û–±—É—á–µ–Ω–∏–µ</strong>: –±–µ–∑ —É—á–∏—Ç–µ–ª—è (unsupervised)</li>
      <li><strong>Bottleneck</strong>: —Å–∂–∞—Ç–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö</li>
    </ul>

    </div>
<div class="block">
    <h2>üî∑ 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</h2>
    <p><strong>Input ‚Üí Encoder ‚Üí Latent Space (z) ‚Üí Decoder ‚Üí Output</strong></p>
    <table>
      <tr><th>–ö–æ–º–ø–æ–Ω–µ–Ω—Ç</th><th>–§—É–Ω–∫—Ü–∏—è</th></tr>
      <tr><td><strong>Encoder</strong></td><td>–°–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ latent space</td></tr>
      <tr><td><strong>Latent space (z)</strong></td><td>–ö–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ</td></tr>
      <tr><td><strong>Decoder</strong></td><td>–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ latent space</td></tr>
    </table>
    <p>–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å latent space << —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞</p>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ü—Ä–æ—Å—Ç–æ–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä (Keras)</h2>
    <pre><code>import tensorflow as tf
from tensorflow.keras import layers, models

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
input_dim = 784  # –Ω–∞–ø—Ä–∏–º–µ—Ä, MNIST 28x28
encoding_dim = 32  # —Ä–∞–∑–º–µ—Ä –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞

# Encoder
encoder_input = layers.Input(shape=(input_dim,))
encoded = layers.Dense(128, activation='relu')(encoder_input)
encoded = layers.Dense(encoding_dim, activation='relu')(encoded)

encoder = models.Model(encoder_input, encoded, name='encoder')

# Decoder
decoder_input = layers.Input(shape=(encoding_dim,))
decoded = layers.Dense(128, activation='relu')(decoder_input)
decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)

decoder = models.Model(decoder_input, decoded, name='decoder')

# Autoencoder
autoencoder_input = layers.Input(shape=(input_dim,))
encoded_output = encoder(autoencoder_input)
decoded_output = decoder(encoded_output)

autoencoder = models.Model(
    autoencoder_input, 
    decoded_output, 
    name='autoencoder'
)

autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder.summary()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –ü—Ä–æ—Å—Ç–æ–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä (PyTorch)</h2>
    <pre><code>import torch
import torch.nn as nn

class Autoencoder(nn.Module):
    def __init__(self, input_dim, encoding_dim):
        super(Autoencoder, self).__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, encoding_dim),
            nn.ReLU()
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim, 128),
            nn.ReLU(),
            nn.Linear(128, input_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded
    
    def encode(self, x):
        return self.encoder(x)
    
    def decode(self, z):
        return self.decoder(z)

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = Autoencoder(input_dim=784, encoding_dim=32)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Convolutional Autoencoder</h2>
    <pre><code># –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
from tensorflow.keras import layers, models

def build_conv_autoencoder(input_shape):
    # Encoder
    encoder_input = layers.Input(shape=input_shape)
    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)
    x = layers.MaxPooling2D((2, 2), padding='same')(x)
    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling2D((2, 2), padding='same')(x)
    encoded = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    
    encoder = models.Model(encoder_input, encoded, name='encoder')
    
    # Decoder
    decoder_input = layers.Input(shape=encoded.shape[1:])
    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(decoder_input)
    x = layers.UpSampling2D((2, 2))(x)
    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = layers.UpSampling2D((2, 2))(x)
    decoded = layers.Conv2D(input_shape[-1], (3, 3), 
                           activation='sigmoid', padding='same')(x)
    
    decoder = models.Model(decoder_input, decoded, name='decoder')
    
    # Autoencoder
    autoencoder_input = layers.Input(shape=input_shape)
    autoencoder = models.Model(
        autoencoder_input,
        decoder(encoder(autoencoder_input))
    )
    
    return encoder, decoder, autoencoder

encoder, decoder, autoencoder = build_conv_autoencoder((28, 28, 1))
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. –û–±—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞</h2>
    <pre><code># Keras
from tensorflow.keras.callbacks import EarlyStopping

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, MNIST)
from tensorflow.keras.datasets import mnist
(x_train, _), (x_test, _) = mnist.load_data()

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

# Flatten –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ AE
x_train = x_train.reshape((len(x_train), -1))
x_test = x_test.reshape((len(x_test), -1))

# –û–±—É—á–µ–Ω–∏–µ
history = autoencoder.fit(
    x_train, x_train,  # –≤—Ö–æ–¥ = –≤—ã—Ö–æ–¥!
    epochs=50,
    batch_size=256,
    shuffle=True,
    validation_data=(x_test, x_test),
    callbacks=[EarlyStopping(patience=5)]
)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
encoded_imgs = encoder.predict(x_test)
decoded_imgs = decoder.predict(encoded_imgs)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –¢–∏–ø—ã –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤</h2>
    <table>
      <tr><th>–¢–∏–ø</th><th>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>Vanilla</strong></td><td>–ü—Ä–æ—Å—Ç–æ–π MLP</td><td>–ë–∞–∑–æ–≤–æ–µ —Å–∂–∞—Ç–∏–µ</td></tr>
      <tr><td><strong>Convolutional</strong></td><td>CNN —Å–ª–æ–∏</td><td>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è</td></tr>
      <tr><td><strong>Sparse</strong></td><td>–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è</td><td>Feature learning</td></tr>
      <tr><td><strong>Denoising</strong></td><td>–®—É–º –Ω–∞ –≤—Ö–æ–¥–µ</td><td>–®—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ</td></tr>
      <tr><td><strong>Variational (VAE)</strong></td><td>–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π</td><td>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è</td></tr>
      <tr><td><strong>Contractive</strong></td><td>–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</td><td>–†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 8. Sparse Autoencoder</h2>
    <pre><code># –î–æ–±–∞–≤–ª–µ–Ω–∏–µ L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏
from tensorflow.keras import regularizers

encoded = layers.Dense(
    encoding_dim, 
    activation='relu',
    activity_regularizer=regularizers.l1(1e-5)
)(encoder_input)

# –ò–ª–∏ KL-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è –¥–ª—è sparsity
from tensorflow.keras import backend as K

def kl_divergence_regularizer(rho=0.05):
    def loss(y_true, y_pred):
        rho_hat = K.mean(y_pred, axis=0)
        kl = rho * K.log(rho / rho_hat) + \
             (1 - rho) * K.log((1 - rho) / (1 - rho_hat))
        return K.sum(kl)
    return loss

# rho ‚Äî –∂–µ–ª–∞–µ–º–∞—è —Å—Ä–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è (–æ–±—ã—á–Ω–æ 0.05)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. Denoising Autoencoder</h2>
    <pre><code>import numpy as np

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –∫ –¥–∞–Ω–Ω—ã–º
noise_factor = 0.5
x_train_noisy = x_train + noise_factor * np.random.normal(
    loc=0.0, scale=1.0, size=x_train.shape
)
x_test_noisy = x_test + noise_factor * np.random.normal(
    loc=0.0, scale=1.0, size=x_test.shape
)

# Clip –∑–Ω–∞—á–µ–Ω–∏—è –≤ [0, 1]
x_train_noisy = np.clip(x_train_noisy, 0., 1.)
x_test_noisy = np.clip(x_test_noisy, 0., 1.)

# –û–±—É—á–µ–Ω–∏–µ: –≤—Ö–æ–¥ ‚Äî –∑–∞—à—É–º–ª—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –≤—ã—Ö–æ–¥ ‚Äî —á–∏—Å—Ç—ã–µ
autoencoder.fit(
    x_train_noisy, x_train,
    epochs=50,
    batch_size=256,
    validation_data=(x_test_noisy, x_test)
)

# –¢–µ–ø–µ—Ä—å –º–æ–¥–µ–ª—å —É–º–µ–µ—Ç —É–±–∏—Ä–∞—Ç—å —à—É–º!</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Variational Autoencoder (VAE)</h2>
    <pre><code>from tensorflow.keras import backend as K

class Sampling(layers.Layer):
    """Reparameterization trick"""
    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = K.shape(z_mean)[0]
        dim = K.shape(z_mean)[1]
        epsilon = K.random_normal(shape=(batch, dim))
        return z_mean + K.exp(0.5 * z_log_var) * epsilon

# Encoder
encoder_input = layers.Input(shape=(input_dim,))
x = layers.Dense(128, activation='relu')(encoder_input)
z_mean = layers.Dense(latent_dim)(x)
z_log_var = layers.Dense(latent_dim)(x)
z = Sampling()([z_mean, z_log_var])

encoder = models.Model(encoder_input, [z_mean, z_log_var, z])

# Decoder
decoder_input = layers.Input(shape=(latent_dim,))
x = layers.Dense(128, activation='relu')(decoder_input)
decoder_output = layers.Dense(input_dim, activation='sigmoid')(x)

decoder = models.Model(decoder_input, decoder_output)

# VAE loss
def vae_loss(x, x_decoded):
    # Reconstruction loss
    reconstruction_loss = K.binary_crossentropy(x, x_decoded)
    reconstruction_loss *= input_dim
    
    # KL divergence
    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
    kl_loss = K.sum(kl_loss, axis=-1)
    kl_loss *= -0.5
    
    return K.mean(reconstruction_loss + kl_loss)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤</h2>
    <ul>
      <li><strong>–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏</strong> ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ PCA</li>
      <li><strong>–®—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ</strong> ‚Äî denoising autoencoder</li>
      <li><strong>–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π</strong> ‚Äî –≤—ã—Å–æ–∫–∞—è –æ—à–∏–±–∫–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏</li>
      <li><strong>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö</strong> ‚Äî VAE</li>
      <li><strong>–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ</strong> ‚Äî transfer learning</li>
      <li><strong>Feature extraction</strong> ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å encoder</li>
      <li><strong>–°–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö</strong> ‚Äî lossy compression</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 12. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π</h2>
    <pre><code># –û–±—É—á–∏—Ç—å –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
autoencoder.fit(x_train_normal, x_train_normal, epochs=50)

# –í—ã—á–∏—Å–ª–∏—Ç—å –æ—à–∏–±–∫—É —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
reconstructed = autoencoder.predict(x_test)
mse = np.mean(np.square(x_test - reconstructed), axis=1)

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ—Ä–æ–≥
threshold = np.percentile(mse, 95)

# –ê–Ω–æ–º–∞–ª–∏–∏ ‚Äî —ç—Ç–æ –¥–∞–Ω–Ω—ã–µ —Å –≤—ã—Å–æ–∫–æ–π –æ—à–∏–±–∫–æ–π
anomalies = mse > threshold

print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {anomalies.sum()}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import matplotlib.pyplot as plt
plt.hist(mse, bins=50)
plt.axvline(threshold, color='r', linestyle='--', label='Threshold')
plt.xlabel('Reconstruction Error')
plt.ylabel('Count')
plt.legend()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 13. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞</h2>
    <pre><code>import matplotlib.pyplot as plt

# –î–ª—è 2D –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
encoding_dim = 2
# ... –æ–±—É—á–∏—Ç—å autoencoder ...

# –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
encoded_imgs = encoder.predict(x_test)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(10, 8))
plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], 
           c=y_test, cmap='viridis', alpha=0.5)
plt.colorbar()
plt.xlabel('Latent dimension 1')
plt.ylabel('Latent dimension 2')
plt.title('Latent Space Visualization')
plt.show()

# –î–ª—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ > 2 –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å t-SNE –∏–ª–∏ UMAP
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=42)
encoded_2d = tsne.fit_transform(encoded_imgs)

plt.scatter(encoded_2d[:, 0], encoded_2d[:, 1], 
           c=y_test, cmap='viridis', alpha=0.5)
plt.colorbar()
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (VAE)</h2>
    <pre><code># –°–µ–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å –∏–∑ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
import numpy as np

# –°–ª—É—á–∞–π–Ω—ã–µ —Ç–æ—á–∫–∏ –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
n_samples = 10
random_latent_vectors = np.random.normal(size=(n_samples, latent_dim))

# –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
generated_images = decoder.predict(random_latent_vectors)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import matplotlib.pyplot as plt
n = 10
plt.figure(figsize=(20, 2))
for i in range(n):
    ax = plt.subplot(1, n, i + 1)
    plt.imshow(generated_images[i].reshape(28, 28), cmap='gray')
    plt.axis('off')
plt.show()

# –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–æ—á–∫–∞–º–∏
def interpolate(encoder, decoder, img1, img2, steps=10):
    z1 = encoder.predict(img1.reshape(1, -1))[0]  # mean –¥–ª—è VAE
    z2 = encoder.predict(img2.reshape(1, -1))[0]
    
    interpolated = []
    for alpha in np.linspace(0, 1, steps):
        z = (1 - alpha) * z1 + alpha * z2
        interpolated.append(decoder.predict(z.reshape(1, -1)))
    
    return interpolated</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 15. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</h2>
    <div class="good-vs-bad">
      <div class="good">
        <h3>‚úÖ –•–æ—Ä–æ—à–æ</h3>
        <ul>
          <li>–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏</li>
          <li>–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π</li>
          <li>–®—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ</li>
          <li>Feature extraction</li>
          <li>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (VAE)</li>
          <li>–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á</li>
        </ul>
      </div>
      <div class="bad">
        <h3>‚ùå –ü–ª–æ—Ö–æ / –ï—Å—Ç—å –ª—É—á—à–µ</h3>
        <ul>
          <li>–ü—Ä–æ—Å—Ç–æ–µ —Å–∂–∞—Ç–∏–µ ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ PCA</li>
          <li>–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ supervised learning</li>
          <li>–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ GAN</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="block">
    <h2>üî∑ 16. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞</h2>
    <pre><code># 1. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
# - –°–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π encoder/decoder
# - –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ/—É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏

# 2. –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
# Binary crossentropy –¥–ª—è [0,1]
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# MSE –¥–ª—è –¥—Ä—É–≥–∏—Ö
autoencoder.compile(optimizer='adam', loss='mse')

# 3. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
from tensorflow.keras import regularizers
layers.Dense(128, activation='relu',
            kernel_regularizer=regularizers.l2(1e-4))

# 4. Batch Normalization
from tensorflow.keras.layers import BatchNormalization
x = layers.Dense(128)(x)
x = BatchNormalization()(x)
x = layers.Activation('relu')(x)

# 5. Learning rate scheduling
from tensorflow.keras.callbacks import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)

history = autoencoder.fit(
    x_train, x_train,
    epochs=100,
    callbacks=[reduce_lr]
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞</h2>
    <pre><code># 1. Reconstruction Loss
from sklearn.metrics import mean_squared_error

reconstructed = autoencoder.predict(x_test)
mse = mean_squared_error(x_test.flatten(), 
                         reconstructed.flatten())
print(f"MSE: {mse:.4f}")

# 2. SSIM (–¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
from skimage.metrics import structural_similarity as ssim

ssim_scores = []
for i in range(len(x_test)):
    score = ssim(x_test[i], reconstructed[i])
    ssim_scores.append(score)

print(f"Mean SSIM: {np.mean(ssim_scores):.4f}")

# 3. Visualization
n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    # –û—Ä–∏–≥–∏–Ω–∞–ª
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')
    plt.axis('off')
    
    # –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')
    plt.axis('off')
plt.show()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 18. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (0-1 –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è)</li>
      <li>[ ] –í—ã–±—Ä–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–∏–ø AE –¥–ª—è –∑–∞–¥–∞—á–∏</li>
      <li>[ ] –ü–æ–¥–æ–±—Ä–∞—Ç—å —Ä–∞–∑–º–µ—Ä –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞</li>
      <li>[ ] –°–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ encoder/decoder</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å appropriate activation functions</li>
      <li>[ ] –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (BCE vs MSE)</li>
      <li>[ ] –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è</li>
      <li>[ ] –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏</li>
      <li>[ ] –î–ª—è VAE: –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å reconstruction –∏ KL loss</li>
    </ul>

    <h3>üí° –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫—É:</h3>
    <blockquote>
      ¬´–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä ‚Äî —ç—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—Å—è —Å–∂–∏–º–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–æ —Å–∞–º–æ–≥–æ –≤–∞–∂–Ω–æ–≥–æ, –∞ –ø–æ—Ç–æ–º –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –∏—Ö –æ–±—Ä–∞—Ç–Ω–æ. –ö–∞–∫ ZIP-–∞—Ä—Ö–∏–≤, –Ω–æ "—É–º–Ω—ã–π" ‚Äî –æ–Ω –ø–æ–Ω–∏–º–∞–µ—Ç, —á—Ç–æ –≤–∞–∂–Ω–æ –≤ –¥–∞–Ω–Ω—ã—Ö, –∞ —á—Ç–æ –º–æ–∂–Ω–æ –æ—Ç–±—Ä–æ—Å–∏—Ç—å¬ª.
    </blockquote>
  </div>



</div>
</body>
</html>

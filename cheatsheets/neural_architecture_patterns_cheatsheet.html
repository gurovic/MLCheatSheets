<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Neural Architecture Design Patterns Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    .good-vs-bad {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .good-vs-bad div {
      flex: 1;
      padding: 6px 8px;
      border-radius: 4px;
    }

    .good {
      background-color: #f0f9f4;
      border-left: 3px solid #2e8b57;
    }

    .bad {
      background-color: #fdf0f2;
      border-left: 3px solid #d32f2f;
    }

    .good h3, .bad h3 {
      margin: 0 0 4px;
      font-size: 1em;
      font-weight: 700;
    }

    .good ul, .bad ul {
      padding-left: 20px;
      margin: 0;
    }

    .good li::before { content: "‚úÖ "; font-weight: bold; }
    .bad li::before { content: "‚ùå "; font-weight: bold; }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>

</head>
<body>

<div class="container">

  <h1>üèóÔ∏è Neural Architecture Design Patterns</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã</h2>
    <ul>
      <li><strong>Skip Connections (ResNet)</strong>: –ø—Ä—è–º—ã–µ —Å–≤—è–∑–∏ —á–µ—Ä–µ–∑ —Å–ª–æ–∏</li>
      <li><strong>Bottleneck</strong>: —Å–∂–∞—Ç–∏–µ-—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏</li>
      <li><strong>Inception Module</strong>: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Å–≤–µ—Ä—Ç–∫–∏ —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞</li>
      <li><strong>Squeeze-and-Excitation</strong>: channel attention</li>
      <li><strong>Separable Convolutions</strong>: depthwise + pointwise</li>
      <li><strong>Multi-Head</strong>: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ attention heads</li>
    </ul>

  <div class="block">
    <h2>üî∑ 2. Skip Connections</h2>
    <pre><code>import torch.nn as nn

class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        residual = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual  # Skip connection
        out = self.relu(out)
        return out</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. Bottleneck Architecture</h2>
    <pre><code>class Bottleneck(nn.Module):
    """1x1 conv –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è/–ø–æ–≤—ã—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏"""
    def __init__(self, in_channels, bottleneck_channels, out_channels):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, bottleneck_channels, 1)
        self.conv2 = nn.Conv2d(bottleneck_channels, bottleneck_channels, 3, padding=1)
        self.conv3 = nn.Conv2d(bottleneck_channels, out_channels, 1)
        self.bn1 = nn.BatchNorm2d(bottleneck_channels)
        self.bn2 = nn.BatchNorm2d(bottleneck_channels)
        self.bn3 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU()
        
        # Projection shortcut –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã–µ
        self.shortcut = nn.Sequential()
        if in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        return self.relu(out)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. Inception Module</h2>
    <pre><code>class InceptionModule(nn.Module):
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Å–≤–µ—Ä—Ç–∫–∏ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤"""
    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_pool):
        super().__init__()
        # 1x1 conv branch
        self.branch1 = nn.Conv2d(in_channels, out_1x1, 1)
        
        # 3x3 conv branch (—Å reduction)
        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, red_3x3, 1),
            nn.Conv2d(red_3x3, out_3x3, 3, padding=1)
        )
        
        # 5x5 conv branch (—Å reduction)
        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, red_5x5, 1),
            nn.Conv2d(red_5x5, out_5x5, 5, padding=2)
        )
        
        # Pool branch
        self.branch4 = nn.Sequential(
            nn.MaxPool2d(3, stride=1, padding=1),
            nn.Conv2d(in_channels, out_pool, 1)
        )
    
    def forward(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)
        # Concatenate –ø–æ –∫–∞–Ω–∞–ª–∞–º
        return torch.cat([branch1, branch2, branch3, branch4], 1)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. Depthwise Separable Convolutions</h2>
    <pre><code>class SeparableConv2d(nn.Module):
    """Depthwise + Pointwise convolution"""
    def __init__(self, in_channels, out_channels, kernel_size):
        super().__init__()
        # Depthwise: –∫–∞–∂–¥—ã–π –∫–∞–Ω–∞–ª –æ—Ç–¥–µ–ª—å–Ω–æ
        self.depthwise = nn.Conv2d(
            in_channels, in_channels, kernel_size,
            padding=kernel_size//2, groups=in_channels
        )
        # Pointwise: 1x1 conv
        self.pointwise = nn.Conv2d(in_channels, out_channels, 1)
    
    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x

# –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ ~9 —Ä–∞–∑ –º–µ–Ω—å—à–µ —á–µ–º –≤ –æ–±—ã—á–Ω–æ–π —Å–≤–µ—Ä—Ç–∫–µ!</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. Squeeze-and-Excitation (SE)</h2>
    <pre><code>class SEBlock(nn.Module):
    """Channel attention mechanism"""
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.squeeze = nn.AdaptiveAvgPool2d(1)
        self.excitation = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        b, c, _, _ = x.size()
        # Squeeze: global pooling
        y = self.squeeze(x).view(b, c)
        # Excitation: FC layers
        y = self.excitation(y).view(b, c, 1, 1)
        # Scale: channel-wise multiplication
        return x * y.expand_as(x)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. –ì–∏–±—Ä–∏–¥–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã</h2>
    <ul>
      <li><strong>CNN + RNN</strong>: feature extraction + sequence modeling</li>
      <li><strong>CNN + Transformer</strong>: Vision Transformer (ViT)</li>
      <li><strong>Multi-scale processing</strong>: FPN, U-Net</li>
      <li><strong>Attention + Convolution</strong>: ConvNeXt, CoAtNet</li>
    </ul>
    <pre><code>class HybridCNNRNN(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.cnn = models.resnet18(pretrained=True)
        self.cnn.fc = nn.Identity()
        
        self.rnn = nn.LSTM(512, 256, 2, batch_first=True)
        self.fc = nn.Linear(256, num_classes)
    
    def forward(self, x):
        # x: (batch, seq_len, C, H, W)
        batch_size, seq_len = x.size(0), x.size(1)
        
        # CNN features –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–µ–π–º–∞
        features = []
        for t in range(seq_len):
            feat = self.cnn(x[:, t])
            features.append(feat)
        
        features = torch.stack(features, dim=1)  # (batch, seq, 512)
        
        # RNN
        out, _ = self.rnn(features)
        out = self.fc(out[:, -1, :])  # –ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥
        return out</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. Best Practices</h2>
    <ul>
      <li><strong>Normalization</strong>: BatchNorm, LayerNorm, GroupNorm</li>
      <li><strong>Activation</strong>: ReLU ‚Üí GELU, Swish –¥–ª—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π</li>
      <li><strong>Regularization</strong>: Dropout, DropPath, Stochastic Depth</li>
      <li><strong>Initialization</strong>: He –¥–ª—è ReLU, Xavier –¥–ª—è Tanh</li>
      <li><strong>Skip connections</strong>: –∫—Ä–∏—Ç–∏—á–Ω—ã –¥–ª—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π</li>
      <li><strong>Bottlenecks</strong>: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 9. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤</h2>
    <table>
      <tr><th>–ü–∞—Ç—Ç–µ—Ä–Ω</th><th>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ</th><th>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</th></tr>
      <tr><td><strong>Skip Connections</strong></td><td>–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –ø–æ—Ç–æ–∫</td><td>–ì–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏</td></tr>
      <tr><td><strong>Bottleneck</strong></td><td>–ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</td><td>ResNet, EfficientNet</td></tr>
      <tr><td><strong>Inception</strong></td><td>Multi-scale</td><td>GoogLeNet</td></tr>
      <tr><td><strong>SE Block</strong></td><td>Channel attention</td><td>SENet, EfficientNet</td></tr>
      <tr><td><strong>Separable Conv</strong></td><td>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å</td><td>MobileNet, Xception</td></tr>
    </table>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ß–µ–∫-–ª–∏—Å—Ç</h2>
    <ul>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å skip connections –¥–ª—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π</li>
      <li>[ ] –î–æ–±–∞–≤–∏—Ç—å normalization –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —Å–≤–µ—Ä—Ç–∫–∏</li>
      <li>[ ] –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å bottleneck –¥–ª—è efficiency</li>
      <li>[ ] –ü—Ä–∏–º–µ–Ω–∏—Ç—å separable convolutions –Ω–∞ mobile</li>
      <li>[ ] –î–æ–±–∞–≤–∏—Ç—å attention –º–µ—Ö–∞–Ω–∏–∑–º—ã</li>
      <li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é</li>
      <li>[ ] –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è: Dropout, DropPath</li>
      <li>[ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å gradient flow</li>
    </ul>
    <blockquote>
      ¬´–•–æ—Ä–æ—à–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ = –ø–∞—Ç—Ç–µ—Ä–Ω—ã + domain knowledge. Skip connections, bottlenecks –∏ attention ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –±–ª–æ–∫–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π¬ª.
    </blockquote>
  </div>

</div>

</div>
</body>
</html>

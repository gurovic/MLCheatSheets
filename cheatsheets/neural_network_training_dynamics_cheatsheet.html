<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>–î–∏–Ω–∞–º–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen {
      body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        color: #333;
        background: #fafcff;
        padding: 10px;
      
        min-width: 900px;
      }
    }
    @media print {
      body {
        background: white;
        padding: 0;
      }
      @page {
        size: A4 landscape;
        margin: 10mm;
      }
    }

        .container {
      max-width: 100%;
    }

    /* Responsive columns: 3 columns on wide screens, fewer on narrow */
    @media (min-width: 900px) {
      .container {
        column-count: 3;
        column-gap: 20px;
      }
    }
    
    @media (min-width: 600px) and (max-width: 899px) {
      .container {
        column-count: 2;
        column-gap: 20px;
      }
    }
    
    @media (max-width: 599px) {
      .container {
        column-count: 1;
      }
    }

    .block {
      break-inside: avoid;
      margin-bottom: 1.2em;
      padding: 12px;
      background: white;
      border-radius: 6px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }

    h1 {
      font-size: 1.6em;
      font-weight: 700;
      color: #1a5fb4;
      text-align: center;
      margin: 0 0 8px;
      column-span: all;
    }

    .subtitle {
      text-align: center;
      color: #666;
      font-size: 0.9em;
      margin-bottom: 12px;
      column-span: all;
    }

    h2 {
      font-size: 1.15em;
      font-weight: 700;
      color: #1a5fb4;
      margin: 0 0 8px;
      padding-bottom: 4px;
      border-bottom: 1px solid #e0e7ff;
    }

    p, ul, ol {
      font-size: 0.92em;
      margin: 0.6em 0;
    }

    ul, ol {
      padding-left: 18px;
    }

    li {
      margin-bottom: 4px;
    }

    code {
      font-family: 'Consolas', 'Courier New', monospace;
      background-color: #f0f4ff;
      padding: 1px 4px;
      border-radius: 3px;
      font-size: 0.88em;
    }

    pre {
      background-color: #f0f4ff;
      padding: 8px;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.84em;
      margin: 6px 0;
    }

    pre code {
      padding: 0;
      background: none;
      white-space: pre-wrap;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.82em;
      margin: 6px 0;
    }

    th {
      background-color: #e6f0ff;
      text-align: left;
      padding: 4px 6px;
      font-weight: 600;
    }

    td {
      padding: 4px 6px;
      border-bottom: 1px solid #f0f4ff;
    }

    tr:nth-child(even) {
      background-color: #f8fbff;
    }

    blockquote {
      font-style: italic;
      margin: 8px 0;
      padding: 6px 10px;
      background: #f8fbff;
      border-left: 2px solid #1a5fb4;
      font-size: 0.88em;
    }

    @media print {
      .container { column-gap: 12px; }
      .block { box-shadow: none; }
      code, pre, table { font-size: 0.78em; }
      h1 { font-size: 1.4em; }
      h2 { font-size: 1em; }
    }
  </style>
</head>
<body>

<div class="container">

  <h1>üß† –î–∏–Ω–∞–º–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π</h1>
  <div class="subtitle">üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –í–≤–µ–¥–µ–Ω–∏–µ –≤ –¥–∏–Ω–∞–º–∏–∫—É –æ–±—É—á–µ–Ω–∏—è</h2>
    <p><strong>–î–∏–Ω–∞–º–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è</strong> ‚Äî —ç—Ç–æ –∏–∑—É—á–µ–Ω–∏–µ —Ç–æ–≥–æ, –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∏–∑–º–µ–Ω—è—é—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.</p>
    <ul>
      <li><strong>–¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –≤–µ—Å–æ–≤</strong>: –ø—É—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ</li>
      <li><strong>–°–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏</strong>: –∫–∞–∫ –±—ã—Å—Ç—Ä–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º</li>
      <li><strong>–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</strong>: —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≤–æ–∑–º—É—â–µ–Ω–∏—è–º</li>
      <li><strong>–û–±–æ–±—â–µ–Ω–∏–µ</strong>: —Å–≤—è–∑—å —Å –∫–∞—á–µ—Å—Ç–≤–æ–º –Ω–∞ —Ç–µ—Å—Ç–µ</li>
    </ul>
    <blockquote>üí° –ü–æ–Ω–∏–º–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏–∫–∏ –ø–æ–º–æ–≥–∞–µ—Ç –≤—ã–±—Ä–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã</blockquote>

    </div>
<div class="block">
    <h2>üî∑ 2. –õ–∞–Ω–¥—à–∞—Ñ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å</h2>
    <p><strong>Loss landscape</strong> ‚Äî –≥–µ–æ–º–µ—Ç—Ä–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.</p>
    
    <p><strong>–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏</strong>:</p>
    <ul>
      <li>–õ–æ–∫–∞–ª—å–Ω—ã–µ –º–∏–Ω–∏–º—É–º—ã –∏ –º–∞–∫—Å–∏–º—É–º—ã</li>
      <li>–°–µ–¥–ª–æ–≤—ã–µ —Ç–æ—á–∫–∏ (–ø—Ä–µ–æ–±–ª–∞–¥–∞—é—Ç –≤ –≤—ã—Å–æ–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è—Ö)</li>
      <li>–ü–ª–∞—Ç–æ (–æ–±–ª–∞—Å—Ç–∏ —Å –º–∞–ª—ã–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–º)</li>
      <li>–û—Å—Ç—Ä—ã–µ vs –ø–ª–æ—Å–∫–∏–µ –º–∏–Ω–∏–º—É–º—ã</li>
    </ul>

    <p><strong>–í–∞–∂–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ</strong>: –ü–ª–æ—Å–∫–∏–µ –º–∏–Ω–∏–º—É–º—ã –æ–±—ã—á–Ω–æ –æ–±–æ–±—â–∞—é—Ç –ª—É—á—à–µ, —á–µ–º –æ—Å—Ç—Ä—ã–µ.</p>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –∏ –≤–∞—Ä–∏–∞—Ü–∏–∏</h2>
    <p><strong>–û—Å–Ω–æ–≤–Ω–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ</strong>:</p>
    <p>Œ∏(t+1) = Œ∏(t) - Œ∑ ‚àáL(Œ∏(t))</p>
    
    <p><strong>–§–∞–∫—Ç–æ—Ä—ã –≤–ª–∏—è–Ω–∏—è</strong>:</p>
    <ul>
      <li><strong>Learning rate (Œ∑)</strong>: —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è</li>
      <li><strong>Batch size</strong>: —à—É–º –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö</li>
      <li><strong>Momentum</strong>: –∏–Ω–µ—Ä—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏—è</li>
      <li><strong>Adaptive rates</strong>: Adam, RMSProp</li>
    </ul>

    <pre><code># –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
import matplotlib.pyplot as plt
import numpy as np

# –õ–æ–≥–∏—Ä—É–µ–º –≤–µ—Å–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
weights_history = []

for epoch in range(epochs):
    optimizer.zero_grad()
    loss = criterion(model(X), y)
    loss.backward()
    optimizer.step()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–Ω–∏–º–æ–∫ –≤–µ—Å–æ–≤
    weights_history.append(
        model.state_dict()['layer.weight'].clone()
    )</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 4. –§–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è</h2>
    <p>–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–∑:</p>
    
    <p><strong>1. Warm-up (—Ä–∞–∑–æ–≥—Ä–µ–≤)</strong>:</p>
    <ul>
      <li>–ù–∞—á–∞–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</li>
      <li>–ë—ã—Å—Ç—Ä–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ loss</li>
      <li>Learning rate –º–æ–∂–µ—Ç —Ä–∞—Å—Ç–∏</li>
    </ul>

    <p><strong>2. Linear regime (–ª–∏–Ω–µ–π–Ω–∞—è —Ñ–∞–∑–∞)</strong>:</p>
    <ul>
      <li>–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</li>
      <li>–ü—Ä–∏–º–µ—Ä–Ω–æ –ª–∏–Ω–µ–π–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ loss</li>
      <li>–û—Å–Ω–æ–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∑–¥–µ—Å—å</li>
    </ul>

    <p><strong>3. Fine-tuning (—Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞)</strong>:</p>
    <ul>
      <li>–ú–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å</li>
      <li>–ú–∞–ª—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è loss</li>
      <li>–ß–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–µ–Ω—å—à–∏–π learning rate</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 5. Learning rate scheduling</h2>
    <p>–ò–∑–º–µ–Ω–µ–Ω–∏–µ learning rate –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ.</p>

    <p><strong>–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏</strong>:</p>
    <pre><code># Step decay
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)

# Cosine annealing
scheduler = CosineAnnealingLR(optimizer, T_max=100)

# ReduceLROnPlateau
scheduler = ReduceLROnPlateau(
    optimizer, 
    mode='min',
    factor=0.1,
    patience=10
)

# –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–π LR
scheduler = CyclicLR(
    optimizer,
    base_lr=0.001,
    max_lr=0.01,
    step_size_up=2000
)</code></pre>

    <blockquote>‚ö° –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π schedule –º–æ–∂–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ 2-3 —Ä–∞–∑–∞</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 6. Warm-up —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏</h2>
    <p>–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ LR –≤ –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è:</p>

    <pre><code># Linear warm-up
def get_lr(epoch, warmup_epochs=5, max_lr=0.001):
    if epoch < warmup_epochs:
        return max_lr * (epoch + 1) / warmup_epochs
    return max_lr

# –í PyTorch
from torch.optim.lr_scheduler import LambdaLR

def warmup_lambda(epoch):
    if epoch < 5:
        return (epoch + 1) / 5
    return 1.0

scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)</code></pre>

    <p><strong>–ó–∞—á–µ–º –Ω—É–∂–µ–Ω warm-up?</strong></p>
    <ul>
      <li>–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –≤ –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è</li>
      <li>–ò–∑–±–µ–∂–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤</li>
      <li>–õ—É—á—à–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö batch size</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 7. Batch size –∏ –¥–∏–Ω–∞–º–∏–∫–∞</h2>
    <p>–†–∞–∑–º–µ—Ä batch —Å–∏–ª—å–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –¥–∏–Ω–∞–º–∏–∫—É –æ–±—É—á–µ–Ω–∏—è.</p>

    <table>
      <tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–ú–∞–ª—ã–π batch</th><th>–ë–æ–ª—å—à–æ–π batch</th></tr>
      <tr>
        <td><strong>–®—É–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞</strong></td>
        <td>–í—ã—Å–æ–∫–∏–π</td>
        <td>–ù–∏–∑–∫–∏–π</td>
      </tr>
      <tr>
        <td><strong>–°—Ö–æ–¥–∏–º–æ—Å—Ç—å</strong></td>
        <td>–ú–µ–¥–ª–µ–Ω–Ω–∞—è</td>
        <td>–ë—ã—Å—Ç—Ä–∞—è</td>
      </tr>
      <tr>
        <td><strong>–û–±–æ–±—â–µ–Ω–∏–µ</strong></td>
        <td>–õ—É—á—à–µ</td>
        <td>–•—É–∂–µ</td>
      </tr>
      <tr>
        <td><strong>–ú–∏–Ω–∏–º—É–º—ã</strong></td>
        <td>–ü–ª–æ—Å–∫–∏–µ</td>
        <td>–û—Å—Ç—Ä—ã–µ</td>
      </tr>
      <tr>
        <td><strong>–ü–∞–º—è—Ç—å GPU</strong></td>
        <td>–ú–µ–Ω—å—à–µ</td>
        <td>–ë–æ–ª—å—à–µ</td>
      </tr>
    </table>

    <p><strong>–ü—Ä–∞–≤–∏–ª–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è LR</strong>:</p>
    <p>–ü—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ batch –≤ k —Ä–∞–∑, —É–≤–µ–ª–∏—á—å—Ç–µ LR –≤ ‚àök —Ä–∞–∑</p>
  </div>

  <div class="block">
    <h2>üî∑ 8. Gradient accumulation</h2>
    <p>–≠–º—É–ª—è—Ü–∏—è –±–æ–ª—å—à–æ–≥–æ batch –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏:</p>

    <pre><code>accumulation_steps = 4
model.zero_grad()

for i, (X, y) in enumerate(train_loader):
    # Forward pass
    output = model(X)
    loss = criterion(output, y)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º loss
    loss = loss / accumulation_steps
    
    # Backward pass (–Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã)
    loss.backward()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()</code></pre>

    <p>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size = batch_size √ó accumulation_steps</p>
  </div>

  <div class="block">
    <h2>üî∑ 9. Momentum –∏ –∏–Ω–µ—Ä—Ü–∏—è</h2>
    <p>Momentum –¥–æ–±–∞–≤–ª—è–µ—Ç "–ø–∞–º—è—Ç—å" –ø—Ä–æ—à–ª—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤:</p>
    <p>v(t) = Œ≤¬∑v(t-1) + ‚àáL(Œ∏)</p>
    <p>Œ∏(t+1) = Œ∏(t) - Œ∑¬∑v(t)</p>

    <p><strong>–≠—Ñ—Ñ–µ–∫—Ç—ã</strong>:</p>
    <ul>
      <li>–°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏</li>
      <li>–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö</li>
      <li>–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤</li>
      <li>–°–Ω–∏–∂–µ–Ω–∏–µ –∫–æ–ª–µ–±–∞–Ω–∏–π</li>
    </ul>

    <pre><code># SGD —Å momentum
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.01,
    momentum=0.9,
    nesterov=True  # Nesterov momentum
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã</h2>
    <p><strong>Adam</strong>: –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç LR –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞</p>
    <p>m(t) = Œ≤‚ÇÅ¬∑m(t-1) + (1-Œ≤‚ÇÅ)¬∑‚àáL</p>
    <p>v(t) = Œ≤‚ÇÇ¬∑v(t-1) + (1-Œ≤‚ÇÇ)¬∑‚àáL¬≤</p>
    <p>Œ∏(t+1) = Œ∏(t) - Œ∑¬∑mÃÇ(t) / (‚àövÃÇ(t) + Œµ)</p>

    <pre><code># –†–∞–∑–ª–∏—á–Ω—ã–µ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã
from torch import optim

# Adam (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.999)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# AdamW (—Å weight decay)
optimizer = optim.AdamW(
    model.parameters(),
    lr=0.001,
    weight_decay=0.01
)

# RMSprop
optimizer = optim.RMSprop(
    model.parameters(),
    lr=0.01,
    alpha=0.99
)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Gradient clipping</h2>
    <p>–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –≤–∑—Ä—ã–≤–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (exploding gradients):</p>

    <pre><code># Gradient clipping –ø–æ –Ω–æ—Ä–º–µ
import torch.nn as nn

max_norm = 1.0

for epoch in range(epochs):
    optimizer.zero_grad()
    loss = criterion(model(X), y)
    loss.backward()
    
    # –û–±—Ä–µ–∑–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
    nn.utils.clip_grad_norm_(
        model.parameters(), 
        max_norm
    )
    
    optimizer.step()

# Clipping –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é
nn.utils.clip_grad_value_(
    model.parameters(),
    clip_value=0.5
)</code></pre>

    <p><strong>–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å</strong>:</p>
    <ul>
      <li>RNN –∏ LSTM (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!)</li>
      <li>–û—á–µ–Ω—å –≥–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏</li>
      <li>–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 12. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –¥–∏–Ω–∞–º–∏–∫–∞</h2>
    <p>–ù–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è.</p>

    <p><strong>–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã</strong>:</p>
    <pre><code># Xavier/Glorot initialization
nn.init.xavier_uniform_(layer.weight)

# He initialization (–¥–ª—è ReLU)
nn.init.kaiming_normal_(
    layer.weight,
    mode='fan_in',
    nonlinearity='relu'
)

# Orthogonal (–¥–ª—è RNN)
nn.init.orthogonal_(layer.weight)

# –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
nn.init.normal_(layer.weight, mean=0, std=0.01)</code></pre>

    <blockquote>‚ö†Ô∏è –ü–ª–æ—Ö–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ gradient vanishing/exploding</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 13. Mode connectivity</h2>
    <p>–î–≤–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–∞ —á–∞—Å—Ç–æ —Å–æ–µ–¥–∏–Ω–µ–Ω—ã –ø—É—Ç–µ–º —Å –Ω–∏–∑–∫–∏–º loss.</p>

    <p><strong>–ù–∞–±–ª—é–¥–µ–Ω–∏–µ</strong>: –ú–æ–∂–Ω–æ –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏:</p>
    <pre><code># –õ–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è
alpha = 0.5  # —Å–µ—Ä–µ–¥–∏–Ω–∞ –ø—É—Ç–∏
theta_mid = alpha * theta_1 + (1 - alpha) * theta_2

# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏
model.load_state_dict(theta_mid)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º loss (—á–∞—Å—Ç–æ –æ—Å—Ç–∞–µ—Ç—Å—è –Ω–∏–∑–∫–∏–º!)
loss_mid = evaluate(model, test_loader)</code></pre>

    <p><strong>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ</strong>:</p>
    <ul>
      <li>Model averaging (–∞–Ω—Å–∞–º–±–ª—å)</li>
      <li>–ü–æ–Ω–∏–º–∞–Ω–∏–µ –ª–∞–Ω–¥—à–∞—Ñ—Ç–∞</li>
      <li>Fast geometric ensembling (FGE)</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 14. Loss curve analysis</h2>
    <p>–ê–Ω–∞–ª–∏–∑ –∫—Ä–∏–≤–æ–π loss –ø–æ–º–æ–≥–∞–µ—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã:</p>

    <pre><code>import matplotlib.pyplot as plt

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train')
plt.plot(val_losses, label='Validation')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Learning Curves')

plt.subplot(1, 2, 2)
plt.plot(learning_rates)
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.title('LR Schedule')
plt.show()</code></pre>

    <p><strong>–ü—Ä–∏–∑–Ω–∞–∫–∏ –ø—Ä–æ–±–ª–µ–º</strong>:</p>
    <ul>
      <li>Loss –Ω–µ —É–±—ã–≤–∞–µ—Ç ‚Üí —Å–ª–∏—à–∫–æ–º –º–∞–ª—ã–π LR</li>
      <li>Loss –≤–∑—Ä—ã–≤–∞–µ—Ç—Å—è ‚Üí —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π LR</li>
      <li>–ö–æ–ª–µ–±–∞–Ω–∏—è ‚Üí batch size –∏–ª–∏ LR</li>
      <li>Train &lt;&lt; Val ‚Üí –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ</li>
    </ul>
  </div>

  <div class="block">
    <h2>üî∑ 15. Learning rate finder</h2>
    <p>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ LR:</p>

    <pre><code>def find_lr(model, train_loader, optimizer, 
            start_lr=1e-7, end_lr=10, num_iter=100):
    lr_mult = (end_lr / start_lr) ** (1 / num_iter)
    lr = start_lr
    optimizer.param_groups[0]['lr'] = lr
    
    lrs = []
    losses = []
    
    for i, (X, y) in enumerate(train_loader):
        if i >= num_iter:
            break
            
        optimizer.zero_grad()
        loss = criterion(model(X), y)
        loss.backward()
        optimizer.step()
        
        lrs.append(lr)
        losses.append(loss.item())
        
        lr *= lr_mult
        optimizer.param_groups[0]['lr'] = lr
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.plot(lrs, losses)
    plt.xscale('log')
    plt.xlabel('Learning Rate')
    plt.ylabel('Loss')
    plt.show()
    
    return lrs, losses</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 16. Catastrophic forgetting</h2>
    <p>–ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—å "–∑–∞–±—ã–≤–∞–µ—Ç" —Å—Ç–∞—Ä—ã–µ:</p>

    <p><strong>–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –±–æ—Ä—å–±—ã</strong>:</p>
    <ul>
      <li><strong>Elastic Weight Consolidation (EWC)</strong>: —à—Ç—Ä–∞—Ñ –∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–∞–∂–Ω—ã—Ö –≤–µ—Å–æ–≤</li>
      <li><strong>Progressive networks</strong>: –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–ª–æ–µ–≤</li>
      <li><strong>Rehearsal</strong>: –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö</li>
      <li><strong>Knowledge distillation</strong>: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤ —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏</li>
    </ul>

    <pre><code># –ü—Ä–æ—Å—Ç–æ–π rehearsal
old_data = ...  # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ

for epoch in range(epochs):
    # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    for X_new, y_new in new_loader:
        loss = criterion(model(X_new), y_new)
        
    # –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å—Ç–∞—Ä—ã—Ö
    for X_old, y_old in old_loader:
        loss += criterion(model(X_old), y_old)
        
    loss.backward()
    optimizer.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 17. Double descent phenomenon</h2>
    <p>–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–µ —è–≤–ª–µ–Ω–∏–µ: –ø–æ—Å–ª–µ "–ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è" –∫–∞—á–µ—Å—Ç–≤–æ —Å–Ω–æ–≤–∞ —É–ª—É—á—à–∞–µ—Ç—Å—è!</p>

    <p><strong>–¢—Ä–∏ —Ä–µ–∂–∏–º–∞</strong>:</p>
    <ul>
      <li><strong>Underparameterized</strong>: –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π bias-variance tradeoff</li>
      <li><strong>Interpolation threshold</strong>: –ø–∏–∫ test error</li>
      <li><strong>Overparameterized</strong>: test error —Å–Ω–æ–≤–∞ –ø–∞–¥–∞–µ—Ç!</li>
    </ul>

    <p>–ù–∞–±–ª—é–¥–∞–µ—Ç—Å—è –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏:</p>
    <ul>
      <li>–†–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ (–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)</li>
      <li>–ß–∏—Å–ª–∞ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è</li>
      <li>–ö–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö</li>
    </ul>

    <blockquote>ü§Ø –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π —Ç–µ–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è!</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 18. Lottery ticket hypothesis</h2>
    <p>–í —Å–ª—É—á–∞–π–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–µ—Ç–∏ –µ—Å—Ç—å –ø–æ–¥—Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç—Å—è —Ç–∞–∫–∂–µ —Ö–æ—Ä–æ—à–æ.</p>

    <p><strong>–ü—Ä–æ—Ü–µ–¥—É—Ä–∞</strong>:</p>
    <ol>
      <li>–û–±—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å–µ—Ç—å</li>
      <li>–£–¥–∞–ª–∏—Ç—å k% –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –≤–µ—Å–æ–≤ (pruning)</li>
      <li>–í–µ—Ä–Ω—É—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –≤–µ—Å–∞ –∫ –Ω–∞—á–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º</li>
      <li>–ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å ‚Üí –ø–æ–ª—É—á–∞–µ–º —Ç–æ –∂–µ –∫–∞—á–µ—Å—Ç–≤–æ!</li>
    </ol>

    <pre><code># –ù–∞—Ö–æ–¥–∏–º "–≤—ã–∏–≥—Ä—ã—à–Ω—ã–π –±–∏–ª–µ—Ç"
original_weights = copy.deepcopy(model.state_dict())

# –û–±—É—á–∞–µ–º
train(model)

# –ù–∞—Ö–æ–¥–∏–º –º–∞—Å–∫—É (—Ç–æ–ø-20% –≤–µ—Å–æ–≤)
mask = create_mask(model, keep_ratio=0.2)

# –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫ –Ω–∞—á–∞–ª—å–Ω—ã–º –≤–µ—Å–∞–º
model.load_state_dict(original_weights)

# –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É –∏ –ø–µ—Ä–µ–æ–±—É—á–∞–µ–º
apply_mask(model, mask)
train(model)  # –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ–≥–æ –∂–µ –∫–∞—á–µ—Å—Ç–≤–∞!</code></pre>
  </div>



</div>
</body>
</html>

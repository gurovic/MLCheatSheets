<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Actor-Critic –º–µ—Ç–æ–¥—ã Cheatsheet ‚Äî 3 –∫–æ–ª–æ–Ω–∫–∏</title>
  <style>
    @media screen { body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; color: #333; background: #fafcff; padding: 10px; } }
    @media print { body { background: white; padding: 0; } @page { size: A4 landscape; margin: 10mm; } }
    .container { column-count: 3; column-gap: 20px; max-width: 100%; }
    .block { break-inside: avoid; margin-bottom: 1.2em; padding: 12px; background: white; border-radius: 6px; box-shadow: 0 1px 3px rgba(0,0,0,0.05); }
    h1 { font-size: 1.6em; font-weight: 700; color: #1a5fb4; text-align: center; margin: 0 0 8px; column-span: all; }
    .subtitle { text-align: center; color: #666; font-size: 0.9em; margin-bottom: 12px; column-span: all; }
    h2 { font-size: 1.15em; font-weight: 700; color: #1a5fb4; margin: 0 0 8px; padding-bottom: 4px; border-bottom: 1px solid #e0e7ff; }
    p, ul, ol { font-size: 0.92em; margin: 0.6em 0; }
    ul, ol { padding-left: 18px; }
    li { margin-bottom: 4px; }
    code { font-family: 'Consolas', 'Courier New', monospace; background-color: #f0f4ff; padding: 1px 4px; border-radius: 3px; font-size: 0.88em; }
    pre { background-color: #f0f4ff; padding: 8px; border-radius: 4px; overflow-x: auto; font-size: 0.84em; margin: 6px 0; }
    pre code { padding: 0; background: none; white-space: pre-wrap; }
    table { width: 100%; border-collapse: collapse; font-size: 0.82em; margin: 6px 0; }
    th { background-color: #e6f0ff; text-align: left; padding: 4px 6px; font-weight: 600; }
    td { padding: 4px 6px; border-bottom: 1px solid #f0f4ff; }
    tr:nth-child(even) { background-color: #f8fbff; }
    blockquote { font-style: italic; margin: 8px 0; padding: 6px 10px; background: #f8fbff; border-left: 2px solid #1a5fb4; font-size: 0.88em; }
    @media print { .container { column-gap: 12px; } .block { box-shadow: none; } code, pre, table { font-size: 0.78em; } h1 { font-size: 1.4em; } h2 { font-size: 1em; } }
  </style>
</head>
<body>

<div class="container">

  <h1>üé≠ Actor-Critic –º–µ—Ç–æ–¥—ã Cheatsheet</h1>
  <div class="subtitle">Reinforcement Learning ‚Ä¢ Policy Gradient ‚Ä¢ Value-based ‚Ä¢ A3C ‚Ä¢ PPO<br>üìÖ –Ø–Ω–≤–∞—Ä—å 2026</div>

  <div class="block">
    <h2>üî∑ 1. –°—É—Ç—å Actor-Critic</h2>
<ul><li><strong>Actor</strong>: –ø–æ–ª–∏—Ç–∏–∫–∞ œÄ(a|s) - –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è</li><li><strong>Critic</strong>: —Ñ—É–Ω–∫—Ü–∏—è —Ü–µ–Ω–Ω–æ—Å—Ç–∏ V(s) - –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è</li><li><strong>–ö–æ–º–±–∏–Ω–∞—Ü–∏—è</strong>: policy gradient + value-based</li><li><strong>–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ</strong>: —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å</li></ul><blockquote>Actor —Ä–µ—à–∞–µ—Ç, —á—Ç–æ –¥–µ–ª–∞—Ç—å. Critic –≥–æ–≤–æ—Ä–∏—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –ø–æ–ª—É—á–∏–ª–æ—Å—å.</blockquote>
  </div>

  <div class="block">
    <h2>üî∑ 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</h2>
<pre><code># Actor: –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è
œÄ_Œ∏(a|s) - –ø–æ–ª–∏—Ç–∏–∫–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ Œ∏

# Critic: –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è
V_w(s) - —Ñ—É–Ω–∫—Ü–∏—è —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ w

# Advantage
A(s,a) = Q(s,a) - V(s)
       = r + Œ≥V(s') - V(s)  (TD error)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 3. –ë–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º</h2>
<ol><li>–ù–∞–±–ª—é–¥–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ s</li><li>Actor –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ a ~ œÄ_Œ∏(a|s)</li><li>–ü–æ–ª—É—á–∞–µ–º reward r –∏ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ s'</li><li>Critic –≤—ã—á–∏—Å–ª—è–µ—Ç TD error: Œ¥ = r + Œ≥V_w(s') - V_w(s)</li><li>–û–±–Ω–æ–≤–ª—è–µ–º Critic: w ‚Üê w + Œ±_w Œ¥ ‚àá_w V_w(s)</li><li>–û–±–Ω–æ–≤–ª—è–µ–º Actor: Œ∏ ‚Üê Œ∏ + Œ±_Œ∏ Œ¥ ‚àá_Œ∏ log œÄ_Œ∏(a|s)</li></ol>
  </div>

  <div class="block">
    <h2>üî∑ 4. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è PyTorch</h2>
<pre><code>import torch
import torch.nn as nn

class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU()
        )
        # Actor head
        self.actor = nn.Linear(128, action_dim)
        # Critic head
        self.critic = nn.Linear(128, 1)
    
    def forward(self, state):
        features = self.shared(state)
        action_probs = F.softmax(self.actor(features), dim=-1)
        value = self.critic(features)
        return action_probs, value</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 5. –û–±—É—á–µ–Ω–∏–µ</h2>
<pre><code>def train_step(state, action, reward, next_state, done):
    # Forward pass
    probs, value = model(state)
    _, next_value = model(next_state)
    
    # TD error (advantage)
    td_target = reward + gamma * next_value * (1 - done)
    advantage = td_target - value
    
    # Critic loss
    critic_loss = advantage.pow(2).mean()
    
    # Actor loss
    log_prob = torch.log(probs[action])
    actor_loss = -(log_prob * advantage.detach()).mean()
    
    # Total loss
    loss = actor_loss + critic_loss
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 6. A2C (Advantage Actor-Critic)</h2>
<pre><code># –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π parallel training
for episode in range(num_episodes):
    states, actions, rewards = [], [], []
    
    # –°–±–æ—Ä —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
    for step in range(max_steps):
        action_probs, _ = model(state)
        action = sample(action_probs)
        next_state, reward, done = env.step(action)
        
        states.append(state)
        actions.append(action)
        rewards.append(reward)
        
        if done:
            break
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ
    update(states, actions, rewards)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 7. A3C (Asynchronous)</h2>
<pre><code>import multiprocessing as mp

class Worker(mp.Process):
    def __init__(self, global_model):
        super().__init__()
        self.global_model = global_model
        self.local_model = ActorCritic()
    
    def run(self):
        while not done:
            # –°–æ–±—Ä–∞—Ç—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é
            trajectory = collect_trajectory()
            
            # –í—ã—á–∏—Å–ª–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
            gradients = compute_gradients(trajectory)
            
            # –û–±–Ω–æ–≤–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
            self.global_model.apply_gradients(gradients)
            
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
            self.local_model.load_state_dict(
                self.global_model.state_dict()
            )</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 8. GAE (Generalized Advantage)</h2>
<pre><code># –°–≥–ª–∞–∂–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ advantage
Œ¥_t = r_t + Œ≥V(s_{t+1}) - V(s_t)

A^{GAE}_t = Œ£ (Œ≥Œª)^l Œ¥_{t+l}

Œª ‚àà [0, 1] - trade-off:
  Œª = 0: —Ç–æ–ª—å–∫–æ TD error
  Œª = 1: Monte Carlo return</code></pre><pre><code>def compute_gae(rewards, values, gamma=0.99, lam=0.95):
    advantages = []
    gae = 0
    
    for t in reversed(range(len(rewards))):
        delta = rewards[t] + gamma * values[t+1] - values[t]
        gae = delta + gamma * lam * gae
        advantages.insert(0, gae)
    
    return advantages</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 9. PPO (Proximal Policy Optimization)</h2>
<pre><code># Clipped surrogate objective
r_t(Œ∏) = œÄ_Œ∏(a|s) / œÄ_Œ∏_old(a|s)  # importance sampling ratio

L^{CLIP}(Œ∏) = E[min(
    r_t(Œ∏) * A_t,
    clip(r_t(Œ∏), 1-Œµ, 1+Œµ) * A_t
)]

Œµ - clip parameter (–æ–±—ã—á–Ω–æ 0.2)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 10. Continuous actions</h2>
<pre><code># –î–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π: Gaussian policy
class ContinuousActor(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.mu = nn.Linear(state_dim, action_dim)
        self.log_std = nn.Parameter(torch.zeros(action_dim))
    
    def forward(self, state):
        mu = self.mu(state)
        std = torch.exp(self.log_std)
        dist = Normal(mu, std)
        return dist

# Sampling
dist = actor(state)
action = dist.sample()
log_prob = dist.log_prob(action)</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 11. Tricks –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏</h2>
<ul><li><strong>Baseline subtraction</strong>: —É–±—Ä–∞—Ç—å V(s) –∏–∑ returns</li><li><strong>Entropy bonus</strong>: +Œ≤*H(œÄ) –¥–ª—è exploration</li><li><strong>Value clipping</strong>: –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è V</li><li><strong>Gradient clipping</strong>: norm ‚â§ 0.5</li><li><strong>Orthogonal initialization</strong></li></ul>
  </div>

  <div class="block">
    <h2>üî∑ 12. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤</h2>
<table><tr><th>–ú–µ—Ç–æ–¥</th><th>–ü–ª—é—Å—ã</th><th>–ú–∏–Ω—É—Å—ã</th></tr><tr><td>A2C</td><td>–ü—Ä–æ—Å—Ç–æ—Ç–∞, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</td><td>–ú–µ–¥–ª–µ–Ω–Ω–µ–µ A3C</td></tr><tr><td>A3C</td><td>–°–∫–æ—Ä–æ—Å—Ç—å (parallel)</td><td>–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</td></tr><tr><td>PPO</td><td>State-of-the-art</td><td>–°–ª–æ–∂–Ω–æ—Å—Ç—å</td></tr></table>
  </div>

  <div class="block">
    <h2>üî∑ 13. Gym –ø—Ä–∏–º–µ—Ä</h2>
<pre><code>import gym

env = gym.make("CartPole-v1")
model = ActorCritic(4, 2)
optimizer = torch.optim.Adam(model.parameters())

for episode in range(1000):
    state = env.reset()
    done = False
    
    while not done:
        # Select action
        probs, value = model(torch.FloatTensor(state))
        action = torch.multinomial(probs, 1).item()
        
        # Environment step
        next_state, reward, done, _ = env.step(action)
        
        # Train
        train_step(state, action, reward, next_state, done)
        
        state = next_state</code></pre>
  </div>

  <div class="block">
    <h2>üî∑ 14. Hyperparameters</h2>
<table><tr><th>–ü–∞—Ä–∞–º–µ—Ç—Ä</th><th>–ó–Ω–∞—á–µ–Ω–∏–µ</th></tr><tr><td>Learning rate (actor)</td><td>1e-4</td></tr><tr><td>Learning rate (critic)</td><td>1e-3</td></tr><tr><td>Gamma (Œ≥)</td><td>0.99</td></tr><tr><td>GAE lambda (Œª)</td><td>0.95</td></tr><tr><td>Entropy coef</td><td>0.01</td></tr></table>
  </div>

  <div class="block">
    <h2>üî∑ 15. Best Practices</h2>
<ul><li>[ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GAE –¥–ª—è advantage estimation</li><li>[ ] –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å rewards</li><li>[ ] Gradient clipping</li><li>[ ] Entropy bonus –¥–ª—è exploration</li><li>[ ] –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤</li><li>[ ] –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ (reward, loss)</li></ul>
  </div>

  <div class="block">
    <h2>üî∑ 16. –ß–µ–∫-–ª–∏—Å—Ç</h2>
<ul><li>[ ] –û–∫—Ä—É–∂–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–æ</li><li>[ ] Actor –∏ Critic —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã</li><li>[ ] GAE –Ω–∞—Å—Ç—Ä–æ–µ–Ω</li><li>[ ] –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–æ–±—Ä–∞–Ω—ã</li><li>[ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–±–æ—Ç–∞–µ—Ç</li></ul><blockquote>¬´Actor-Critic ‚Äî —ç—Ç–æ –∫–∞–∫ –≤–æ–¥–∏—Ç–µ–ª—å (actor) –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä (critic): –æ–¥–∏–Ω —É–ø—Ä–∞–≤–ª—è–µ—Ç, –¥—Ä—É–≥–æ–π –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏ –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ¬ª.</blockquote>
  </div>

</div>

</body>
</html>
